{
  "cells": [
    {
      "cell_type": "raw",
      "id": "58886322-b8be-44a6-8e31-a25b0ceb6415",
      "metadata": {
        "id": "58886322-b8be-44a6-8e31-a25b0ceb6415",
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "---\n",
        "title: \"From Noise to Numbers: A Hands-On Guide to Generating MNIST Digits with Diffusion Models (Part 2)\"\n",
        "description: \"??\"\n",
        "date: '2025-02-15'\n",
        "image: images/2025-02-10-diffusion-model-mnist-part2.jpeg\n",
        "output-file: 2025-02-15-diffusion-model-mnist-part2\n",
        "toc: true\n",
        "\n",
        "categories:\n",
        "- python\n",
        "- dl\n",
        "\n",
        "keyword:\n",
        "- Pytorch\n",
        "- Stable diffusion\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fd6dd069",
      "metadata": {
        "id": "fd6dd069"
      },
      "source": [
        "![image source: https://www.artbreeder.com/image/6b4df6c697078f0e2cda42348ec6](images/2025-02-10-diffusion-model-mnist-part2.jpeg)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "106dae32-324e-4732-98d8-624224e27bba",
      "metadata": {
        "id": "106dae32-324e-4732-98d8-624224e27bba"
      },
      "source": [
        "## Introduction\n",
        "\n",
        "Welcome back to Part 2 of our journey into diffusion models! In the [first part](https://hassaanbinaslam.github.io/myblog/posts/2025-02-10-diffusion-model-mnist-part1.html), we successfully built a basic Convolutional UNet from scratch and trained it to directly predict denoised MNIST digits.  We saw that it could indeed remove some noise, but the results were still a bit blurry, and it wasn't quite the \"diffusion model magic\" we were hoping for.\n",
        "\n",
        "One of the key limitations we hinted at was the simplicity of our `BasicUNet` architecture.  For this second part, we're going to address that head-on. We'll be upgrading our UNet architecture to something more powerful and feature-rich.\n",
        "\n",
        "To do this, we'll be leveraging the fantastic `diffusers` library from Hugging Face.  `diffusers` is a widely adopted toolkit in the world of diffusion models, providing pre-built and optimized components that can significantly simplify our development process and boost performance.\n",
        "\n",
        "In this part, we'll replace our `BasicUNet` with a `UNet2DModel` from `diffusers`.  We'll keep the core task the same – direct image prediction – but with a more advanced UNet under the hood. This will allow us to see firsthand how architectural improvements can impact the quality of our denoising results, setting the stage for even more exciting explorations in future parts! Let's dive in!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4fcb7eca-4c9c-45d0-b966-8f400d9efc8a",
      "metadata": {
        "id": "4fcb7eca-4c9c-45d0-b966-8f400d9efc8a"
      },
      "source": [
        "### Credits\n",
        "\n",
        "This post is inspired by the [Hugging Face Diffusion Course](https://huggingface.co/learn/diffusion-course/en/unit1/3)\n",
        "\n",
        "### Environment Details\n",
        "\n",
        "You can access and run this Jupyter Notebook from the GitHub repository on this link [2025-02-15-diffusion-model-mnist-part2.ipynb](https://github.com/hassaanbinaslam/myblog/blob/main/posts/2025-02-15-diffusion-model-mnist-part2.ipynb)\n",
        "\n",
        "Run the following cell to install the required packages.\n",
        "\n",
        "* This notebook can be run with [Google Colab](https://colab.research.google.com/) T4 GPU runtime.\n",
        "* I have also tested this notebook with AWS SageMaker Jupyter Notebook running on instance \"ml.g5.xlarge\" and image \"SageMaker Distribution 2.3.0\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "47c651f8-0f51-44cb-8df6-54dd96313c33",
      "metadata": {
        "id": "47c651f8-0f51-44cb-8df6-54dd96313c33"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install datasets[vision]\n",
        "!pip install diffusers\n",
        "!pip install watermark\n",
        "!pip install torchinfo\n",
        "!pip install matplotlib"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c4846699",
      "metadata": {
        "id": "c4846699"
      },
      "source": [
        "[WaterMark](https://github.com/rasbt/watermark) is an IPython magic extension for printing date and time stamps, version numbers, and hardware information. Let's load this extension and print the environment details."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "64dd2d3c-631e-43b2-ab87-b0e9e0c1bafa",
      "metadata": {
        "id": "64dd2d3c-631e-43b2-ab87-b0e9e0c1bafa"
      },
      "outputs": [],
      "source": [
        "%load_ext watermark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "9af53e1f-1b21-4ba4-84a9-f5df6c2c1f6b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9af53e1f-1b21-4ba4-84a9-f5df6c2c1f6b",
        "outputId": "1ded64c8-c97b-4e32-9c10-958b87cbfd57"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Python implementation: CPython\n",
            "Python version       : 3.11.11\n",
            "IPython version      : 7.34.0\n",
            "\n",
            "torch      : 2.5.1+cu124\n",
            "torchvision: 0.20.1+cu124\n",
            "datasets   : 3.2.0\n",
            "diffusers  : 0.32.2\n",
            "matplotlib : 3.10.0\n",
            "watermark  : 2.5.0\n",
            "torchinfo  : 1.8.0\n",
            "\n",
            "Compiler    : GCC 11.4.0\n",
            "OS          : Linux\n",
            "Release     : 6.1.85+\n",
            "Machine     : x86_64\n",
            "Processor   : x86_64\n",
            "CPU cores   : 2\n",
            "Architecture: 64bit\n",
            "\n"
          ]
        }
      ],
      "source": [
        "%watermark -v -m -p torch,torchvision,datasets,diffusers,matplotlib,watermark,torchinfo"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "91069e59",
      "metadata": {},
      "source": [
        "## Diving into `diffusers` and `UNet2DModel`\n",
        "\n",
        "So, what exactly *is* this `diffusers` library we're so excited about?  Think of `diffusers` as a comprehensive, community-driven library in PyTorch specifically designed for working with diffusion models. It's maintained by Hugging Face, the same team behind the popular Transformers library, so you know it's built with quality and ease of use in mind.\n",
        "\n",
        "Why are we using `diffusers` now?  Several reasons!  First, it provides well-tested and optimized implementations of various diffusion model components, saving us from writing everything from scratch.  Second, it's a vibrant ecosystem, constantly evolving with the latest research and techniques in diffusion models.  By using `diffusers`, we're standing on the shoulders of giants!\n",
        "\n",
        "For Part 2, the star of the show is the `UNet2DModel` class from `diffusers`. This is a more sophisticated UNet architecture compared to our `BasicUNet`.  It's like upgrading from a standard bicycle to a mountain bike – both are bikes, but the mountain bike is built for more challenging terrain and better performance.\n",
        "\n",
        "What makes `UNet2DModel` more advanced? Let's look at some key architectural improvements under the hood:\n",
        "\n",
        "*   **ResNet Blocks:**  Instead of simple convolutional layers, `UNet2DModel` utilizes ResNet blocks within its downsampling and upsampling paths. ResNet blocks are known for making it easier to train deeper networks, which can capture more complex features in images. Think of them as more efficient and powerful building blocks for our UNet.\n",
        "\n",
        "*   **Attention Mechanisms:**  `UNet2DModel` incorporates attention mechanisms, specifically \"Attention Blocks,\" in its architecture.  Attention is a powerful concept in deep learning that allows the model to focus on the most relevant parts of the input when processing information.  In image generation, attention can help the model selectively focus on different regions of the image, potentially leading to finer details and more coherent structures.\n",
        "\n",
        "*   **Group Normalization:**  Instead of Batch Normalization, `UNet2DModel` uses Group Normalization. Group Normalization is often favored in generative models, especially when working with smaller batch sizes, as it tends to be more stable and perform better in those scenarios.\n",
        "\n",
        "*   **Timestep Embedding:**  Even though we are still doing direct image prediction in this part,  `UNet2DModel` is designed with diffusion models in mind.  It includes a `TimestepEmbedding` layer, which is a standard component in diffusion models to handle the timestep information (which we'll explore in later parts!).  For now, we'll just be passing in a timestep of 0, but this layer is there, ready for when we move to true diffusion.\n",
        "\n",
        "These architectural enhancements in `UNet2DModel` give it a greater capacity to learn and potentially denoise images more effectively than our `BasicUNet`. Let's see if it lives up to the hype!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aa9dda09",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "fb920269",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
