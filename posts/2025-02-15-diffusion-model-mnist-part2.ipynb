{
  "cells": [
    {
      "cell_type": "raw",
      "id": "58886322-b8be-44a6-8e31-a25b0ceb6415",
      "metadata": {
        "id": "58886322-b8be-44a6-8e31-a25b0ceb6415",
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "---\n",
        "title: \"From Noise to Numbers: A Hands-On Guide to Generating MNIST Digits with Diffusion Models (Part 2)\"\n",
        "description: \"??\"\n",
        "date: '2025-02-15'\n",
        "image: images/2025-02-15-diffusion-model-mnist-part2.jpeg\n",
        "output-file: 2025-02-15-diffusion-model-mnist-part2\n",
        "toc: true\n",
        "\n",
        "categories:\n",
        "- python\n",
        "- dl\n",
        "\n",
        "keyword:\n",
        "- Pytorch\n",
        "- Stable diffusion\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fd6dd069",
      "metadata": {
        "id": "fd6dd069"
      },
      "source": [
        "![image source: https://www.artbreeder.com/image/6b4df6c697078f0e2cda42348ec6](images/2025-02-15-diffusion-model-mnist-part2.jpeg)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "106dae32-324e-4732-98d8-624224e27bba",
      "metadata": {
        "id": "106dae32-324e-4732-98d8-624224e27bba"
      },
      "source": [
        "## Introduction\n",
        "\n",
        "Welcome back to Part 2 of our journey into diffusion models! In the [first part](https://hassaanbinaslam.github.io/myblog/posts/2025-02-10-diffusion-model-mnist-part1.html), we successfully built a basic Convolutional UNet from scratch and trained it to directly predict denoised MNIST digits.  We saw that it could indeed remove some noise, but the results were still a bit blurry, and it wasn't quite the \"diffusion model magic\" we were hoping for.\n",
        "\n",
        "One of the key limitations we hinted at was the simplicity of our `BasicUNet` architecture.  For this second part, we're going to address that and we'll be upgrading our UNet architecture to something more powerful and feature-rich.\n",
        "\n",
        "To do this, we'll be leveraging the fantastic `diffusers` library from [Hugging Face](https://huggingface.co/).  [`diffusers`](https://huggingface.co/docs/diffusers/en/index) is a widely adopted toolkit in the world of diffusion models, providing pre-built and optimized components that can significantly simplify our development process and boost performance.\n",
        "\n",
        "In this part, we'll replace our `BasicUNet` with a `UNet2DModel` from `diffusers`.  We'll keep the core task the same – direct image prediction – but with a more advanced UNet under the hood. This will allow us to see firsthand how architectural improvements can impact the quality of our denoising results, setting the stage for even more exciting explorations in future parts! Let's dive in!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4fcb7eca-4c9c-45d0-b966-8f400d9efc8a",
      "metadata": {
        "id": "4fcb7eca-4c9c-45d0-b966-8f400d9efc8a"
      },
      "source": [
        "### Credits\n",
        "\n",
        "This post is inspired by the [Hugging Face Diffusion Course](https://huggingface.co/learn/diffusion-course/en/unit1/3)\n",
        "\n",
        "### Environment Details\n",
        "\n",
        "You can access and run this Jupyter Notebook from the GitHub repository on this link [2025-02-15-diffusion-model-mnist-part2.ipynb](https://github.com/hassaanbinaslam/myblog/blob/main/posts/2025-02-15-diffusion-model-mnist-part2.ipynb)\n",
        "\n",
        "Run the following cell to install the required packages.\n",
        "\n",
        "* This notebook can be run with [Google Colab](https://colab.research.google.com/) T4 GPU runtime.\n",
        "* I have also tested this notebook with AWS SageMaker Jupyter Notebook running on instance \"ml.g5.xlarge\" and image \"SageMaker Distribution 2.3.0\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "47c651f8-0f51-44cb-8df6-54dd96313c33",
      "metadata": {
        "id": "47c651f8-0f51-44cb-8df6-54dd96313c33"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install datasets[vision]\n",
        "!pip install diffusers\n",
        "!pip install watermark\n",
        "!pip install torchinfo\n",
        "!pip install matplotlib"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c4846699",
      "metadata": {
        "id": "c4846699"
      },
      "source": [
        "[WaterMark](https://github.com/rasbt/watermark) is an IPython magic extension for printing date and time stamps, version numbers, and hardware information. Let's load this extension and print the environment details."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "64dd2d3c-631e-43b2-ab87-b0e9e0c1bafa",
      "metadata": {
        "id": "64dd2d3c-631e-43b2-ab87-b0e9e0c1bafa"
      },
      "outputs": [],
      "source": [
        "%load_ext watermark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "9af53e1f-1b21-4ba4-84a9-f5df6c2c1f6b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9af53e1f-1b21-4ba4-84a9-f5df6c2c1f6b",
        "outputId": "1ded64c8-c97b-4e32-9c10-958b87cbfd57"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Python implementation: CPython\n",
            "Python version       : 3.11.11\n",
            "IPython version      : 7.34.0\n",
            "\n",
            "torch      : 2.5.1+cu124\n",
            "torchvision: 0.20.1+cu124\n",
            "datasets   : 3.2.0\n",
            "diffusers  : 0.32.2\n",
            "matplotlib : 3.10.0\n",
            "watermark  : 2.5.0\n",
            "torchinfo  : 1.8.0\n",
            "\n",
            "Compiler    : GCC 11.4.0\n",
            "OS          : Linux\n",
            "Release     : 6.1.85+\n",
            "Machine     : x86_64\n",
            "Processor   : x86_64\n",
            "CPU cores   : 2\n",
            "Architecture: 64bit\n",
            "\n"
          ]
        }
      ],
      "source": [
        "%watermark -v -m -p torch,torchvision,datasets,diffusers,matplotlib,watermark,torchinfo"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "91069e59",
      "metadata": {},
      "source": [
        "## Diving into `diffusers` and `UNet2DModel`\n",
        "\n",
        "So, what exactly *is* this [`diffusers`](https://huggingface.co/docs/diffusers/en/index) library we're so excited about?  Think of `diffusers` as a comprehensive, community-driven library in [PyTorch](https://pytorch.org/) specifically designed for working with diffusion models. It's maintained by Hugging Face, the same team behind the popular [Transformers](https://huggingface.co/docs/transformers/en/index) library, so you know it's built with quality and ease of use in mind.\n",
        "\n",
        "Why are we using `diffusers` now?  Several reasons!  First, it provides well-tested and optimized implementations of various diffusion model components, saving us from writing everything from scratch.  Second, it's a vibrant ecosystem, constantly evolving with the latest research and techniques in diffusion models.  By using `diffusers`, we're standing on the shoulders of giants!\n",
        "\n",
        "For Part 2, the star of the show is the [`UNet2DModel`](https://huggingface.co/docs/diffusers/main/en/api/models/unet2d) class from `diffusers`. This is a more sophisticated UNet architecture compared to our `BasicUNet`.  It's like upgrading from a standard bicycle to a mountain bike – both are bikes, but the mountain bike is built for more challenging terrain and better performance.\n",
        "\n",
        "What makes `UNet2DModel` more advanced? Let's look at some key architectural improvements under the hood:\n",
        "\n",
        "*   **Configurable Block Types:** `UNet2DModel` is designed to be flexible. Instead of being fixed to a single type of block, it allows you to choose different block types for its downsampling and upsampling paths using parameters like `down_block_types` and `up_block_types`.  While the *default* `DownBlock2D` and `UpBlock2D` blocks are primarily convolutional,  `diffusers` also provides option for selecting \"resnet\" layers.\n",
        "\n",
        "*   **Attention Mechanisms:**  `UNet2DModel` incorporates attention mechanisms, specifically \"Attention Blocks,\" in its architecture.  Attention is a powerful concept in deep learning that allows the model to focus on the most relevant parts of the input when processing information.  In image generation, attention can help the model selectively focus on different regions of the image, potentially leading to finer details and more coherent structures.\n",
        "\n",
        "*   **Group Normalization:**  Instead of Batch Normalization, `UNet2DModel` uses Group Normalization. Group Normalization is often favored in generative models, especially when working with smaller batch sizes, as it tends to be more stable and perform better in those scenarios.\n",
        "\n",
        "*   **Timestep Embedding:**  Even though we are still doing direct image prediction in this part,  `UNet2DModel` is designed with diffusion models in mind.  It includes a `TimestepEmbedding` layer, which is a standard component in diffusion models to handle the timestep information (which we'll explore in later parts!).  For now, we'll just be passing in a timestep of 0, but this layer is there, ready for when we move to true diffusion.\n",
        "\n",
        "These architectural enhancements in `UNet2DModel` give it a greater capacity to learn and potentially denoise images more effectively than our `BasicUNet`. Let's see if it lives up to the hype!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1d397e59",
      "metadata": {},
      "source": [
        "## Data Preparation and Preprocessing for MNIST\n",
        "\n",
        "As we are building upon the foundations laid in Part 1, we will reuse the same data preparation and preprocessing steps for the MNIST dataset. For a more in-depth explanation of these steps, please refer back to the first part of this guide. Here, we will quickly outline the process to ensure our data is ready for training our enhanced UNet.\n",
        "\n",
        "First, we load the MNIST dataset using the `datasets` library:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8c9203a3",
      "metadata": {},
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "dataset = load_dataset(\"mnist\")\n",
        "print(dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3b3cf281",
      "metadata": {},
      "source": [
        "This code snippet downloads and loads the MNIST dataset.  As we saw in Part 1, this dataset is provided as a `DatasetDict` with 'train' and 'test' splits, each containing 'image' and 'label' features.\n",
        "\n",
        "Next, we define our preprocessing pipeline using `torchvision.transforms`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "622fd34b",
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from torchvision import transforms\n",
        "\n",
        "image_size = 32  # Define the target image size\n",
        "\n",
        "preprocess = transforms.Compose([\n",
        "    transforms.Resize((image_size, image_size)),\n",
        "    transforms.ToTensor(),\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "81c92d10",
      "metadata": {},
      "source": [
        "This `preprocess` pipeline consists of two transformations:\n",
        "\n",
        "*   `transforms.Resize((image_size, image_size))`: Resizes each image to a fixed size of 32x32 pixels. This ensures consistent input dimensions for our UNet model.\n",
        "*   `transforms.ToTensor()`: Converts the images to PyTorch tensors and scales the pixel values to the range \\[0, 1]. This normalization is crucial for training deep learning models effectively.\n",
        "\n",
        "To apply this preprocessing to the dataset efficiently, we define a `transform` function and set it for our dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "119c7480",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define the transform function\n",
        "def transform(examples):\n",
        "    examples = [preprocess(image) for image in examples[\"image\"]]\n",
        "    return {\"images\": examples}\n",
        "\n",
        "# Apply the transform to the dataset\n",
        "dataset.set_transform(transform)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c20fb384",
      "metadata": {},
      "source": [
        "This `transform` function applies our `preprocess` pipeline to each image in the dataset on-the-fly, meaning preprocessing happens only when the data is accessed, saving memory and keeping our dataset efficient.\n",
        "\n",
        "With the MNIST dataset loaded and preprocessed, we are now ready to introduce our enhanced UNet architecture from the `diffusers` library! Let's move on to explore the `UNet2DModel`."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aa9dda09",
      "metadata": {},
      "source": [
        "## Implementing `UNet2DModel`\n",
        "\n",
        "Now, let's see how to put the `diffusers` `UNet2DModel` into action for our MNIST digit denoising task.  Here's the code snippet we'll use to instantiate the model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f56ae62f",
      "metadata": {},
      "outputs": [],
      "source": [
        "from diffusers import UNet2DModel\n",
        "\n",
        "model = UNet2DModel(\n",
        "    sample_size=32,\n",
        "    in_channels=1,\n",
        "    out_channels=1,\n",
        "    layers_per_block=2,\n",
        "    block_out_channels=(32, 64, 64),\n",
        "    down_block_types=(\n",
        "        \"DownBlock2D\",\n",
        "        \"AttnDownBlock2D\",\n",
        "        \"AttnDownBlock2D\",\n",
        "    ),\n",
        "    up_block_types=(\n",
        "        \"AttnUpBlock2D\",\n",
        "        \"AttnUpBlock2D\",\n",
        "        \"UpBlock2D\",\n",
        "    ),\n",
        ")\n",
        "\n",
        "print(model) # Or summary(model) for a more detailed view"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4c83dca6",
      "metadata": {},
      "source": [
        "Let's break down the parameters we've used when creating our `UNet2DModel` instance:\n",
        "\n",
        "*   `sample_size=32`:  This specifies the size of the input images. We're still working with 32x32 MNIST images after preprocessing, so we set this to 32.\n",
        "\n",
        "*   `in_channels=1`: MNIST images are grayscale, meaning they have a single color channel.  Therefore, `in_channels` is set to 1.\n",
        "\n",
        "*   `out_channels=1`:  We want our UNet to output denoised grayscale images, so `out_channels` is also 1.\n",
        "\n",
        "*   `layers_per_block=2`: This parameter controls the number of ResNet layers within each UNet block (both downsampling and upsampling blocks). We've chosen 2, meaning each block will have two ResNet layers. Increasing this would make the model deeper and potentially more powerful, but also increase training time.\n",
        "\n",
        "*   `block_out_channels=(32, 64, 64)`: This is a crucial parameter that defines the number of output channels for each block in the downsampling path.\n",
        "    *   The first value, `32`, corresponds to the output channels of the initial downsampling block.\n",
        "    *   The second value, `64`, is for the next downsampling block, and so on.\n",
        "    *   We've chosen `(32, 64, 64)`, which is \"roughly matching our basic unet example\" as we noted in the code comments. This is a deliberate choice to keep the model size somewhat comparable to our `BasicUNet` while still benefiting from the architectural improvements of `UNet2DModel`.\n",
        "\n",
        "*   `down_block_types=(\"DownBlock2D\", \"AttnDownBlock2D\", \"AttnDownBlock2D\")`: This list specifies the type of downsampling blocks to use in the encoder path.\n",
        "    *   `\"DownBlock2D\"`:  A standard ResNet downsampling block.\n",
        "    *   `\"AttnDownBlock2D\"`: A ResNet downsampling block with added attention mechanisms.\n",
        "    *   We're using a mix of standard and attention-based downsampling blocks to leverage the benefits of attention in capturing important image features.\n",
        "\n",
        "*   `up_block_types=(\"AttnUpBlock2D\", \"AttnUpBlock2D\", \"UpBlock2D\")`:  Similarly, this list defines the types of upsampling blocks in the decoder path, mirroring the downsampling path and also incorporating attention blocks in the upsampling process.\n",
        "\n",
        "By carefully configuring these parameters, we've created a `UNet2DModel` tailored for our MNIST denoising task, leveraging the power of `diffusers` and incorporating more advanced architectural components compared to our `BasicUNet`.  The `print(model)` output (or `summary(model)`) will show the detailed architecture and confirm the parameter settings we've defined.  You'll likely notice a significantly larger number of parameters compared to `BasicUNet`, hinting at the increased capacity of this enhanced model."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e0de5861",
      "metadata": {},
      "source": [
        "## Training the Enhanced UNet\n",
        "\n",
        "With our `UNet2DModel` defined, the next step is to train it!  The training process for this enhanced UNet will be remarkably similar to what we did in Part 1 with our `BasicUNet`.  This is intentional! By keeping the training process consistent, we can isolate the impact of the architectural changes we've made by switching to `UNet2DModel`.\n",
        "\n",
        "We will still be using:\n",
        "\n",
        "*   **Direct Image Prediction:**  Our model will still be trained to directly predict the denoised version of a noisy MNIST image in a single forward pass.\n",
        "*   **Mean Squared Error (MSE) Loss:** We'll continue to use MSE loss (`F.mse_loss`) to measure the difference between the predicted denoised image and the clean target image.\n",
        "*   **Adam Optimizer:**  We'll stick with the Adam optimizer (`torch.optim.Adam`) to update the model's weights during training.\n",
        "\n",
        "Here's a snippet of the training loop code. You'll notice it's almost identical to the training loop from Part 1:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "11f86cf6",
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "from torch.optim import Adam\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# --- Setup (Device, Model, Optimizer, Loss History, Hyperparameters) ---\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device) # Our UNet2DModel from diffusers\n",
        "optimizer = Adam(model.parameters(), lr=1e-3) # Same learning rate as Part 1\n",
        "losses = []\n",
        "num_epochs = 5 # Same number of epochs as Part 1\n",
        "batch_size = 128 # Same batch size as Part 1\n",
        "\n",
        "train_dataloader = torch.utils.data.DataLoader(\n",
        "    dataset[\"train\"], batch_size=batch_size, shuffle=True\n",
        ")\n",
        "\n",
        "# --- Training Loop ---\n",
        "for epoch in range(num_epochs):\n",
        "    for batch in train_dataloader:\n",
        "        clean_images = batch[\"images\"].to(device)\n",
        "        noise = torch.randn_like(clean_images).to(device)\n",
        "        noise_amount = torch.randn(clean_images.shape[0]).to(device)\n",
        "        noisy_images = corrupt(clean_images, noise, noise_amount) # Same corrupt function\n",
        "\n",
        "        predicted_images = model(noisy_images, 0).sample # Still passing timestep 0\n",
        "\n",
        "        loss = F.mse_loss(predicted_images, clean_images)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        losses.append(loss.item())\n",
        "\n",
        "    avg_loss = sum(losses[-len(train_dataloader):]) / len(train_dataloader)\n",
        "    print(f\"Finished epoch {epoch+1}. Average loss: {avg_loss:.4f}\")\n",
        "\n",
        "# --- Plotting Loss Curve ---\n",
        "plt.figure(figsize=(8, 4))\n",
        "plt.plot(losses, label=\"Training Loss\")\n",
        "plt.title(\"Training Loss Curve (UNet2DModel - Direct Prediction)\") # Updated title\n",
        "plt.xlabel(\"Iteration\")\n",
        "plt.ylabel(\"MSE Loss\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "07eb0b5c",
      "metadata": {},
      "source": [
        "As you can see, the core training logic remains the same. We load batches, generate noise, corrupt images, feed them to the `UNet2DModel` (still with a timestep of 0), calculate MSE loss, and update the weights using Adam.  We've also kept the hyperparameters (learning rate, batch size, number of epochs) consistent with Part 1 for a direct comparison.\n",
        "\n",
        "After running this training code, we obtain the following loss curve:\n",
        "\n",
        "[**Insert Loss Curve Plot Image Here -  *This is where you would insert the actual plot image generated by your code***]\n",
        "\n",
        "This loss curve shows the training progress of our `UNet2DModel`.  [**Add a brief observation about the loss curve here - e.g., Does it converge lower than in Part 1? Does it converge faster? Is it more stable?  Even a qualitative observation is good.**]\n",
        "\n",
        "Now that our enhanced UNet is trained, let's see how it performs in denoising MNIST digits!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fb920269",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
