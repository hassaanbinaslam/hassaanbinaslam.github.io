{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "id": "kdcZSuoctukO"
      },
      "source": [
        "---\n",
        "title: Generating Text with Recurrent Neural Networks in PyTorch\n",
        "description: This is a practice notebook to build a character-level language model with LSTM using PyTorch. We will train a model on an input text, and our goal will be to generate some new text. \n",
        "image: images/2022-11-19-pytorch-lstm-text-generation.jpeg\n",
        "date: '2022-11-19'\n",
        "toc: true\n",
        "badges: true\n",
        "output-file: 22022-11-19-pytorch-lstm-text-generation.html\n",
        "\n",
        "categories:\n",
        "- pytorch\n",
        "- lstm\n",
        "\n",
        "keyword:\n",
        "- ml\n",
        "- dl\n",
        "- nn\n",
        "- pytorch\n",
        "- LSTM\n",
        "- text\n",
        "- nlp\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u2O8j6ezu0x9"
      },
      "source": [
        "![](images/2022-11-19-pytorch-lstm-text-generation.jpeg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xsza7iAIu0x_"
      },
      "source": [
        "## Credits\n",
        "This notebook takes inspiration and ideas from the following sources.\n",
        "\n",
        "* \"Machine learning with PyTorch and Scikit-Learn\" by \"Sebastian Raschka, Yuxi (Hayden) Liu, and Vahid Mirjalili\". You can get the book from its website: [Machine learning with PyTorch and Scikit-Learn](https://sebastianraschka.com/books/#machine-learning-with-pytorch-and-scikit-learn). In addition, the GitHub repository for this book has valuable notebooks: [github.com/rasbt/machine-learning-book](https://github.com/rasbt/machine-learning-book). Parts of the code you see in this notebook are taken from [chapter 15](https://github.com/rasbt/machine-learning-book/blob/main/ch15/ch15_part3.ipynb) notebook of the same book.\n",
        "* \"Intro to Deep Learning and Generative Models Course\" lecture series from \"Sebastian Raschka\". Course website: [stat453-ss2021](https://sebastianraschka.com/teaching/stat453-ss2021/). YouTube Link: [Intro to Deep Learning and Generative Models Course](https://www.youtube.com/playlist?list=PLTKMiZHVd_2KJtIXOW0zFhFfBaJJilH51). Lectures that are related to this post are [L15.5 Long Short-Term Memory](https://youtu.be/k6fSgUaWUF8) and [L15.7 An RNN Sentiment Classifier in PyTorch](https://youtu.be/KgrdifrlDxg)\n",
        "* \"Andrej Karpathy\" blog post [The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wtohdezgu0x_"
      },
      "source": [
        "## Environment\n",
        "This notebook [GitHub link here](https://github.com/hassaanbinaslam/myblog/blob/main/posts/2022-11-19-pytorch-lstm-text-generation.ipynb) is prepared with Google Colab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E9kZUBv9DobO",
        "outputId": "8b4cefed-e790-4b6c-9998-e49b8f2d6b47"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "python==3.7.15\n",
            "numpy==1.21.6\n",
            "torch==1.12.1+cu113\n",
            "matplotlib==3.2.2\n"
          ]
        }
      ],
      "source": [
        "#| code-fold: true\n",
        "from platform import python_version\n",
        "import numpy, matplotlib, pandas, torch\n",
        "\n",
        "print(\"python==\" + python_version())\n",
        "print(\"numpy==\" + numpy.__version__)\n",
        "print(\"torch==\" + torch.__version__)\n",
        "print(\"matplotlib==\" + matplotlib.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MBWOSLktKMoD"
      },
      "source": [
        "## Introduction\n",
        "Recurrent Neural Network (RNN) works well for sequence problems, i.e., predicting the next sequence item. Stock prices, for example, are a type of sequence data more commonly known as time-series data. A similar notion can be applied to the NLP domain to build a character-level language model. Here language textual data becomes the sequence data, and from our model, we try to predict the next character in the input text. For training, the input text is broken into a sequence of characters and fed to the model one character at a time. The network will process the new character in relation to previously seen characters and use this information to predict the next alphabet."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oWPcusVG6oC8"
      },
      "source": [
        "## Data Preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UvqRfcfd6qBM"
      },
      "source": [
        "### Download data\n",
        "For input text, we will use a famous English folk story (though any other text will work equally well) with the name [Cinderella](https://en.wikipedia.org/wiki/Cinderella). To download the story text, you may use [Project Gutenberg](https://www.gutenberg.org/cache/epub/10830/pg10830.txt) site or [Archive.org](https://ia600204.us.archive.org/30/items/cinderella10830gut/10830.txt)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Ib_HGLXwuxIa"
      },
      "outputs": [],
      "source": [
        "download_link = \"https://ia600204.us.archive.org/30/items/cinderella10830gut/10830.txt\"\n",
        "\n",
        "## alternate download link\n",
        "# download_link = \"https://www.gutenberg.org/cache/epub/10830/pg10830.txt\"\n",
        "\n",
        "file_name = 'input.txt'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8IgzpFma3B21",
        "outputId": "08ab0427-083e-441f-c596-868284544089"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 45278  100 45278    0     0  38865      0  0:00:01  0:00:01 --:--:-- 38831\n"
          ]
        }
      ],
      "source": [
        "##\n",
        "# download the story text and save it as {file_name}\n",
        "! curl {download_link} -o {file_name}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XTB60IRm3SD3"
      },
      "source": [
        "The download is complete. We can now open the file and read its contents."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "kfsJCpko3I6p"
      },
      "outputs": [],
      "source": [
        "##\n",
        "# Reading and processing text\n",
        "with open(file_name, \"r\", encoding=\"utf8\") as fp:\n",
        "    text = fp.read()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FO_RBkaI3jz6"
      },
      "source": [
        "### Preprocess data\n",
        "\n",
        "The downloaded text has been published as a volunteer effort under **Project Gutenberg**. They have added some project and license information after the original story text as part of the project requirements. We are not interested in that text (boilerplate text), so let's omit that and limit our input text to the folk story."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4NwL2Ytz3VTA",
        "outputId": "b9127f6f-d88f-41bf-9b03-b29d1af1c715"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total Length (character count): 21831\n"
          ]
        }
      ],
      "source": [
        "##\n",
        "# truncate text till story start and end\n",
        "start_indx = text.find(\n",
        "    \"There once lived a gentleman and his wife, who were the parents of a\\nlovely little daughter.\"\n",
        ")\n",
        "end_indx = text.find(\"*       *       *       *       *\")\n",
        "\n",
        "text = text[start_indx:end_indx]\n",
        "\n",
        "# total length of the text\n",
        "print(\"Total Length (character count):\", len(text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DdmC50u76FjL"
      },
      "source": [
        "### How does the data look?\n",
        "\n",
        "Let's view the first 500 characters from the story text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "qGjFlqU854GX",
        "outputId": "372bb33a-850f-471e-e086-d5ffc4f86298"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'There once lived a gentleman and his wife, who were the parents of a\\nlovely little daughter.\\n\\nWhen this child was only nine years of age, her mother fell sick.\\nFinding her death coming on, she called her child to her and said to\\nher, \"My child, always be good; bear every thing that happens to you\\nwith patience, and whatever evil and troubles you may suffer, you will\\nbe happy in the end if you are so.\" Then the poor lady died, and her\\ndaughter was full of great grief at the loss of a mother so go'"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# view the text start\n",
        "text[:500]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hCjnocCI6NA6"
      },
      "source": [
        "And the last 500 characters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "3vvBwMtp6K8n",
        "outputId": "83201752-ee9e-44a6-b39e-71acbc27bec3"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'their affection.\\nShe was then taken to the palace of the young prince, in whose eyes she\\nappeared yet more lovely than before, and who married her shortly after.\\n\\nCinderella, who was as good as she was beautiful, allowed her sisters to\\nlodge in the palace, and gave them in marriage, that same day, to two\\nlords belonging to the court.\\n\\n[Illustration: MARRIAGE OF THE PRINCE AND CINDERELLA.]\\n\\nThe amiable qualities of Cinderella were as conspicuous after as they\\nhad been before marriage.\\n\\n\\n\\n\\n       '"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# view the text end\n",
        "text[-500:]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1FIRf2jP9Hxz"
      },
      "source": [
        "### Preparing data dictionary\n",
        "Our data is a string and can't be used to train a model. So instead, we have to convert it into integers. For this encoding, we will use a simple methodology where each unique character in the text is assigned an integer and then replaced with all occurrences of that character in the text with that integer value.\n",
        "\n",
        "For this, let's first create a set of all the unique characters in the text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KG1uvKEH6SDC",
        "outputId": "e8c52889-57ad-47cc-a8c8-b4bef4adcacb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unique Characters: 65\n",
            "['\\n', ' ', '!', '\"', \"'\", ',', '-', '.', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'Y', 'Z', '[', ']', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# find unique chars from text\n",
        "char_set = set(text)\n",
        "print(\"Unique Characters:\", len(char_set))\n",
        "\n",
        "# sort char set\n",
        "chars_sorted = sorted(char_set)\n",
        "print(chars_sorted)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VXDo73J0-zHv"
      },
      "source": [
        "We now know all the unique characters in our input text. Accordingly, we can create a dictionary and assign each character in `char_set` a unique integer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q_Q1T-v_-vyC",
        "outputId": "0b7a8e2c-d7f7-4ae0-acc4-818af3912c53"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'\\n': 0, ' ': 1, '!': 2, '\"': 3, \"'\": 4, ',': 5, '-': 6, '.': 7, ':': 8, ';': 9, '?': 10, 'A': 11, 'B': 12, 'C': 13, 'D': 14, 'E': 15, 'F': 16, 'G': 17, 'H': 18, 'I': 19, 'J': 20, 'K': 21, 'L': 22, 'M': 23, 'N': 24, 'O': 25, 'P': 26, 'Q': 27, 'R': 28, 'S': 29, 'T': 30, 'U': 31, 'V': 32, 'W': 33, 'Y': 34, 'Z': 35, '[': 36, ']': 37, '_': 38, 'a': 39, 'b': 40, 'c': 41, 'd': 42, 'e': 43, 'f': 44, 'g': 45, 'h': 46, 'i': 47, 'j': 48, 'k': 49, 'l': 50, 'm': 51, 'n': 52, 'o': 53, 'p': 54, 'q': 55, 'r': 56, 's': 57, 't': 58, 'u': 59, 'v': 60, 'w': 61, 'x': 62, 'y': 63, 'z': 64}\n"
          ]
        }
      ],
      "source": [
        "# encode chars\n",
        "char2int = {ch: i for i, ch in enumerate(chars_sorted)}\n",
        "\n",
        "# `char2int` dictionary for char -> int\n",
        "print(char2int)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IqkThDbH_Lus"
      },
      "source": [
        "But more than just the encoding, we also need a way to convert the encoded characters back to the original form. For this, we will use a separate array that will hold the index of each `char` in the dictionary. Together with `char2int` and `int2char` we can move back and forth between encoded and decoded characters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yPfiZDCU-9hC",
        "outputId": "c3767341-eced-4fab-f94e-3e8951aa7c5d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['\\n' ' ' '!' '\"' \"'\" ',' '-' '.' ':' ';' '?' 'A' 'B' 'C' 'D' 'E' 'F' 'G'\n",
            " 'H' 'I' 'J' 'K' 'L' 'M' 'N' 'O' 'P' 'Q' 'R' 'S' 'T' 'U' 'V' 'W' 'Y' 'Z'\n",
            " '[' ']' '_' 'a' 'b' 'c' 'd' 'e' 'f' 'g' 'h' 'i' 'j' 'k' 'l' 'm' 'n' 'o'\n",
            " 'p' 'q' 'r' 's' 't' 'u' 'v' 'w' 'x' 'y' 'z']\n"
          ]
        }
      ],
      "source": [
        "int2char = np.array(chars_sorted)\n",
        "\n",
        "# `int2char` for int -> char\n",
        "print(int2char)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UdUTmXib_vb8"
      },
      "source": [
        "### Encode input text\n",
        "In this step, we will use the `char2int` dictionary to encode our story text. The encoded version of `text` is called `text_encoded`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ReYfQ7nk_1nC",
        "outputId": "85147933-cc61-4443-b731-926d8361ed5d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Text encoded shape:  (21831,)\n"
          ]
        }
      ],
      "source": [
        "##\n",
        "# encode original text\n",
        "text_encoded = np.array([char2int[ch] for ch in text], dtype=np.int32)\n",
        "\n",
        "print(\"Text encoded shape: \", text_encoded.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vb2w7KeVAd8d"
      },
      "source": [
        "Let's use `int2char` to decode and return the original text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V8pEgkR_FqYp",
        "outputId": "3dfc5e40-bfe5-4d42-c3d4-e1deb39e5bb0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "30 -> T\n",
            "46 -> h\n",
            "43 -> e\n",
            "56 -> r\n",
            "43 -> e\n"
          ]
        }
      ],
      "source": [
        "##\n",
        "# decoding original text\n",
        "for ex in text_encoded[:5]:\n",
        "    print(\"{} -> {}\".format(ex, int2char[ex]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2iCFx2i4F4f2"
      },
      "source": [
        "Another example of encoding and decoding. This time I used multiple words together."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AtH-SI0x_7si",
        "outputId": "9e2b9f46-a56f-4901-9524-e0b10ad483c1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "There once lived a      == Encoding ==>  [30 46 43 56 43  1 53 52 41 43  1 50 47 60 43 42  1 39]\n",
            "[45 43 52 58 50 43 51 39 52  1 39 52 42  1 46 47 57  1 61 47 44 43]  == Reverse  ==>  gentleman and his wife\n"
          ]
        }
      ],
      "source": [
        "print(text[:18], \"     == Encoding ==> \", text_encoded[:18])\n",
        "print(text_encoded[19:41], \" == Reverse  ==> \", \"\".join(int2char[text_encoded[19:41]]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UYckMTJrGOn5"
      },
      "source": [
        "### Prepare data sequences\n",
        "We have our encoded data ready. Next, we will convert it into sequences of fixed length. The last sequence element will act as a target, and the remaining elements will be the input. For sequencing, we will use length 41. \n",
        "\n",
        "* The first 40 characters in sequence form the input\n",
        "* The last character in sequence (41) represents the output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "RadZ2v91GTWG"
      },
      "outputs": [],
      "source": [
        "##\n",
        "# make sequences of encoded text as `text_chunks`\n",
        "seq_length = 40\n",
        "chunk_size = seq_length + 1\n",
        "\n",
        "text_chunks = [\n",
        "    text_encoded[i : i + chunk_size] for i in range(len(text_encoded) - chunk_size + 1)\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e0x11xBWKidH",
        "outputId": "fe404cbb-adcc-4d22-d784-b3842a02de76"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[30 46 43 56 43  1 53 52 41 43  1 50 47 60 43 42  1 39  1 45 43 52 58 50\n",
            " 43 51 39 52  1 39 52 42  1 46 47 57  1 61 47 44]  ->  43\n",
            "'There once lived a gentleman and his wif'  ->  'e'\n"
          ]
        }
      ],
      "source": [
        "##\n",
        "# inspect the first chuck\n",
        "for seq in text_chunks[:1]:\n",
        "    input_seq = seq[:-1]\n",
        "    target = seq[-1]\n",
        "\n",
        "    print(input_seq, \" -> \", target)\n",
        "    print(repr(\"\".join(int2char[input_seq])), \" -> \", repr(\"\".join(int2char[target])))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NPOv3GEgKnRj",
        "outputId": "a01e82c2-7a34-420d-e98f-64e573d98589"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[46 43 56 43  1 53 52 41 43  1 50 47 60 43 42  1 39  1 45 43 52 58 50 43\n",
            " 51 39 52  1 39 52 42  1 46 47 57  1 61 47 44 43]  ->  5\n",
            "'here once lived a gentleman and his wife'  ->  ','\n"
          ]
        }
      ],
      "source": [
        "##\n",
        "# inspect the second chuck\n",
        "for seq in text_chunks[1:2]:\n",
        "    input_seq = seq[:-1]\n",
        "    target = seq[-1]\n",
        "\n",
        "    print(input_seq, \" -> \", target)\n",
        "    print(repr(\"\".join(int2char[input_seq])), \" -> \", repr(\"\".join(int2char[target])))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f8laz0rgN9fH"
      },
      "source": [
        "## Load Data into Dataset and DataLoader class\n",
        "In this section, we will load our encoded data sequences into Dataset and DataLoader class to prepare batches for model training.\n",
        "\n",
        "### Load data into Dataset class\n",
        "class `TextDataset` is derived from PyTorch `Dataset`. When we get a sequence using this class, it will return the sequence as a tuple of input and target."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0E0s3hsMGToh",
        "outputId": "b5befe45-930a-45fa-c85c-768c1dd6317d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:15: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:201.)\n",
            "  from ipykernel import kernelapp as app\n"
          ]
        }
      ],
      "source": [
        "#| code-fold: show\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, text_chunks):\n",
        "        self.text_chunks = text_chunks\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.text_chunks)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text_chunk = self.text_chunks[idx]\n",
        "        return text_chunk[:-1].long(), text_chunk[1:].long()  # return input, target\n",
        "\n",
        "\n",
        "seq_dataset = TextDataset(torch.tensor(text_chunks))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LviQsBnYPF-w"
      },
      "source": [
        "Each element from the `seq_dataset` consists of \n",
        "\n",
        "* `input` data that we will feed to the model for training\n",
        "* `target` data that we will use to compare the model output\n",
        "\n",
        "Remember that both `input` and `target` sequences are derived from the same encoded text. We train our model to predict the next character from the given input. One character is given as an input to the model, and one character output comes out of the model. In an ideal case, the model output character should represent the next character in a sequence. And our `target` sequence is just that: one next character from the input sequence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8kk5uUXrOSGN",
        "outputId": "7da935b2-0c86-4736-ca4a-26d5ed4df0f8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Input (x): 'There once lived a gentleman and his wif'\n",
            "Target (y): 'here once lived a gentleman and his wife'\n",
            "\n",
            " Input (x): 'here once lived a gentleman and his wife'\n",
            "Target (y): 'ere once lived a gentleman and his wife,'\n",
            "\n"
          ]
        }
      ],
      "source": [
        "for i, (seq, target) in enumerate(seq_dataset):\n",
        "    print(\" Input (x):\", repr(\"\".join(int2char[seq])))\n",
        "    print(\"Target (y):\", repr(\"\".join(int2char[target])))\n",
        "    print()\n",
        "    if i == 1:\n",
        "        break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vmnlopS0Wgvw"
      },
      "source": [
        "### Load data into DataLoader class to prepare batches\n",
        "In this step, we have prepared training batches using the PyTorch [DataLoader](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader) class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "6ajuSPNhO7ue"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "batch_size = 64\n",
        "\n",
        "torch.manual_seed(1)\n",
        "seq_dl = DataLoader(seq_dataset, batch_size=batch_size, shuffle=True, drop_last=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4XCeL7qwW2By"
      },
      "source": [
        "## Model Configuration and Training\n",
        "\n",
        "In this section, we will configure a model for character-level language modeling. This model will have an Embedding layer at the start. Next, output from the embedding layer will be passed to the LSTM layer. Finally, at the output, we have a fully connected linear layer.\n",
        "\n",
        "For an in-depth analysis of the working of an Embedding layer, I recommend this article [Embeddings in Machine Learning: Everything You Need to Know](https://www.featureform.com/post/the-definitive-guide-to-embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "y76zG-lTWxmB"
      },
      "outputs": [],
      "source": [
        "#| code-fold: show\n",
        "import torch.nn as nn\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "class RNN(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, rnn_hidden_size):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.rnn_hidden_size = rnn_hidden_size\n",
        "        self.rnn = nn.LSTM(embed_dim, rnn_hidden_size, batch_first=True)\n",
        "        self.fc = nn.Linear(rnn_hidden_size, vocab_size)\n",
        "\n",
        "    def forward(self, x, hidden, cell):\n",
        "        out = self.embedding(x).unsqueeze(1)\n",
        "        out, (hidden, cell) = self.rnn(out, (hidden, cell))\n",
        "        out = self.fc(out).reshape(out.size(0), -1)\n",
        "        return out, hidden, cell\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        hidden = torch.zeros(1, batch_size, self.rnn_hidden_size)\n",
        "        cell = torch.zeros(1, batch_size, self.rnn_hidden_size)\n",
        "        return hidden.to(device), cell.to(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5bjQZUmxW69X",
        "outputId": "0d50bde3-f39f-4900-900b-cc4eb488e8eb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "RNN(\n",
              "  (embedding): Embedding(65, 256)\n",
              "  (rnn): LSTM(256, 512, batch_first=True)\n",
              "  (fc): Linear(in_features=512, out_features=65, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.manual_seed(1)\n",
        "\n",
        "# define model dimensions\n",
        "vocab_size = len(int2char)\n",
        "embed_dim = 256\n",
        "rnn_hidden_size = 512\n",
        "\n",
        "# initialize model\n",
        "model = RNN(vocab_size, embed_dim, rnn_hidden_size)\n",
        "model = model.to(device)\n",
        "model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lhPHlhCUdvT-"
      },
      "source": [
        "### Configure loss function and optimizer\n",
        "\n",
        "* For the loss function, we will use [CrossEntropyLoss](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html). This is because we are dealing with a classification problem, and our model has to predict the next character from `vocab_size` of 65 classes.\n",
        "* For optimization, we will use [torch.optim.Adam](https://pytorch.org/docs/stable/generated/torch.optim.Adam.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "kNRg8U2xXIC3"
      },
      "outputs": [],
      "source": [
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.005)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JMigjW8-d2Si"
      },
      "source": [
        "### Model training\n",
        "\n",
        "All parts are ready so let's start the training. Google Colab \"CPU\" runtime can take significantly longer to train. I would suggest using \"GPU\" runtime instead."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fHmfxuIWdzWJ",
        "outputId": "2f289099-3541-49b3-9e38-f124466370ca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0 loss: 2.6252\n",
            "Epoch 500 loss: 0.3377\n",
            "Epoch 1000 loss: 0.2502\n",
            "Epoch 1500 loss: 0.2403\n",
            "Epoch 2000 loss: 0.2501\n",
            "Epoch 2500 loss: 0.2374\n",
            "Epoch 3000 loss: 0.2368\n",
            "Epoch 3500 loss: 0.2499\n",
            "Epoch 4000 loss: 0.2643\n",
            "Epoch 4500 loss: 0.2555\n",
            "Epoch 5000 loss: 0.3854\n",
            "Epoch 5500 loss: 0.2326\n",
            "Epoch 6000 loss: 0.2390\n",
            "Epoch 6500 loss: 0.2270\n",
            "Epoch 7000 loss: 0.2663\n",
            "Epoch 7500 loss: 0.3403\n",
            "Epoch 8000 loss: 0.2475\n",
            "Epoch 8500 loss: 0.2370\n",
            "Epoch 9000 loss: 0.2126\n",
            "Epoch 9500 loss: 0.2308\n",
            "Total execution time in seconds:  378.14\n",
            "Device type:  cuda\n"
          ]
        }
      ],
      "source": [
        "#| code-fold: show\n",
        "# for execution time measurement\n",
        "from timeit import default_timer as timer\n",
        "\n",
        "num_epochs = 10000\n",
        "model.train()\n",
        "\n",
        "start = timer()  # timer start\n",
        "for epoch in range(num_epochs):\n",
        "    hidden, cell = model.init_hidden(batch_size)\n",
        "\n",
        "    seq_batch, target_batch = next(iter(seq_dl))\n",
        "    seq_batch = seq_batch.to(device)\n",
        "    target_batch = target_batch.to(device)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss = 0\n",
        "\n",
        "    for c in range(seq_length):\n",
        "        pred, hidden, cell = model(seq_batch[:, c], hidden, cell)\n",
        "        loss += loss_fn(pred, target_batch[:, c])\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    loss = loss.item() / seq_length\n",
        "    if epoch % 500 == 0:\n",
        "        print(f\"Epoch {epoch} loss: {loss:.4f}\")\n",
        "\n",
        "end = timer()  # timer end\n",
        "print(\"Total execution time in seconds: \", \"%.2f\" % (end - start))\n",
        "print(\"Device type: \", device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lMVXN21SyjCI"
      },
      "source": [
        "## Process output from the model\n",
        "\n",
        "Getting a prediction (text generation) from the model takes some extra work. Since the model is trained on encoded text, the output generated from the model is also encoded. Further, any input used for prediction itself needs to be encoded using the same encoding dictionary model it is trained with. For this, we have defined a helper function.\n",
        "\n",
        "* This function will take the input text and encode it before passing it to the model\n",
        "* It will take the output from the model and decode it before returning\n",
        "* Note that LSTM model output has `logits, hidden state, and cell state` . Logits give us the next predicted character. Hidden state and cell state are for keeping the context (or memory) of characters processed so far and are supplied to the model for the next prediction.\n",
        "* For the output logits, we can predict the next character using the index of the highest logit value. This will make our model predict the exact text on the same input each time. To introduce some randomness, we take help from PyTorch class [torch.distributions.categorical.Categorical](https://pytorch.org/docs/stable/distributions.html#categorical). This is how it works\n",
        "\n",
        "    * We obtain output probabilities by applying softmax to logits and pass them to a Categorical object to create a distribution.\n",
        "    * Generate a sample from a Categorical object. Samples generated from the same distribution may be different. This way, we get different outputs with the same input text.\n",
        "    * This way, we can also control the predictability of the model output by controlling the probability distribution (calculated from logits) passed to the Categorical object. If we can make probabilities a lot more similar (through scaling), the sample generated by Categorical will also be mostly the same. On the other hand, if we can make the probabilities further apart, then we can also increase the randomness of the output from the Categorical class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "sSrnPAcGwyZz"
      },
      "outputs": [],
      "source": [
        "#| code-fold: show\n",
        "from torch.distributions.categorical import Categorical\n",
        "\n",
        "def sample(model, starting_str, len_generated_text=500, scale_factor=1.0):\n",
        "\n",
        "    encoded_input = torch.tensor([char2int[s] for s in starting_str])\n",
        "    encoded_input = torch.reshape(encoded_input, (1, -1))\n",
        "\n",
        "    generated_str = starting_str\n",
        "\n",
        "    model.eval()\n",
        "    hidden, cell = model.init_hidden(1)\n",
        "    hidden = hidden.to(\"cpu\")\n",
        "    cell = cell.to(\"cpu\")\n",
        "    for c in range(len(starting_str) - 1):\n",
        "        _, hidden, cell = model(encoded_input[:, c].view(1), hidden, cell)\n",
        "\n",
        "    last_char = encoded_input[:, -1]\n",
        "    for i in range(len_generated_text):\n",
        "        logits, hidden, cell = model(last_char.view(1), hidden, cell)\n",
        "        logits = torch.squeeze(logits, 0)\n",
        "        scaled_logits = logits * scale_factor\n",
        "        m = Categorical(logits=scaled_logits)\n",
        "        last_char = m.sample()\n",
        "        generated_str += str(int2char[last_char])\n",
        "\n",
        "    return generated_str"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lLiTZzddFYMw"
      },
      "source": [
        "## Generating new text passages\n",
        "\n",
        "We are processing text and model output on the 'CPU' device in the 'sample' function. So let's also move the model to the same device."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WTSOGh1mE9rl",
        "outputId": "1911eba3-6f14-466e-d427-971a2f1cc322"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "RNN(\n",
              "  (embedding): Embedding(65, 256)\n",
              "  (rnn): LSTM(256, 512, batch_first=True)\n",
              "  (fc): Linear(in_features=512, out_features=65, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "##\n",
        "# move model to cpu\n",
        "model.to('cpu')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5PF8P8PAFSdd"
      },
      "source": [
        "Before generating some lengthy text, let's experiment with simple words and see if our model can complete them.\n",
        "\n",
        "At first, I used the string \"fat\" and asked the model to generate the following three characters to complete this word. But at the same time, I have passed a tiny scaling factor meaning I have decreased the model's predictability."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "62SznZR2AZA7",
        "outputId": "567aab7e-229d-47e7-9058-8f35b195ef39"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "fat, i\n"
          ]
        }
      ],
      "source": [
        "print(sample(model, starting_str=\"fat\", len_generated_text=3, scale_factor=0.1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kAyeFtWoGf0T"
      },
      "source": [
        "Next, I asked the model to use the same input and predict the following three characters, but I increased the model's predictability ten times. So let's see the output this time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lxk1dnAaAbne",
        "outputId": "64b81ecd-a1f5-4133-a93b-3d7e03db9c0c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "father\n"
          ]
        }
      ],
      "source": [
        "print(sample(model, starting_str='fat', len_generated_text=3, scale_factor=1.0))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r0lE_yuHG2PH"
      },
      "source": [
        "The second time model generated the correct word \"father\" it had seen before in the training text. So let's now generate some lengthy texts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LHYSjkKtFCpa",
        "outputId": "789757fd-0635-4444-bd01-43e34ad61f48"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The father too was she was one of those good faeries who protect children. Her\n",
            "spirits revived, and she wiped away her tears.\n",
            "\n",
            "The faery took Cinderella by the hand, and old woman, assuming her character of Queen of the\n",
            "Faeries, that only jumped up behind the\n",
            "carriage as nimbly as if they had been footmen and laced so tight, touched Cinderella's clothes with her wand, and said, \"Now, my dear good child,\" said the faery, \"here you have a coach and\n",
            "horses, much handsomer than your sisters', to say the least\n"
          ]
        }
      ],
      "source": [
        "##\n",
        "# text generation example 1\n",
        "print(sample(model, starting_str=\"The father\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v_Lo6AfbHHQg",
        "outputId": "9bf79006-546a-4d37-82d5-c96c81f95841"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The mother so good crust. But\n",
            "if you like to give the household. It was she who washed the dishes, and\n",
            "scrubbed down the step-sisters were very cruel to Cinderella,\n",
            "that he did not eat one morsel of the supper.\n",
            "\n",
            "Cinderella drew the fellow slipper\n",
            "out of her godmother\n",
            "would do with it. Her godmother took the pumpkin, and scooped out the\n",
            "inside of it, leaving nothing but rind; she then struck it with her\n",
            "godmother then said, \"My dear Cinderella,\n",
            "that he did not eat one morsel of the supper.\n",
            "\n",
            "Cinderella drew\n"
          ]
        }
      ],
      "source": [
        "##\n",
        "# text generation example 2\n",
        "print(sample(model, starting_str=\"The mother\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3t4scGtdHR7A",
        "outputId": "abd71fa5-7526-41b4-dbd6-6d19032151c9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The three sisters were very cruel to Cinderella,\n",
            "that he delicacies which she had\n",
            "received from the prince:  but they did not eat one morsel for a\n",
            "couple of days. They spent their whole time before a looking-glass, and\n",
            "they would be laced so tight, tossing her head disdainfully, \"that I\n",
            "should lend my clothes to a dirty Cinderella like you!\"\n",
            "\n",
            "Cinderella quite amazed; but their\n",
            "astonishment at her dancing was still greater.\n",
            "\n",
            "Gracefulness seemed to play in the attempt.\n",
            "\n",
            "The long-wished-for evening came at last, an\n"
          ]
        }
      ],
      "source": [
        "##\n",
        "# text generation example 3\n",
        "print(sample(model, starting_str=\"The three sisters\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ItjiCjxzHXDC",
        "outputId": "e1aabb58-eed7-405c-b970-fe504107e75e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The lovely prince\n",
            "immediately jumped up behind the\n",
            "carriage as nimbly as conspicuous after as they\n",
            "had been before mocking me,\" replied the poor girl to do all the\n",
            "drudgery of the household. It was she who washed the dishes, and\n",
            "scrubbed down the stairs, who tried with all their might to force their unwould stration: CINDERELLA IS PRESENTED BY THE PRINCE TO THE KING AND\n",
            "QUEEN, WHO WELCOME HER WITH THE HONORS DUE TO A GREAT PRINCESS, AND IS\n",
            "THEN LED INTO THE ROYAL BY THE HER WITH THE HONORS DUE TO A GREAT PRINCES\n"
          ]
        }
      ],
      "source": [
        "##\n",
        "# text generation example 4\n",
        "print(sample(model, starting_str=\"The lovely prince\"))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "instance_type": "ml.t3.medium",
    "kernelspec": {
      "display_name": "Python 3.8.8 ('myblog')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    },
    "vscode": {
      "interpreter": {
        "hash": "bbac80ad2bfd54975e0c0f7ddf300156d5b24b5126f35dc262fa887f22fb28f1"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
