{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "title: Streaming Responses in FastAPI\n",
    "description: In this blog post, I explore how to stream responses in FastAPI using Server-Sent Events, StreamingResponse, and WebSockets. Through simple examples that simulate LLM outputs, I demonstrate how you can efficiently stream real-time data in your applications.\n",
    "date: '2025-01-19'\n",
    "image: images/2023-05-29-tips-aws-cert-pearsonvue-online-pakistan.jpeg\n",
    "output-file: 2025-01-19-streaming-responses-fastapi\n",
    "toc: true\n",
    "\n",
    "categories:\n",
    "- python\n",
    "\n",
    "keyword:\n",
    "- FastAPI\n",
    "- Python\n",
    "- Streaming\n",
    "- WebSockets\n",
    "- Web Development\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "When working with LLMs, you quickly realize that getting a response all at once doesn't always cut it. For a better user experience, especially with long outputs, streaming the response is the way to go. But standard REST APIs aren't built for this – they hold the client hostage until the entire response is ready. I've been exploring how to overcome this with FastAPI, and in this post, I'll walk you through creating streaming APIs using StreamingResponse, Server-Sent Events (SSE), and WebSockets. I'll use a simple dummy LLM to illustrate the concepts, and you can find all the code examples in the linked GitHub repository."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment Details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python==3.12.8\n",
      "fastapi==0.115.6\n"
     ]
    }
   ],
   "source": [
    "#| code-fold: true\n",
    "#| code-summary: \"Show the code\"\n",
    "\n",
    "from platform import python_version\n",
    "import fastapi\n",
    "\n",
    "print(\"python==\" + python_version())\n",
    "print(\"fastapi==\" + fastapi.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulating LLM Streaming Output\n",
    "As we dive into implementing streaming APIs, it's crucial to have a way to mimic how an LLM would generate text. Rather than connecting to a real model for every example, we'll use a simple, controlled simulation. This allows us to focus on the streaming mechanisms without getting bogged down in the complexities of LLM inference.\n",
    "\n",
    "Our simulation is based on a straightforward asynchronous generator function called `event_generator`. Here’s the Python code:\n",
    "\n",
    "```python\n",
    "# Test messages\n",
    "MESSAGES = [\n",
    "    \"This is\",\n",
    "    \" a large\",\n",
    "    \" response\",\n",
    "    \" being\",\n",
    "    \" streamed\",\n",
    "    \" through FastAPI.\",\n",
    "    \" Here's the final\",\n",
    "    \" chunk!\",\n",
    "]\n",
    "\n",
    "async def event_generator():\n",
    "    for message in MESSAGES:\n",
    "        yield message\n",
    "        await asyncio.sleep(1)  # Simulate an async delay\n",
    "\n",
    "```\n",
    "\n",
    "As you can see, the `event_generator` iterates through a list of predefined messages. With each iteration, it yields a dictionary containing the data and then pauses for 1 second using `asyncio.sleep(1)`. This pause mimics the time it might take for an LLM to generate the next portion of a response. This approach makes the simulation both easy to understand and also representative of the asynchronous nature of LLM output. We'll use this `event_generator` function across all the streaming examples in this post."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Method 1: Streaming with StreamingResponse\n",
    "FastAPI's` StreamingResponse` is a powerful tool for handling situations where you need to send large or continuous data to a client. Unlike traditional REST API responses, which require the server to generate the entire response before sending, `StreamingResponse` allows the server to transmit data in chunks. This is especially useful for use cases such as streaming audio/video, large file downloads, or, as in our case, delivering output from an LLM model.\n",
    "\n",
    "#### Key Differences from REST API Responses\n",
    "\n",
    "- **REST API**: In a typical REST scenario, the entire response body is generated on the server and then sent to the client as a single unit. This works fine for smaller datasets, but it can be inefficient when dealing with large amounts of data. The client has to wait until the whole response is built before it can begin processing.\n",
    "\n",
    "- **StreamingResponse**: With `StreamingResponse`, data is transmitted in a series of chunks over a single HTTP connection. As soon as the first chunk is ready, it's sent to the client. This allows the client to begin processing data while the server is still generating the rest of the response. This incremental delivery significantly enhances the user experience, particularly when dealing with long processing tasks like those often found in LLM interactions.\n",
    "\n",
    "#### HTTP Protocol Differences\n",
    "\n",
    "- **Content-Length**: In a traditional REST API response, the `Content-Length` header specifies the total size of the response in bytes. However, with `StreamingResponse`, the total size of the data is not always known upfront. As such the `Content-Length` header might be absent, or more commonly the header `Transfer-Encoding: chunked` will be used instead.\n",
    "\n",
    "- **Transfer-Encoding: chunked**: This HTTP header indicates that the response body is being sent in chunks. Each chunk is prefaced by its size, allowing the client to process data as it arrives without knowing the total size of the response beforehand.\n",
    "\n",
    "#### How Clients Recognize Incoming Data is Streamed?\n",
    "\n",
    "The client recognizes that it is receiving a streamed response when it encounters the `Transfer-Encoding: chunked` header. Each chunk is prefaced by its size, and the client waits for the next chunk. This process continues until the stream is closed, indicating that there's no more data to receive.\n",
    "\n",
    "The server signals the end of the stream in one of the following ways:\n",
    "\n",
    "- **Empty Chunk**: The server sends a final chunk that has a size of 0. This indicates that there is no more data to send.\n",
    "\n",
    "- **Connection Closure**: If the stream ends naturally (for example, because the generator function has exhausted), FastAPI will automatically close the connection. In essence, the server sends a TCP \"FIN\" packet to the client signaling the end of transmission.\n",
    "\n",
    "In the following example, we'll see how we can use FastAPI's `StreamingResponse` along with our simulated LLM to demonstrate how to construct a streaming API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing StreamingResponse: A Practical Example\n",
    "\n",
    "Now, let's solidify our understanding with a concrete example using FastAPI's StreamingResponse. On the server side, our endpoint will use the `event_generator` we defined earlier to stream data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-fold: true\n",
    "#| code-summary: \"Show the code\"\n",
    "#| filename: \"app_stream_response.py\"\n",
    "\n",
    "from fastapi import FastAPI\n",
    "from fastapi.responses import StreamingResponse\n",
    "import asyncio\n",
    "\n",
    "# Test messages (same as before)\n",
    "MESSAGES = [\n",
    "    \"This is\",\n",
    "    \" a large\",\n",
    "    \" response\",\n",
    "    \" being\",\n",
    "    \" streamed\",\n",
    "    \" through FastAPI.\",\n",
    "    \" Here's the final\",\n",
    "    \" chunk!\",\n",
    "]\n",
    "\n",
    "async def event_generator():\n",
    "    for message in MESSAGES:\n",
    "        yield message\n",
    "        await asyncio.sleep(1)  # Simulate an async delay\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "@app.get(\"/stream\")\n",
    "async def stream_response():\n",
    "    return StreamingResponse(\n",
    "        event_generator(),\n",
    "        media_type=\"text/plain\",)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this code, the `/stream` endpoint utilizes `StreamingResponse`. We're passing in the output of the `event_generator` after extracting the data field, which will be streamed as `text/plain`. The key here is that the data isn't collected into a single string first; instead, it is yielded piece-by-piece, and `StreamingResponse` handles the chunking and transmission.\n",
    "\n",
    "### Client-Side Implementation\n",
    "\n",
    "On the client side, fetching this stream requires a slightly different approach than traditional REST calls. Instead of waiting for a single, complete JSON response, we read from the stream as it becomes available. Here is the JavaScript code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "javascript"
    }
   },
   "outputs": [],
   "source": [
    "#| code-fold: true\n",
    "#| code-summary: \"Show the code\"\n",
    "#| filename: \"stream.html\"\n",
    "\n",
    "async function streamResponse() {\n",
    "   try {\n",
    "      const response = await fetch('/stream');\n",
    "      const reader = response.body.getReader();\n",
    "      const decoder = new TextDecoder();\n",
    "\n",
    "      while (true) {\n",
    "         const { value, done } = await reader.read();\n",
    "         if (done) break;\n",
    "\n",
    "          const text = decoder.decode(value);\n",
    "          const container = document.getElementById('response-container');\n",
    "          container.innerText += text;\n",
    "      }\n",
    "  } catch (error) {\n",
    "      console.error('Streaming error:', error);\n",
    "  }\n",
    "}\n",
    "\n",
    "// Start streaming when page loads\n",
    "streamResponse();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explanation of the Client-Side Code\n",
    "\n",
    "- **response.body.getReader()**: This line retrieves a `ReadableStreamReader` from the response body. The `ReadableStream` allows us to process incoming data in chunks as they are received, rather than having to wait for the entire response to be downloaded.\n",
    "- **const { value, done } = await reader.read()**: This is the heart of the stream processing. It asynchronously reads the next chunk of data from the stream. The return value is an object that contains two properties:\n",
    "- **value**: This is a `Uint8Array` (binary data) containing the latest chunk of data.\n",
    "- **done**: A boolean that signals if the stream has completed. If done is true, it indicates that there is no more data to process, and the loop should terminate.\n",
    "- **const text = decoder.decode(value)**: The binary chunk (value) is converted to a string using `TextDecoder`, making it usable for display or further processing.\n",
    "- **container.innerText += text**: The decoded string is appended to an HTML element with the ID response-container. This provides visible feedback on the incoming data.\n",
    "\n",
    "#### Comparison to REST API calls\n",
    "\n",
    "In contrast to this streaming approach, a typical REST API call looks like this:\n",
    "\n",
    "```javascript\n",
    "const response = await fetch('/api/data');  // Make the REST API request\n",
    "const data = await response.json(); // Parse the JSON response body\n",
    "```\n",
    "\n",
    "The difference is stark. In a REST API call, we wait for the entire response before processing, while with `StreamingResponse` (and the client-side code), we handle each chunk of data as it becomes available. This allows us to display output to the user almost instantly rather than after a long wait."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspecting Streaming Responses: A Visual Walkthrough\n",
    "\n",
    "To truly understand what's happening under the hood with `StreamingResponse`, let's take a look at the actual HTTP requests and responses using network inspection tools. We'll use Chrome DevTools and Wireshark to examine the data being transmitted between the client and the server.\n",
    "\n",
    "#### 1. Chrome DevTools: Examining HTTP Headers\n",
    "\n",
    "![](images/2025-01-19-streaming-responses-fastapi/stream/stream-html-chrome-dev-tools.PNG){.lightbox}\n",
    "\n",
    "The first screenshot is from Chrome's DevTools, specifically the \"Network\" tab. We can see the request made to the `/stream` endpoint. The key thing to notice in the response headers is the `Transfer-Encoding: chunked` header, highlighted in yellow. This confirms that the server is sending a chunked response, which is essential for streaming. The `Content-Type` is also set to `text/plain`, as specified in our server-side code.\n",
    "\n",
    "#### 2. Wireshark: Diving into the Data Packets\n",
    "\n",
    "![](images/2025-01-19-streaming-responses-fastapi/stream/wireshark-chunks.PNG){.lightbox}\n",
    "\n",
    "The second screenshot comes from Wireshark, a powerful network protocol analyzer. This tool allows us to inspect the raw packets being transmitted over the network. Here, we can see that multiple HTTP \"chunked\" responses are being sent from the server to the client as part of a single HTTP connection. This provides a visual confirmation that the server is indeed streaming data in chunks.\n",
    "\n",
    "#### 3. Analyzing Individual Data Chunks\n",
    "\n",
    "**Sixth Data Chunk:**\n",
    "\n",
    "![](images/2025-01-19-streaming-responses-fastapi/stream/wireshark-chunks-01.PNG){.lightbox}\n",
    "\n",
    "\n",
    "We've opened the sixth data chunk in Wireshark, where we see the chunk has a size of 35 octets. This chunk corresponds to the text \" through FastAPI StreamingResponse.\".\n",
    "\n",
    "**Seventh Data Chunk:**\n",
    "\n",
    "![](images/2025-01-19-streaming-responses-fastapi/stream/wireshark-chunks-02.PNG){.lightbox}\n",
    "\n",
    "Here we have the seventh data chunk, indicated by a size of 17 octets. This maps to the message \" Here's the final\".\n",
    "\n",
    "**Eighth Data Chunk:**\n",
    "\n",
    "![](images/2025-01-19-streaming-responses-fastapi/stream/wireshark-chunks-03.PNG){.lightbox}\n",
    "\n",
    "In this screenshot we opened the eighth data chunk. Here the chunk size is 7 octets and it corresponds to the message \" chunk!\".\n",
    "\n",
    "**Last Data Chunk:**\n",
    "\n",
    "![](images/2025-01-19-streaming-responses-fastapi/stream/wireshark-chunks-04.PNG){.lightbox}\n",
    "\n",
    "Finally, we can see the last chunk being sent. It has a size of 0 octets. This zero-size chunk tells the client that the server has finished sending the stream, and it can close the connection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312quart",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
