{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "title: Streaming Responses in FastAPI\n",
    "description: In this blog post, I explore how to stream responses in FastAPI using Server-Sent Events, StreamingResponse, and WebSockets. Through simple examples that simulate LLM outputs, I demonstrate how you can efficiently stream real-time data in your applications.\n",
    "date: '2025-01-19'\n",
    "image: images/2023-05-29-tips-aws-cert-pearsonvue-online-pakistan.jpeg\n",
    "output-file: 2025-01-19-streaming-responses-fastapi\n",
    "toc: true\n",
    "\n",
    "categories:\n",
    "- python\n",
    "\n",
    "keyword:\n",
    "- FastAPI\n",
    "- Python\n",
    "- Streaming\n",
    "- WebSockets\n",
    "- Web Development\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "When working with LLMs, you quickly realize that getting a response all at once doesn't always cut it. For a better user experience, especially with long outputs, streaming the response is the way to go. But standard REST APIs aren't built for this – they hold the client hostage until the entire response is ready. I've been exploring how to overcome this with FastAPI, and in this post, I'll walk you through creating streaming APIs using StreamingResponse, Server-Sent Events (SSE), and WebSockets. I'll use a simple dummy LLM to illustrate the concepts, and you can find all the code examples in the linked GitHub repository."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment Details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python==3.12.8\n",
      "fastapi==0.115.6\n",
      "sse_starlette==2.2.1\n"
     ]
    }
   ],
   "source": [
    "#| code-fold: true\n",
    "#| code-summary: \"Show the code\"\n",
    "\n",
    "from platform import python_version\n",
    "import fastapi\n",
    "import sse_starlette\n",
    "\n",
    "print(\"python==\" + python_version())\n",
    "print(\"fastapi==\" + fastapi.__version__)\n",
    "print(\"sse_starlette==\" + sse_starlette.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulating LLM Streaming Output\n",
    "As we dive into implementing streaming APIs, it's crucial to have a way to mimic how an LLM would generate text. Rather than connecting to a real model for every example, we'll use a simple, controlled simulation. This allows us to focus on the streaming mechanisms without getting bogged down in the complexities of LLM inference.\n",
    "\n",
    "Our simulation is based on a straightforward asynchronous generator function called `event_generator`. Here’s the Python code:\n",
    "\n",
    "```python\n",
    "# Test messages\n",
    "MESSAGES = [\n",
    "    \"This is\",\n",
    "    \" a large\",\n",
    "    \" response\",\n",
    "    \" being\",\n",
    "    \" streamed\",\n",
    "    \" through FastAPI.\",\n",
    "    \" Here's the final\",\n",
    "    \" chunk!\",\n",
    "]\n",
    "\n",
    "async def event_generator():\n",
    "    for message in MESSAGES:\n",
    "        yield message\n",
    "        await asyncio.sleep(1)  # Simulate an async delay\n",
    "\n",
    "```\n",
    "\n",
    "As you can see, the `event_generator` iterates through a list of predefined messages. With each iteration, it yields a dictionary containing the data and then pauses for 1 second using `asyncio.sleep(1)`. This pause mimics the time it might take for an LLM to generate the next portion of a response. This approach makes the simulation both easy to understand and also representative of the asynchronous nature of LLM output. We'll use this `event_generator` function across all the streaming examples in this post."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Method 1: Streaming with StreamingResponse\n",
    "FastAPI's` StreamingResponse` is a powerful tool for handling situations where you need to send large or continuous data to a client. Unlike traditional REST API responses, which require the server to generate the entire response before sending, `StreamingResponse` allows the server to transmit data in chunks. This is especially useful for use cases such as streaming audio/video, large file downloads, or, as in our case, delivering output from an LLM model.\n",
    "\n",
    "#### Key Differences from REST API Responses\n",
    "\n",
    "- **REST API**: In a typical REST scenario, the entire response body is generated on the server and then sent to the client as a single unit. This works fine for smaller datasets, but it can be inefficient when dealing with large amounts of data. The client has to wait until the whole response is built before it can begin processing.\n",
    "\n",
    "- **StreamingResponse**: With `StreamingResponse`, data is transmitted in a series of chunks over a single HTTP connection. As soon as the first chunk is ready, it's sent to the client. This allows the client to begin processing data while the server is still generating the rest of the response. This incremental delivery significantly enhances the user experience, particularly when dealing with long processing tasks like those often found in LLM interactions.\n",
    "\n",
    "#### HTTP Protocol Differences\n",
    "\n",
    "- **Content-Length**: In a traditional REST API response, the `Content-Length` header specifies the total size of the response in bytes. However, with `StreamingResponse`, the total size of the data is not always known upfront. As such the `Content-Length` header might be absent, or more commonly the header `Transfer-Encoding: chunked` will be used instead.\n",
    "\n",
    "- **Transfer-Encoding: chunked**: This HTTP header indicates that the response body is being sent in chunks. Each chunk is prefaced by its size, allowing the client to process data as it arrives without knowing the total size of the response beforehand.\n",
    "\n",
    "#### How Clients Recognize Incoming Data is Streamed?\n",
    "\n",
    "The client recognizes that it is receiving a streamed response when it encounters the `Transfer-Encoding: chunked` header. Each chunk is prefaced by its size, and the client waits for the next chunk. This process continues until the stream is closed, indicating that there's no more data to receive.\n",
    "\n",
    "The server signals the end of the stream in one of the following ways:\n",
    "\n",
    "- **Empty Chunk**: The server sends a final chunk that has a size of 0. This indicates that there is no more data to send.\n",
    "\n",
    "- **Connection Closure**: If the stream ends naturally (for example, because the generator function has exhausted), FastAPI will automatically close the connection. In essence, the server sends a TCP \"FIN\" packet to the client signaling the end of transmission.\n",
    "\n",
    "In the following example, we'll see how we can use FastAPI's `StreamingResponse` along with our simulated LLM to demonstrate how to construct a streaming API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing StreamingResponse: A Practical Example\n",
    "\n",
    "Now, let's solidify our understanding with a concrete example using FastAPI's StreamingResponse. On the server side, our endpoint will use the `event_generator` we defined earlier to stream data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-fold: true\n",
    "#| code-summary: \"Show the code\"\n",
    "#| filename: \"app_stream_response.py\"\n",
    "\n",
    "from fastapi import FastAPI\n",
    "from fastapi.responses import StreamingResponse\n",
    "import asyncio\n",
    "\n",
    "# Test messages (same as before)\n",
    "MESSAGES = [\n",
    "    \"This is\",\n",
    "    \" a large\",\n",
    "    \" response\",\n",
    "    \" being\",\n",
    "    \" streamed\",\n",
    "    \" through FastAPI.\",\n",
    "    \" Here's the final\",\n",
    "    \" chunk!\",\n",
    "]\n",
    "\n",
    "async def event_generator():\n",
    "    for message in MESSAGES:\n",
    "        yield message\n",
    "        await asyncio.sleep(1)  # Simulate an async delay\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "@app.get(\"/stream\")\n",
    "async def stream_response():\n",
    "    return StreamingResponse(\n",
    "        event_generator(),\n",
    "        media_type=\"text/plain\",)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this code, the `/stream` endpoint utilizes `StreamingResponse`. We're passing in the output of the `event_generator` after extracting the data field, which will be streamed as `text/plain`. The key here is that the data isn't collected into a single string first; instead, it is yielded piece-by-piece, and `StreamingResponse` handles the chunking and transmission.\n",
    "\n",
    "### Client-Side Implementation\n",
    "\n",
    "On the client side, fetching this stream requires a slightly different approach than traditional REST calls. Instead of waiting for a single, complete JSON response, we read from the stream as it becomes available. Here is the JavaScript code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "javascript"
    }
   },
   "outputs": [],
   "source": [
    "#| code-fold: true\n",
    "#| code-summary: \"Show the code\"\n",
    "#| filename: \"stream.html\"\n",
    "\n",
    "async function streamResponse() {\n",
    "   try {\n",
    "      const response = await fetch('/stream');\n",
    "      const reader = response.body.getReader();\n",
    "      const decoder = new TextDecoder();\n",
    "\n",
    "      while (true) {\n",
    "         const { value, done } = await reader.read();\n",
    "         if (done) break;\n",
    "\n",
    "          const text = decoder.decode(value);\n",
    "          const container = document.getElementById('response-container');\n",
    "          container.innerText += text;\n",
    "      }\n",
    "  } catch (error) {\n",
    "      console.error('Streaming error:', error);\n",
    "  }\n",
    "}\n",
    "\n",
    "// Start streaming when page loads\n",
    "streamResponse();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explanation of the Client-Side Code\n",
    "\n",
    "- **response.body.getReader()**: This line retrieves a `ReadableStreamReader` from the response body. The `ReadableStream` allows us to process incoming data in chunks as they are received, rather than having to wait for the entire response to be downloaded.\n",
    "- **const { value, done } = await reader.read()**: This is the heart of the stream processing. It asynchronously reads the next chunk of data from the stream. The return value is an object that contains two properties:\n",
    "- **value**: This is a `Uint8Array` (binary data) containing the latest chunk of data.\n",
    "- **done**: A boolean that signals if the stream has completed. If done is true, it indicates that there is no more data to process, and the loop should terminate.\n",
    "- **const text = decoder.decode(value)**: The binary chunk (value) is converted to a string using `TextDecoder`, making it usable for display or further processing.\n",
    "- **container.innerText += text**: The decoded string is appended to an HTML element with the ID response-container. This provides visible feedback on the incoming data.\n",
    "\n",
    "#### Comparison to REST API calls\n",
    "\n",
    "In contrast to this streaming approach, a typical REST API call looks like this:\n",
    "\n",
    "```javascript\n",
    "const response = await fetch('/api/data');  // Make the REST API request\n",
    "const data = await response.json(); // Parse the JSON response body\n",
    "```\n",
    "\n",
    "The difference is stark. In a REST API call, we wait for the entire response before processing, while with `StreamingResponse` (and the client-side code), we handle each chunk of data as it becomes available. This allows us to display output to the user almost instantly rather than after a long wait."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspecting Streaming Responses: A Visual Walkthrough\n",
    "\n",
    "To truly understand what's happening under the hood with `StreamingResponse`, let's take a look at the actual HTTP requests and responses using network inspection tools. We'll use Chrome DevTools and Wireshark to examine the data being transmitted between the client and the server.\n",
    "\n",
    "#### 1. Chrome DevTools: Examining HTTP Headers\n",
    "\n",
    "![](images/2025-01-19-streaming-responses-fastapi/stream/stream-html-chrome-dev-tools.PNG){.lightbox}\n",
    "\n",
    "The first screenshot is from Chrome's DevTools, specifically the \"Network\" tab. We can see the request made to the `/stream` endpoint. The key thing to notice in the response headers is the `Transfer-Encoding: chunked` header, highlighted in yellow. This confirms that the server is sending a chunked response, which is essential for streaming. The `Content-Type` is also set to `text/plain`, as specified in our server-side code.\n",
    "\n",
    "#### 2. Wireshark: Diving into the Data Packets\n",
    "\n",
    "![](images/2025-01-19-streaming-responses-fastapi/stream/wireshark-chunks.PNG){.lightbox}\n",
    "\n",
    "The second screenshot comes from Wireshark, a powerful network protocol analyzer. This tool allows us to inspect the raw packets being transmitted over the network. Here, we can see that multiple HTTP \"chunked\" responses are being sent from the server to the client as part of a single HTTP connection. This provides a visual confirmation that the server is indeed streaming data in chunks.\n",
    "\n",
    "#### 3. Analyzing Individual Data Chunks\n",
    "\n",
    "**Sixth Data Chunk:**\n",
    "\n",
    "![](images/2025-01-19-streaming-responses-fastapi/stream/wireshark-chunks-01.PNG){.lightbox}\n",
    "\n",
    "\n",
    "We've opened the sixth data chunk in Wireshark, where we see the chunk has a size of 35 octets. This chunk corresponds to the text \" through FastAPI StreamingResponse.\".\n",
    "\n",
    "**Seventh Data Chunk:**\n",
    "\n",
    "![](images/2025-01-19-streaming-responses-fastapi/stream/wireshark-chunks-02.PNG){.lightbox}\n",
    "\n",
    "Here we have the seventh data chunk, indicated by a size of 17 octets. This maps to the message \" Here's the final\".\n",
    "\n",
    "**Eighth Data Chunk:**\n",
    "\n",
    "![](images/2025-01-19-streaming-responses-fastapi/stream/wireshark-chunks-03.PNG){.lightbox}\n",
    "\n",
    "In this screenshot we opened the eighth data chunk. Here the chunk size is 7 octets and it corresponds to the message \" chunk!\".\n",
    "\n",
    "**Last Data Chunk:**\n",
    "\n",
    "![](images/2025-01-19-streaming-responses-fastapi/stream/wireshark-chunks-04.PNG){.lightbox}\n",
    "\n",
    "Finally, we can see the last chunk being sent. It has a size of 0 octets. This zero-size chunk tells the client that the server has finished sending the stream, and it can close the connection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Method 2: Streaming with Server-Sent Events (SSE)\n",
    "\n",
    "Server-Sent Events (SSE) are another powerful mechanism for pushing data from the server to the client in a stream. While similar in purpose to StreamingResponse, SSE operates at a higher level, utilizing a structured, event-based approach. This method is particularly well-suited for scenarios where the server needs to continuously send updates to the client, such as live notifications or updates to a real-time dashboard.\n",
    "\n",
    "#### Key Features of SSE\n",
    "- **Unidirectional**: SSE is a one-way communication channel. The server pushes data to the client, but the client cannot send data back to the server over the same connection. (For bidirectional communication, we would need to use WebSockets, which we'll discuss later).\n",
    "- **Text-Based**: SSE is a text-based protocol. Data is formatted as simple text events, which are easy to parse on the client side.\n",
    "- **Automatic Reconnection**: If the connection between the server and client is interrupted, the client will automatically try to reconnect to the server after a short delay. This is a key benefit of using SSE because it makes sure your connection is always live.\n",
    "\n",
    "### Server-Side Implementation (FastAPI)\n",
    "\n",
    "Here's how we can implement SSE with FastAPI, using our existing `event_generator`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-fold: true\n",
    "#| code-summary: \"Show the code\"\n",
    "#| filename: \"app_ess.py\"\n",
    "\n",
    "from fastapi import FastAPI\n",
    "from sse_starlette.sse import EventSourceResponse\n",
    "import asyncio\n",
    "\n",
    "# Test messages (same as before)\n",
    "MESSAGES = [\n",
    "    \"This is\",\n",
    "    \" a large\",\n",
    "    \" response\",\n",
    "    \" being\",\n",
    "    \" streamed\",\n",
    "    \" through FastAPI SSE.\",\n",
    "    \" Here's the final\",\n",
    "    \" chunk!\",\n",
    "]\n",
    "\n",
    "async def event_generator():\n",
    "    for message in MESSAGES:\n",
    "        yield {\"data\": message}\n",
    "        await asyncio.sleep(1)  # Simulate an async delay\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "@app.get(\"/stream\")\n",
    "async def stream_response():\n",
    "    return EventSourceResponse(event_generator())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, it's incredibly straightforward. Instead of `StreamingResponse`, we use `EventSourceResponse` from the `sse-starlette` package (which you'll need to install). `EventSourceResponse` takes the asynchronous generator (which produces a dictionary) and formats it as SSE messages that can be consumed by the browser. Note that in order to send raw text we have to format the yield as `{\"data\": \"your_text\"}`\n",
    "\n",
    "### Client-Side Implementation (JavaScript)\n",
    "\n",
    "On the client side, connecting to an SSE stream is also remarkably simple:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "javascript"
    }
   },
   "outputs": [],
   "source": [
    "#| code-fold: true\n",
    "#| code-summary: \"Show the code\"\n",
    "#| filename: \"ess.html\"\n",
    "\n",
    "const eventSource = new EventSource('/stream');\n",
    "\n",
    "eventSource.onmessage = function (event) {\n",
    "  const container = document.getElementById('response-container');\n",
    "  container.innerText += event.data;\n",
    "};"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explanation of the Client-Side Code\n",
    "\n",
    "- **const eventSource = new EventSource('/stream')**: This line establishes the connection to the SSE endpoint. When the connection is successful, the client begins to receive server-sent events.\n",
    "- **eventSource.onmessage = function (event) { ... }**: This defines a callback function that is executed when a message is received from the server.\n",
    "- The event object contains the data that the server sent in event.data .\n",
    "- We simply append the event.data to an HTML element with the ID response-container.\n",
    "\n",
    "#### Comparison with StreamingResponse\n",
    "- **Structure**: `StreamingResponse` provides raw data that the client must interpret. SSE provides a structured format with an event-driven approach through which you can have multiple event types.\n",
    "- **Protocol Overhead**: SSE has slightly more protocol overhead than StreamingResponse due to the additional text-based formatting and event structure.\n",
    "- **Client-Side Ease**: The client-side API for SSE is cleaner since the event handling is baked in (EventSource API).\n",
    "- **Use Cases**: `StreamingResponse` might be better if you are trying to stream very large files because it has less overhead. SSE shines in cases where the messages are structured, which allows clients to act on different event types."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diving Deeper: SSE Specification and Message Fields\n",
    "\n",
    "To gain an even more detailed understanding of Server-Sent Events, let's refer to the official documentation. The [Mozilla Developer Network (MDN) page on Using server-sent events](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events) provides a comprehensive overview of the SSE specification and its various features.\n",
    "\n",
    "In particular, the MDN documentation highlights that SSE messages can include various fields, not just the data field we used in our initial example. These fields allow for richer event structures, enabling the client to handle different kinds of events or provide additional context. Here's a breakdown of the key fields as defined by the SSE specification:\n",
    "\n",
    "- **data**: This is the core field containing the event data. It can be any text that is sent from the server. Each message can have multiple data fields, and they will be concatenated together.\n",
    "- **event**: The event field can be used to specify the event type. This allows the client to use a specific callback handler to react to different events originating from the same SSE stream. If no event type is specified, the client uses the default onmessage callback.\n",
    "- **id**: The id field sets the event ID. This is crucial for client-side error handling and reconnection. When a connection is interrupted and the client reconnects, the client will include the last seen event ID in the Last-Event-ID header. The server can use this information to determine where to resume the stream.\n",
    "- **retry**: The retry field specifies the amount of time (in milliseconds) that the client should wait before trying to reconnect if the connection is lost. This allows the server to control the client-side reconnection behavior.\n",
    "\n",
    "These additional fields allow you to build highly structured and robust applications. The server can now send messages such as:\n",
    "\n",
    "```yaml\n",
    "id: 12345\n",
    "event: user-logged-in\n",
    "retry: 10000\n",
    "data: John Doe\n",
    "data: User ID : 123\n",
    "\n",
    "id: 12346\n",
    "event: message\n",
    "data: hello world\n",
    "```\n",
    "\n",
    "In the above message stream we are using `id`, `event`, `retry` and multiple `data` fields. You can also format the message with data in one line, like the last message example.\n",
    "\n",
    "By understanding these additional message fields, you can design more sophisticated real-time applications leveraging the flexibility of Server-Sent Events."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Visual Network Inspection\n",
    "To fully grasp how Server-Sent Events operate, let's examine the HTTP requests and responses using network inspection tools. Just as with `StreamingResponse`, visualizing the data flow helps clarify what’s happening behind the scenes. We'll again use Chrome DevTools and Wireshark to see the structure and details of the transmitted data.\n",
    "\n",
    "#### 1. Chrome DevTools: Examining HTTP Headers\n",
    "\n",
    "![](images/2025-01-19-streaming-responses-fastapi/ess/headers.PNG){.lightbox}\n",
    "\n",
    "The first screenshot shows the \"Network\" tab in Chrome DevTools for our SSE endpoint. Here, you can see the `Content-Type` header is set to `text/event-stream`, which is the critical header indicating that we are dealing with an SSE stream. This is different from the text/plain we saw when using the StreamingResponse. The `Transfer-Encoding` is also set to chunked which signals that the content is being sent in parts.\n",
    "\n",
    "#### 2. Chrome DevTools: EventStream Tab\n",
    "\n",
    "![](images/2025-01-19-streaming-responses-fastapi/ess/event-stream.PNG){.lightbox}\n",
    "\n",
    "One of the nice things about Chrome is that when it receives `text/event-stream` it provides a separate \"EventStream\" tab to display the events in a more readable format. This view helps in tracing the exact sequence of events sent from the server. Each event includes a timestamp and message data, making debugging straightforward. Here we can also see that each of the messages has a message type of message.\n",
    "\n",
    "#### 3. Chrome DevTools: Response Tab\n",
    "\n",
    "![](images/2025-01-19-streaming-responses-fastapi/ess/response.PNG){.lightbox}\n",
    "\n",
    "In this view we can inspect the raw HTTP response that was received from the server. In this view each of the messages that we received are in a separate line. Each of these lines have the prefix of `data`.\n",
    "\n",
    "#### 4. Wireshark: Diving into the Data Packets\n",
    "\n",
    "![](images/2025-01-19-streaming-responses-fastapi/ess/wireshark.PNG){.lightbox}\n",
    "\n",
    "This Wireshark screenshot shows the raw packets for the SSE stream. We see the familiar chunked transfer encoding. Notice that, like `StreamingResponse`, SSE also sends data in chunks. The final chunk of size 0 signals the end of the stream, and all the messages from the server have been received."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312quart",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
