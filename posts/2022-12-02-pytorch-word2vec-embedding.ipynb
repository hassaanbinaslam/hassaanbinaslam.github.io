{"cells":[{"cell_type":"raw","metadata":{"id":"kdcZSuoctukO"},"source":["---\n","title: Implementing Word2Vec with PyTorch\n","description: This is a practice notebook to implement word2vec in PyTorch. \n","image: images/2022-12-02-pytorch-word2vec-embedding.png\n","date: '2022-12-02'\n","toc: true\n","badges: true\n","output-file: 2022-12-02-pytorch-word2vec-embedding.html\n","\n","categories:\n","- pytorch\n","- nlp\n","\n","keyword:\n","- ml\n","- dl\n","- nn\n","- pytorch\n","- text\n","- nlp\n","- word2vec\n","- embedding\n","---"]},{"cell_type":"markdown","metadata":{"id":"u2O8j6ezu0x9"},"source":["![](images/2022-12-02-pytorch-word2vec-embedding.png)"]},{"cell_type":"markdown","metadata":{"id":"xsza7iAIu0x_"},"source":["## Credits\n","This notebook takes inspiration and ideas from the following sources.\n","\n","* An excellent word2vec introduction from \"Jay Alammar\": [illustrated-word2vec](https://jalammar.github.io/illustrated-word2vec/). \n","* Blog post by \"Musashi (sometimes Jacobs-) Harukawa\" with the same title. You can find the original post here: [word2vec-from-scratch](https://muhark.github.io/python/ml/nlp/2021/10/21/word2vec-from-scratch.html). Parts of the code you see in this notebook are taken from this post.\n","* Another very detailed and well-explained blog post by \"Olga Chernytska\". You can find the original post here: [word2vec-with-pytorch-implementing-original-paper](https://towardsdatascience.com/word2vec-with-pytorch-implementing-original-paper-2cd7040120b0). Parts of the code you see in this notebook are taken from this post."]},{"cell_type":"markdown","metadata":{"id":"wtohdezgu0x_"},"source":["## Environment\n","This notebook [GitHub link here](https://github.com/hassaanbinaslam/myblog/blob/main/posts/2022-12-02-pytorch-word2vec-embedding.ipynb) is prepared with Google Colab. [Hugging Face Datasets](https://huggingface.co/docs/datasets/index) library is required for this post."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4182,"status":"ok","timestamp":1670327669270,"user":{"displayName":"Hassaan Bin Aslam","userId":"13411116435427293553"},"user_tz":-300},"id":"iXlDTdvi6nlU","outputId":"4bef8920-d475-4392-c626-eff7bec46cd2"},"outputs":[],"source":["# install hugging face datasets\n","! pip install datasets"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1902,"status":"ok","timestamp":1670327671155,"user":{"displayName":"Hassaan Bin Aslam","userId":"13411116435427293553"},"user_tz":-300},"id":"E9kZUBv9DobO","outputId":"88ceae6c-0eb7-4b3b-b23e-eddf05a0cfe0"},"outputs":[{"name":"stdout","output_type":"stream","text":["python==3.8.15\n","numpy==1.21.6\n","torch==1.12.1+cu113\n","matplotlib==3.2.2\n","datasets==2.7.1\n"]}],"source":["#| code-fold: true\n","from platform import python_version\n","import numpy, matplotlib, pandas, torch, datasets\n","\n","print(\"python==\" + python_version())\n","print(\"numpy==\" + numpy.__version__)\n","print(\"torch==\" + torch.__version__)\n","print(\"matplotlib==\" + matplotlib.__version__)\n","print(\"datasets==\" + datasets.__version__)"]},{"cell_type":"markdown","metadata":{"id":"MBWOSLktKMoD"},"source":["## Introduction\n","> Following are the `word2vec` and `word embedding` explanations provided by [OpenAI ChatGPT](https://chat.openai.com/chat). If unsatisfied, I suggest using the links provided in the \"Credits\" section ([illustrated-word2vec](https://jalammar.github.io/illustrated-word2vec/) from Jay Alammar). \n","\n","`prompt > word2vec blog introduction`\n","\n","**Word2vec**\n","\n","Word2vec is a widely-used natural language processing (NLP) algorithm that uses deep learning to learn the relationships between words in a corpus (a large collection of text data). A team of researchers developed it at Google, led by Tomas Mikolov in 2013.\n","\n","Word2vec uses a neural network to learn the relationships between words by training on a large corpus of text data. The algorithm uses a shallow neural network with a single hidden layer to predict the surrounding words of a given word based on its context. The network's hidden layer is trained to capture the relationships between words, and the resulting vectors (or \"word embeddings\") can be used to represent the words in the corpus in a lower-dimensional space.\n","\n","The benefits of using word2vec include the ability to capture the relationships between words in a corpus and the ability to handle large amounts of data efficiently. This makes it a useful tool for many NLP tasks, such as text classification, language translation and named entity recognition.\n","\n","Overall, word2vec is a powerful and effective algorithm for learning the relationships between words in a corpus and has become a cornerstone of modern NLP techniques.\n","\n","`prompt > word embedding short description`\n","\n","**Word Embedding**\n","\n","A word embedding represents a word in a low-dimensional, continuous vector space. This representation captures the semantic relationships between words in a corpus, allowing for more accurate and efficient natural language processing. Word embeddings are typically learned using neural network models, such as the word2vec algorithm. These learned embeddings can then be used in various NLP tasks, such as text classification and language translation.\n","\n","**Summary of the steps followed in this notebook**\n","\n","* Download hugging face [tweets_hate_speech_detection](https://huggingface.co/datasets/tweets_hate_speech_detection) dataset\n","* Preprocess the data to remove special characters and normalize the text\n","* Create tokens from the text and their vocabulary, and finally encode the tokens\n","* Create training dataset, model configuration, loss, and optimizer\n","* Train the model and extract the embedding layer weights\n","* Analyse the word embeddings learned from the tweets"]},{"cell_type":"markdown","metadata":{"id":"oWPcusVG6oC8"},"source":["## Data Preparation"]},{"cell_type":"markdown","metadata":{"id":"UvqRfcfd6qBM"},"source":["### Load Hugging Face Dataset\n","For this notebook, I will use Hugging Face [Twitter Hate Speech Dataset](https://huggingface.co/datasets/tweets_hate_speech_detection). It contains training data with approximately 32K tweets divided into two groups: `hate speech` and `not a hate speech`. This dataset is originally for classification tasks, but we may use it to learn word embeddings (or word contexts). This approach can also be used to identify inherent biases present in the data."]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":86,"referenced_widgets":["fd43128fc4d74e10a6d4d64df3d3195a","427f8f7d1a374f70915a8c10ae7ecc14","4d4a8f416b2e405fbdc80d0fe6cce353","97ed0d18e7a14227872981fc581f550f","d00236fd7d5b4a0dac70c4389d20e61d","0e1dbf44aaa447e99f1c449bb5294e33","3e433938116347a59ce144ef862af928","0eddd7378b7047e3ad7a1216ba184918","e562176c3e0341a3a1bb881645c59029","a24de2b54c604b0784c52177cfbd6fd0","0700ca7be4e24998a2a3205b7b9c7bf0"]},"executionInfo":{"elapsed":2234,"status":"ok","timestamp":1670327673382,"user":{"displayName":"Hassaan Bin Aslam","userId":"13411116435427293553"},"user_tz":-300},"id":"Ib_HGLXwuxIa","outputId":"16456549-3281-4124-aea6-451e98eeaf01"},"outputs":[{"name":"stderr","output_type":"stream","text":["WARNING:datasets.builder:Found cached dataset tweets_hate_speech_detection (/root/.cache/huggingface/datasets/tweets_hate_speech_detection/default/0.0.0/c6b6f41e91ac9113e1c032c5ecf7a49b4e1e9dc8699ded3c2d8425c9217568b2)\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"fd43128fc4d74e10a6d4d64df3d3195a","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"}],"source":["import datasets\n","\n","dataset = datasets.load_dataset(\"tweets_hate_speech_detection\")"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":34,"status":"ok","timestamp":1670327673383,"user":{"displayName":"Hassaan Bin Aslam","userId":"13411116435427293553"},"user_tz":-300},"id":"vXvE5Q6NCs0p","outputId":"fb6fd427-9d13-4044-c9e0-a381d6d5eb21"},"outputs":[{"data":{"text/plain":["DatasetDict({\n","    train: Dataset({\n","        features: ['label', 'tweet'],\n","        num_rows: 31962\n","    })\n","})"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["# Let's check the downloaded object\n","dataset"]},{"cell_type":"markdown","metadata":{"id":"0b4M1bfCUj6p"},"source":["It shows that we have only a training set, and each element from the set has a `label` and `tweet` text."]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":32,"status":"ok","timestamp":1670327673385,"user":{"displayName":"Hassaan Bin Aslam","userId":"13411116435427293553"},"user_tz":-300},"id":"HBYHKZpgCn04","outputId":"5e54a2b7-279a-409d-d906-66e551d1c071"},"outputs":[{"data":{"text/plain":["{'label': ClassLabel(names=['no-hate-speech', 'hate-speech'], id=None),\n"," 'tweet': Value(dtype='string', id=None)}"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["train_ds = dataset[\"train\"]\n","train_ds.features"]},{"cell_type":"markdown","metadata":{"id":"JP-sNnO-V1JM"},"source":["Let's check what the raw data looks like."]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":30,"status":"ok","timestamp":1670327673387,"user":{"displayName":"Hassaan Bin Aslam","userId":"13411116435427293553"},"user_tz":-300},"id":"GAIFMdK_Vf9N","outputId":"1a57b624-37ba-4873-ba11-745ce4576d7c"},"outputs":[{"name":"stdout","output_type":"stream","text":["** labels **\n"," [0, 0, 0, 0, 0]\n","\n","** tweets **\n","@user when a father is dysfunctional and is so selfish he drags his kids into his dysfunction.   #run\n","@user @user thanks for #lyft credit i can't use cause they don't offer wheelchair vans in pdx.    #disapointed #getthanked\n","bihday your majesty\n","#model   i love u take with u all the time in urð±!!! ððððð¦ð¦ð¦  \n","factsguide: society now    #motivation\n"]}],"source":["# print a few labels\n","print(\"** labels **\\n\", train_ds[\"label\"][:5])\n","\n","# print a few tweet texts\n","print(\"\\n** tweets **\")\n","for t in train_ds[\"tweet\"][:5]:\n","    print(t)"]},{"cell_type":"markdown","metadata":{"id":"k0en-nnjVOO7"},"source":["This raw data shows that the labels are encoded as `0` and `1` for `no-hate` and `hate` speech, respectively. Therefore, we can improve our view by putting labels and tweets in Pandas's DataFrame and analyzing them side by side."]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":580},"executionInfo":{"elapsed":30,"status":"ok","timestamp":1670327673391,"user":{"displayName":"Hassaan Bin Aslam","userId":"13411116435427293553"},"user_tz":-300},"id":"SQSB3Fx-DhMi","outputId":"2424417e-661f-42e1-edfd-767bf7110512"},"outputs":[{"data":{"text/html":["\n","  <div id=\"df-61dd778d-af34-4e0c-affc-b54453c4d2c8\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>label</th>\n","      <th>tweet</th>\n","      <th>label_name</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>@user when a father is dysfunctional and is so selfish he drags his kids into his dysfunction.   #run</td>\n","      <td>no-hate-speech</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0</td>\n","      <td>@user @user thanks for #lyft credit i can't use cause they don't offer wheelchair vans in pdx.    #disapointed #getthanked</td>\n","      <td>no-hate-speech</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0</td>\n","      <td>bihday your majesty</td>\n","      <td>no-hate-speech</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0</td>\n","      <td>#model   i love u take with u all the time in urð±!!! ððððð¦ð¦ð¦</td>\n","      <td>no-hate-speech</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0</td>\n","      <td>factsguide: society now    #motivation</td>\n","      <td>no-hate-speech</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>0</td>\n","      <td>[2/2] huge fan fare and big talking before they leave. chaos and pay disputes when they get there. #allshowandnogo</td>\n","      <td>no-hate-speech</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>0</td>\n","      <td>@user camping tomorrow @user @user @user @user @user @user @user dannyâ¦</td>\n","      <td>no-hate-speech</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>0</td>\n","      <td>the next school year is the year for exams.ð¯ can't think about that ð­ #school #exams   #hate #imagine #actorslife #revolutionschool #girl</td>\n","      <td>no-hate-speech</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>0</td>\n","      <td>we won!!! love the land!!! #allin #cavs #champions #cleveland #clevelandcavaliers  â¦</td>\n","      <td>no-hate-speech</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>0</td>\n","      <td>@user @user welcome here !  i'm   it's so #gr8 !</td>\n","      <td>no-hate-speech</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-61dd778d-af34-4e0c-affc-b54453c4d2c8')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-61dd778d-af34-4e0c-affc-b54453c4d2c8 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-61dd778d-af34-4e0c-affc-b54453c4d2c8');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "],"text/plain":["   label  \\\n","0      0   \n","1      0   \n","2      0   \n","3      0   \n","4      0   \n","5      0   \n","6      0   \n","7      0   \n","8      0   \n","9      0   \n","\n","                                                                                                                                             tweet  \\\n","0                                            @user when a father is dysfunctional and is so selfish he drags his kids into his dysfunction.   #run   \n","1                       @user @user thanks for #lyft credit i can't use cause they don't offer wheelchair vans in pdx.    #disapointed #getthanked   \n","2                                                                                                                              bihday your majesty   \n","3                                                           #model   i love u take with u all the time in urð±!!! ðððð\n","ð¦ð¦ð¦     \n","4                                                                                                           factsguide: society now    #motivation   \n","5                             [2/2] huge fan fare and big talking before they leave. chaos and pay disputes when they get there. #allshowandnogo     \n","6                                                                        @user camping tomorrow @user @user @user @user @user @user @user dannyâ¦   \n","7  the next school year is the year for exams.ð¯ can't think about that ð­ #school #exams   #hate #imagine #actorslife #revolutionschool #girl   \n","8                                                          we won!!! love the land!!! #allin #cavs #champions #cleveland #clevelandcavaliers  â¦    \n","9                                                                                                @user @user welcome here !  i'm   it's so #gr8 !    \n","\n","       label_name  \n","0  no-hate-speech  \n","1  no-hate-speech  \n","2  no-hate-speech  \n","3  no-hate-speech  \n","4  no-hate-speech  \n","5  no-hate-speech  \n","6  no-hate-speech  \n","7  no-hate-speech  \n","8  no-hate-speech  \n","9  no-hate-speech  "]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["import pandas as pd\n","\n","pd.set_option(\"display.max_colwidth\", None)\n","\n","train_ds.set_format(type=\"pandas\")\n","df = train_ds[:]\n","\n","# a function to convert label codes to string value\n","def label_int2str(row):\n","    return train_ds.features[\"label\"].int2str(row)\n","\n","\n","df[\"label_name\"] = df[\"label\"].apply(label_int2str)\n","df.head(10)"]},{"cell_type":"markdown","metadata":{"id":"66crb_zjXruH"},"source":["All these tweets are labeled as `no-hate-speech`. Let's view some of the tweets from the other class."]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":442},"executionInfo":{"elapsed":30,"status":"ok","timestamp":1670327673393,"user":{"displayName":"Hassaan Bin Aslam","userId":"13411116435427293553"},"user_tz":-300},"id":"EST7LbZbX6MQ","outputId":"088ea4cc-f69c-4f7a-c898-d26f56c03500"},"outputs":[{"data":{"text/html":["\n","  <div id=\"df-1fdf6e0d-078c-45e1-914b-bc6ffdc075f3\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>label</th>\n","      <th>tweet</th>\n","      <th>label_name</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>13</th>\n","      <td>1</td>\n","      <td>@user #cnn calls #michigan middle school 'build the wall' chant '' #tcot</td>\n","      <td>hate-speech</td>\n","    </tr>\n","    <tr>\n","      <th>14</th>\n","      <td>1</td>\n","      <td>no comment!  in #australia   #opkillingbay #seashepherd #helpcovedolphins #thecove  #helpcovedolphins</td>\n","      <td>hate-speech</td>\n","    </tr>\n","    <tr>\n","      <th>17</th>\n","      <td>1</td>\n","      <td>retweet if you agree!</td>\n","      <td>hate-speech</td>\n","    </tr>\n","    <tr>\n","      <th>23</th>\n","      <td>1</td>\n","      <td>@user @user lumpy says i am a . prove it lumpy.</td>\n","      <td>hate-speech</td>\n","    </tr>\n","    <tr>\n","      <th>34</th>\n","      <td>1</td>\n","      <td>it's unbelievable that in the 21st century we'd need something like this. again. #neverump  #xenophobia</td>\n","      <td>hate-speech</td>\n","    </tr>\n","    <tr>\n","      <th>56</th>\n","      <td>1</td>\n","      <td>@user lets fight against  #love #peace</td>\n","      <td>hate-speech</td>\n","    </tr>\n","    <tr>\n","      <th>68</th>\n","      <td>1</td>\n","      <td>ð©the white establishment can't have blk folx running around loving themselves and promoting our greatness</td>\n","      <td>hate-speech</td>\n","    </tr>\n","    <tr>\n","      <th>77</th>\n","      <td>1</td>\n","      <td>@user hey, white people: you can call people 'white' by @user  #race  #identity #medâ¦</td>\n","      <td>hate-speech</td>\n","    </tr>\n","    <tr>\n","      <th>82</th>\n","      <td>1</td>\n","      <td>how the #altright uses  &amp;amp; insecurity to lure men into #whitesupremacy</td>\n","      <td>hate-speech</td>\n","    </tr>\n","    <tr>\n","      <th>111</th>\n","      <td>1</td>\n","      <td>@user i'm not interested in a #linguistics that doesn't address #race &amp;amp; . racism is about #power. #raciolinguistics bringsâ¦</td>\n","      <td>hate-speech</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1fdf6e0d-078c-45e1-914b-bc6ffdc075f3')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-1fdf6e0d-078c-45e1-914b-bc6ffdc075f3 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-1fdf6e0d-078c-45e1-914b-bc6ffdc075f3');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "],"text/plain":["     label  \\\n","13       1   \n","14       1   \n","17       1   \n","23       1   \n","34       1   \n","56       1   \n","68       1   \n","77       1   \n","82       1   \n","111      1   \n","\n","                                                                                                                                 tweet  \\\n","13                                                          @user #cnn calls #michigan middle school 'build the wall' chant '' #tcot     \n","14                               no comment!  in #australia   #opkillingbay #seashepherd #helpcovedolphins #thecove  #helpcovedolphins   \n","17                                                                                                              retweet if you agree!    \n","23                                                                                     @user @user lumpy says i am a . prove it lumpy.   \n","34                            it's unbelievable that in the 21st century we'd need something like this. again. #neverump  #xenophobia    \n","56                                                                                             @user lets fight against  #love #peace    \n","68                      ð©the white establishment can't have blk folx running around loving themselves and promoting our greatness     \n","77                                             @user hey, white people: you can call people 'white' by @user  #race  #identity #medâ¦   \n","82                                                       how the #altright uses  &amp; insecurity to lure men into #whitesupremacy       \n","111  @user i'm not interested in a #linguistics that doesn't address #race &amp; . racism is about #power. #raciolinguistics bringsâ¦   \n","\n","      label_name  \n","13   hate-speech  \n","14   hate-speech  \n","17   hate-speech  \n","23   hate-speech  \n","34   hate-speech  \n","56   hate-speech  \n","68   hate-speech  \n","77   hate-speech  \n","82   hate-speech  \n","111  hate-speech  "]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["df[df[\"label\"]==1].head(10)"]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":30,"status":"ok","timestamp":1670327673394,"user":{"displayName":"Hassaan Bin Aslam","userId":"13411116435427293553"},"user_tz":-300},"id":"WEdxi1ahYUDs"},"outputs":[],"source":["# reset dataset to its original format\n","train_ds.reset_format()"]},{"cell_type":"markdown","metadata":{"id":"FO_RBkaI3jz6"},"source":["### Preprocess data\n","\n","We can not use raw text directly to train a model because machines understand numbers and not alphabets. But to properly encode our text (convert it into numbers), we need to do some extra steps.\n","\n","* tweets also contain special characters from emoticons or emojis. However, they do not help learn word embeddings. So we need to strip them from the text.\n","* split the tweet text into proper words. Even though we can use all the words from the tweet text, experience has shown that not all are useful in learning embeddings. Words that are commonly omitted are either uncommon or rare words, or stopwords (commonly used words)\n","* Create a dictionary or vocabulary to filter words and encode them. This vocabulary helps move between encoded (integer) and character (or string) representations of words.\n","\n","For some of these preprocessing tasks, we will use [Natural Language Toolkit (NLTK library)](https://www.nltk.org/).\n","\n","> NLTK is a leading platform for building Python programs to work with human language data. It provides easy-to-use interfaces to over 50 corpora and lexical resources such as WordNet, along with a suite of text processing libraries for classification, tokenization, stemming, tagging, parsing, and semantic reasoning, wrappers for industrial-strength NLP libraries."]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":914,"status":"ok","timestamp":1670327674278,"user":{"displayName":"Hassaan Bin Aslam","userId":"13411116435427293553"},"user_tz":-300},"id":"4NwL2Ytz3VTA","outputId":"b93b19a3-9300-49fe-f5d1-51df4f0d8882"},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n","[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n","[nltk_data]   Package omw-1.4 is already up-to-date!\n"]},{"data":{"text/plain":["True"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["import nltk\n","\n","nltk.download(\"stopwords\")  # for filtering common words\n","nltk.download(\"wordnet\")  # for lemmatization of words\n","nltk.download(\"omw-1.4\")  # required for 'wordnet'"]},{"cell_type":"markdown","metadata":{"id":"iZmrzVRiF2nf"},"source":["I have created a helper function in the next cell to preprocess and split the tweet text into tokens."]},{"cell_type":"code","execution_count":11,"metadata":{"executionInfo":{"elapsed":38,"status":"ok","timestamp":1670327674282,"user":{"displayName":"Hassaan Bin Aslam","userId":"13411116435427293553"},"user_tz":-300},"id":"Uz93BglEDZKa"},"outputs":[],"source":["from nltk.corpus import stopwords\n","from nltk.stem import WordNetLemmatizer\n","\n","import re\n","\n","sw = stopwords.words(\"english\")\n","wl = WordNetLemmatizer()\n","\n","\n","def split_tokens(row):\n","    # step 1: lower the text\n","    t = row[\"tweet\"].lower()\n","    # step 2: remove all other characters except alphabets, numbers, and a space\n","    t = re.sub(r\"[^a-z 0-9]\", \"\", t)\n","    # step 3: split the text into words or tokens. split is made at each \"space\" character\n","    t = re.split(r\" +\", t)\n","    # step 4: remove stop words\n","    t = [i for i in t if (i not in sw) and (i not in [\"user\"]) and len(i)]\n","    # step 5: lemmatize words\n","    t = [wl.lemmatize(i) for i in t]\n","\n","    row[\"all_tokens\"] = t\n","    return row"]},{"cell_type":"markdown","metadata":{"id":"SoSycK8SGB3f"},"source":["Let's use a sample tweet text to uncover the working of this function."]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"elapsed":38,"status":"ok","timestamp":1670327674283,"user":{"displayName":"Hassaan Bin Aslam","userId":"13411116435427293553"},"user_tz":-300},"id":"UKjUZwZJGSaQ","outputId":"1a68394b-7cf6-479f-9730-0bd16f8a4629"},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'@user when a father is dysfunctional and is so selfish he drags his kids into his dysfunction.   #run'"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["sample_tweet = train_ds[0][\"tweet\"]\n","sample_tweet"]},{"cell_type":"markdown","metadata":{"id":"spnAM4zxHN-s"},"source":["**Step 1: lower the text**"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"elapsed":41,"status":"ok","timestamp":1670327674288,"user":{"displayName":"Hassaan Bin Aslam","userId":"13411116435427293553"},"user_tz":-300},"id":"UitTHzPAGYCm","outputId":"763b9635-593e-4cae-8dbc-9e608f00a843"},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'@user when a father is dysfunctional and is so selfish he drags his kids into his dysfunction.   #run'"]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["t = sample_tweet.lower()\n","t"]},{"cell_type":"markdown","metadata":{"id":"SDMJm9k8HSyB"},"source":["**Step 2: remove all other characters except alphabets, numbers, and spaces**"]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"elapsed":41,"status":"ok","timestamp":1670327674289,"user":{"displayName":"Hassaan Bin Aslam","userId":"13411116435427293553"},"user_tz":-300},"id":"HlGiszAJGqC-","outputId":"b4d25011-3464-4e4d-80d3-2af1befcb4da"},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'user when a father is dysfunctional and is so selfish he drags his kids into his dysfunction   run'"]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":["t = re.sub(r\"[^a-z 0-9]\", \"\",t)\n","t"]},{"cell_type":"markdown","metadata":{"id":"qqTyaf7IHWv3"},"source":["**Step 3: split the text into words or tokens. split is made at each \"space\" character**"]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":40,"status":"ok","timestamp":1670327674290,"user":{"displayName":"Hassaan Bin Aslam","userId":"13411116435427293553"},"user_tz":-300},"id":"9HzzQNcmGwfi","outputId":"681c3f73-3e91-45d3-e7b4-ceeb6e191a73"},"outputs":[{"data":{"text/plain":["['user',\n"," 'when',\n"," 'a',\n"," 'father',\n"," 'is',\n"," 'dysfunctional',\n"," 'and',\n"," 'is',\n"," 'so',\n"," 'selfish',\n"," 'he',\n"," 'drags',\n"," 'his',\n"," 'kids',\n"," 'into',\n"," 'his',\n"," 'dysfunction',\n"," 'run']"]},"execution_count":15,"metadata":{},"output_type":"execute_result"}],"source":["t = re.split(r\" +\", t)\n","t"]},{"cell_type":"markdown","metadata":{"id":"8GZGzMPVHaNb"},"source":["**Step 4: remove stop words**\n","Besides stop words, I have also filtered \"user\" from the text. This is because in the original tweet text, any reference to a Twitter user (e.g., @hassaanbinaslam) is replaced with `@user` to hide identity."]},{"cell_type":"code","execution_count":16,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":36,"status":"ok","timestamp":1670327674291,"user":{"displayName":"Hassaan Bin Aslam","userId":"13411116435427293553"},"user_tz":-300},"id":"ob5X9D1oG3fP","outputId":"4f91ba1f-d70b-4360-b699-dd3ba027f787"},"outputs":[{"data":{"text/plain":["['father', 'dysfunctional', 'selfish', 'drags', 'kids', 'dysfunction', 'run']"]},"execution_count":16,"metadata":{},"output_type":"execute_result"}],"source":["t = [i for i in t if (i not in sw) and (i not in [\"user\"]) and len(i)]\n","t"]},{"cell_type":"markdown","metadata":{"id":"DYyXOKrQIMP_"},"source":["English language stop words taken from the NLTK library include the following list."]},{"cell_type":"code","execution_count":17,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":33,"status":"ok","timestamp":1670327674293,"user":{"displayName":"Hassaan Bin Aslam","userId":"13411116435427293553"},"user_tz":-300},"id":"4ZXH6I6OIXQg","outputId":"02e41b35-c74f-484e-83e8-a6f3f7270eca"},"outputs":[{"name":"stdout","output_type":"stream","text":["['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"]}],"source":["# englist language stopwords from NLTK\n","print(sw)"]},{"cell_type":"markdown","metadata":{"id":"zyI-dgmCHd5h"},"source":["**Step 5: lemmatize words**\n","*What is lemmatization anyway?*\n","Lemmatization is applied to normalize the text. It considers the context and converts the word to its meaningful base form, which is called Lemma. to read more about it, refer to [what-is-the-difference-between-lemmatization-vs-stemming](https://stackoverflow.com/questions/1787110/what-is-the-difference-between-lemmatization-vs-stemming)"]},{"cell_type":"code","execution_count":18,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1659,"status":"ok","timestamp":1670327675924,"user":{"displayName":"Hassaan Bin Aslam","userId":"13411116435427293553"},"user_tz":-300},"id":"BnBUED9XHDlH","outputId":"ace851b5-5a4b-4721-f2da-f42d8d71b355"},"outputs":[{"data":{"text/plain":["['father', 'dysfunctional', 'selfish', 'drag', 'kid', 'dysfunction', 'run']"]},"execution_count":18,"metadata":{},"output_type":"execute_result"}],"source":["t = [wl.lemmatize(i) for i in t]\n","t"]},{"cell_type":"markdown","metadata":{"id":"Ws0eEXiDKEVG"},"source":["Let's use the helper function `split_tokens` and compare our output."]},{"cell_type":"code","execution_count":19,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":56,"status":"ok","timestamp":1670327675924,"user":{"displayName":"Hassaan Bin Aslam","userId":"13411116435427293553"},"user_tz":-300},"id":"GYOnWY7rHJ0t","outputId":"25c4e1d8-1fe8-4930-9491-f6655483461c"},"outputs":[{"data":{"text/plain":["{'label': 0,\n"," 'tweet': '@user when a father is dysfunctional and is so selfish he drags his kids into his dysfunction.   #run',\n"," 'all_tokens': ['father',\n","  'dysfunctional',\n","  'selfish',\n","  'drag',\n","  'kid',\n","  'dysfunction',\n","  'run']}"]},"execution_count":19,"metadata":{},"output_type":"execute_result"}],"source":["split_tokens(train_ds[0])"]},{"cell_type":"markdown","metadata":{"id":"fnGmcU3MLZ_f"},"source":["Our preprocessing steps and tokenize function is ready. So let's apply it to our entire tweet dataset."]},{"cell_type":"code","execution_count":20,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":50,"status":"ok","timestamp":1670327675925,"user":{"displayName":"Hassaan Bin Aslam","userId":"13411116435427293553"},"user_tz":-300},"id":"nJvh4YpfK46r","outputId":"2aaedbd6-ef73-4299-d773-383a22447258"},"outputs":[{"name":"stderr","output_type":"stream","text":["WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/tweets_hate_speech_detection/default/0.0.0/c6b6f41e91ac9113e1c032c5ecf7a49b4e1e9dc8699ded3c2d8425c9217568b2/cache-4f6808d01b1a8972.arrow\n"]},{"data":{"text/plain":["Dataset({\n","    features: ['label', 'tweet', 'all_tokens'],\n","    num_rows: 31962\n","})"]},"execution_count":20,"metadata":{},"output_type":"execute_result"}],"source":["# tokenize tweet dataset\n","train_ds_all_tokens = train_ds.map(split_tokens)\n","train_ds_all_tokens"]},{"cell_type":"markdown","metadata":{"id":"7uAY4CbHLumy"},"source":["### Create vocabulary\n","A vocabulary is a dictionary where each `key` represents the token (word ) string form. The `value` obtained from the dictionary defines the token's unique integer or encoded form. To create our vocabulary, we will follow the following approach.\n","\n","1. Iterate through all the tweets (entire dataset) and count the frequency of all the tokens.\n","2. Remove the rare or uncommon tokens. This helps in reducing the size of the vocabulary. However, these uncommon tokens also include words created with typos e.g., while tweeting someone incorrectly typed `fahtr` instead of `father`. In our case, we will remove all tokens with a frequency of less than 10.\n","3. After that, we will put our filtered tokens in an ordered dictionary\n","4. Pass the ordered dictionary to Pytorch [Vocab class](https://pytorch.org/text/stable/vocab.html). A vocab object automatically maps tokens to indices."]},{"cell_type":"code","execution_count":21,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":48,"status":"ok","timestamp":1670327675928,"user":{"displayName":"Hassaan Bin Aslam","userId":"13411116435427293553"},"user_tz":-300},"id":"7Y5WTPveLYv9","outputId":"d0a494d4-c7ea-4af6-eccf-e9321638101b"},"outputs":[{"name":"stdout","output_type":"stream","text":["Number of tokens found:  38988\n"]}],"source":["# create a frequency map for all toekns\n","from collections import Counter\n","\n","token_count = Counter()\n","for row_tokens in train_ds_all_tokens[\"all_tokens\"]:\n","    token_count.update(row_tokens)\n","\n","print(\"Number of tokens found: \", len(token_count))"]},{"cell_type":"code","execution_count":22,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":44,"status":"ok","timestamp":1670327675929,"user":{"displayName":"Hassaan Bin Aslam","userId":"13411116435427293553"},"user_tz":-300},"id":"QVWkk8Q8NL6u","outputId":"c4c93c5c-353f-4301-9343-377750dc58fb"},"outputs":[{"name":"stdout","output_type":"stream","text":["Number of tokens after filtering:  3007\n"]}],"source":["# remove uncommon tokens\n","min_token_freq = 10\n","token_count_filtered = {k: v for k, v in token_count.items() if v > min_token_freq}\n","\n","print(\"Number of tokens after filtering: \", len(token_count_filtered))"]},{"cell_type":"markdown","metadata":{"id":"RwDF5mGzXh43"},"source":["Notice that by removing uncommon tokens, we have significantly reduced the size of our vocabulary (almost 13x less). In the next step, we will sort them, convert them to [OrdreredDict](https://realpython.com/python-ordereddict/), and pass them to `torchtext.vocab`."]},{"cell_type":"code","execution_count":23,"metadata":{"executionInfo":{"elapsed":41,"status":"ok","timestamp":1670327675929,"user":{"displayName":"Hassaan Bin Aslam","userId":"13411116435427293553"},"user_tz":-300},"id":"lLgFqEkjNVRR"},"outputs":[],"source":["from torchtext.vocab import vocab\n","from collections import OrderedDict\n","\n","# sort the tokens based on their frequency\n","sorted_by_freq_tuples = sorted(\n","    token_count_filtered.items(), key=lambda x: x[1], reverse=True\n",")\n","# create a dictionary of tokens\n","ordered_dict = OrderedDict(sorted_by_freq_tuples)\n","# convert the dictionary into a vocabulary\n","vb = vocab(ordered_dict)"]},{"cell_type":"markdown","metadata":{"id":"sDybiA2KOjjx"},"source":["Using the following methods, we can use the vocabulary to move between the token's integer and string form.\n","\n","* `vb['love']` to get the id '0' for token 'love'\n","* `vb.get_stoi()['love']` to get id '0' for token 'love'\n","* `vb.lookup_token('love')` to get id '0' for token 'love'\n","* `vb.lookup_tokens(['love', 'happy'])` to get id '0' and '2' for token 'love' and 'happy'\n","* `vb.get_itos()[0]` to get token 'love' from id '0'\n","* `vb.lookup_indices([0, 2])` to get token 'love' and 'happy' from id '0' and '2'"]},{"cell_type":"code","execution_count":24,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":40,"status":"ok","timestamp":1670327675929,"user":{"displayName":"Hassaan Bin Aslam","userId":"13411116435427293553"},"user_tz":-300},"id":"JWQv05Q5bpuE","outputId":"2e083e6d-5d6c-4e39-8e15-27b07a7570ac"},"outputs":[{"data":{"text/plain":["(0, 0, 'love')"]},"execution_count":24,"metadata":{},"output_type":"execute_result"}],"source":["vb[\"love\"], vb.get_stoi()[\"love\"], vb.get_itos()[0]"]},{"cell_type":"code","execution_count":25,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":38,"status":"ok","timestamp":1670327675930,"user":{"displayName":"Hassaan Bin Aslam","userId":"13411116435427293553"},"user_tz":-300},"id":"IAOteZtfNj6c","outputId":"5f7097ef-d395-403e-c9fd-c85b4a0b16b8"},"outputs":[{"name":"stdout","output_type":"stream","text":["0 ---> love\n","love ---> 0\n","\n","1 ---> day\n","day ---> 1\n","\n","2 ---> happy\n","happy ---> 2\n","\n","3 ---> u\n","u ---> 3\n","\n","4 ---> amp\n","amp ---> 4\n","\n"]}],"source":["# let's check a few more tokens\n","for i in range(5):\n","    token = vb.get_itos()[i]\n","    print(f\"{i} ---> {token}\")\n","    print(f\"{token} ---> {vb.get_stoi()[token]}\")\n","    print()"]},{"cell_type":"markdown","metadata":{"id":"-bJQ0zftdGCB"},"source":["Alright, we have our vocabulary ready. Remember that we have filtered our vocabulary to remove uncommon tokens. Let's use this vocabulary to do the same for our tweet text tokens."]},{"cell_type":"code","execution_count":26,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":32,"status":"ok","timestamp":1670327675930,"user":{"displayName":"Hassaan Bin Aslam","userId":"13411116435427293553"},"user_tz":-300},"id":"Uqzgg_puNrkx","outputId":"4a263a03-04bf-4554-db72-1406f3ec4d9e"},"outputs":[{"name":"stderr","output_type":"stream","text":["WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/tweets_hate_speech_detection/default/0.0.0/c6b6f41e91ac9113e1c032c5ecf7a49b4e1e9dc8699ded3c2d8425c9217568b2/cache-84d95f24a0e6d681.arrow\n"]}],"source":["# use vocabulary to filter uncommon tokens from tweets\n","def remove_rare_tokens(row):\n","    row[\"tokens\"] = [t for t in row[\"all_tokens\"] if t in vb]\n","    return row\n","\n","\n","train_ds_tokens = train_ds_all_tokens.map(remove_rare_tokens)"]},{"cell_type":"markdown","metadata":{"id":"UfL3-6tMdw3Y"},"source":["In the following cell output, note that only three tokens are left for a sample tweet after removing uncommon tokens."]},{"cell_type":"code","execution_count":27,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":30,"status":"ok","timestamp":1670327675931,"user":{"displayName":"Hassaan Bin Aslam","userId":"13411116435427293553"},"user_tz":-300},"id":"YoVk-OgvddsH","outputId":"8bcc2aea-2080-4533-b30d-f136e8c1495f"},"outputs":[{"data":{"text/plain":["{'label': 0,\n"," 'tweet': '@user when a father is dysfunctional and is so selfish he drags his kids into his dysfunction.   #run',\n"," 'all_tokens': ['father',\n","  'dysfunctional',\n","  'selfish',\n","  'drag',\n","  'kid',\n","  'dysfunction',\n","  'run'],\n"," 'tokens': ['father', 'kid', 'run']}"]},"execution_count":27,"metadata":{},"output_type":"execute_result"}],"source":["# verify the dataset after filtering\n","train_ds_tokens[0]"]},{"cell_type":"markdown","metadata":{"id":"dWq-u7lxpxWR"},"source":["### Create training samples using skip-gram\n","The next step is to create training samples using a \" skip-gram \" technique. To understand it, let's take an example of a sample tweet text: `i get to see my daddy today!`. After applying preprocessing steps, the word tokens produced from the tweet are: `['get', 'see', 'daddy', 'today']`. The vocabulary indices (integer value) for these tokens are provided below.\n","\n","| **Tokens** \t| **Indices** \t|\n","|:----------:\t|:-----------:\t|\n","|     get    \t|      10     \t|\n","|     see    \t|      22     \t|\n","|    daddy   \t|     404     \t|\n","|    today   \t|      9      \t|\n","\n","For creating training samples using skip-gram, we take each token as an input and a surrounding token as the label. But we also need to decide the window size, meaning how many surrounding tokens to create training samples. Suppose we take `window_size=2`; then the training sample for this tweet will be."]},{"cell_type":"markdown","metadata":{"id":"TBu6wcvVulIW"},"source":["| text    \t|         \t|           \t|           \t|      \t| input \t| output \t|      \t| input \t| output \t|\n","|---------\t|---------\t|-----------\t|-----------\t|------\t|-------\t|--------\t|------\t|-------\t|--------\t|\n","| **get** \t| see     \t| daddy     \t| today     \t| ---> \t| get   \t| see    \t| ---> \t|    10 \t|     22 \t|\n","|         \t|         \t|           \t|           \t|      \t| get   \t| daddy  \t|      \t|    10 \t|    404 \t|\n","| get     \t| **see** \t| daddy     \t| today     \t| ---> \t| see   \t| get    \t| ---> \t|    22 \t|     10 \t|\n","|         \t|         \t|           \t|           \t|      \t| see   \t| daddy  \t|      \t|    22 \t|    404 \t|\n","|         \t|         \t|           \t|           \t|      \t| see   \t| today  \t|      \t|    22 \t|      9 \t|\n","| get     \t| see     \t| **daddy** \t| today     \t| ---> \t| daddy \t| get    \t| ---> \t|   404 \t|     10 \t|\n","|         \t|         \t|           \t|           \t|      \t| daddy \t| see    \t|      \t|   404 \t|     22 \t|\n","|         \t|         \t|           \t|           \t|      \t| daddy \t| today  \t|      \t|   404 \t|      9 \t|\n","| get     \t| see     \t| daddy     \t| **today** \t| ---> \t| today \t| see    \t| ---> \t|     9 \t|     22 \t|\n","|         \t|         \t|           \t|           \t|      \t| today \t| daddy  \t|      \t| 9     \t| 404    \t|"]},{"cell_type":"markdown","metadata":{"id":"AR_y37WDvYa3"},"source":["In the above table, we have generated ten training samples from 4 tokens with a window size of 2. In each sample, we have two tokens\n","\n","* input or the training data\n","* output or the label value\n","\n","Given the tokens and window size, let's create a function to create training samples for us."]},{"cell_type":"code","execution_count":28,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":26,"status":"ok","timestamp":1670327675931,"user":{"displayName":"Hassaan Bin Aslam","userId":"13411116435427293553"},"user_tz":-300},"id":"YoqQ9-ildlAF","outputId":"e2dcd800-ce92-4da4-8aa7-f1115a165f62"},"outputs":[{"name":"stderr","output_type":"stream","text":["WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/tweets_hate_speech_detection/default/0.0.0/c6b6f41e91ac9113e1c032c5ecf7a49b4e1e9dc8699ded3c2d8425c9217568b2/cache-e1806b88a0961df3.arrow\n"]}],"source":["# code source: https://muhark.github.io/python/ml/nlp/2021/10/21/word2vec-from-scratch.html\n","def windowizer(row, wsize=3):\n","    # default 'window size' (wsize) is three\n","    doc = row[\"tokens\"]\n","    wsize = wsize\n","    out = []\n","    for i, wd in enumerate(doc):\n","        target = vb[wd]\n","        window = [\n","            i + j\n","            for j in range(-wsize, wsize + 1, 1)\n","            if (i + j >= 0) & (i + j < len(doc)) & (j != 0)\n","        ]\n","\n","        out += [(target, vb[doc[w]]) for w in window]\n","    row[\"moving_window\"] = out\n","    return row\n","\n","\n","train_ds_tokens = train_ds_tokens.map(windowizer)"]},{"cell_type":"markdown","metadata":{"id":"Mdb5Z2x1wLuf"},"source":["Let's check a sample tweet with generated training samples."]},{"cell_type":"code","execution_count":29,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":23,"status":"ok","timestamp":1670327675931,"user":{"displayName":"Hassaan Bin Aslam","userId":"13411116435427293553"},"user_tz":-300},"id":"6TgnFFNeicAA","outputId":"2dd79388-84f6-48f0-a044-1ce98da4a104"},"outputs":[{"data":{"text/plain":["{'label': 0,\n"," 'tweet': 'i get to see my daddy today!!   #80days #gettingfed',\n"," 'all_tokens': ['get', 'see', 'daddy', 'today', '80days', 'gettingfed'],\n"," 'tokens': ['get', 'see', 'daddy', 'today'],\n"," 'moving_window': [[10, 22],\n","  [10, 404],\n","  [10, 9],\n","  [22, 10],\n","  [22, 404],\n","  [22, 9],\n","  [404, 10],\n","  [404, 22],\n","  [404, 9],\n","  [9, 10],\n","  [9, 22],\n","  [9, 404]]}"]},"execution_count":29,"metadata":{},"output_type":"execute_result"}],"source":["# note that we have used skip-gram window size=3\n","train_ds_tokens[12]"]},{"cell_type":"markdown","metadata":{"id":"vk2Paho_oxQc"},"source":["### Create dataset and dataloader\n","The preprocessing part of the data is complete. Now we only need to load this data into the Pytorch Dataset class and create batches for model training using DataLoader class. Both are utilities or helper classes provided by PyTorch to make our dataset (data preparation) code decoupled from our model training code for better readability and modularity. \n","\n","* `torch.utils.data.Dataset` stores the samples and their corresponding labels\n","* `torch.utils.data.DataLoader` wraps an iterable around the Dataset to enable easy access to the samples."]},{"cell_type":"code","execution_count":30,"metadata":{"executionInfo":{"elapsed":18,"status":"ok","timestamp":1670327675932,"user":{"displayName":"Hassaan Bin Aslam","userId":"13411116435427293553"},"user_tz":-300},"id":"wrsDH5yqiiS_"},"outputs":[],"source":["# source: https://muhark.github.io/python/ml/nlp/2021/10/21/word2vec-from-scratch.html\n","# Helper class to make our data compatible with PyTorch Dataset\n","from torch.utils.data import Dataset\n","\n","class Word2VecDataset(Dataset):\n","    def __init__(self, dataset, vocab_size):\n","        self.dataset = dataset\n","        self.vocab_size = vocab_size\n","        self.data = [i for s in dataset[\"moving_window\"] for i in s]\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, idx):\n","        return self.data[idx]"]},{"cell_type":"code","execution_count":31,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3554,"status":"ok","timestamp":1670327679470,"user":{"displayName":"Hassaan Bin Aslam","userId":"13411116435427293553"},"user_tz":-300},"id":"j2cCEkK7w_Pr","outputId":"6ae75b90-0a0d-4e0e-b1f6-75516c33ad86"},"outputs":[{"name":"stdout","output_type":"stream","text":["a word2vec_ds entry: [13, 123] \n","number of word2vec_ds entries: 761044\n"]}],"source":["vb_size = len(vb)\n","\n","word2vec_ds = Word2VecDataset(train_ds_tokens, vocab_size=vb_size)\n","\n","print(\n","    f\"a word2vec_ds entry: {word2vec_ds[0]}\",\n","    f\"\\nnumber of word2vec_ds entries: {len(word2vec_ds)}\",\n",")"]},{"cell_type":"markdown","metadata":{"id":"SwoBmQ1hF-hy"},"source":["`word2vec_ds` is our training dataset, and a single entry from it is of shape `(input, label)`. Notice that there are many entries (or training samples) in `word2vec_ds`, and these samples represent all of the tweets tokens. For training, we need to create batches from them for faster processing. So let's do that next."]},{"cell_type":"code","execution_count":32,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1670327679471,"user":{"displayName":"Hassaan Bin Aslam","userId":"13411116435427293553"},"user_tz":-300},"id":"I_1C0Be9FjOu","outputId":"60244f48-959c-4a7e-9cb5-dfdf8315107a"},"outputs":[{"name":"stdout","output_type":"stream","text":["number of training batches: 47\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:563: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n"]}],"source":["from torch.utils.data import DataLoader\n","\n","BATCH_SIZE = 2 ** 14\n","N_LOADER_PROCS = 10\n","\n","dataloader = DataLoader(\n","    word2vec_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=N_LOADER_PROCS\n",")\n","\n","print(f\"number of training batches: {len(dataloader)}\")"]},{"cell_type":"code","execution_count":33,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2187,"status":"ok","timestamp":1670327681651,"user":{"displayName":"Hassaan Bin Aslam","userId":"13411116435427293553"},"user_tz":-300},"id":"TTV59vp7HWOg","outputId":"fda7bb7f-553c-4d24-ae83-992e9730e252"},"outputs":[{"data":{"text/plain":["[tensor([  46, 1743,    0,  ...,   28,    8,  122]),\n"," tensor([  15, 2197,  197,  ...,  179,  846,    0])]"]},"execution_count":33,"metadata":{},"output_type":"execute_result"}],"source":["tmp = next(iter(dataloader))\n","tmp"]},{"cell_type":"markdown","metadata":{"id":"m0jvSOZWIEVK"},"source":["In the last cell, I fetched a single batch from the dataloader. Its output is two long tensors.\n","\n","* The first tensor has all the entries for the input data\n","* The second tensor has all the entries for the labels\n","\n","The size of both these tensors is the same."]},{"cell_type":"markdown","metadata":{"id":"wcfcBZTiItO_"},"source":["## Model Configuration\n","\n","Now we will configure the model to be used for training. \n","\n","**What are we trying to solve with our model?**\n","\n","We are trying to train a model that can predict the surrounding words of a given input. And this is how we have designed our training data too. Each training sample has `(input, label)`. Where `input` is the given token, and `label` is some nearby token. It is like forcing the model to predict the context of an input word.\n","\n","**How can such a model be helpful to us?**\n","\n","Such a model that can predict the context of a word is not helpful to us in any actual scenario. So we are not going to use it. Instead, we are only interested in the learned weights of such a model, and we call them `word embedding`. It is like faking a problem (creating a pseudo-problem), training a model, and then using the learned weights for other tasks."]},{"cell_type":"code","execution_count":34,"metadata":{"executionInfo":{"elapsed":11,"status":"ok","timestamp":1670327681652,"user":{"displayName":"Hassaan Bin Aslam","userId":"13411116435427293553"},"user_tz":-300},"id":"j1cImalCHjJs"},"outputs":[],"source":["# source: https://muhark.github.io/python/ml/nlp/2021/10/21/word2vec-from-scratch.html\n","import torch.nn as nn\n","\n","class Word2Vec(nn.Module):\n","    def __init__(self, vocab_size, embedding_size):\n","        super().__init__()\n","        self.embed = nn.Embedding(vocab_size, embedding_size)\n","        self.expand = nn.Linear(embedding_size, vocab_size, bias=False)\n","\n","    def forward(self, input):\n","        # Encode input to lower-dimensional representation\n","        hidden = self.embed(input)\n","        # Expand hidden layer to predictions\n","        logits = self.expand(hidden)\n","        return logits"]},{"cell_type":"markdown","metadata":{"id":"UCSj4AbCFfpY"},"source":["Notice that in the model configuration, the size of the output layer (nn.Linear) is equal to our vocabulary size. So, our model is configured for a multi-classification problem."]},{"cell_type":"code","execution_count":35,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3241,"status":"ok","timestamp":1670327684884,"user":{"displayName":"Hassaan Bin Aslam","userId":"13411116435427293553"},"user_tz":-300},"id":"G-HiLdQcLcA6","outputId":"31a39ef4-fa97-4536-9183-c0c2e137138e"},"outputs":[{"name":"stdout","output_type":"stream","text":["cuda\n"]},{"data":{"text/plain":["Word2Vec(\n","  (embed): Embedding(3007, 100)\n","  (expand): Linear(in_features=100, out_features=3007, bias=False)\n",")"]},"execution_count":35,"metadata":{},"output_type":"execute_result"}],"source":["EMBED_SIZE = 100\n","model = Word2Vec(vb_size, EMBED_SIZE)\n","\n","# Relevant if you have a GPU\n","device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n","print(device)\n","\n","model.to(device)"]},{"cell_type":"markdown","metadata":{"id":"vrT-lOfYLmOU"},"source":["Now configure the loss function and a training optimizer."]},{"cell_type":"code","execution_count":36,"metadata":{"executionInfo":{"elapsed":15,"status":"ok","timestamp":1670327684885,"user":{"displayName":"Hassaan Bin Aslam","userId":"13411116435427293553"},"user_tz":-300},"id":"lY8BO6xOLJne"},"outputs":[],"source":["import torch\n","\n","LR = 3e-4  # learning rate\n","loss_fn = nn.CrossEntropyLoss()\n","optimizer = torch.optim.AdamW(model.parameters(), lr=LR)"]},{"cell_type":"markdown","metadata":{"id":"MCinHWCvL1r1"},"source":["## Model Training\n","In this section we are going to train our model for 100 epochs. "]},{"cell_type":"code","execution_count":37,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":359323,"status":"ok","timestamp":1670328044195,"user":{"displayName":"Hassaan Bin Aslam","userId":"13411116435427293553"},"user_tz":-300},"id":"Wdfi_-G2L4a5","outputId":"d08cc511-af3d-4865-cfac-970136f99685"},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|█████████▉| 4699/4700 [05:59<00:00, 40.21it/s]"]}],"source":["# source: https://muhark.github.io/python/ml/nlp/2021/10/21/word2vec-from-scratch.html\n","from tqdm import tqdm  # For progress bar. https://github.com/tqdm/tqdm\n","\n","EPOCHS = 100\n","progress_bar = tqdm(range(EPOCHS * len(dataloader)))\n","running_loss = []\n","\n","for epoch in range(EPOCHS):\n","    epoch_loss = 0\n","    for center, context in dataloader:\n","        center, context = center.to(device), context.to(device)\n","\n","        optimizer.zero_grad()\n","        logits = model(input=context)\n","        loss = loss_fn(logits, center)\n","        epoch_loss += loss.item()\n","        loss.backward()\n","        optimizer.step()\n","\n","        progress_bar.update(1)\n","    epoch_loss /= len(dataloader)\n","    running_loss.append(epoch_loss)"]},{"cell_type":"markdown","metadata":{"id":"2LXowLF9GkLX"},"source":["Let's plot the training loss."]},{"cell_type":"code","execution_count":38,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":282},"executionInfo":{"elapsed":50,"status":"ok","timestamp":1670328044200,"user":{"displayName":"Hassaan Bin Aslam","userId":"13411116435427293553"},"user_tz":-300},"id":"lk_WzvU_L45W","outputId":"6f41f22b-b0bc-4aa4-db2b-38a71670ceeb"},"outputs":[{"data":{"text/plain":["[<matplotlib.lines.Line2D at 0x7fd2f2d74130>]"]},"execution_count":38,"metadata":{},"output_type":"execute_result"},{"data":{"image/png":"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","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"needs_background":"light"},"output_type":"display_data"}],"source":["import matplotlib.pyplot as plt\n","plt.plot(running_loss)"]},{"cell_type":"markdown","metadata":{"id":"C8_8ZQHkIMGx"},"source":["## Analyze Word Embeddings (Word Vectors)\n","Training is complete, and our model has learned something. In this section, we will analyze the learned weights and their quality.\n","\n","So, let's extract the weights.\n","\n","**We have weights in two layers: embedding and linear. Both have similar dimensions, so which should we use as embeddings?**\n","I have experimented with weights from both these layers, and here are some suggestions.\n","\n","* Embeddings from the layer closer to the output layer give better results. In our case, it is the Linear layer. Our linear layer has the same dimensions as the embedding layer, and we will use weights from this layer.\n","* If the dimensions of the layers closer to the output layer are different than the Embedding layer, then we should stick to the embedding layer. Sometimes, people prefer to concatenate weights from multiple layers to create the final output."]},{"cell_type":"code","execution_count":39,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":40,"status":"ok","timestamp":1670328044200,"user":{"displayName":"Hassaan Bin Aslam","userId":"13411116435427293553"},"user_tz":-300},"id":"RN-eggwDMJlP","outputId":"9549b15f-6def-4198-e198-19ed217c0e78"},"outputs":[{"data":{"text/plain":["(3007, 100)"]},"execution_count":39,"metadata":{},"output_type":"execute_result"}],"source":["# embedding = model.embed.weight.cpu().detach().numpy()\n","embedding = model.expand.weight.cpu().detach().numpy()\n","embedding.shape"]},{"cell_type":"markdown","metadata":{"id":"vx6CqHZAM4TF"},"source":["Next, I have created some functions to calculate the distance between these word vectors. We are doing this to find a word similarity score with other words (cosine similarity). A smaller distance between two vectors means that these words are often used in similar contexts."]},{"cell_type":"code","execution_count":40,"metadata":{"executionInfo":{"elapsed":34,"status":"ok","timestamp":1670328044201,"user":{"displayName":"Hassaan Bin Aslam","userId":"13411116435427293553"},"user_tz":-300},"id":"GEu1lYZ3Ia2t"},"outputs":[],"source":["# source: https://muhark.github.io/python/ml/nlp/2021/10/21/word2vec-from-scratch.html\n","from scipy.spatial import distance\n","import numpy as np\n","\n","def get_distance_matrix(embedding, metric):\n","    dist_matrix = distance.squareform(distance.pdist(embedding, metric))\n","    return dist_matrix\n","\n","\n","def get_k_similar_words(word, dist_matrix, k=10):\n","    idx = vb[word]\n","    dists = dist_matrix[idx]\n","    ind = np.argpartition(dists, k)[: k + 1]\n","    ind = ind[np.argsort(dists[ind])][1:]\n","    out = [(i, vb.lookup_token(i), dists[i]) for i in ind]\n","    return out"]},{"cell_type":"code","execution_count":41,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":805,"status":"ok","timestamp":1670328044973,"user":{"displayName":"Hassaan Bin Aslam","userId":"13411116435427293553"},"user_tz":-300},"id":"xIakLE2zIgB-","outputId":"1c7723ae-ade8-4503-c76d-fd913cc2423e"},"outputs":[{"data":{"text/plain":["(3007, 3007)"]},"execution_count":41,"metadata":{},"output_type":"execute_result"}],"source":["# calculate 2d distance matrix\n","dmat = get_distance_matrix(embedding, \"cosine\")\n","dmat.shape"]},{"cell_type":"markdown","metadata":{"id":"OeI9u7fAwdJv"},"source":["Another helper function is to print similar words identified based on their distance."]},{"cell_type":"code","execution_count":42,"metadata":{"executionInfo":{"elapsed":22,"status":"ok","timestamp":1670328044974,"user":{"displayName":"Hassaan Bin Aslam","userId":"13411116435427293553"},"user_tz":-300},"id":"qD29bthXIhhx"},"outputs":[],"source":["def similar_words(tokens):\n","    for word in tokens:\n","        print(word, [t[1] for t in get_k_similar_words(word, dmat)], \"\\n\")"]},{"cell_type":"markdown","metadata":{},"source":["Next, I selected some tokens and used embeddings to find words most similar to them."]},{"cell_type":"code","execution_count":43,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":23,"status":"ok","timestamp":1670328044975,"user":{"displayName":"Hassaan Bin Aslam","userId":"13411116435427293553"},"user_tz":-300},"id":"q_zptbRycNf_","outputId":"b5d256af-d190-4645-b458-3ddb3bb4e32e"},"outputs":[{"name":"stdout","output_type":"stream","text":["father ['dad', 'fathersday', 'day', 'happy', 'love', 'one', 'bihday', 'kid', 'u', 'today'] \n","\n","mother ['care', 'child', 'son', 'died', 'father', 'kill', 'men', 'ten', 'born', 'wife'] \n","\n","boy ['girl', 'guy', 'man', 'smile', 'little', 'family', 'love', 'selfie', 'day', 'face'] \n","\n","girl ['smile', 'summer', 'love', 'guy', 'happy', 'fun', 'friend', 'boy', 'beautiful', 'today'] \n","\n","job ['one', 'amp', 'im', 'getting', 'got', 'going', 'first', 'even', 'get', 'cant'] \n","\n","sad ['make', 'people', 'life', 'know', 'dont', 'help', 'world', 'amp', 'like', 'say'] \n","\n","happy ['day', 'today', 'love', 'great', 'life', 'make', 'amp', 'weekend', 'smile', 'one'] \n","\n","hate ['people', 'america', 'say', 'dont', 'even', 'many', 'still', 'world', 'much', 'amp'] \n","\n","america ['hate', 'orlando', 'many', 'trump', 'people', 'say', 'even', 'american', 'still', 'shooting'] \n","\n","england ['eng', 'football', 'euro2016', 'v', 'wale', 'soccer', 'match', 'russia', 'player', 'clinton'] \n","\n","india ['received', 'test', 'sign', 'local', 'em', 'rude', 'amount', 'forget', 'called', 'shocking'] \n","\n","football ['england', 'euro2016', 'france', 'fan', 'v', 'making', 'review', 'match', 'game', 'award'] \n","\n","swimming ['swim', 'bbq', 'sunglass', 'ink', 'lovemylife', 'ceremony', 'placement', 'follow4follow', 'brunette', 'pool'] \n","\n","cycling ['pub', 'nutrition', 'musictherapy', 'exploring', 'letsgo', 'niece', 'cook', 'taste', 'pougal', 'mount'] \n","\n","exercise ['wellness', 'weightloss', 'lifeisgood', 'fitfam', 'inspire', 'madrid', 'namaste', 'runner', 'yoga', 'workout'] \n","\n","health ['healthy', 'food', 'happiness', 'amazing', 'fitness', 'beauty', 'gym', 'positive', 'run', 'lifestyle'] \n","\n","fitness ['workout', 'gym', 'fit', 'food', 'running', 'run', 'yoga', 'health', 'selfie', 'monday'] \n","\n"]}],"source":["tokens = [\"father\", \"mother\", \"boy\", \"girl\"]  # tokens for relations\n","tokens += [\"job\", \"sad\", \"happy\", \"hate\"]  # for emotions\n","tokens += [\"america\", \"england\", \"india\"]  # for regions\n","tokens += [\"football\", \"swimming\", \"cycling\"]  # for sports\n","tokens += [\"exercise\", \"health\", \"fitness\"]  # for health\n","\n","similar_words(tokens)"]},{"cell_type":"markdown","metadata":{"id":"bW5c5Vym4hfj"},"source":["### Visualize embeddings\n","We can also analyze the embeddings by visualizing them on a plot. Similar word vectors should appear closer in the plot. But each word vector has 300 dimensions, so how can we plot them on a 2D surface? For this, we can take the help of the dimension reduction technique `t-SNE` to reduce the word vectors dimensions from 300 to 2. PCA can also be applied to achieve similar results.\n","\n","We need first to normalize the weights to get better results."]},{"cell_type":"code","execution_count":44,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":19,"status":"ok","timestamp":1670328044976,"user":{"displayName":"Hassaan Bin Aslam","userId":"13411116435427293553"},"user_tz":-300},"id":"UrVlfmxVd04S","outputId":"cd5aa0b1-6b13-4c65-c482-564880993dde"},"outputs":[{"data":{"text/plain":["(3007, 100)"]},"execution_count":44,"metadata":{},"output_type":"execute_result"}],"source":["# source: https://github.com/OlgaChernytska/word2vec-pytorch/blob/main/notebooks/Inference.ipynb\n","# normalization of weights\n","norms = (embedding ** 2).sum(axis=1) ** (1 / 2)\n","norms = np.reshape(norms, (len(norms), 1))\n","embedding_norm = embedding / norms\n","\n","embedding_norm.shape"]},{"cell_type":"markdown","metadata":{"id":"i2yvzfcaQGyn"},"source":["Now apply t-distributed Stochastic Neighbor Embedding ([t-SNE](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html)) to reduce the dimensions of our embedding.\n"]},{"cell_type":"code","execution_count":45,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":29761,"status":"ok","timestamp":1670328074723,"user":{"displayName":"Hassaan Bin Aslam","userId":"13411116435427293553"},"user_tz":-300},"id":"waWq50yW4GB3","outputId":"6c1a1cfa-45cf-4b87-fc22-62bb04cd1164"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.8/dist-packages/sklearn/manifold/_t_sne.py:780: FutureWarning: The default initialization in TSNE will change from 'random' to 'pca' in 1.2.\n","  warnings.warn(\n","/usr/local/lib/python3.8/dist-packages/sklearn/manifold/_t_sne.py:790: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n","  warnings.warn(\n","100%|██████████| 4700/4700 [06:10<00:00, 40.21it/s]"]}],"source":["# source: https://github.com/OlgaChernytska/word2vec-pytorch/blob/main/notebooks/Inference.ipynb\n","from sklearn.manifold import TSNE\n","\n","# embeddings DataFrame\n","embedding_df = pd.DataFrame(embedding)\n","\n","# t-SNE transform\n","tsne = TSNE(n_components=2)\n","embedding_df_trans = tsne.fit_transform(embedding_df)\n","embedding_df_trans = pd.DataFrame(embedding_df_trans)"]},{"cell_type":"markdown","metadata":{"id":"b464aKAbQxIc"},"source":["We have some tokens selected in the last section. Let's highlight them in the plot to locate them easily. For this, I have created a color coding for their indices."]},{"cell_type":"code","execution_count":46,"metadata":{"executionInfo":{"elapsed":27,"status":"ok","timestamp":1670328074724,"user":{"displayName":"Hassaan Bin Aslam","userId":"13411116435427293553"},"user_tz":-300},"id":"1aMJ00Zx4Tyn"},"outputs":[],"source":["# get token order\n","embedding_df_trans.index = vb.get_itos()\n","\n","# create color codes for selected tokens\n","color_codes = []\n","for s in embedding_df_trans.index:\n","    if s in tokens:\n","        color_codes.append(True)\n","    else:\n","        color_codes.append(False)"]},{"cell_type":"markdown","metadata":{"id":"An8IXiAHSFMa"},"source":["Now create a scatter plot of these embeddings using [Plotly](https://plotly.com/python-api-reference/generated/plotly.graph_objects.Scatter.html)."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":542},"executionInfo":{"elapsed":28,"status":"ok","timestamp":1670328074726,"user":{"displayName":"Hassaan Bin Aslam","userId":"13411116435427293553"},"user_tz":-300},"id":"hWRADjvf4Vih","outputId":"0577001b-6040-472f-c88d-6a4f364f2d70"},"outputs":[],"source":["# source: https://github.com/OlgaChernytska/word2vec-pytorch/blob/main/notebooks/Inference.ipynb\n","import plotly.graph_objects as go\n","\n","color = np.where(color_codes, \"red\", \"grey\")\n","fig = go.Figure()\n","\n","fig.add_trace(\n","    go.Scatter(\n","        x=embedding_df_trans[0],\n","        y=embedding_df_trans[1],\n","        mode=\"text\",\n","        text=embedding_df_trans.index,\n","        textposition=\"middle center\",\n","        textfont=dict(color=color),\n","    )\n",")"]},{"cell_type":"markdown","metadata":{},"source":["![](images/2022-12-02-pytorch-word2vec-embedding/plot-1.png)"]},{"cell_type":"markdown","metadata":{},"source":["I am zooming in on some of the highlighted tokens at the top right corner."]},{"cell_type":"markdown","metadata":{},"source":["![](images/2022-12-02-pytorch-word2vec-embedding/plot-2.png)"]},{"cell_type":"markdown","metadata":{},"source":["Further zooming."]},{"cell_type":"markdown","metadata":{},"source":["![](images/2022-12-02-pytorch-word2vec-embedding/plot-3.png)"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[],"toc_visible":true},"gpuClass":"standard","instance_type":"ml.t3.medium","kernelspec":{"display_name":"Python 3.8.8 ('myblog')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"},"varInspector":{"cols":{"lenName":16,"lenType":16,"lenVar":40},"kernels_config":{"python":{"delete_cmd_postfix":"","delete_cmd_prefix":"del ","library":"var_list.py","varRefreshCmd":"print(var_dic_list())"},"r":{"delete_cmd_postfix":") ","delete_cmd_prefix":"rm(","library":"var_list.r","varRefreshCmd":"cat(var_dic_list()) "}},"types_to_exclude":["module","function","builtin_function_or_method","instance","_Feature"],"window_display":false},"vscode":{"interpreter":{"hash":"bbac80ad2bfd54975e0c0f7ddf300156d5b24b5126f35dc262fa887f22fb28f1"}},"widgets":{"application/vnd.jupyter.widget-state+json":{"0700ca7be4e24998a2a3205b7b9c7bf0":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0e1dbf44aaa447e99f1c449bb5294e33":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0eddd7378b7047e3ad7a1216ba184918":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3e433938116347a59ce144ef862af928":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"427f8f7d1a374f70915a8c10ae7ecc14":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0e1dbf44aaa447e99f1c449bb5294e33","placeholder":"​","style":"IPY_MODEL_3e433938116347a59ce144ef862af928","value":"100%"}},"4d4a8f416b2e405fbdc80d0fe6cce353":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_0eddd7378b7047e3ad7a1216ba184918","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_e562176c3e0341a3a1bb881645c59029","value":1}},"97ed0d18e7a14227872981fc581f550f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a24de2b54c604b0784c52177cfbd6fd0","placeholder":"​","style":"IPY_MODEL_0700ca7be4e24998a2a3205b7b9c7bf0","value":" 1/1 [00:00&lt;00:00, 27.97it/s]"}},"a24de2b54c604b0784c52177cfbd6fd0":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d00236fd7d5b4a0dac70c4389d20e61d":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e562176c3e0341a3a1bb881645c59029":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"fd43128fc4d74e10a6d4d64df3d3195a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_427f8f7d1a374f70915a8c10ae7ecc14","IPY_MODEL_4d4a8f416b2e405fbdc80d0fe6cce353","IPY_MODEL_97ed0d18e7a14227872981fc581f550f"],"layout":"IPY_MODEL_d00236fd7d5b4a0dac70c4389d20e61d"}}}}},"nbformat":4,"nbformat_minor":0}
