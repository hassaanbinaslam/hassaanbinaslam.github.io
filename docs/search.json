[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hi there!\nI am Hassaan Bin Aslam, and welcome to my blog.\nI started this blog to document and share my learning. I make a living by working as a Machine Learning Solutions Architect. AI/ML, Cloud Architecture, DevOps are fascinating topics and also close to my heart. Every day I face exciting problems, and I like to share my understanding of them here. I am passionate about AWS as a strategic cloud platform and using AI/ML to solve business problems. I’m currently focusing a lot of my time on applying DevOps practices to Machine Learning workloads (MLOps) to enable customers to adopt Machine Learning at scale.\nYou can find and connect with me on - LinkedIn: linkedin.com/in/hassaanbinaslam - Twitter: twitter.com/hassaanbinaslam - GitHub: github.com/hassaanbinaslam"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Random Thoughts",
    "section": "",
    "text": "Implementing AutoEncoder with PyTorch\n\n\n\n\n\n\n\npytorch\n\n\n\n\nThis is a practice notebook to implement AutoEncoder in PyTorch. An autoencoder takes an image as input, stores it in a lower dimension (term encoder), and tries to reproduce the same image as output, hence the term auto. Autoencoders come in handy to identify and group similar images.\n\n\n\n\n\n\nDec 14, 2022\n\n\n11 min\n\n\n\n\n\n\n  \n\n\n\n\nImplementing Word2Vec with PyTorch\n\n\n\n\n\n\n\npytorch\n\n\nnlp\n\n\n\n\nThis is a practice notebook to implement word2vec in PyTorch.\n\n\n\n\n\n\nDec 2, 2022\n\n\n14 min\n\n\n\n\n\n\n  \n\n\n\n\nGenerating Text with Recurrent Neural Networks in PyTorch\n\n\n\n\n\n\n\npytorch\n\n\nlstm\n\n\n\n\nThis is a practice notebook to build a character-level language model with LSTM using PyTorch. We will train a model on an input text, and our goal will be to generate some new text.\n\n\n\n\n\n\nNov 19, 2022\n\n\n7 min\n\n\n\n\n\n\n  \n\n\n\n\nPredicting the Sentiment of IMDB Movie Reviews using LSTM in PyTorch\n\n\n\n\n\n\n\npytorch\n\n\nlstm\n\n\n\n\nThis is a practice notebook to work with a dataset of 50,000 movie reviews from the Internet Movie Database (IMDB) and build an LSTM predictor to distinguish between positive and negative reviews.\n\n\n\n\n\n\nNov 9, 2022\n\n\n8 min\n\n\n\n\n\n\n  \n\n\n\n\nBuild Temporal Models for Univariate Time series Data with RNN, GRU, LSTM, CNN using PyTorch\n\n\n\n\n\n\n\npytorch\n\n\ndl\n\n\n\n\nThis is a practice notebook to understand and build models for time series data. We will explore some popular neural network architectures including RNN, GRU, LSTM, and 1D CNN.\n\n\n\n\n\n\nNov 7, 2022\n\n\n14 min\n\n\n\n\n\n\n  \n\n\n\n\nDetect Vanishing Gradients in Deep Neural Networks by Plotting Gradient Distributions\n\n\n\n\n\n\n\npytorch\n\n\ndl\n\n\n\n\nIn this notebook, we will explore how vanishing gradients can affect the training of a deep neural network. We will visualize the gradient flow from the deeper to starting layers during the backpropagation for two popular activation functions, Sigmoid and ReLU.\n\n\n\n\n\n\nOct 23, 2022\n\n\n10 min\n\n\n\n\n\n\n  \n\n\n\n\nConvolutional Neural Networks Filters and Feature Maps with PyTorch\n\n\n\n\n\n\n\npytorch\n\n\n\n\nThis is a practice notebook for implementing a convolutional neural network (CNN) on the MNIST dataset with PyTorch. We will implement the now famous LeNet-5 from Yann LeCun, a 7-layer CNN from 1989. Then we will explore and visualize the layers learned by our network including filters, feature maps, and output layers.\n\n\n\n\n\n\nOct 18, 2022\n\n\n9 min\n\n\n\n\n\n\n  \n\n\n\n\nTwo Class (Binary) Logistic Regression in Pytorch\n\n\n\n\n\n\n\npytorch\n\n\n\n\nThis is a practice notebook for implementing a two class logistic regression model in PyTorch. We will start by generating some synthetic data and then build an end-to-end pipeline to train a model. We will also see two ways to implement logistic regression models.\n\n\n\n\n\n\nOct 11, 2022\n\n\n10 min\n\n\n\n\n\n\n  \n\n\n\n\nLinear Regression with PyTorch\n\n\n\n\n\n\n\npytorch\n\n\n\n\nThis is a practice notebook for implementing a linear regression model in PyTorch. We will start by generating some synthetic linear data and then load it into DataLoader class for creating mini-batches. Then build the complete pipeline to train the model and visualize its loss progress in TensorBoard.\n\n\n\n\n\n\nOct 10, 2022\n\n\n4 min\n\n\n\n\n\n\n  \n\n\n\n\nTrain an Image Classifier using Fastai (Deep Dive Analysis)\n\n\n\n\n\n\n\naws\n\n\nml\n\n\nsagemaker\n\n\nfastai\n\n\n\n\nThis is a practice notebook using the fastai library to build a simple image classifier for cricket, tennis, basketball, and soccer.\n\n\n\n\n\n\nAug 10, 2022\n\n\n20 min\n\n\n\n\n\n\n  \n\n\n\n\nBuilding a Feature Repository with SageMaker Feature Store\n\n\n\n\n\n\n\naws\n\n\nml\n\n\nsagemaker\n\n\n\n\nThis notebook demonstrates how to build a central feature repository using Amazon SageMaker Feature Store. Feature Store is used to store, retrieve, and share machine learning features.\n\n\n\n\n\n\nAug 5, 2022\n\n\n10 min\n\n\n\n\n\n\n  \n\n\n\n\nDeploy Scikit-learn Models to Amazon SageMaker with the SageMaker Python SDK using Script mode\n\n\n\n\n\n\n\naws\n\n\nml\n\n\nsagemaker\n\n\n\n\nThe aim of this notebook is to demonstrate how to train and deploy a scikit-learn model in Amazon SageMaker using script mode.\n\n\n\n\n\n\nJul 7, 2022\n\n\n19 min\n\n\n\n\n\n\n  \n\n\n\n\nLoading SageMaker Linear Learner Model with Apache MXNet in Python\n\n\n\n\n\n\n\naws\n\n\nml\n\n\nsagemaker\n\n\n\n\nHow to load SageMaker builtin Linear Learner model with Apache MXNet in Python.\n\n\n\n\n\n\nJul 5, 2022\n\n\n3 min\n\n\n\n\n\n\n  \n\n\n\n\nServerless Inference with SageMaker Serverless Endpoints\n\n\n\n\n\n\n\naws\n\n\nml\n\n\nsagemaker\n\n\n\n\nHow to call an ML model endpoint hosted by SageMaker using serverless technology.\n\n\n\n\n\n\nJun 17, 2022\n\n\n8 min\n\n\n\n\n\n\n  \n\n\n\n\nDemystifying Amazon SageMaker Training for scikit-learn Lovers\n\n\n\n\n\n\n\naws\n\n\nml\n\n\nsagemaker\n\n\n\n\nHow to build, train, and deploy a machine learning model with Amazon SageMaker.\n\n\n\n\n\n\nJun 8, 2022\n\n\n14 min\n\n\n\n\n\n\n  \n\n\n\n\nstoremagic - Don’t lose your variables in Jupyter Notebook\n\n\n\n\n\n\n\njupyter\n\n\npython\n\n\n\n\nA post on IPython extension ‘storemagic’ to keep your important variables in persistent storage. Use this trick to keep them safe and stop pulling your hair (if there are any left)\n\n\n\n\n\n\nMay 30, 2022\n\n\n1 min\n\n\n\n\n\n\n  \n\n\n\n\nData Preparation with SageMaker Data Wrangler (Part 5)\n\n\n\n\n\n\n\naws\n\n\nml\n\n\nsagemaker\n\n\n\n\nA detailed guide on AWS SageMaker Data Wrangler to prepare data for machine learning models. This is a five parts series where we will prepare, import, explore, process, and export data using AWS Data Wrangler. You are reading Part 5:Export data for ML training.\n\n\n\n\n\n\nMay 26, 2022\n\n\n2 min\n\n\n\n\n\n\n  \n\n\n\n\nData Preparation with SageMaker Data Wrangler (Part 4)\n\n\n\n\n\n\n\naws\n\n\nml\n\n\nsagemaker\n\n\n\n\nA detailed guide on AWS SageMaker Data Wrangler to prepare data for machine learning models. This is a five parts series where we will prepare, import, explore, process, and export data using AWS Data Wrangler. You are reading Part 4:Preprocess data using Data Wrangler.\n\n\n\n\n\n\nMay 25, 2022\n\n\n3 min\n\n\n\n\n\n\n  \n\n\n\n\nData Preparation with SageMaker Data Wrangler (Part 3)\n\n\n\n\n\n\n\naws\n\n\nml\n\n\nsagemaker\n\n\n\n\nA detailed guide on AWS SageMaker Data Wrangler to prepare data for machine learning models. This is a five parts series where we will prepare, import, explore, process, and export data using AWS Data Wrangler. You are reading Part 3:Explore data with Data Wrangler visualizations.\n\n\n\n\n\n\nMay 24, 2022\n\n\n2 min\n\n\n\n\n\n\n  \n\n\n\n\nData Preparation with SageMaker Data Wrangler (Part 2)\n\n\n\n\n\n\n\naws\n\n\nml\n\n\nsagemaker\n\n\n\n\nA detailed guide on AWS SageMaker Data Wrangler to prepare data for machine learning models. This is a five parts series where we will prepare, import, explore, process, and export data using AWS Data Wrangler. You are reading Part 2:Import data from multiple sources using Data Wrangler.\n\n\n\n\n\n\nMay 23, 2022\n\n\n3 min\n\n\n\n\n\n\n  \n\n\n\n\nCollecting metrics and logs from on-premises servers with the CloudWatch agent\n\n\n\n\n\n\n\naws\n\n\ncloudwatch\n\n\n\n\nA detailed guide on using cloudwatch agent to collect logs and metrics from an on-premises Ubuntu server.\n\n\n\n\n\n\nMay 21, 2022\n\n\n5 min\n\n\n\n\n\n\n  \n\n\n\n\nData Preparation with SageMaker Data Wrangler (Part 1)\n\n\n\n\n\n\n\naws\n\n\nml\n\n\nsagemaker\n\n\n\n\nA detailed guide on AWS SageMaker Data Wrangler to prepare data for machine learning models. This is a five parts series where we will prepare, import, explore, process, and export data using AWS Data Wrangler. You are reading Part 1:Prepare synthetic data and place it on multiple sources.\n\n\n\n\n\n\nMay 17, 2022\n\n\n4 min\n\n\n\n\n\n\n  \n\n\n\n\nAWS Machine Learning Certification Notes (MLS-C01)\n\n\n\n\n\n\n\naws\n\n\nml\n\n\n\n\nMy notes for AWS ML speciality exam passed on May 14, 2022.\n\n\n\n\n\n\nMay 14, 2022\n\n\n10 min\n\n\n\n\n\n\n  \n\n\n\n\nKaggle - Blue Book for Bulldozers\n\n\n\n\n\n\n\nml\n\n\n\n\nblue book\n\n\n\n\n\n\nApr 25, 2022\n\n\n17 min\n\n\n\n\n\n\n  \n\n\n\n\nDocker - Send Container Logs to AWS CloudWatch\n\n\n\n\n\n\n\ndocker\n\n\npython\n\n\naws\n\n\ncloudwatch\n\n\n\n\nA tutorial on sending docker application logs to aws cloudwatch.\n\n\n\n\n\n\nApr 11, 2022\n\n\n6 min\n\n\n\n\n\n\n  \n\n\n\n\nMachine Learning Nomenclature\n\n\n\n\n\n\n\nml\n\n\n\n\nA collection of machine learning terminologies\n\n\n\n\n\n\nMar 31, 2022\n\n\n3 min\n\n\n\n\n\n\n  \n\n\n\n\nAWS EFS Sync to S3 Using DataSync\n\n\n\n\n\n\n\naws\n\n\nlambda\n\n\nefs\n\n\ns3\n\n\nsynchonization\n\n\ndatasync\n\n\n\n\nA tutorial to synchronize EFS with S3 bucket using DataSync service.\n\n\n\n\n\n\nMar 29, 2022\n\n\n7 min\n\n\n\n\n\n\n  \n\n\n\n\nAWS EFS Sync to S3 Using Lambda\n\n\n\n\n\n\n\naws\n\n\nlambda\n\n\nefs\n\n\ns3\n\n\nsynchonization\n\n\n\n\nA tutorial on synchronizing EFS with S3 bucket using a Lambda function.\n\n\n\n\n\n\nMar 28, 2022\n\n\n9 min\n\n\n\n\n\n\n  \n\n\n\n\nAWS Lambda - Test and Debug Locally in Visual Studio Code\n\n\n\n\n\n\n\naws\n\n\nlambda\n\n\ndocker\n\n\n\n\nA tutorial on testing and debugging AWS Lambda function from Visual Studio Code.\n\n\n\n\n\n\nMar 16, 2022\n\n\n7 min\n\n\n\n\n\n\n  \n\n\n\n\nDocker - Debugging Python Application\n\n\n\n\n\n\n\ndocker\n\n\npython\n\n\n\n\nA tutorial on debugging Python application running on Docker inside WSL2.\n\n\n\n\n\n\nMar 14, 2022\n\n\n5 min\n\n\n\n\n\n\n  \n\n\n\n\nDocker - Accessing Python Application Logs\n\n\n\n\n\n\n\ndocker\n\n\npython\n\n\n\n\nA tutorial on getting Python application logs running on Docker inside WSL2.\n\n\n\n\n\n\nMar 11, 2022\n\n\n7 min\n\n\n\n\n\n\n  \n\n\n\n\nSklearn Pipeline and Transformers Deep Dive\n\n\n\n\n\n\n\nml\n\n\nsklearn\n\n\n\n\nA detailed tutorial on Sklearn Pipeline, ColumnTransformer, FunctionTransformer, and a custom transformer.\n\n\n\n\n\n\nMar 4, 2022\n\n\n11 min\n\n\n\n\n\n\n  \n\n\n\n\nAWS CloudFormation Template, Functions, and Commands\n\n\n\n\n\n\n\naws\n\n\n\n\nSome useful notes on AWS CloudFormation template sections, intrinsic functions, and other tips.\n\n\n\n\n\n\nFeb 28, 2022\n\n\n10 min\n\n\n\n\n\n\n  \n\n\n\n\nAWS IAM Policy Types\n\n\n\n\n\n\n\naws\n\n\n\n\nA summary of different AWS IAM policies.\n\n\n\n\n\n\nFeb 23, 2022\n\n\n2 min\n\n\n\n\n\n\n  \n\n\n\n\nAWS DNS Records - A, CNAME, ALIAS, & MX\n\n\n\n\n\n\n\naws\n\n\n\n\nA summary of differences between AWS Route53 DNS Records\n\n\n\n\n\n\nFeb 22, 2022\n\n\n3 min\n\n\n\n\n\n\n  \n\n\n\n\nPython - A collection of output formatting tips\n\n\n\n\n\n\n\npython\n\n\n\n\nSome handy Python output tips\n\n\n\n\n\n\nFeb 18, 2022\n\n\n0 min\n\n\n\n\n\n\n  \n\n\n\n\nPython - Getting more information from Tracebacks\n\n\n\n\n\n\n\npython\n\n\n\n\nA tutorial to get more information from Python exception stack traceback.\n\n\n\n\n\n\nFeb 11, 2022\n\n\n4 min\n\n\n\n\n\n\n  \n\n\n\n\nPython Dictionary - Multiple ways to get items\n\n\n\n\n\n\n\npython\n\n\n\n\nA tutorial on multiple ways to get items from a Python dictionary.\n\n\n\n\n\n\nFeb 10, 2022\n\n\n1 min\n\n\n\n\n\n\n  \n\n\n\n\nMy First Blog Post from Jupyter Notebook\n\n\n\n\n\n\n\njupyter\n\n\nfastpages\n\n\n\n\nTrying to check if everything is working as intended\n\n\n\n\n\n\nFeb 9, 2022\n\n\n0 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2022-02-09-hello-world.html",
    "href": "posts/2022-02-09-hello-world.html",
    "title": "My First Blog Post from Jupyter Notebook",
    "section": "",
    "text": "Well, this is my first post using Jupyter notebook as a publishing medium. Besides this notebook, I am also using ‘nbdev’ library from FastAI as tooling to convert notebooks into static HTML pages. Once pushed to GitHub they will become new posts on my blog. I need to learn more about this setup, but it is looking very interesting.\n\n\nCode\n# I can also include some code directly into the blog post. No need for GitHub snippets.\nprint(\"nbdev and fastpages from Fast.AI are so cool! \")\n\n\nnbdev and fastpages from Fast.AI are so cool!"
  },
  {
    "objectID": "posts/2022-02-10-python-dictionary.html#about",
    "href": "posts/2022-02-10-python-dictionary.html#about",
    "title": "Python Dictionary - Multiple ways to get items",
    "section": "About",
    "text": "About\nThis notebook demonstrates multiple ways to get items from a Python dictionary.\n\nEnvironment Details\n\n\nCode\nfrom platform import python_version\n\nprint(\"python==\" + python_version())\n\n\npython==3.8.5\n\n\n\n\nExample Dictionaries\n\n##\n# simple dictionary\ncar = {\n    \"brand\": \"ford\",\n    \"model\": \"mustang\"\n}\n\ncar\n\n{'brand': 'ford', 'model': 'mustang'}\n\n\n\n##\n# nested dictionary\nfamily = {\n    'gfather' : {\n        'father': {\n            'son': {'love':'python'}\n        }\n    }\n}\n\nfamily\n\n{'gfather': {'father': {'son': {'love': 'python'}}}}"
  },
  {
    "objectID": "posts/2022-02-10-python-dictionary.html#method-1-square-brackets",
    "href": "posts/2022-02-10-python-dictionary.html#method-1-square-brackets",
    "title": "Python Dictionary - Multiple ways to get items",
    "section": "Method 1: Square brackets",
    "text": "Method 1: Square brackets\nA square bracket is the simplest approach to getting any item from a dictionary. You can get a value from a dictionary by providing it a key in [] brackets. For example, to get a value of model from a car\n\ncar['model']\n\n'mustang'\n\n\nProblem with this approach is that if the provided key is not available in the dictionary then it will throw a KeyError exception.\n\ncar['year']\n\nKeyError: 'year'\n\n\nTo avoid KeyError, you can first check if the key is available in dictionary.\n\nif 'year' in car: # check if given key is available in dictionary\n    year = car['year'] # now get the value\nelse:\n    year = '1964' # (Optional) otherwise give this car a default value\n\nyear\n\n'1964'\n\n\nAn alternate approach could be to use a Try-Except block to handle the KeyError exception.\n\ntry:\n    year = car['year']\nexcept KeyError:\n    year = '1964' # give this car a default value\n\nyear\n\n'1964'\n\n\nFor nested dictionaries, you can use chained [] brackets. But beware that if any of the Keys is missing in the chain, you will get a KeyError exception.\n\n##\n# this will work. All keys are present.\nfamily['gfather']['father']['son']\n\n{'love': 'python'}\n\n\n\n##\n# this will not work. 'mother' key is not in dictionary\nfamily['gfather']['mother']['son']\n\nKeyError: 'mother'"
  },
  {
    "objectID": "posts/2022-02-10-python-dictionary.html#method-2-get-function",
    "href": "posts/2022-02-10-python-dictionary.html#method-2-get-function",
    "title": "Python Dictionary - Multiple ways to get items",
    "section": "Method 2: Get function",
    "text": "Method 2: Get function\n\nhttps://docs.python.org/3/library/stdtypes.html#dict.get  get(key[, default])\n\nGet function will return the value for key if key is in the dictionary. Otherwise, it will return a default value which is None. You can provide your default value as well.\n\nyear = car.get('year', '1964')\nyear\n# year key is not present so get function will return a default value '1964'\n\n'1964'\n\n\nDepending on your use case there can be confusion with this approach when your item can also have None value. In that case, you will not know whether the None value was returned from the dictionary or it was the Get function.\n\nowner = car.get('owner')\nowner\n# owner has a None value. But is this value coming from dic or from Get function? \n# This can be confusing for large nested dictionaries.\n\nFor nested dictionaries you can use chained Get functions. But beware that missing Key items needs to be properly handled otherwise you will still get an exception.\n\n##\n# this will work. All keys are present.\nfamily.get('gfather').get('father').get('son')\n\n{'love': 'python'}\n\n\n\n##\n# this will still work. 'daughter' key is missing\n# but since it is at the end of chain it will return a default None value\nfamily.get('gfather').get('father').get('daughter')\n\n\n##\n# this will NOT work. 'mother' key is missing and it returned a default None value.\n# but since it is not at the end, and we called Get function on returned value 'None'\nfamily.get('gfather').get('mother').get('son')\n\nAttributeError: 'NoneType' object has no attribute 'get'\n\n\n\n##\n# this will work. 'mother' key is missing and it returned a default value.\n# but we have properly handled all the default values with empty dictionaries.\nfamily.get('gfather', {}).get('mother', {}).get('son', {})\n\n{}"
  },
  {
    "objectID": "posts/2022-02-11-python-stack-traceback-more-info.html#about",
    "href": "posts/2022-02-11-python-stack-traceback-more-info.html#about",
    "title": "Python - Getting more information from Tracebacks",
    "section": "About",
    "text": "About\nThis notebook demonstrates what the Python Traceback object is, and how can we get more information out of it to better diagnose exception messages.\n\nCredit\nThis blog post is based on an article originally written in Python Cookbook published by O'Reilly Media, Inc. and released July 2002. In book’s chapter 15, there is a section with the title Getting More Information from Tracebacks written by Bryn Keller. An online version of this article is available at https://www.oreilly.com/library/view/python-cookbook/0596001673/ch14s05.html.\nThe original article uses Python 2.2, but I have adapted it for Python 3.8. Also, I have added some commentary to give more insights on Python Traceback object.\n\n\nEnvironment Details\n\n\nCode\nfrom platform import python_version\n\nprint(\"python==\" + python_version())\n\n\npython==3.8.5"
  },
  {
    "objectID": "posts/2022-02-11-python-stack-traceback-more-info.html#discussion",
    "href": "posts/2022-02-11-python-stack-traceback-more-info.html#discussion",
    "title": "Python - Getting more information from Tracebacks",
    "section": "Discussion",
    "text": "Discussion\nConsider the following toy example where we are getting some data from an external source (an API call, a DB call, etc.), and we need to find the length of individual items provided in the list. We know that items in the list will be of type str so we have used a len() function on it.\nWe got an exception when we ran our function on received data, and now we are trying to investigate what caused the error.\n\n\nCode\n# this is intentionally hidden as we don't know about the data received from an external source. \ndata = [\"1\", \"22\", 333, \"4444\"]\n\n\n\n##\n# our toy example function.\nimport sys, traceback\n\ndef get_items_len(items: list) -> list:\n    \"\"\"\n    this function returns the length of items received in a list.\n    \"\"\"\n    items_len = []\n    for i in items:\n        items_len.append(len(i))\n    \n    return items_len\n\n\n##\n# let's run our function on \"data\" received from an external source\ntry:\n    get_items_len(data)\nexcept Exception as e:\n    print(traceback.print_exc())\n\nNone\n\n\nTraceback (most recent call last):\n  File \"<ipython-input-4-42cd486e1858>\", line 4, in <module>\n    get_items_len(data)\n  File \"<ipython-input-3-8421f841ba77>\", line 11, in get_items_len\n    items_len.append(len(i))\nTypeError: object of type 'int' has no len()\n\n\nWe got an exception while data processing and the Traceback message gives us some details. It tells us that we have received some data of type integer instead of string, and we are trying to call len() function on it. But we don’t know the actual data value that caused the exception, and we don’t know the index of the item in the list that caused this error. Depending on the use case, information about the local variables, or input data that caused the error can be crucial in diagnosing the root cause of an error.\nFortunately, all this information is already available to us in the Traceback object, but there are no built-in methods that give this information directly. Let us try some of the built-in methods on the Traceback object to see the kind of information we could get from them.\n\n#collapse-output\n# calling traceback module built-in methods\ntry:\n    get_items_len(data)\nexcept Exception as e:\n    print(\"***** Exception *****\")\n    print(e)\n\n    exc_type, exc_value, exc_traceback = sys.exc_info()\n    print(\"\\n***** print_tb *****\")\n    traceback.print_tb(exc_traceback, limit=1, file=sys.stdout)\n\n    print(\"\\n***** print_exception *****\")\n    # exc_type below is ignored on 3.5 and later\n    traceback.print_exception(exc_type, exc_value, exc_traceback,\n                                limit=2, file=sys.stdout)\n    \n    print(\"\\n***** print_exc *****\")\n    traceback.print_exc(limit=2, file=sys.stdout)\n\n    print(\"\\n***** format_exc, first and last line *****\")\n    formatted_lines = traceback.format_exc().splitlines()\n    print(formatted_lines[0])\n    print(formatted_lines[-1])\n\n    print(\"\\n***** format_exception *****\")\n    # exc_type below is ignored on 3.5 and later\n    print(repr(traceback.format_exception(exc_type, exc_value,\n                                            exc_traceback)))\n                                            \n    print(\"\\n***** extract_tb *****\")\n    print(repr(traceback.extract_tb(exc_traceback)))\n\n    print(\"\\n***** format_tb *****\")\n    print(repr(traceback.format_tb(exc_traceback)))\n\n    print(\"\\n***** tb_lineno *****\", exc_traceback.tb_lineno)\n\n***** Exception *****\nobject of type 'int' has no len()\n\n***** print_tb *****\n  File \"<ipython-input-5-73d5b316a567>\", line 4, in <module>\n    get_items_len(data)\n\n***** print_exception *****\nTraceback (most recent call last):\n  File \"<ipython-input-5-73d5b316a567>\", line 4, in <module>\n    get_items_len(data)\n  File \"<ipython-input-3-8421f841ba77>\", line 11, in get_items_len\n    items_len.append(len(i))\nTypeError: object of type 'int' has no len()\n\n***** print_exc *****\nTraceback (most recent call last):\n  File \"<ipython-input-5-73d5b316a567>\", line 4, in <module>\n    get_items_len(data)\n  File \"<ipython-input-3-8421f841ba77>\", line 11, in get_items_len\n    items_len.append(len(i))\nTypeError: object of type 'int' has no len()\n\n***** format_exc, first and last line *****\nTraceback (most recent call last):\nTypeError: object of type 'int' has no len()\n\n***** format_exception *****\n['Traceback (most recent call last):\\n', '  File \"<ipython-input-5-73d5b316a567>\", line 4, in <module>\\n    get_items_len(data)\\n', '  File \"<ipython-input-3-8421f841ba77>\", line 11, in get_items_len\\n    items_len.append(len(i))\\n', \"TypeError: object of type 'int' has no len()\\n\"]\n\n***** extract_tb *****\n[<FrameSummary file <ipython-input-5-73d5b316a567>, line 4 in <module>>, <FrameSummary file <ipython-input-3-8421f841ba77>, line 11 in get_items_len>]\n\n***** format_tb *****\n['  File \"<ipython-input-5-73d5b316a567>\", line 4, in <module>\\n    get_items_len(data)\\n', '  File \"<ipython-input-3-8421f841ba77>\", line 11, in get_items_len\\n    items_len.append(len(i))\\n']\n\n***** tb_lineno ***** 4\n\n\nAll these methods are useful but we are still short on information about the state of local variables when the system crashed. Before writing our custom function to get the variables state at the time of exception, let us spend some time to understand the working of Traceback object.\n\nTraceback Module\n\nhttps://docs.python.org/3/library/traceback.html \n\nThis module provides an easy-to-use interface to work with traceback objects. It provides multiple functions that we can use to extract the required information from traceback. So far, we have used methods from this module in the above examples.\n\n\nTraceback Objects\n\nhttps://docs.python.org/3/reference/datamodel.html  On this page search for term “Traceback objects”\n\nTraceback objects represent a stack trace of an exception. A traceback object is implicitly created when an exception occurs and may also be explicitly created by initializing an instance of class types.TracebackType. traceback object is also an instance of types.TracebackType class. When an exception occurs, a traceback object is initialized for us, and we can obtain it from any of the following two methods. 1. It is available as a third item of the tuple returned by sys.exc_info() “(type, value, traceback)” 2. It is available as the __traceback__ object of the caught exception. “Exception.__traceback__”\nA traceback object is a linked list of nodes, where each node is a Frame object. Frame objects form their own linked list but in the opposite direction of traceback objects. Together they work like a doubly-linked list, and we can use them to move back and forth in the stack trace history. It is the frame objects that hold all the stack’s important information. traceback object has some special attributes * tb_next point to the next level in the stack trace (towards the frame where the exception occurred), or None if there is no next level * tb_frame points to the execution frame of the current level * tb_lineno gives the line number where the exception occurred\n\n##\n# method 1: get traceback object using sys.exc_info()\ntry:\n    get_items_len(data)\nexcept Exception as e:\n    print(sys.exc_info()[2])\n\n<traceback object at 0x7f5c6c60e9c0>\n\n\n\n##\n# method 2: get traceback object using Exception.__traceback__\ntry:\n    get_items_len(data)\nexcept Exception as e:\n    print(e.__traceback__ )\n\n<traceback object at 0x7f5c6c5c0180>\n\n\nIf there is no exception in the system, then calling sys.exc_info() will only return None values.\n\n##\n# no exception is generated so sys.exc_info() will return None values.\ntry:\n    get_items_len(['1','2','3','4'])\nexcept Exception as e:\n    print(sys.exc_info()[2])\n\n\n\nFrame Objects\n\nhttps://docs.python.org/3/reference/datamodel.html  On this page search for term “Frame objects”\n\nFrame objects represent execution frames. It has some special attributes * f_back is a reference to the previous stack frame (towards the caller), or None if this is the bottom stack frame * f_code is the code object being executed in this frame. We will discuss Code Objects in next the section * f_lineno is the current line number of the frame — writing to this from within a trace function jumps to the given line (only for the bottom-most frame). A debugger can implement a Jump command (aka Set Next Statement) by writing to f_lineno. This attribute will give you the line number in the code on which exception occurred * f_locals is a dictionary used to lookup local variables. From this dictionary we can get all the local variables and their state at the time of exception * f_globals is a dictionary for global varaibles\n\n\nCode Objects\n\nhttps://docs.python.org/3/reference/datamodel.html  On this page search for term “Code Objects”\n\nCode objects represent byte-compiled executable Python code or bytecode. Some of its attributes include * co_name gives the function name being executed * co_filename gives the filename from which the code was compiled\nThere are many other helpful attributes in this object, and you may read about them from the docs.\n\n\nVisual representation of Traceback, Frame and Code Objects\n\n*\n\nfigure 1: Visual representation of Traceback, Frame and Code Objects\n\n\n\n\n\n\nCustom fuction for additional exception info\nNow with this additional information on stack trace objects, let us create a function to get variables state at the time of exception.\n\n\nCode\ndef exc_info_plus():\n    \"\"\"\n    Provides the usual traceback information, followed by a listing of all the\n    local variables in each frame.\n    \"\"\"\n    tb = sys.exc_info()[2]\n\n    # iterate forward to the last (most recent) traceback object.\n    while 1:\n        if not tb.tb_next:\n            break\n        tb = tb.tb_next\n    stack = []\n\n    # get the most recent traceback frame\n    f = tb.tb_frame\n\n    # iterate backwards from recent to oldest traceback frame \n    while f:\n        stack.append(f)\n        f = f.f_back\n    \n    # stack.reverse() # uncomment to get innermost (most recent) frame at the last\n\n    # get exception information and stack trace entries from most recent traceback object\n    exc_msg = traceback.format_exc()\n\n    exc_msg += \"\\n*** Locals by frame, innermost first ***\"\n    for frame in stack:\n        exc_msg += f\"\\nFrame {frame.f_code.co_name} in {frame.f_code.co_filename} at line {frame.f_lineno}\"\n        for key, value in frame.f_locals.items():\n            exc_msg += f\"\\n\\t {key:20} = \"\n            try:\n                data = str(value)\n                # limit variable's output to a certain number. You can adjust it as per your requirement.\n                # But not to remove it as output from large objects (e.g. Pandas DataFrame) can be troublesome. \n                output_limit = 50\n                exc_msg += (data[:output_limit] + \"...\") if len(data) > output_limit else data\n            except:\n                exc_msg += \"<ERROR WHILE PRINTING VALUE>\"\n\n    return exc_msg\n\n\n\n#collapse-output\n#now let us try our custom exception function and see the ouput\ntry:\n    get_items_len(data)\nexcept Exception as e:\n    print(exc_info_plus())\n\nTraceback (most recent call last):\n  File \"<ipython-input-10-01264d9e470a>\", line 4, in <module>\n    get_items_len(data)\n  File \"<ipython-input-3-8421f841ba77>\", line 11, in get_items_len\n    items_len.append(len(i))\nTypeError: object of type 'int' has no len()\n\n*** Locals by frame, innermost first ***\nFrame get_items_len in <ipython-input-3-8421f841ba77> at line 11\n     items                = ['1', '22', 333, '4444']\n     items_len            = [1, 2]\n     i                    = 333\nFrame <module> in <ipython-input-10-01264d9e470a> at line 6\n     __name__             = __main__\n     __doc__              = Automatically created module for IPython interacti...\n     __package__          = None\n     __loader__           = None\n     __spec__             = None\n     __builtin__          = <module 'builtins' (built-in)>\n     __builtins__         = <module 'builtins' (built-in)>\n     _ih                  = ['', '#collapse-hide\\nfrom platform import python_...\n     _oh                  = {}\n     _dh                  = ['/data/_notebooks']\n     In                   = ['', '#collapse-hide\\nfrom platform import python_...\n     Out                  = {}\n     get_ipython          = <bound method InteractiveShell.get_ipython of <ipy...\n     exit                 = <IPython.core.autocall.ZMQExitAutocall object at 0...\n     quit                 = <IPython.core.autocall.ZMQExitAutocall object at 0...\n     _                    = \n     __                   = \n     ___                  = \n     _i                   = #collapse-show\ndef exc_info_plus():\n    \"\"\"\n    Pr...\n     _ii                  = ##\n# no exception is generated so sys.exc_info() w...\n     _iii                 = ##\n# method 2: get traceback object using Exceptio...\n     _i1                  = #collapse-hide\nfrom platform import python_version...\n     python_version       = <function python_version at 0x7f5c72dbc430>\n     _i2                  = #collapse-hide\n# this is intentionally hidden as w...\n     data                 = ['1', '22', 333, '4444']\n     _i3                  = ##\n# our toy example function.\nimport sys, traceba...\n     sys                  = <module 'sys' (built-in)>\n     traceback            = <module 'traceback' from '/usr/lib/python3.8/trace...\n     get_items_len        = <function get_items_len at 0x7f5c6c62c790>\n     _i4                  = ##\n# let's run our function on \"data\" received fro...\n     _i5                  = #collapse-output\n# calling traceback module built-...\n     exc_type             = <class 'TypeError'>\n     exc_value            = object of type 'int' has no len()\n     exc_traceback        = <traceback object at 0x7f5c6c5cf700>\n     formatted_lines      = ['Traceback (most recent call last):', '  File \"<i...\n     _i6                  = ##\n# method 1: get traceback object using sys.exc_...\n     _i7                  = ##\n# method 2: get traceback object using Exceptio...\n     _i8                  = ##\n# no exception is generated so sys.exc_info() w...\n     _i9                  = #collapse-show\ndef exc_info_plus():\n    \"\"\"\n    Pr...\n     exc_info_plus        = <function exc_info_plus at 0x7f5c6c62cc10>\n     _i10                 = #collapse-output\n#now let us try our custom except...\n     e                    = object of type 'int' has no len()\nFrame run_code in /usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py at line 3418\n     self                 = <ipykernel.zmqshell.ZMQInteractiveShell object at ...\n     code_obj             = <code object <module> at 0x7f5c6c62eea0, file \"<ip...\n     result               = <ExecutionResult object at 7f5c6c5c88e0, execution...\n     async_               = False\n     __tracebackhide__    = __ipython_bottom__\n     old_excepthook       = <bound method IPKernelApp.excepthook of <ipykernel...\n     outflag              = True\nFrame run_ast_nodes in /usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py at line 3338\n     self                 = <ipykernel.zmqshell.ZMQInteractiveShell object at ...\n     nodelist             = [<_ast.Try object at 0x7f5c6c5c8850>]\n     cell_name            = <ipython-input-10-01264d9e470a>\n     interactivity        = none\n     compiler             = <IPython.core.compilerop.CachingCompiler object at...\n     result               = <ExecutionResult object at 7f5c6c5c88e0, execution...\n     to_run_exec          = [<_ast.Try object at 0x7f5c6c5c8850>]\n     to_run_interactive   = []\n     mod                  = <_ast.Module object at 0x7f5c6c5c8430>\n     compare              = <function InteractiveShell.run_ast_nodes.<locals>....\n     to_run               = [(<_ast.Try object at 0x7f5c6c5c8850>, 'exec')]\n     node                 = <_ast.Try object at 0x7f5c6c5c8850>\n     mode                 = exec\n     code                 = <code object <module> at 0x7f5c6c62eea0, file \"<ip...\n     asy                  = False\n     _async               = False\nFrame run_cell_async in /usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py at line 3146\n     raw_cell             = #collapse-output\n#now let us try our custom except...\n     silent               = False\n     shell_futures        = True\n     transformed_cell     = #collapse-output\n#now let us try our custom except...\n     preprocessing_exc_tuple = None\n     info                 = <ExecutionInfo object at 7f5c6c5c8be0, raw_cell=\"#...\n     error_before_exec    = <function InteractiveShell.run_cell_async.<locals>...\n     cell                 = #collapse-output\n#now let us try our custom except...\n     compiler             = <IPython.core.compilerop.CachingCompiler object at...\n     _run_async           = False\n     cell_name            = <ipython-input-10-01264d9e470a>\n     code_ast             = <_ast.Module object at 0x7f5c6c5c85e0>\n     interactivity        = last_expr\n     result               = <ExecutionResult object at 7f5c6c5c88e0, execution...\n     self                 = <ipykernel.zmqshell.ZMQInteractiveShell object at ...\n     store_history        = True\nFrame _pseudo_sync_runner in /usr/local/lib/python3.8/dist-packages/IPython/core/async_helpers.py at line 68\n     coro                 = <coroutine object InteractiveShell.run_cell_async ...\nFrame _run_cell in /usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py at line 2923\n     self                 = <ipykernel.zmqshell.ZMQInteractiveShell object at ...\n     raw_cell             = #collapse-output\n#now let us try our custom except...\n     store_history        = True\n     silent               = False\n     shell_futures        = True\n     preprocessing_exc_tuple = None\n     transformed_cell     = #collapse-output\n#now let us try our custom except...\n     coro                 = <coroutine object InteractiveShell.run_cell_async ...\n     runner               = <function _pseudo_sync_runner at 0x7f5c724ba040>\nFrame run_cell in /usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py at line 2877\n     self                 = <ipykernel.zmqshell.ZMQInteractiveShell object at ...\n     raw_cell             = #collapse-output\n#now let us try our custom except...\n     store_history        = True\n     silent               = False\n     shell_futures        = True\n     result               = None\nFrame run_cell in /usr/local/lib/python3.8/dist-packages/ipykernel/zmqshell.py at line 539\n     self                 = <ipykernel.zmqshell.ZMQInteractiveShell object at ...\n     args                 = ('#collapse-output\\n#now let us try our custom exc...\n     kwargs               = {'store_history': True, 'silent': False}\n     __class__            = <class 'ipykernel.zmqshell.ZMQInteractiveShell'>\nFrame do_execute in /usr/local/lib/python3.8/dist-packages/ipykernel/ipkernel.py at line 302\n     self                 = <ipykernel.ipkernel.IPythonKernel object at 0x7f5c...\n     code                 = #collapse-output\n#now let us try our custom except...\n     silent               = False\n     store_history        = True\n     user_expressions     = {}\n     allow_stdin          = True\n     reply_content        = {}\n     run_cell             = <bound method InteractiveShell.run_cell_async of <...\n     should_run_async     = <bound method InteractiveShell.should_run_async of...\n     shell                = <ipykernel.zmqshell.ZMQInteractiveShell object at ...\nFrame wrapper in /usr/local/lib/python3.8/dist-packages/tornado/gen.py at line 234\n     args                 = (<ipykernel.ipkernel.IPythonKernel object at 0x7f5...\n     kwargs               = {}\n     future               = <Future pending>\n     ctx_run              = <built-in method run of Context object at 0x7f5c6c...\n     result               = <generator object IPythonKernel.do_execute at 0x7f...\n     func                 = <function IPythonKernel.do_execute at 0x7f5c6f6978...\nFrame execute_request in /usr/local/lib/python3.8/dist-packages/ipykernel/kernelbase.py at line 540\n     self                 = <ipykernel.ipkernel.IPythonKernel object at 0x7f5c...\n     stream               = <zmq.eventloop.zmqstream.ZMQStream object at 0x7f5...\n     ident                = [b'e2e3826d25fb4c63876268cdc5a787ad']\n     parent               = {'header': {'msg_id': '218114cb9837444cbd29466d87b...\n     content              = {'code': '#collapse-output\\n#now let us try our cu...\n     code                 = #collapse-output\n#now let us try our custom except...\n     silent               = False\n     store_history        = True\n     user_expressions     = {}\n     allow_stdin          = True\n     stop_on_error        = True\n     metadata             = {'started': datetime.datetime(2022, 2, 14, 9, 30, ...\nFrame wrapper in /usr/local/lib/python3.8/dist-packages/tornado/gen.py at line 234\n     args                 = (<ipykernel.ipkernel.IPythonKernel object at 0x7f5...\n     kwargs               = {}\n     future               = <Future pending>\n     ctx_run              = <built-in method run of Context object at 0x7f5c6c...\n     result               = <generator object Kernel.execute_request at 0x7f5c...\n     func                 = <function Kernel.execute_request at 0x7f5c6f747f70...\nFrame dispatch_shell in /usr/local/lib/python3.8/dist-packages/ipykernel/kernelbase.py at line 265\n     self                 = <ipykernel.ipkernel.IPythonKernel object at 0x7f5c...\n     stream               = <zmq.eventloop.zmqstream.ZMQStream object at 0x7f5...\n     msg                  = {'header': {'msg_id': '218114cb9837444cbd29466d87b...\n     idents               = [b'e2e3826d25fb4c63876268cdc5a787ad']\n     msg_type             = execute_request\n     handler              = <bound method Kernel.execute_request of <ipykernel...\nFrame wrapper in /usr/local/lib/python3.8/dist-packages/tornado/gen.py at line 234\n     args                 = (<ipykernel.ipkernel.IPythonKernel object at 0x7f5...\n     kwargs               = {}\n     future               = <Future pending>\n     ctx_run              = <built-in method run of Context object at 0x7f5c6f...\n     result               = <generator object Kernel.dispatch_shell at 0x7f5c6...\n     func                 = <function Kernel.dispatch_shell at 0x7f5c6f7473a0>\nFrame process_one in /usr/local/lib/python3.8/dist-packages/ipykernel/kernelbase.py at line 362\n     self                 = <ipykernel.ipkernel.IPythonKernel object at 0x7f5c...\n     wait                 = True\n     priority             = 10\n     t                    = 13\n     dispatch             = <bound method Kernel.dispatch_shell of <ipykernel....\n     args                 = (<zmq.eventloop.zmqstream.ZMQStream object at 0x7f...\nFrame run in /usr/local/lib/python3.8/dist-packages/tornado/gen.py at line 775\n     self                 = <tornado.gen.Runner object at 0x7f5c6c60f8e0>\n     future               = None\n     exc_info             = None\n     value                = (10, 13, <bound method Kernel.dispatch_shell of <i...\nFrame inner in /usr/local/lib/python3.8/dist-packages/tornado/gen.py at line 814\n     f                    = None\n     self                 = <tornado.gen.Runner object at 0x7f5c6c60f8e0>\nFrame _run_callback in /usr/local/lib/python3.8/dist-packages/tornado/ioloop.py at line 741\n     self                 = <tornado.platform.asyncio.AsyncIOMainLoop object a...\n     callback             = functools.partial(<function Runner.handle_yield.<l...\nFrame <lambda> in /usr/local/lib/python3.8/dist-packages/tornado/ioloop.py at line 688\n     f                    = <Future finished result=(10, 13, <bound method...7...\n     callback             = <function Runner.handle_yield.<locals>.inner at 0x...\n     future               = <Future finished result=(10, 13, <bound method...7...\n     self                 = <tornado.platform.asyncio.AsyncIOMainLoop object a...\nFrame _run in /usr/lib/python3.8/asyncio/events.py at line 81\n     self                 = <Handle IOLoop.add_future.<locals>.<lambda>(<Futur...\nFrame _run_once in /usr/lib/python3.8/asyncio/base_events.py at line 1859\n     self                 = <_UnixSelectorEventLoop running=True closed=False ...\n     sched_count          = 0\n     handle               = <Handle IOLoop.add_future.<locals>.<lambda>(<Futur...\n     timeout              = 0\n     event_list           = []\n     end_time             = 113697.83311910101\n     ntodo                = 2\n     i                    = 0\nFrame run_forever in /usr/lib/python3.8/asyncio/base_events.py at line 570\n     self                 = <_UnixSelectorEventLoop running=True closed=False ...\n     old_agen_hooks       = asyncgen_hooks(firstiter=None, finalizer=None)\nFrame start in /usr/local/lib/python3.8/dist-packages/tornado/platform/asyncio.py at line 199\n     self                 = <tornado.platform.asyncio.AsyncIOMainLoop object a...\n     old_loop             = <_UnixSelectorEventLoop running=True closed=False ...\nFrame start in /usr/local/lib/python3.8/dist-packages/ipykernel/kernelapp.py at line 612\n     self                 = <ipykernel.kernelapp.IPKernelApp object at 0x7f5c7...\nFrame launch_instance in /usr/local/lib/python3.8/dist-packages/traitlets/config/application.py at line 845\n     cls                  = <class 'ipykernel.kernelapp.IPKernelApp'>\n     argv                 = None\n     kwargs               = {}\n     app                  = <ipykernel.kernelapp.IPKernelApp object at 0x7f5c7...\nFrame <module> in /usr/local/lib/python3.8/dist-packages/ipykernel_launcher.py at line 16\n     __name__             = __main__\n     __doc__              = Entry point for launching an IPython kernel.\n\nThis...\n     __package__          = \n     __loader__           = <_frozen_importlib_external.SourceFileLoader objec...\n     __spec__             = ModuleSpec(name='ipykernel_launcher', loader=<_fro...\n     __annotations__      = {}\n     __builtins__         = <module 'builtins' (built-in)>\n     __file__             = /usr/local/lib/python3.8/dist-packages/ipykernel_l...\n     __cached__           = /usr/local/lib/python3.8/dist-packages/__pycache__...\n     sys                  = <module 'sys' (built-in)>\n     app                  = <module 'ipykernel.kernelapp' from '/usr/local/lib...\nFrame _run_code in /usr/lib/python3.8/runpy.py at line 87\n     code                 = <code object <module> at 0x7f5c7317e030, file \"/us...\n     run_globals          = {'__name__': '__main__', '__doc__': 'Entry point f...\n     init_globals         = None\n     mod_name             = __main__\n     mod_spec             = ModuleSpec(name='ipykernel_launcher', loader=<_fro...\n     pkg_name             = \n     script_name          = None\n     loader               = <_frozen_importlib_external.SourceFileLoader objec...\n     fname                = /usr/local/lib/python3.8/dist-packages/ipykernel_l...\n     cached               = /usr/local/lib/python3.8/dist-packages/__pycache__...\nFrame _run_module_as_main in /usr/lib/python3.8/runpy.py at line 194\n     mod_name             = ipykernel_launcher\n     alter_argv           = 1\n     mod_spec             = ModuleSpec(name='ipykernel_launcher', loader=<_fro...\n     code                 = <code object <module> at 0x7f5c7317e030, file \"/us...\n     main_globals         = {'__name__': '__main__', '__doc__': 'Entry point f...\n\n\nNote the output from the first stack frame in the above stack trace. It is easy now to see (items) that we received in our function. The item at index i is also available (333) on which our function crashed. Using our custom function unexpected errors are logged in a format that makes it a lot easier to find and fix the errors. Let’s fix our function to handle unexpected integer values.\n\n##\n# let's fix our function to handle unexpected 'int' items by converting them to 'str'\ndef get_items_len(items: list) -> list:\n    \"\"\"\n    this function returns the length of items received in a list.\n    \"\"\"\n    items_len = []\n    for i in map(str, items):\n        items_len.append(len(i))\n    \n    return items_len\n\n# test it again\nget_items_len(data)\n\n[1, 2, 3, 4]"
  },
  {
    "objectID": "posts/2022-02-18-python-tips-output-formatting.html#about",
    "href": "posts/2022-02-18-python-tips-output-formatting.html#about",
    "title": "Python - A collection of output formatting tips",
    "section": "About",
    "text": "About\nThis notebook is a collection of useful tips to format Python string literals and output."
  },
  {
    "objectID": "posts/2022-02-18-python-tips-output-formatting.html#environment",
    "href": "posts/2022-02-18-python-tips-output-formatting.html#environment",
    "title": "Python - A collection of output formatting tips",
    "section": "Environment",
    "text": "Environment\n\n\nCode\nfrom platform import python_version\n\nprint(\"python==\" + python_version())\n\n\npython==3.8.5"
  },
  {
    "objectID": "posts/2022-02-18-python-tips-output-formatting.html#f-string-expressions-inside-a-string",
    "href": "posts/2022-02-18-python-tips-output-formatting.html#f-string-expressions-inside-a-string",
    "title": "Python - A collection of output formatting tips",
    "section": "f-string: Expressions inside a string",
    "text": "f-string: Expressions inside a string\n\nr = 'red'\ng = 'green'\nb = 1001\n\n# f-string has a simple syntax. Put 'f' at the start of string, and put expressions in {}\nf\"Stop = {r}, Go = {g}\"\n\n'Stop = red, Go = green'\n\n\n\n##\n# 'F' can also be used to start an f-string\nF\"binary = {b}. If you need value in brackets {{{b}}}\"\n\n'binary = 1001. If you need value in brackets {1001}'\n\n\n\n##\n# f-string can also be started with \"\"\" quotes\nf\"\"\"{r} or {g}\"\"\"\n\n'red or green'\n\n\n\n##\n# f-string on multiple lines. \n# 1. Use \"\"\" with backslash \\\nf\"\"\"{r}\\\n or \\\n{g}\"\"\"\n\n'red or green'\n\n\n\n##\n# f-string on multiple lines. \n# 2. Use only backslash \\\nf\"{r}\" \\\n\" or \" \\\nf\"{g}\"\n\n'red or green'\n\n\n\n##\n# f-string on multiple lines. \n# 3. Use  brackets ()\n(f\"{r}\"\n\" or \"\nf\"{g}\")\n\n'red or green'\n\n\n\n##\n# you can also compute an expression in an f-string\nf\"{ 40 + 2}\"\n\n'42'\n\n\n\n##\n# functions can also be called from inside an f-string\nf\"This is in CAPS: { str.upper(r) }\"\n\n# same as above\nf\"This is in CAPS: { r.upper() }\"\n\n'This is in CAPS: RED'\n\n\n\nf-string: Padding the output\n\n##\n# Inside f-string, passing an integer after ':' will cause that field to be a minimum number of characters wide. \n# This is useful for making columns line up.\n\ngroups = {\n    'small': 100,\n    'medium': 100100,\n    'large': 100100100\n}\n\nfor group, value in groups.items():\n    print(f\"{value:10} ==> {group:20}\")\n\nprint(f\"{'****'*10}\") # another nice trick\n\nfor group, value in groups.items():\n    print(f\"{group:10} ==> {value:20}\")\n\n       100 ==> small               \n    100100 ==> medium              \n 100100100 ==> large               \n****************************************\nsmall      ==>                  100\nmedium     ==>               100100\nlarge      ==>            100100100\n\n\n\n\nf-string: Binary and hexadecimal format\n\n##\n# you can convert integers to binary and hexadecimal format\nprint( f\"5 in binary {5:b}\" )\n\nprint( f\"5 in hexadecimal {5:#b}\" )\n\n5 in binary 101\n5 in hexadecimal 0b101\n\n\n\n\nf-string: Controlling the decimal places\n\nimport math\nprint(f'The value of pi is approximately (no formatting) {math.pi}')\nprint(f'The value of pi is approximately {math.pi :.3f}')\n\nThe value of pi is approximately (no formatting) 3.141592653589793\nThe value of pi is approximately 3.142\n\n\n\n\nf-string: Putting commas in numerical output\n\nnum = 3214298342.234\nf\"{num:,}\"\n\n'3,214,298,342.234'"
  },
  {
    "objectID": "posts/2022-02-22-aws-dns-records.html#domain-name",
    "href": "posts/2022-02-22-aws-dns-records.html#domain-name",
    "title": "AWS DNS Records - A, CNAME, ALIAS, & MX",
    "section": "Domain Name",
    "text": "Domain Name\n\n\nDomain + TLD = Domain Name\nWhen you buy a ‘domain’ from a a registrar or reseller, you buy the rights to a specific domain name (example.com), and any subdomains you want to create (my-site.example.com, mail.example.com, etc).\nThe domain name (example.com) is also called the apex, root or naked domain name.\nExamples of protocol are http, ftp, TCP, UDP, FTP, SMTP etc.\nExamples of top level domains are .org, .net, .com, .ai etc."
  },
  {
    "objectID": "posts/2022-02-22-aws-dns-records.html#example-use-cases",
    "href": "posts/2022-02-22-aws-dns-records.html#example-use-cases",
    "title": "AWS DNS Records - A, CNAME, ALIAS, & MX",
    "section": "Example use cases",
    "text": "Example use cases\n\n\nYou can point your root domain name example.com to an Elastic IP Address 192.0.2.23\n\n\nWe can also map EC2 instances IPv4 Public IP Address to an A record. But this is not recommended as EC2 instances public IP addresses change when you stop/start your server. We should always use Elastic IP addresses instead."
  },
  {
    "objectID": "posts/2022-02-22-aws-dns-records.html#example-use-cases-1",
    "href": "posts/2022-02-22-aws-dns-records.html#example-use-cases-1",
    "title": "AWS DNS Records - A, CNAME, ALIAS, & MX",
    "section": "Example use cases",
    "text": "Example use cases\n\n\n\nNAME\nTYPE\nVALUE\n\n\n\n\nwww.example.com\nCNAME\nexample.com\n\n\nexample.com\nA\n192.0.2.23\n\n\n\n\n\nAn A record for example.com (root domain) points to server IP address\n\n\nA CNAME record points www.example.com to example.com\n\n\nNow if the IP address of your server has changed you will have to update it only at one place A record. www.example.com and example.com will automatically inherit the changes.\nIMPORTANT\n\n\nCNAME entry for the root domain is not allowed.\n\n\n\n\n\n\n\n\n\n\nNAME\nTYPE\nVALUE\n\n\n\n\n example.com\nCNAME\napp.example.com\n\n\napp.example.com\nA\n192.0.2.23"
  },
  {
    "objectID": "posts/2022-02-23-aws-policy-types.html#identity-based-policies",
    "href": "posts/2022-02-23-aws-policy-types.html#identity-based-policies",
    "title": "AWS IAM Policy Types",
    "section": "Identity-based policies",
    "text": "Identity-based policies\nIdentity-based policies – Attach managed and inline policies to IAM identities (users, groups to which users belong, or roles). Identity-based policies grant permissions to an identity."
  },
  {
    "objectID": "posts/2022-02-23-aws-policy-types.html#resource-based-policies",
    "href": "posts/2022-02-23-aws-policy-types.html#resource-based-policies",
    "title": "AWS IAM Policy Types",
    "section": "Resource-based policies",
    "text": "Resource-based policies\nResource-based policies – Attach inline policies to resources. The most common examples of resource-based policies are Amazon S3 bucket policies and IAM role trust policies. Resource-based policies grant permissions to the principal that is specified in the policy. Principals can be in the same account as the resource or in other accounts.\n\nIdentity vs Resource based policy\n  * Identity-based policies are applied to IAM identities, and grant them access to AWS resources. * Resource-based policies are applied to AWS resources, and they grant access to Principals (IAM identities, and applications)"
  },
  {
    "objectID": "posts/2022-02-23-aws-policy-types.html#permissions-boundaries",
    "href": "posts/2022-02-23-aws-policy-types.html#permissions-boundaries",
    "title": "AWS IAM Policy Types",
    "section": "Permissions boundaries",
    "text": "Permissions boundaries\nPermissions boundaries – Use a customer or AWS managed policy as the permissions boundary for an IAM entity (user or role). That policy defines the maximum permissions that the identity-based policies can grant to an entity, but does not grant permissions. Permissions boundaries do not define the maximum permissions that a resource-based policy can grant to an entity."
  },
  {
    "objectID": "posts/2022-02-23-aws-policy-types.html#organizations-scps",
    "href": "posts/2022-02-23-aws-policy-types.html#organizations-scps",
    "title": "AWS IAM Policy Types",
    "section": "Organizations SCPs",
    "text": "Organizations SCPs\nOrganizations SCPs – Use an AWS Organizations service control policy (SCP) to define the maximum permissions for account members of an organization or organizational unit (OU). SCPs limit permissions that identity-based policies or resource-based policies grant to entities (users or roles) within the account, but do not grant permissions.\n\nPermission boundries vs Organization SCP\n\nBoth permission boundries and SCP only limit permissions. They don’t give any permissions.\nPermission boundries limits permissions of identity-based policies only.\nSCP limits permissions on both identity and resource based policies."
  },
  {
    "objectID": "posts/2022-02-23-aws-policy-types.html#access-control-lists-acls",
    "href": "posts/2022-02-23-aws-policy-types.html#access-control-lists-acls",
    "title": "AWS IAM Policy Types",
    "section": "Access control lists (ACLs)",
    "text": "Access control lists (ACLs)\nAccess control lists (ACLs) – Use ACLs to control which principals in other accounts can access the resource to which the ACL is attached. ACLs are similar to resource-based policies, although they are the only policy type that does not use the JSON policy document structure. ACLs are cross-account permissions policies that grant permissions to the specified principal. ACLs cannot grant permissions to entities within the same account.\n\nACL vs Resource based policy\n\nResource based policies can grant permission to entities in same or different account\nACL can only grant permissions to entities in different account\nOnly a few resources support ACL including AWS Amazon S3, AWS WAF, and Amazon VPC. It is a legacy IAM policy type and AWS recommends not to use it."
  },
  {
    "objectID": "posts/2022-02-23-aws-policy-types.html#session-policies",
    "href": "posts/2022-02-23-aws-policy-types.html#session-policies",
    "title": "AWS IAM Policy Types",
    "section": "Session policies",
    "text": "Session policies\nSession policies – Pass advanced session policies when you use the AWS CLI or AWS API to assume a role or a federated user. Session policies limit the permissions that the role or user’s identity-based policies grant to the session. Session policies limit permissions for a created session, but do not grant permissions. For more information, see Session Policies."
  },
  {
    "objectID": "posts/2022-02-28-aws-cloudformation-template.html#awstemplateformatversion-optional",
    "href": "posts/2022-02-28-aws-cloudformation-template.html#awstemplateformatversion-optional",
    "title": "AWS CloudFormation Template, Functions, and Commands",
    "section": "AWSTemplateFormatVersion (optional)",
    "text": "AWSTemplateFormatVersion (optional)\nThe AWS CloudFormation template version that the template conforms to. ### Syntax\nAWSTemplateFormatVersion: \"2010-09-09\""
  },
  {
    "objectID": "posts/2022-02-28-aws-cloudformation-template.html#description-optional",
    "href": "posts/2022-02-28-aws-cloudformation-template.html#description-optional",
    "title": "AWS CloudFormation Template, Functions, and Commands",
    "section": "Description (optional)",
    "text": "Description (optional)\nA text string that describes the template. This section must always follow the template format version section. ### Syntax\nDescription: >\n  Here are some\n  details about\n  the template."
  },
  {
    "objectID": "posts/2022-02-28-aws-cloudformation-template.html#metadata-optional",
    "href": "posts/2022-02-28-aws-cloudformation-template.html#metadata-optional",
    "title": "AWS CloudFormation Template, Functions, and Commands",
    "section": "Metadata (optional)",
    "text": "Metadata (optional)\nObjects that provide additional information about the template.\n\nDifference between Metadata and Description is that some cloudformation features can refer to the objects that are defined in Metadata section. For example, you can use a metadata key AWS::CloudFormation::Interface to define how parameters are grouped and sorted on AWS cloudformation console. By default, cloudformation console alphbetically sorts the parameters by their logical ID.\nAWS strongly recommends not to use this section for storing sensitive information such as passwords or secrets.\n\n\nSyntax\nMetadata:\n  Instances:\n    Description: \"Information about the instances\"\n  Databases: \n    Description: \"Information about the databases\""
  },
  {
    "objectID": "posts/2022-02-28-aws-cloudformation-template.html#parameters-optional",
    "href": "posts/2022-02-28-aws-cloudformation-template.html#parameters-optional",
    "title": "AWS CloudFormation Template, Functions, and Commands",
    "section": "Parameters (optional)",
    "text": "Parameters (optional)\nParameters enable you to input custom values to your template each time you create or update a stack. You can refer to parameters from the Resources and Outputs sections of the template using Ref intrinsic function.\nCloudFormation currently supports the following parameter types\n\nString – A literal string\nNumber – An integer or float\nList<Number> – An array of integers or floats\nCommaDelimitedList – An array of literal strings that are separated by commas\nAWS::EC2::KeyPair::KeyName – An Amazon EC2 key pair name\nAWS::EC2::SecurityGroup::Id – A security group ID\nAWS::EC2::Subnet::Id – A subnet ID\nAWS::EC2::VPC::Id – A VPC ID\nList<AWS::EC2::VPC::Id> – An array of VPC IDs\nList<AWS::EC2::SecurityGroup::Id> – An array of security group IDs\nList<AWS::EC2::Subnet::Id> – An array of subnet IDs\n\n\nSyntax\nThe following example declares a parameter named InstanceTypeParameter. This parameter lets you specify the Amazon EC2 instance type for the stack to use when you create or update the stack.\nNote that InstanceTypeParameter has a default value of t2.micro. This is the value that AWS CloudFormation will use to provision the stack unless another value is provided.\nParameters:\n  InstanceTypeParameter:\n    Type: String\n    Default: t2.micro\n    AllowedValues:\n      - t2.micro\n      - m1.small\n      - m1.large\n    Description: Enter t2.micro, m1.small, or m1.large. Default is t2.micro.\n\nReferencing a parameter in template (Ref function)\nIn the following example, the InstanceType property of the EC2 instance resource references the InstanceTypeParameter parameter value.\nEc2Instance:\n  Type: AWS::EC2::Instance\n  Properties:\n    InstanceType:\n      Ref: InstanceTypeParameter\n    ImageId: ami-0ff8a91507f77f867"
  },
  {
    "objectID": "posts/2022-02-28-aws-cloudformation-template.html#rules-optional",
    "href": "posts/2022-02-28-aws-cloudformation-template.html#rules-optional",
    "title": "AWS CloudFormation Template, Functions, and Commands",
    "section": "Rules (optional)",
    "text": "Rules (optional)\nValidates a parameter or a combination of parameters that are passed to a template during a stack creation or stack update.\nYou can use the following rule-specific intrinsic functions to define rule conditions and assertions: * Fn::And * Fn::Contains * Fn::EachMemberEquals * Fn::EachMemberIn * Fn::Equals * Fn::If * Fn::Not * Fn::Or * Fn::RefAll * Fn::ValueOf * Fn::ValueOfAll\n\nSyntax\nIn the following example, the rule checks the value of the InstanceType parameter. The user must specify a1.medium, if the value of the environment parameter is test.\nRules:\n  testInstanceType:\n    RuleCondition: !Equals \n      - !Ref Environment\n      - test\n    Assertions:\n      - Assert:\n          'Fn::Contains':\n            - - a1.medium\n            - !Ref InstanceType\n        AssertDescription: 'For a test environment, the instance type must be a1.medium'"
  },
  {
    "objectID": "posts/2022-02-28-aws-cloudformation-template.html#mappings-optional",
    "href": "posts/2022-02-28-aws-cloudformation-template.html#mappings-optional",
    "title": "AWS CloudFormation Template, Functions, and Commands",
    "section": "Mappings (optional)",
    "text": "Mappings (optional)\nThe optional Mappings section matches a key to a corresponding set of named values similar to a lookup table. For example, if you want to set values based on a region, you can create a mapping that uses the region name as a key and contains the values you want to specify for each specific region. You use the Fn::FindInMap intrinsic function in the Resources and Outputs to retrieve values in a map. Note that you can’t include parameters, pseudo parameters, or intrinsic functions in the Mappings section.\n\nFn::FindInMap\nThe intrinsic function Fn::FindInMap returns the value corresponding to keys in a two-level map that’s declared in the Mappings section.\nSyntax for the short form:\n!FindInMap [ MapName, TopLevelKey, SecondLevelKey ] \nParameters * MapName * The logical name of a mapping declared in the Mappings section that contains the keys and values. * TopLevelKey * The top-level key name. Its value is a list of key-value pairs. * SecondLevelKey * The second-level key name, which is set to one of the keys from the list assigned to TopLevelKey.\nA more concrete example\nMappings: \n  RegionMap: \n    us-east-1: \n      HVM64: \"ami-0ff8a91507f77f867\"\n      HVMG2: \"ami-0a584ac55a7631c0c\"\nResources: \n  myEC2Instance: \n    Type: \"AWS::EC2::Instance\"\n    Properties: \n      ImageId: !FindInMap\n        - RegionMap\n        - !Ref 'AWS::Region' # us-east-1\n        - HVM64\n      InstanceType: m1.small"
  },
  {
    "objectID": "posts/2022-02-28-aws-cloudformation-template.html#conditions-optional",
    "href": "posts/2022-02-28-aws-cloudformation-template.html#conditions-optional",
    "title": "AWS CloudFormation Template, Functions, and Commands",
    "section": "Conditions (optional)",
    "text": "Conditions (optional)\nConditions that control whether certain resources are created or whether certain resource properties are assigned a value during stack creation or update. For example, you could conditionally create a resource that depends on whether the stack is for a production or test environment.\nConditions are defined in Conditions section, and are then applied in following sections. * Parameters * Resources * Outputs\nYou can use following intrinsic functions to define your conditions * Fn::And * Fn::Equals * Fn::If * Fn::Not * Fn::Or\n\nSyntax\nConditions:\n  Logical ID:\n    Intrinsic function\nA more concrete example\nAWSTemplateFormatVersion: 2010-09-09\nParameters:\n  EnvType:\n    Description: Environment type.\n    Default: test\n    Type: String\n    AllowedValues:\n      - prod\n      - test\n    ConstraintDescription: must specify prod or test.\nConditions:\n  CreateProdResources: !Equals \n    - !Ref EnvType\n    - prod\nResources:\n  EC2Instance:\n    Type: 'AWS::EC2::Instance'\n    Properties:\n      ImageId: ami-0ff8a91507f77f867\n  MountPoint:\n    Type: 'AWS::EC2::VolumeAttachment'\n    Condition: CreateProdResources\n    Properties:\n      InstanceId: !Ref EC2Instance\n      VolumeId: !Ref NewVolume\n      Device: /dev/sdh\n  NewVolume:\n    Type: 'AWS::EC2::Volume'\n    Condition: CreateProdResources\n    Properties:\n      Size: 100\n      AvailabilityZone: !GetAtt \n        - EC2Instance\n        - AvailabilityZone\n\n\nDifference between Rules and Conditions usage?\n\nRules are used to evaluate the input given by the user in Parameters\nConditions turn come after all rules have been evaluated\nConditions are not limited to Parameters and can also work with Resources and Outputs"
  },
  {
    "objectID": "posts/2022-02-28-aws-cloudformation-template.html#transform-optional",
    "href": "posts/2022-02-28-aws-cloudformation-template.html#transform-optional",
    "title": "AWS CloudFormation Template, Functions, and Commands",
    "section": "Transform (optional)",
    "text": "Transform (optional)\nFor serverless applications (also referred to as Lambda-based applications), specifies the version of the AWS Serverless Application Model (AWS SAM) to use. When you specify a transform, you can use AWS SAM syntax to declare resources in your template. The model defines the syntax that you can use and how it's processed.\nYou can also use AWS::Include transforms to work with template snippets that are stored separately from the main AWS CloudFormation template. You can store your snippet files in an Amazon S3 bucket and then reuse the functions across multiple templates.\n\nSyntax\nTransform:\n  - MyMacro\n  - 'AWS::Serverless'\n\nAWS::Include transform\nUse the AWS::Include transform, which is a macro hosted by AWS CloudFormation, to insert boilerplate content into your templates. The AWS::Include transform lets you create a reference to a template snippet in an Amazon S3 bucket. The AWS::Include function behaves similarly to an include, copy, or import directive in programming languages.\n\nExample\nTransform:\n  Name: 'AWS::Include'\n  Parameters:\n    Location: 's3://MyAmazonS3BucketName/MyFileName.yaml'"
  },
  {
    "objectID": "posts/2022-02-28-aws-cloudformation-template.html#resources-required",
    "href": "posts/2022-02-28-aws-cloudformation-template.html#resources-required",
    "title": "AWS CloudFormation Template, Functions, and Commands",
    "section": "Resources (required)",
    "text": "Resources (required)\nSpecifies the stack resources and their properties, such as an Amazon Elastic Compute Cloud instance or an Amazon Simple Storage Service bucket. You can refer to resources in the Resources and Outputs sections of the template.\n\nSyntax\nResources:\n  Logical ID:\n    Type: Resource type\n    Properties:\n      Set of properties\nA more concrete example\nResources:\n  MyEC2Instance:\n    Type: \"AWS::EC2::Instance\"\n    Properties:\n      ImageId: \"ami-0ff8a91507f77f867\""
  },
  {
    "objectID": "posts/2022-02-28-aws-cloudformation-template.html#outputs-optional",
    "href": "posts/2022-02-28-aws-cloudformation-template.html#outputs-optional",
    "title": "AWS CloudFormation Template, Functions, and Commands",
    "section": "Outputs (optional)",
    "text": "Outputs (optional)\nThe optional Outputs section declares output values that you can import into other stacks (to create cross-stack references), return in response (to describe stack calls), or view on the AWS CloudFormation console. For example, you can output the S3 bucket name from a stack to make the bucket easier to find.\nNotes * You can declare a maximum of 200 outputs in a template. * AWS strongly recommend you don’t use this section to output sensitive information, such as passwords or secrets * Output values are available after the stack operation is complete. Stack output values aren’t available when a stack status is in any of the IN_PROGRESS status. * AWS also does not recommend establishing dependencies between a service runtime and the stack output value because output values might not be available at all times.\n\nSyntax\nOutputs:\n  Logical ID:\n    Description: Information about the value\n    Value: Value to return\n    Export:\n      Name: Name of resource to export\nA more concrete example where certain values are shown as output at the end of stack creation.\nOutputs:\n  BackupLoadBalancerDNSName:\n    Description: The DNSName of the backup load balancer\n    Value: !GetAtt BackupLoadBalancer.DNSName\n    Condition: CreateProdResources\n  InstanceID:\n    Description: The Instance ID\n    Value: !Ref EC2Instance\nFor Cross-Stack output use Export tag. Values outputed with “Export” tag can be imported in other stacks “in the same region”. Then, use the Fn::ImportValue intrinsic function to import the value in another stack “in the same region”.\nOutputs:\n  StackVPC:\n    Description: The ID of the VPC\n    Value: !Ref MyVPC\n    Export:\n      Name: !Sub \"${AWS::StackName}-VPCID\""
  },
  {
    "objectID": "posts/2022-02-28-aws-cloudformation-template.html#fngetatt",
    "href": "posts/2022-02-28-aws-cloudformation-template.html#fngetatt",
    "title": "AWS CloudFormation Template, Functions, and Commands",
    "section": "Fn::GetAtt",
    "text": "Fn::GetAtt\nThe Fn::GetAtt intrinsic function returns the value of an attribute from a resource in the template.\n\nSyntax\n!GetAtt logicalNameOfResource.attributeName\n\nlogicalNameOfResource\n\nThe logical name (also called logical ID) of the resource that contains the attribute that you want.\n\nattributeName\n\nThe name of the resource-specific attribute whose value you want. See the resource’s reference page for details about the attributes available for that resource type.\n\nReturn value\n\nThe attribute value.\n\n\nA more concrete example\n!GetAtt myELB.DNSName\nNotes: * For the Fn::GetAtt logical resource name, you can’t use functions. You must specify a string that’s a resource’s logical ID. * For the Fn::GetAtt attribute name, you can use the Ref function."
  },
  {
    "objectID": "posts/2022-02-28-aws-cloudformation-template.html#fnimportvalue",
    "href": "posts/2022-02-28-aws-cloudformation-template.html#fnimportvalue",
    "title": "AWS CloudFormation Template, Functions, and Commands",
    "section": "Fn::ImportValue",
    "text": "Fn::ImportValue\nThe intrinsic function Fn::ImportValue returns the value of an output exported by another stack. You typically use this function to create cross-stack references.\nNotes: * For each AWS account, Export names must be unique within a region. * You can’t create cross-stack references across regions. You can use the intrinsic function Fn::ImportValue to import only values that have been exported within the same region. * You can’t delete a stack if another stack references one of its outputs. * You can’t modify or remove an output value that is referenced by another stack.\n\nSyntax\n!ImportValue sharedValueToImport\nA more concrete example.\nFn::ImportValue:\n  !Sub \"${NetworkStackName}-SecurityGroupID\""
  },
  {
    "objectID": "posts/2022-02-28-aws-cloudformation-template.html#fnsub",
    "href": "posts/2022-02-28-aws-cloudformation-template.html#fnsub",
    "title": "AWS CloudFormation Template, Functions, and Commands",
    "section": "Fn::Sub",
    "text": "Fn::Sub\nThe intrinsic function Fn::Sub substitutes variables in an input string with values that you specify. In your templates, you can use this function to construct commands or outputs that include values that aren’t available until you create or update a stack.\n\nSyntax\n!Sub\n  - String\n  - VarName: VarValue\n\nParameters\n\nString\n\nA string with variables that AWS CloudFormation substitutes with their associated values at runtime. Write variables as ${MyVarName}. Variables can be template parameter names, resource logical IDs, resource attributes, or a variable in a key-value map.\n\nVarName\n\nThe name of a variable that you included in the String parameter.\n\nVarValue\n\nThe value that CloudFormation substitutes for the associated variable name at runtime.\n\n\nA more concrete example. The following example uses a mapping to substitute the ${Domain} variable with the resulting value from the Ref function.\nName: !Sub \n  - 'www.${Domain}'\n  - Domain: !Ref RootDomainName"
  },
  {
    "objectID": "posts/2022-03-04-sklearn-pipeline-deep-dive.html#about",
    "href": "posts/2022-03-04-sklearn-pipeline-deep-dive.html#about",
    "title": "Sklearn Pipeline and Transformers Deep Dive",
    "section": "About",
    "text": "About\nThis notebook shows various ways to work with Skearn Pipelines. * We will start with some of the limitations of pipelines and how to overcome them * We will discuss getting a dataframe from a pipeline instead of a NumPy array, and the benefits of this approach * We will learn how to use CustomTransformer and a FunctionTransformer * We will also build a custom transformer to do some feature engineering * Along the way, we will also see how to avoid common mistakes while creating pipelines"
  },
  {
    "objectID": "posts/2022-03-04-sklearn-pipeline-deep-dive.html#setup",
    "href": "posts/2022-03-04-sklearn-pipeline-deep-dive.html#setup",
    "title": "Sklearn Pipeline and Transformers Deep Dive",
    "section": "Setup",
    "text": "Setup\n\nEnvironment Details\n\n\nCode\nfrom platform import python_version\nimport sklearn, numpy, matplotlib, pandas\n\nprint(\"python==\" + python_version())\nprint(\"sklearn==\" + sklearn.__version__)\nprint(\"numpy==\" + numpy.__version__)\nprint(\"pandas==\" + pandas.__version__)\nprint(\"matplotlib==\" + matplotlib.__version__)\n\n\npython==3.8.8\nsklearn==1.0.2\nnumpy==1.20.1\npandas==1.2.3\nmatplotlib==3.5.1\n\n\n\n\nLoading Data\nFor this example we will use the original Titanic dataset, describing the survival status of individual passengers on the Titanic ship.\nSome notes from original source: * The variables on our extracted dataset are ‘pclass’, ‘name’, ‘sex’, ‘age’, ‘sibsp’, ‘parch’, ‘ticket’, ‘fare’,‘cabin’, ‘embarked’, ‘boat’, ‘body’, and ‘home.dest’. * pclass refers to passenger class (1st, 2nd, 3rd), and is a proxy for socio-economic class. * Age is in years, and some infants had fractional values. * sibsp = Number of Siblings/Spouses aboard * parch = Number of Parents/Children aboard * The target is either a person survived or not (1 or 0)\nImportant note: The purpose of this notebook is not to train a best model on titanic data, but to understand the working of Sklearn pipeline and transformers. So please be mindful of that.\n\nfrom sklearn.datasets import fetch_openml\nfrom sklearn.model_selection import train_test_split\nimport numpy as np\n\nnp.random.seed(42) # for consistency\n\n# Load data from https://www.openml.org/d/40945\nX, y = fetch_openml(\"titanic\", version=1, as_frame=True, return_X_y=True)\n\nX.head()\n\n\n\n\n\n  \n    \n      \n      pclass\n      name\n      sex\n      age\n      sibsp\n      parch\n      ticket\n      fare\n      cabin\n      embarked\n      boat\n      body\n      home.dest\n    \n  \n  \n    \n      0\n      1.0\n      Allen, Miss. Elisabeth Walton\n      female\n      29.0000\n      0.0\n      0.0\n      24160\n      211.3375\n      B5\n      S\n      2\n      NaN\n      St Louis, MO\n    \n    \n      1\n      1.0\n      Allison, Master. Hudson Trevor\n      male\n      0.9167\n      1.0\n      2.0\n      113781\n      151.5500\n      C22 C26\n      S\n      11\n      NaN\n      Montreal, PQ / Chesterville, ON\n    \n    \n      2\n      1.0\n      Allison, Miss. Helen Loraine\n      female\n      2.0000\n      1.0\n      2.0\n      113781\n      151.5500\n      C22 C26\n      S\n      None\n      NaN\n      Montreal, PQ / Chesterville, ON\n    \n    \n      3\n      1.0\n      Allison, Mr. Hudson Joshua Creighton\n      male\n      30.0000\n      1.0\n      2.0\n      113781\n      151.5500\n      C22 C26\n      S\n      None\n      135.0\n      Montreal, PQ / Chesterville, ON\n    \n    \n      4\n      1.0\n      Allison, Mrs. Hudson J C (Bessie Waldo Daniels)\n      female\n      25.0000\n      1.0\n      2.0\n      113781\n      151.5500\n      C22 C26\n      S\n      None\n      NaN\n      Montreal, PQ / Chesterville, ON\n    \n  \n\n\n\n\n\n##\n# let's check the frequency of missing values in each feature\nX.isnull().sum().sort_values(ascending=False)\n\nbody         1188\ncabin        1014\nboat          823\nhome.dest     564\nage           263\nembarked        2\nfare            1\npclass          0\nname            0\nsex             0\nsibsp           0\nparch           0\nticket          0\ndtype: int64\n\n\n\n##\n# let's drop top 4 features with highest percentage of missing data\n# This step is done to make our working with pipeline simpler and easier to understand\n\nX.drop(['body', 'cabin', 'boat', 'home.dest'], axis=1, inplace=True)\nX_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2)\nX_train.head()\n\n\n\n\n\n  \n    \n      \n      pclass\n      name\n      sex\n      age\n      sibsp\n      parch\n      ticket\n      fare\n      embarked\n    \n  \n  \n    \n      999\n      3.0\n      McCarthy, Miss. Catherine 'Katie'\n      female\n      NaN\n      0.0\n      0.0\n      383123\n      7.7500\n      Q\n    \n    \n      392\n      2.0\n      del Carlo, Mrs. Sebastiano (Argenia Genovesi)\n      female\n      24.0\n      1.0\n      0.0\n      SC/PARIS 2167\n      27.7208\n      C\n    \n    \n      628\n      3.0\n      Andersson, Miss. Sigrid Elisabeth\n      female\n      11.0\n      4.0\n      2.0\n      347082\n      31.2750\n      S\n    \n    \n      1165\n      3.0\n      Saad, Mr. Khalil\n      male\n      25.0\n      0.0\n      0.0\n      2672\n      7.2250\n      C\n    \n    \n      604\n      3.0\n      Abelseth, Miss. Karen Marie\n      female\n      16.0\n      0.0\n      0.0\n      348125\n      7.6500\n      S"
  },
  {
    "objectID": "posts/2022-03-04-sklearn-pipeline-deep-dive.html#some-terminology-first",
    "href": "posts/2022-03-04-sklearn-pipeline-deep-dive.html#some-terminology-first",
    "title": "Sklearn Pipeline and Transformers Deep Dive",
    "section": "Some Terminology First",
    "text": "Some Terminology First\n\nDatasets\nScikit-learn deals with learning information from one or more datasets that are represented as 2D arrays. They can be understood as a list of multi-dimensional observations. We say that the first axis of these arrays is the samples axis, while the second is the features axis. > (n_samples, n_features)\n\n##\n# for our titanic dataset:\n# n_samples = 1309\n# n_features = 9\nX.shape\n\n(1309, 9)\n\n\n\n\nEstimator\nAn estimator is any object that learns from data; it may be a classification, regression or clustering algorithm or a transformer that extracts/filters useful features from raw data.\nAll estimator objects expose a fit method that takes a dataset (usually a 2-d array) > estimator.fit(data)\n\n\nTransformer\nAn estimator supporting transform and/or fit_transform methods.\nA transformer, transforms the input, usually only X, into some transformed space. Output is an array or sparse matrix of length n_samples and with the number of columns fixed after fitting.\n\n\nFit\nThe fit method is provided on every estimator. It usually takes some samples X, targets y if the model is supervised, and potentially other sample properties such as sample_weight.\nIt should: * clear any prior attributes stored on the estimator, unless warm_start is used * validate and interpret any parameters, ideally raising an error if invalid * validate the input data * estimate and store model attributes from the estimated parameters and provided data; and * return the now fitted estimator to facilitate method chaining\nNote: * Fitting = Calling fit (or fit_transform, fit_predict) method on an estimator. * Fitted = The state of an estimator after fitting."
  },
  {
    "objectID": "posts/2022-03-04-sklearn-pipeline-deep-dive.html#sklearn-pipeline",
    "href": "posts/2022-03-04-sklearn-pipeline-deep-dive.html#sklearn-pipeline",
    "title": "Sklearn Pipeline and Transformers Deep Dive",
    "section": "Sklearn Pipeline",
    "text": "Sklearn Pipeline\n\nclass sklearn.pipeline.Pipeline(steps, *, memory=None, verbose=False)\n\nIt is a pipeline of transformers with a final estimator.\nIt sequentially applies a list of transforms and a final estimator. Intermediate steps of the pipeline must be transforms, that is, they must implement fit and transform methods. The final estimator only needs to implement fit.\nLets create a simple pipeline to better understand its componets. Steps in our pipeline will be * replace missing values using the mean along each numerical feature column; and * then scale them\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler\n\npipe = Pipeline(steps=[\n    ('imputer', SimpleImputer()),\n    ('scaler', StandardScaler())\n])\n\n# our first pipeline has been initialized\npipe\n\nPipeline(steps=[('imputer', SimpleImputer()), ('scaler', StandardScaler())])\n\n\nWe can also visualize the pipeline as a diagram. It has two steps: imputer and scaler in sequence.\n\nfrom sklearn import set_config\nset_config(display=\"diagram\")\n\npipe\n\nPipeline(steps=[('imputer', SimpleImputer()), ('scaler', StandardScaler())])Please rerun this cell to show the HTML repr or trust the notebook.PipelinePipeline(steps=[('imputer', SimpleImputer()), ('scaler', StandardScaler())])SimpleImputerSimpleImputer()StandardScalerStandardScaler()\n\n\nnow lets call fit_transform method to run this pipeline, and preprocess our loaded data\n\n#collapse-output\npipe.fit_transform(X_train, y_train)\n\nValueError: Cannot use mean strategy with non-numeric data:\ncould not convert string to float: \"McCarthy, Miss. Catherine 'Katie'\"\n\n\nAaargh! this is not what we intended. Let us try to understand why our pipeline did not work and then fix it. The exception message says:\nValueError: Cannot use mean strategy with non-numeric data: could not convert string to float: \"McCarthy, Miss. Catherine 'Katie'\nFrom the error message we can deduce that Pipeline is trying to apply its transformers on all columns in the dataset. This was not our intention, as we wanted to apply the transformers to numeric data only. Let’s limit our simple pipeline to numerical columns and run again.\n\nnum_cols = ['age', 'fare']\npipe.fit_transform(X_train[num_cols], y_train)\n\narray([[ 0.        , -0.49963779],\n       [-0.43641134, -0.09097855],\n       [-1.44872891, -0.01824953],\n       ...,\n       [-0.98150542, -0.49349894],\n       [-0.82576425, -0.44336498],\n       [-0.59215251, -0.49349894]])\n\n\nAlright, our pipeline has run now and we can also observe a few outcomes. * When we apply a pipeline to a dataset it will run transformers to all features in the dataset. * Output from one transformer will be passed on to the next one until we reach the end of the pipeline * If we want to apply different transformers for numerical and categorical features (heterogeneous data) then the pipeline will not work for us. We would have to create separate pipelines for the different feature sets and then join the output.\nTo overcome the limitation of a pipeline for heterogeneous data, Sklearn recommends using ColumnTransformer. With ColumnTransformer we can provide column names against the transformers on which we want to apply them."
  },
  {
    "objectID": "posts/2022-03-04-sklearn-pipeline-deep-dive.html#columntransformer",
    "href": "posts/2022-03-04-sklearn-pipeline-deep-dive.html#columntransformer",
    "title": "Sklearn Pipeline and Transformers Deep Dive",
    "section": "ColumnTransformer",
    "text": "ColumnTransformer\nLet’s see our first ColumnTransformer in action.\n\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder\n\n# Note the sequence when creating a ColumnTransformer\n# 1. a name for the transformer\n# 2. the transformer\n# 3. the column names\n\npipe = ColumnTransformer([\n    ('standardscaler', StandardScaler(), ['age', 'fare'] ),\n    ('onehotencoder', OneHotEncoder(), ['sex'])\n    ])\n\npipe\n\nColumnTransformer(transformers=[('standardscaler', StandardScaler(),\n                                 ['age', 'fare']),\n                                ('onehotencoder', OneHotEncoder(), ['sex'])])Please rerun this cell to show the HTML repr or trust the notebook.ColumnTransformerColumnTransformer(transformers=[('standardscaler', StandardScaler(),\n                                 ['age', 'fare']),\n                                ('onehotencoder', OneHotEncoder(), ['sex'])])standardscaler['age', 'fare']StandardScalerStandardScaler()onehotencoder['sex']OneHotEncoderOneHotEncoder()\n\n\n\n##\n# Let's execute our pipeline to see the output.\npipe.fit_transform(X_train, y_train)\n\narray([[        nan, -0.49939913,  1.        ,  0.        ],\n       [-0.39043136, -0.09093509,  1.        ,  0.        ],\n       [-1.2960919 , -0.01824081,  1.        ,  0.        ],\n       ...,\n       [-0.87809473, -0.49326321,  0.        ,  1.        ],\n       [-0.73876234, -0.4431532 ,  0.        ,  1.        ],\n       [-0.52976375, -0.49326321,  0.        ,  1.        ]])\n\n\n\nget_feature_names_out(input_features=None)\nAt this point I will also introduce a very useful function get_feature_names_out(input_features=None). Using this method we can get output feature names as well.\n\npipe.get_feature_names_out()\n\narray(['standardscaler__age', 'standardscaler__fare',\n       'onehotencoder__sex_female', 'onehotencoder__sex_male'],\n      dtype=object)\n\n\nNotice the output * Output feature names appear as <transformer_name>__<feature_name> * For OneHotEncoded feature “sex”, output feature names have the label attached to them\n\n\nmake_column_transformer\nSklean also provides a wrapper function for ColumnTransformer where we don’t have to provide names for the transformers.\n\nfrom sklearn.compose import make_column_transformer\n\n# Note the sequence when using make_column_transformer\n# 1. the transformer\n# 2. the column names\n\npipe = make_column_transformer(\n    (StandardScaler(), ['age', 'fare']),\n    (OneHotEncoder(), ['sex'] ),\n    verbose_feature_names_out=False # to keep output feature names simple\n)\n\npipe\n\nColumnTransformer(transformers=[('standardscaler', StandardScaler(),\n                                 ['age', 'fare']),\n                                ('onehotencoder', OneHotEncoder(), ['sex'])],\n                  verbose_feature_names_out=False)Please rerun this cell to show the HTML repr or trust the notebook.ColumnTransformerColumnTransformer(transformers=[('standardscaler', StandardScaler(),\n                                 ['age', 'fare']),\n                                ('onehotencoder', OneHotEncoder(), ['sex'])],\n                  verbose_feature_names_out=False)standardscaler['age', 'fare']StandardScalerStandardScaler()onehotencoder['sex']OneHotEncoderOneHotEncoder()\n\n\n\n##\n# Let's execute our pipeline.\npipe.fit_transform(X_train, y_train)\n\narray([[        nan, -0.49939913,  1.        ,  0.        ],\n       [-0.39043136, -0.09093509,  1.        ,  0.        ],\n       [-1.2960919 , -0.01824081,  1.        ,  0.        ],\n       ...,\n       [-0.87809473, -0.49326321,  0.        ,  1.        ],\n       [-0.73876234, -0.4431532 ,  0.        ,  1.        ],\n       [-0.52976375, -0.49326321,  0.        ,  1.        ]])\n\n\n\n##\n# notice the feature names this time. they are shorter.\n# we have used attribute \"verbose_feature_names_out=False\" in our pipeline above.\npipe.get_feature_names_out()\n\narray(['age', 'fare', 'sex_female', 'sex_male'], dtype=object)\n\n\n\n\nImportant difference between Pipeline and ColumnTransformer\n\nPipeline applies transformer in sequence on all columns\nColumnTranformer applies transformers in parallel to specified columns and then concats the output\n\n\n\nOpen questions?\nSo our ColumnTransformer is working. But we have a few more questions to address. * Why is the output from our pipeline or ColumnTransformer not shown as a dataframe with output features nicely separated in different columns? * Our input dataset had more features besides age, fare, and sex. Why are they not present in the output? * What happens if I change the sequence of transformers, and feature names in my ColumnTransformer?\nIn the coming sections, we will try to address these questions.\n\nWhy is the output not a dataframe?\nThe output from a pipeline or a ColumnTransformer is an nd-array where the first index is the number of samples, and second index are the output features (n_samples, n_output_features). Since we are only getting numpy array as an output, we are losing information about the column names.\n\ntemp = pipe.fit_transform(X_train, y_train)\n\nprint(type(temp))\nprint(temp.shape)\n\n<class 'numpy.ndarray'>\n(1047, 4)\n\n\n\n\nCan we get the feature names back?\nWe have already seen that we can get the output feature names using method get_feature_names_out. But this time let’s try to analyze our ColumnsTransformer more closely. The transformer attributes discussed here also applies to Pipeline object.\n\n##\n# print the internals of ColumnTransformer\n\nset_config(display='text')\npipe\n\nColumnTransformer(transformers=[('standardscaler', StandardScaler(),\n                                 ['age', 'fare']),\n                                ('onehotencoder', OneHotEncoder(), ['sex'])],\n                  verbose_feature_names_out=False)\n\n\nColumnTransformer has an attribute ‘transformers’ that is keeping a list of all the provided transformers. Let’s print it.\n\npipe.transformers\n\n[('standardscaler', StandardScaler(), ['age', 'fare']),\n ('onehotencoder', OneHotEncoder(), ['sex'])]\n\n\nThese are the transformers list at the initialization time. If we want to check the transformers after fit function has been called, then we need to print a different attribute transformers_.\n\npipe.transformers_\n\n[('standardscaler', StandardScaler(), ['age', 'fare']),\n ('onehotencoder', OneHotEncoder(), ['sex']),\n ('remainder', 'drop', [0, 1, 4, 5, 6, 8])]\n\n\nYou can see the difference. There is an extra transformer with the name remainder at the end. It was not present at the initialization time. What it does is that it drops all remaining columns from the dataset that have not been explicitly used in the ColumnTransformer. Since, at the initialization time, ColumnTransformer does not know about the other columns that it needs to drop this transformer is missing. During fit it sees the dataset and knows about the other columns, it then keeps a list of them to drop (0, 1, 4, 5, 6, 8).\nWe can also index through the transformers as well to fetch anyone from the list.\n\n##\n# second transformer from the list\npipe.transformers_[1]\n\n('onehotencoder', OneHotEncoder(), ['sex'])\n\n\nNotice the tuple sequence. * First is the name * Second is the transformer * Third are the column names\nWe can also call get_feature_names_out method on a separate transformer from the list.\n\n##\n# output features from second tranformer\npipe.transformers_[1][1].get_feature_names_out()\n\narray(['sex_female', 'sex_male'], dtype=object)\n\n\n\n##\n# output features from last tranformer\npipe.transformers_[-1][1].get_feature_names_out()\n\n# No. We cannot do this on last transformer (remainder).\n\nAttributeError: 'str' object has no attribute 'get_feature_names_out'\n\n\nWe now have output feature names, and the output (nd-array). Can we convert them to a DataFrame?\n\nimport pandas as pd\n\ntemp = pipe.fit_transform(X_train, y_train)\ncol_names = pipe.get_feature_names_out()\n\noutput = pd.DataFrame(temp.T, col_names).T\noutput.head()\n\n\n\n\n\n  \n    \n      \n      age\n      fare\n      sex_female\n      sex_male\n    \n  \n  \n    \n      0\n      NaN\n      -0.499399\n      1.0\n      0.0\n    \n    \n      1\n      -0.390431\n      -0.090935\n      1.0\n      0.0\n    \n    \n      2\n      -1.296092\n      -0.018241\n      1.0\n      0.0\n    \n    \n      3\n      -0.320765\n      -0.510137\n      0.0\n      1.0\n    \n    \n      4\n      -0.947761\n      -0.501444\n      1.0\n      0.0"
  },
  {
    "objectID": "posts/2022-03-04-sklearn-pipeline-deep-dive.html#functiontransformer",
    "href": "posts/2022-03-04-sklearn-pipeline-deep-dive.html#functiontransformer",
    "title": "Sklearn Pipeline and Transformers Deep Dive",
    "section": "FunctionTransformer",
    "text": "FunctionTransformer\nWe know how to convert the transformer output to a DataFrame. It would be much simpler if we don’t have to do an extra step, and can directly get a Dataframe from our fitted ColumnTransformer.\nFor this we can take the help of FunctionTransformer > A FunctionTransformer forwards its X (and optionally y) arguments to a user-defined function or function object and returns the result of this function.\nLet’s see a FunctionTransformer in action.\n\nfrom sklearn.preprocessing import FunctionTransformer\n\npreprocessor = make_column_transformer(\n    (StandardScaler(), ['age', 'fare']),\n    (OneHotEncoder(), ['sex'] ),\n    verbose_feature_names_out=False\n)\n\npipe = Pipeline([\n                    (\"preprocess\", preprocessor),\n                    (\"dataframer\",FunctionTransformer(lambda x: pd.DataFrame(x, columns = preprocessor.get_feature_names_out())))\n                    ])\n\nset_config(display=\"diagram\")\npipe\n\nPipeline(steps=[('preprocess',\n                 ColumnTransformer(transformers=[('standardscaler',\n                                                  StandardScaler(),\n                                                  ['age', 'fare']),\n                                                 ('onehotencoder',\n                                                  OneHotEncoder(), ['sex'])],\n                                   verbose_feature_names_out=False)),\n                ('dataframer',\n                 FunctionTransformer(func=<function <lambda> at 0x0000016196A1B040>))])Please rerun this cell to show the HTML repr or trust the notebook.PipelinePipeline(steps=[('preprocess',\n                 ColumnTransformer(transformers=[('standardscaler',\n                                                  StandardScaler(),\n                                                  ['age', 'fare']),\n                                                 ('onehotencoder',\n                                                  OneHotEncoder(), ['sex'])],\n                                   verbose_feature_names_out=False)),\n                ('dataframer',\n                 FunctionTransformer(func=<function <lambda> at 0x0000016196A1B040>))])preprocess: ColumnTransformerColumnTransformer(transformers=[('standardscaler', StandardScaler(),\n                                 ['age', 'fare']),\n                                ('onehotencoder', OneHotEncoder(), ['sex'])],\n                  verbose_feature_names_out=False)standardscaler['age', 'fare']StandardScalerStandardScaler()onehotencoder['sex']OneHotEncoderOneHotEncoder()FunctionTransformerFunctionTransformer(func=<function <lambda> at 0x0000016196A1B040>)\n\n\nNotice that we have applied FunctionTransformer after ColumnTransformer in a Pipeline. When we fit our pipeline on the dataset, ColumnTransformer will be fitted first and then the FunctionTransformer. Since the ColumnTransformer has been fitted first, we will be able to call get_feature_names_out on it while passing data to FunctionTransformer.\n\n##\n# let's run our pipeline again\ntemp = pipe.fit_transform(X_train, y_train)\ntemp.head()\n\n\n\n\n\n  \n    \n      \n      age\n      fare\n      sex_female\n      sex_male\n    \n  \n  \n    \n      0\n      NaN\n      -0.499399\n      1.0\n      0.0\n    \n    \n      1\n      -0.390431\n      -0.090935\n      1.0\n      0.0\n    \n    \n      2\n      -1.296092\n      -0.018241\n      1.0\n      0.0\n    \n    \n      3\n      -0.320765\n      -0.510137\n      0.0\n      1.0\n    \n    \n      4\n      -0.947761\n      -0.501444\n      1.0\n      0.0\n    \n  \n\n\n\n\nThis is looking good. We are now getting back a dataframe directly from the pipeline. With a dataframe it is a lot easier to view and verify the output from the preprocessor.\nBut we have to be very careful with FunctionTransformer. In Sklearn docs, it says\nNote: If a lambda is used as the function, then the resulting transformer will not be pickleable.\nHuh! that is a very concerning point. We have also used a lambda function, and we will not be able to pickle it. Let’s check it first.\n\nimport pickle\n\n# save our pipeline\ns1 = pickle.dumps(pipe)\n\n# reload it\ns2 = pickle.loads(s1)\ns2\n\nPicklingError: Can't pickle <function <lambda> at 0x0000016196A1B040>: attribute lookup <lambda> on __main__ failed\n\n\nThe documentation was right about it. We have used a Lambda function in our FunctionTranformer and we got a pickle error. Since, the limitation is said for Lambda function, changing it with a normal function should work. Let’s do that.\n\ndef get_dataframe(X, transformer):\n    \"\"\" \n    x: an nd-array\n    transformer: fitted transformer\n    \"\"\"\n    col_names = transformer.get_feature_names_out()\n    output = pd.DataFrame(X.T, col_names).T\n    return output\n\npreprocessor = make_column_transformer(\n    (StandardScaler(), ['age', 'fare']),\n    (OneHotEncoder(), ['sex'] ),\n    verbose_feature_names_out=False\n)\n\ndataframer = FunctionTransformer(func=get_dataframe, kw_args={\"transformer\": preprocessor})\n\npipe = Pipeline([\n                    (\"preprocess\", preprocessor),\n                    (\"dataframer\", dataframer)\n                ])\n\nset_config(display=\"diagram\")\npipe\n\nPipeline(steps=[('preprocess',\n                 ColumnTransformer(transformers=[('standardscaler',\n                                                  StandardScaler(),\n                                                  ['age', 'fare']),\n                                                 ('onehotencoder',\n                                                  OneHotEncoder(), ['sex'])],\n                                   verbose_feature_names_out=False)),\n                ('dataframer',\n                 FunctionTransformer(func=<function get_dataframe at 0x00000161953C3670>,\n                                     kw_args={'transformer': ColumnTransformer(transformers=[('standardscaler',\n                                                                                              StandardScaler(),\n                                                                                              ['age',\n                                                                                               'fare']),\n                                                                                             ('onehotencoder',\n                                                                                              OneHotEncoder(),\n                                                                                              ['sex'])],\n                                                                               verbose_feature_names_out=False)}))])Please rerun this cell to show the HTML repr or trust the notebook.PipelinePipeline(steps=[('preprocess',\n                 ColumnTransformer(transformers=[('standardscaler',\n                                                  StandardScaler(),\n                                                  ['age', 'fare']),\n                                                 ('onehotencoder',\n                                                  OneHotEncoder(), ['sex'])],\n                                   verbose_feature_names_out=False)),\n                ('dataframer',\n                 FunctionTransformer(func=<function get_dataframe at 0x00000161953C3670>,\n                                     kw_args={'transformer': ColumnTransformer(transformers=[('standardscaler',\n                                                                                              StandardScaler(),\n                                                                                              ['age',\n                                                                                               'fare']),\n                                                                                             ('onehotencoder',\n                                                                                              OneHotEncoder(),\n                                                                                              ['sex'])],\n                                                                               verbose_feature_names_out=False)}))])preprocess: ColumnTransformerColumnTransformer(transformers=[('standardscaler', StandardScaler(),\n                                 ['age', 'fare']),\n                                ('onehotencoder', OneHotEncoder(), ['sex'])],\n                  verbose_feature_names_out=False)standardscaler['age', 'fare']StandardScalerStandardScaler()onehotencoder['sex']OneHotEncoderOneHotEncoder()FunctionTransformerFunctionTransformer(func=<function get_dataframe at 0x00000161953C3670>,\n                    kw_args={'transformer': ColumnTransformer(transformers=[('standardscaler',\n                                                                             StandardScaler(),\n                                                                             ['age',\n                                                                              'fare']),\n                                                                            ('onehotencoder',\n                                                                             OneHotEncoder(),\n                                                                             ['sex'])],\n                                                              verbose_feature_names_out=False)})\n\n\nNotice the arguments for FunctionTransformer in the above code. * first argument is the function to be called * second argument are the parameters to be passed to our function\nThe sequence of arguments for the callable function will be * first argument will be the output from any previous step in the pipeline (if there is any). In our case, it is nd-array coming from ColumnTransformer. It will be mapped to X. We don’t have to do anything about it. * second argument (if any) we want to pass to function. In our case we need it to be the fitted transformer from the previous step so we have explicitly passed it using kw_args as key-value pair. Where key name is the same as callable method argument name (‘transformer’ in our case).\nNow let’s do our pickle test one more time.\n\n##\n# save our pipeline\ns1 = pickle.dumps(pipe)\n\n# reload it\ns2 = pickle.loads(s1)\ns2\n\nPipeline(steps=[('preprocess',\n                 ColumnTransformer(transformers=[('standardscaler',\n                                                  StandardScaler(),\n                                                  ['age', 'fare']),\n                                                 ('onehotencoder',\n                                                  OneHotEncoder(), ['sex'])],\n                                   verbose_feature_names_out=False)),\n                ('dataframer',\n                 FunctionTransformer(func=<function get_dataframe at 0x00000161953C3670>,\n                                     kw_args={'transformer': ColumnTransformer(transformers=[('standardscaler',\n                                                                                              StandardScaler(),\n                                                                                              ['age',\n                                                                                               'fare']),\n                                                                                             ('onehotencoder',\n                                                                                              OneHotEncoder(),\n                                                                                              ['sex'])],\n                                                                               verbose_feature_names_out=False)}))])Please rerun this cell to show the HTML repr or trust the notebook.PipelinePipeline(steps=[('preprocess',\n                 ColumnTransformer(transformers=[('standardscaler',\n                                                  StandardScaler(),\n                                                  ['age', 'fare']),\n                                                 ('onehotencoder',\n                                                  OneHotEncoder(), ['sex'])],\n                                   verbose_feature_names_out=False)),\n                ('dataframer',\n                 FunctionTransformer(func=<function get_dataframe at 0x00000161953C3670>,\n                                     kw_args={'transformer': ColumnTransformer(transformers=[('standardscaler',\n                                                                                              StandardScaler(),\n                                                                                              ['age',\n                                                                                               'fare']),\n                                                                                             ('onehotencoder',\n                                                                                              OneHotEncoder(),\n                                                                                              ['sex'])],\n                                                                               verbose_feature_names_out=False)}))])preprocess: ColumnTransformerColumnTransformer(transformers=[('standardscaler', StandardScaler(),\n                                 ['age', 'fare']),\n                                ('onehotencoder', OneHotEncoder(), ['sex'])],\n                  verbose_feature_names_out=False)standardscaler['age', 'fare']StandardScalerStandardScaler()onehotencoder['sex']OneHotEncoderOneHotEncoder()FunctionTransformerFunctionTransformer(func=<function get_dataframe at 0x00000161953C3670>,\n                    kw_args={'transformer': ColumnTransformer(transformers=[('standardscaler',\n                                                                             StandardScaler(),\n                                                                             ['age',\n                                                                              'fare']),\n                                                                            ('onehotencoder',\n                                                                             OneHotEncoder(),\n                                                                             ['sex'])],\n                                                              verbose_feature_names_out=False)})\n\n\nAlright, no more issues so let’s proceed to our next question.\n\nWhere are the rest of the columns?\nBy default, only the specified columns in transformers are transformed and combined in the output, and the non-specified columns are dropped. (default of remainder='drop'). By specifying remainder='passthrough', all remaining columns that were not specified in transformers will be automatically passed through. This subset of columns is concatenated with the output of the transformers.\nLet’s see it in action.\n\npreprocessor = make_column_transformer(\n    (StandardScaler(), ['age', 'fare']),\n    (OneHotEncoder(), ['sex'] ),\n    verbose_feature_names_out=False,\n    remainder='passthrough'\n)\n\n# get_dataframe is already defined in last section. Intentionally omitted here. \ndataframer = FunctionTransformer(func=get_dataframe, kw_args={\"transformer\": preprocessor})\n\npipe = Pipeline([\n                    (\"preprocess\", preprocessor),\n                    (\"dataframer\", dataframer)\n                ])\n\ntemp = pipe.fit_transform(X_train, y_train)\ntemp.head()\n\n\n\n\n\n  \n    \n      \n      age\n      fare\n      sex_female\n      sex_male\n      pclass\n      name\n      sibsp\n      parch\n      ticket\n      embarked\n    \n  \n  \n    \n      0\n      NaN\n      -0.499399\n      1.0\n      0.0\n      3.0\n      McCarthy, Miss. Catherine 'Katie'\n      0.0\n      0.0\n      383123\n      Q\n    \n    \n      1\n      -0.390431\n      -0.090935\n      1.0\n      0.0\n      2.0\n      del Carlo, Mrs. Sebastiano (Argenia Genovesi)\n      1.0\n      0.0\n      SC/PARIS 2167\n      C\n    \n    \n      2\n      -1.296092\n      -0.018241\n      1.0\n      0.0\n      3.0\n      Andersson, Miss. Sigrid Elisabeth\n      4.0\n      2.0\n      347082\n      S\n    \n    \n      3\n      -0.320765\n      -0.510137\n      0.0\n      1.0\n      3.0\n      Saad, Mr. Khalil\n      0.0\n      0.0\n      2672\n      C\n    \n    \n      4\n      -0.947761\n      -0.501444\n      1.0\n      0.0\n      3.0\n      Abelseth, Miss. Karen Marie\n      0.0\n      0.0\n      348125\n      S\n    \n  \n\n\n\n\nWe have our remaining features back now, so let’s proceed to our next question.\n\n\nWhat happens if I change the sequence in ColumnTranformer?\nIt is better to make some changes and then see the results. I am making two changes in ColumnTransformer 1. Changed the order of transformers (OHE before scaling) 2. Changed the order of features inside the transformer (‘fare’ before ‘age’)\n\npreprocessor = make_column_transformer(\n    (OneHotEncoder(), ['sex'] ),\n    (StandardScaler(), ['fare', 'age']),\n    verbose_feature_names_out=False,\n    remainder='passthrough'\n)\n\n# get_dataframe is already defined in last section. Intentionally omitted here. \ndataframer = FunctionTransformer(func=get_dataframe, kw_args={\"transformer\": preprocessor})\n\npipe = Pipeline([\n                    (\"preprocess\", preprocessor),\n                    (\"dataframer\", dataframer)\n                ])\n\ntemp = pipe.fit_transform(X_train, y_train)\ntemp.head()\n\n\n\n\n\n  \n    \n      \n      sex_female\n      sex_male\n      fare\n      age\n      pclass\n      name\n      sibsp\n      parch\n      ticket\n      embarked\n    \n  \n  \n    \n      0\n      1.0\n      0.0\n      -0.499399\n      NaN\n      3.0\n      McCarthy, Miss. Catherine 'Katie'\n      0.0\n      0.0\n      383123\n      Q\n    \n    \n      1\n      1.0\n      0.0\n      -0.090935\n      -0.390431\n      2.0\n      del Carlo, Mrs. Sebastiano (Argenia Genovesi)\n      1.0\n      0.0\n      SC/PARIS 2167\n      C\n    \n    \n      2\n      1.0\n      0.0\n      -0.018241\n      -1.296092\n      3.0\n      Andersson, Miss. Sigrid Elisabeth\n      4.0\n      2.0\n      347082\n      S\n    \n    \n      3\n      0.0\n      1.0\n      -0.510137\n      -0.320765\n      3.0\n      Saad, Mr. Khalil\n      0.0\n      0.0\n      2672\n      C\n    \n    \n      4\n      1.0\n      0.0\n      -0.501444\n      -0.947761\n      3.0\n      Abelseth, Miss. Karen Marie\n      0.0\n      0.0\n      348125\n      S\n    \n  \n\n\n\n\nWe can see that changing the sequence in ColumnTransformer does change the output. Also note * Specified columns in transformers are transformed and combined in the output * Transformers sequence in ColumnTransformer also represents the columns sequence in the output * When remainder=passthrough is used then remaining columns will be appended at the end. Remainder columns sequence will be same as in the input."
  },
  {
    "objectID": "posts/2022-03-04-sklearn-pipeline-deep-dive.html#pipeline-inside-columntransformer",
    "href": "posts/2022-03-04-sklearn-pipeline-deep-dive.html#pipeline-inside-columntransformer",
    "title": "Sklearn Pipeline and Transformers Deep Dive",
    "section": "Pipeline inside ColumnTransformer",
    "text": "Pipeline inside ColumnTransformer\nLet’s assume we have more requirements this time. I want * for numerical features (age, fare): impute the missing values first, and then scale them * for categorical features (sex): one hot encode them\nOur pipeline will look like this.\n\n#collapse-output\nnumeric_features = [\"age\", \"fare\"]\nnumeric_transformer = Pipeline(\n    steps=[\n        (\"imputer\", SimpleImputer(strategy=\"median\")),\n        (\"scaler\", StandardScaler())\n        ])\n\ncategorical_features = [\"sex\"]\ncategorical_transformer = OneHotEncoder(handle_unknown=\"ignore\")\n\npreprocessor = make_column_transformer(\n        (numeric_transformer, numeric_features), # note \"numeric_transformer\" is a pipeline this time\n        (categorical_transformer, categorical_features),\n        remainder='passthrough',\n        verbose_feature_names_out=False\n)\n\ndataframer = FunctionTransformer(func=get_dataframe, kw_args={\"transformer\": preprocessor})\npipe = Pipeline([\n                    (\"preprocess\", preprocessor),\n                    (\"dataframer\", dataframer)\n                ])\n\ntemp = pipe.fit_transform(X_train, y_train)\ntemp.head()\n\nAttributeError: Estimator imputer does not provide get_feature_names_out. Did you mean to call pipeline[:-1].get_feature_names_out()?\n\n\nOh geez! What went wrong this time. The error message says\nAttributeError: Estimator imputer does not provide get_feature_names_out. Did you mean to call pipeline[:-1].get_feature_names_out()?\nFrom the error message I am getting that > Estimator imputer does not provide get_feature_names_out\nHmmm, this is strange. Why is this estimator missing a very useful function? Let’s check the docs first on SimpleImputer. For the docs I indeed could not find this method get_feature_names_out() for this transformer. A little googling lead me to this Sklearn Github issue page Implement get_feature_names_out for all estimators. Developers are actively adding get_feature_names_out() to all estimators and transformers, and it looks like this feature has not been implemented for SimpleImputer till Sklearn version==1.0.2.\nBut no worries we can overcome this limitation, and implement this feature ourselves through a custom transformer."
  },
  {
    "objectID": "posts/2022-03-04-sklearn-pipeline-deep-dive.html#custom-transformer",
    "href": "posts/2022-03-04-sklearn-pipeline-deep-dive.html#custom-transformer",
    "title": "Sklearn Pipeline and Transformers Deep Dive",
    "section": "Custom Transformer",
    "text": "Custom Transformer\nWe can create a custom transformer or an estimator simply by inheriting a class from BaseEstimator and optionally the mixin classes in sklearn.base. Sklean provides a template that we can use to create our custom transformer. Template link is here: https://github.com/scikit-learn-contrib/project-template/blob/master/skltemplate/_template.py#L146\nLet us use the same pipeline as in last cell but replace SimpleImputer with a custom one.\n\nfrom sklearn.base import BaseEstimator, TransformerMixin\n\nclass SimpleImputerCustom(BaseEstimator, TransformerMixin):\n    def __init__(self, strategy='mean'):\n        self.strategy = strategy\n        self.imputer = SimpleImputer(strategy=self.strategy)\n       \n        \n    def fit(self, X, y):\n        self.imputer.fit(X, y)\n        return self\n    \n    def transform(self, X):\n        return self.imputer.transform(X)\n    \n    def get_feature_names_out(self, input_features=None):\n        # we have returned the input features name as out features will have the same name\n        return input_features\n\n\nnumeric_features = [\"age\", \"fare\"]\nnumeric_transformer = Pipeline(\n    steps=[\n            (\"imputer\", SimpleImputerCustom(strategy='mean')),\n            (\"scaler\", StandardScaler())\n             ]\n)\n\n\ncategorical_features = [\"sex\"]\ncategorical_transformer = OneHotEncoder(handle_unknown=\"ignore\")\n\npreprocessor = make_column_transformer(\n        (numeric_transformer, numeric_features), # note \"numeric_transformer\" is a pipeline\n        (categorical_transformer, categorical_features),\n        remainder='passthrough',\n    verbose_feature_names_out=False\n)\n\ndataframer = FunctionTransformer(func=get_dataframe, kw_args={\"transformer\": preprocessor})\npipe = Pipeline([\n                    (\"preprocess\", preprocessor),\n                    (\"dataframer\", dataframer)\n                ])\n\ntemp = pipe.fit_transform(X_train, y_train)\ntemp.head()\n\n\n\n\n\n  \n    \n      \n      age\n      fare\n      sex_female\n      sex_male\n      pclass\n      name\n      sibsp\n      parch\n      ticket\n      embarked\n    \n  \n  \n    \n      0\n      0.0\n      -0.499638\n      1.0\n      0.0\n      3.0\n      McCarthy, Miss. Catherine 'Katie'\n      0.0\n      0.0\n      383123\n      Q\n    \n    \n      1\n      -0.436411\n      -0.090979\n      1.0\n      0.0\n      2.0\n      del Carlo, Mrs. Sebastiano (Argenia Genovesi)\n      1.0\n      0.0\n      SC/PARIS 2167\n      C\n    \n    \n      2\n      -1.448729\n      -0.01825\n      1.0\n      0.0\n      3.0\n      Andersson, Miss. Sigrid Elisabeth\n      4.0\n      2.0\n      347082\n      S\n    \n    \n      3\n      -0.358541\n      -0.510381\n      0.0\n      1.0\n      3.0\n      Saad, Mr. Khalil\n      0.0\n      0.0\n      2672\n      C\n    \n    \n      4\n      -1.059376\n      -0.501684\n      1.0\n      0.0\n      3.0\n      Abelseth, Miss. Karen Marie\n      0.0\n      0.0\n      348125\n      S"
  },
  {
    "objectID": "posts/2022-03-04-sklearn-pipeline-deep-dive.html#feature-engineering-with-custom-transformer",
    "href": "posts/2022-03-04-sklearn-pipeline-deep-dive.html#feature-engineering-with-custom-transformer",
    "title": "Sklearn Pipeline and Transformers Deep Dive",
    "section": "Feature Engineering with Custom Transformer",
    "text": "Feature Engineering with Custom Transformer\nSo far, so good! Let’s assume that we have another requirement and it is about feature engineering. We have to combine ‘sibsp’ and ‘parch’ into two new features: family_size and is_alone.\nLet’s implement this now.\n\nclass FamilyFeatureTransformer(BaseEstimator, TransformerMixin):\n    def __init__(self):\n        pass\n       \n    def fit(self, X, y):\n        return self\n    \n    def transform(self, X):\n        X['family_size'] = X['parch'] + X['sibsp']\n        X.drop(['parch', 'sibsp'], axis=1, inplace=True) # we can drop this feature now\n        X['is_alone'] = 1\n        X.loc[X['family_size'] > 1, 'is_alone'] = 0\n        return X\n    \n    def get_feature_names_out(self, input_features=None):\n        # this time we have created new features. Their names are different from input features.\n        # so we have explicitly mentioned them here.\n        return ['family_size', 'is_alone']\n\nnumeric_features = [\"age\", \"fare\"]\nnumeric_transformer = Pipeline(\n    steps=[\n            (\"imputer\", SimpleImputerCustom(strategy='mean')),\n            (\"scaler\", StandardScaler())\n             ]\n)\n\ncategorical_features = [\"sex\"]\ncategorical_transformer = OneHotEncoder(handle_unknown=\"ignore\")\n\nfamily_features = [\"parch\", \"sibsp\"]\nfamily_transformer = FamilyFeatureTransformer()\n\npreprocessor = make_column_transformer(\n        (numeric_transformer, numeric_features), \n        (categorical_transformer, categorical_features),\n        (family_transformer, family_features),\n        remainder='drop', # let's drop extra features this time\n    verbose_feature_names_out=False\n)\n\ndataframer = FunctionTransformer(func=get_dataframe, kw_args={\"transformer\": preprocessor})\npipe = Pipeline([\n                    (\"preprocess\", preprocessor),\n                    (\"dataframer\", dataframer)\n                ])\n\ntemp = pipe.fit_transform(X_train, y_train)\ntemp.head()\n\n\n\n\n\n  \n    \n      \n      age\n      fare\n      sex_female\n      sex_male\n      family_size\n      is_alone\n    \n  \n  \n    \n      0\n      0.000000\n      -0.499638\n      1.0\n      0.0\n      0.0\n      1.0\n    \n    \n      1\n      -0.436411\n      -0.090979\n      1.0\n      0.0\n      1.0\n      1.0\n    \n    \n      2\n      -1.448729\n      -0.018250\n      1.0\n      0.0\n      6.0\n      0.0\n    \n    \n      3\n      -0.358541\n      -0.510381\n      0.0\n      1.0\n      0.0\n      1.0\n    \n    \n      4\n      -1.059376\n      -0.501684\n      1.0\n      0.0\n      0.0\n      1.0"
  },
  {
    "objectID": "posts/2022-03-04-sklearn-pipeline-deep-dive.html#sklean-pipeline-with-feature-importance",
    "href": "posts/2022-03-04-sklearn-pipeline-deep-dive.html#sklean-pipeline-with-feature-importance",
    "title": "Sklearn Pipeline and Transformers Deep Dive",
    "section": "Sklean Pipeline with Feature Importance",
    "text": "Sklean Pipeline with Feature Importance\nAlright, we have our required features ready and we can now pass them to a classifier. Let’s use RandomForrest as our classifier and run our pipeline with it.\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\n\n# 'preprocessor' and 'dataframer' are already declared in last section, and intentionally omitted here.\npipe = Pipeline([\n                    (\"preprocess\", preprocessor),\n                    (\"dataframer\", dataframer),\n                    ('rf_estimator', RandomForestClassifier())\n\n                ])\n\ntemp = pipe.fit_transform(X_train, y_train)\ntemp.head()\n\nAttributeError: 'RandomForestClassifier' object has no attribute 'transform'\n\n\nOkay, looks like we have made a mistake here. Error message is saying\nAttributeError: 'RandomForestClassifier' object has no attribute 'transform'\nI get that. In our pipeline we have an estimator that does not have a transform method defined for it. We should use predict method instead.\nNote: * Estimators implement predict method (Template reference Estimator, Template reference Classifier) * Transformers implement transform method (Template reference Transformer) * fit_transform is same calling fit and then transform\nLet us fix the error and run our pipeline again.\n\n##\n# pipeline created in last section and intentionally omitted here.\npipe.fit(X_train, y_train)\ny_pred = pipe.predict(X_test)\naccuracy_score(y_test, y_pred)\n\n0.7862595419847328\n\n\nLet’s see how our final pipeline looks visually.\n\n##\n# set_config(display='text')\npipe\n\nPipeline(steps=[('preprocess',\n                 ColumnTransformer(transformers=[('pipeline',\n                                                  Pipeline(steps=[('imputer',\n                                                                   SimpleImputerCustom()),\n                                                                  ('scaler',\n                                                                   StandardScaler())]),\n                                                  ['age', 'fare']),\n                                                 ('onehotencoder',\n                                                  OneHotEncoder(handle_unknown='ignore'),\n                                                  ['sex']),\n                                                 ('familyfeaturetransformer',\n                                                  FamilyFeatureTransformer(),\n                                                  ['parch', 'sibsp'])],\n                                   verbose_feature_names_out=False)),\n                ('data...\n                                     kw_args={'transformer': ColumnTransformer(transformers=[('pipeline',\n                                                                                              Pipeline(steps=[('imputer',\n                                                                                                               SimpleImputerCustom()),\n                                                                                                              ('scaler',\n                                                                                                               StandardScaler())]),\n                                                                                              ['age',\n                                                                                               'fare']),\n                                                                                             ('onehotencoder',\n                                                                                              OneHotEncoder(handle_unknown='ignore'),\n                                                                                              ['sex']),\n                                                                                             ('familyfeaturetransformer',\n                                                                                              FamilyFeatureTransformer(),\n                                                                                              ['parch',\n                                                                                               'sibsp'])],\n                                                                               verbose_feature_names_out=False)})),\n                ('rf_estimator', RandomForestClassifier())])Please rerun this cell to show the HTML repr or trust the notebook.PipelinePipeline(steps=[('preprocess',\n                 ColumnTransformer(transformers=[('pipeline',\n                                                  Pipeline(steps=[('imputer',\n                                                                   SimpleImputerCustom()),\n                                                                  ('scaler',\n                                                                   StandardScaler())]),\n                                                  ['age', 'fare']),\n                                                 ('onehotencoder',\n                                                  OneHotEncoder(handle_unknown='ignore'),\n                                                  ['sex']),\n                                                 ('familyfeaturetransformer',\n                                                  FamilyFeatureTransformer(),\n                                                  ['parch', 'sibsp'])],\n                                   verbose_feature_names_out=False)),\n                ('data...\n                                     kw_args={'transformer': ColumnTransformer(transformers=[('pipeline',\n                                                                                              Pipeline(steps=[('imputer',\n                                                                                                               SimpleImputerCustom()),\n                                                                                                              ('scaler',\n                                                                                                               StandardScaler())]),\n                                                                                              ['age',\n                                                                                               'fare']),\n                                                                                             ('onehotencoder',\n                                                                                              OneHotEncoder(handle_unknown='ignore'),\n                                                                                              ['sex']),\n                                                                                             ('familyfeaturetransformer',\n                                                                                              FamilyFeatureTransformer(),\n                                                                                              ['parch',\n                                                                                               'sibsp'])],\n                                                                               verbose_feature_names_out=False)})),\n                ('rf_estimator', RandomForestClassifier())])preprocess: ColumnTransformerColumnTransformer(transformers=[('pipeline',\n                                 Pipeline(steps=[('imputer',\n                                                  SimpleImputerCustom()),\n                                                 ('scaler', StandardScaler())]),\n                                 ['age', 'fare']),\n                                ('onehotencoder',\n                                 OneHotEncoder(handle_unknown='ignore'),\n                                 ['sex']),\n                                ('familyfeaturetransformer',\n                                 FamilyFeatureTransformer(),\n                                 ['parch', 'sibsp'])],\n                  verbose_feature_names_out=False)pipeline['age', 'fare']SimpleImputerCustomSimpleImputerCustom()StandardScalerStandardScaler()onehotencoder['sex']OneHotEncoderOneHotEncoder(handle_unknown='ignore')familyfeaturetransformer['parch', 'sibsp']FamilyFeatureTransformerFamilyFeatureTransformer()FunctionTransformerFunctionTransformer(func=<function get_dataframe at 0x00000161953C3670>,\n                    kw_args={'transformer': ColumnTransformer(transformers=[('pipeline',\n                                                                             Pipeline(steps=[('imputer',\n                                                                                              SimpleImputerCustom()),\n                                                                                             ('scaler',\n                                                                                              StandardScaler())]),\n                                                                             ['age',\n                                                                              'fare']),\n                                                                            ('onehotencoder',\n                                                                             OneHotEncoder(handle_unknown='ignore'),\n                                                                             ['sex']),\n                                                                            ('familyfeaturetransformer',\n                                                                             FamilyFeatureTransformer(),\n                                                                             ['parch',\n                                                                              'sibsp'])],\n                                                              verbose_feature_names_out=False)})RandomForestClassifierRandomForestClassifier()\n\n\nWe can also get the importance of features in our dataset from RandomForrest classifier.\n\nimport matplotlib.pyplot as plt\n\nclf = pipe[-1] # last estimator is the RF classifier\nimportances = clf.feature_importances_\nfeatures = clf.feature_names_in_\n\nindices = np.argsort(importances)\n\nplt.title('Feature Importances')\nplt.barh(range(len(indices)), importances[indices], color='b', align='center')\nplt.yticks(range(len(indices)), [features[i] for i in indices])\nplt.xlabel('Relative Importance')\nplt.show()\n\n\n\n\nNote that all the feature names were passed to the RF Classifier and that is why we were able to get them back using its attribute feature_names_in_. This can be super useful when you have many model deployed in the environment, and you can just use the model object to get information about the features it was trained on.\nFor a moment let’s also remove the feature names from our pipeline and see how it will effect our feature importance plot.\n\npipe = Pipeline([\n                    (\"preprocess\", preprocessor),\n                    # (\"dataframer\", dataframer),\n                    ('rf_estimator', RandomForestClassifier())\n\n                ])\n\n# fit the pipeline\npipe.fit(X_train, y_train)\n\n# get the feature importance plot\nclf = pipe[-1]\nimportances = clf.feature_importances_\nfeatures = clf.feature_names_in_\n\nindices = np.argsort(importances)\n\nplt.title('Feature Importances')\nplt.barh(range(len(indices)), importances[indices], color='b', align='center')\nplt.yticks(range(len(indices)), [features[i] for i in indices])\nplt.xlabel('Relative Importance')\nplt.show()\n\nAttributeError: 'RandomForestClassifier' object has no attribute 'feature_names_in_'\n\n\nNo feature names were passed to our classifier this time and it is missing feature_names_in_ attribute. We can circumvent this and still get feature importance plot.\n\npipe = Pipeline([\n                    (\"preprocess\", preprocessor),\n                    # (\"dataframer\", dataframer),\n                    ('rf_estimator', RandomForestClassifier())\n\n                ])\n\n# fit the pipeline\npipe.fit(X_train, y_train)\n\n# get the feature importance plot\nclf = pipe[-1]\nimportances = clf.feature_importances_\n# features = clf.feature_names_in_\n\nindices = np.argsort(importances)\n\nplt.title('Feature Importances')\nplt.barh(range(len(indices)), importances[indices], color='b', align='center')\nplt.yticks(range(len(indices)), [i for i in indices])\nplt.xlabel('Relative Importance')\nplt.show()\n\n\n\n\nThis time we get the same plot but not withOUT feature names, and it is not useful anymore. So definitely we need to keep the feature names with the final estimator. Feature names can help us a lot in interpreting the model."
  },
  {
    "objectID": "posts/2022-03-04-sklearn-pipeline-deep-dive.html#the-complete-pipeline",
    "href": "posts/2022-03-04-sklearn-pipeline-deep-dive.html#the-complete-pipeline",
    "title": "Sklearn Pipeline and Transformers Deep Dive",
    "section": "The complete Pipeline",
    "text": "The complete Pipeline\nFor an easy reference, let’s put the whole pipeline in one place. ### Load Data\n\nfrom sklearn.datasets import fetch_openml\nfrom sklearn.model_selection import train_test_split\nimport pandas as pd\nimport numpy as np\n\nnp.random.seed(42) # for consistency\n\nX, y = fetch_openml(\"titanic\", version=1, as_frame=True, return_X_y=True)\n\nX.drop(['body', 'cabin', 'boat', 'home.dest'], axis=1, inplace=True)\nX_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2)\nX_train.head()\n\n\n\n\n\n  \n    \n      \n      pclass\n      name\n      sex\n      age\n      sibsp\n      parch\n      ticket\n      fare\n      embarked\n    \n  \n  \n    \n      999\n      3.0\n      McCarthy, Miss. Catherine 'Katie'\n      female\n      NaN\n      0.0\n      0.0\n      383123\n      7.7500\n      Q\n    \n    \n      392\n      2.0\n      del Carlo, Mrs. Sebastiano (Argenia Genovesi)\n      female\n      24.0\n      1.0\n      0.0\n      SC/PARIS 2167\n      27.7208\n      C\n    \n    \n      628\n      3.0\n      Andersson, Miss. Sigrid Elisabeth\n      female\n      11.0\n      4.0\n      2.0\n      347082\n      31.2750\n      S\n    \n    \n      1165\n      3.0\n      Saad, Mr. Khalil\n      male\n      25.0\n      0.0\n      0.0\n      2672\n      7.2250\n      C\n    \n    \n      604\n      3.0\n      Abelseth, Miss. Karen Marie\n      female\n      16.0\n      0.0\n      0.0\n      348125\n      7.6500\n      S\n    \n  \n\n\n\n\n\nTrain Model\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import OneHotEncoder, FunctionTransformer, StandardScaler\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.compose import make_column_transformer\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.impute import SimpleImputer\n\nclass FamilyFeatureTransformer(BaseEstimator, TransformerMixin):\n    def __init__(self):\n        pass\n       \n    def fit(self, X, y):\n        return self\n    \n    def transform(self, X):\n        X['family_size'] = X['parch'] + X['sibsp']\n        X.drop(['parch', 'sibsp'], axis=1, inplace=True) # we can drop this feature now\n        X['is_alone'] = 1\n        X.loc[X['family_size'] > 1, 'is_alone'] = 0\n        return X\n    \n    def get_feature_names_out(self, input_features=None):\n        # this time we have created new features. Their names are different from input features.\n        # so we have explicitly mentioned them here.\n        return ['family_size', 'is_alone']\n\nclass SimpleImputerCustom(BaseEstimator, TransformerMixin):\n    def __init__(self, strategy='mean'):\n        self.strategy = strategy\n        self.imputer = SimpleImputer(strategy=self.strategy)\n       \n        \n    def fit(self, X, y):\n        self.imputer.fit(X, y)\n        return self\n    \n    def transform(self, X):\n        return self.imputer.transform(X)\n    \n    def get_feature_names_out(self, input_features=None):\n        # we have returned the input features name as out features will have the same name\n        return input_features\n\ndef get_dataframe(X, transformer):\n    \"\"\" \n    x: an nd-array\n    transformer: fitted transformer\n    \"\"\"\n    col_names = transformer.get_feature_names_out()\n    output = pd.DataFrame(X.T, col_names).T\n    return output\n    \nnumeric_features = [\"age\", \"fare\"]\nnumeric_transformer = Pipeline(\n    steps=[\n            (\"imputer\", SimpleImputerCustom(strategy='mean')),\n            (\"scaler\", StandardScaler())\n             ]\n)\n\ncategorical_features = [\"sex\"]\ncategorical_transformer = OneHotEncoder(handle_unknown=\"ignore\")\n\nfamily_features = [\"parch\", \"sibsp\"]\nfamily_transformer = FamilyFeatureTransformer()\n\npreprocessor = make_column_transformer(\n        (numeric_transformer, numeric_features), \n        (categorical_transformer, categorical_features),\n        (family_transformer, family_features),\n        remainder='drop', # let's drop extra features this time\n    verbose_feature_names_out=False\n)\n\ndataframer = FunctionTransformer(func=get_dataframe, kw_args={\"transformer\": preprocessor})\npipe = Pipeline([\n                    (\"preprocess\", preprocessor),\n                    (\"dataframer\", dataframer),\n                    ('rf_estimator', RandomForestClassifier())\n\n                ])\n\npipe.fit(X_train, y_train)\ny_pred = pipe.predict(X_test)\naccuracy_score(y_test, y_pred)\n\n0.8015267175572519\n\n\n\n\nPlot Feature Importance\n\nimport matplotlib.pyplot as plt\n\nclf = pipe[-1] # last estimator is the RF classifier\nimportances = clf.feature_importances_\nfeatures = clf.feature_names_in_\n\nindices = np.argsort(importances)\n\nplt.title('Feature Importances')\nplt.barh(range(len(indices)), importances[indices], color='b', align='center')\nplt.yticks(range(len(indices)), [features[i] for i in indices])\nplt.xlabel('Relative Importance')\nplt.show()\n\n\n\n\n\n\nPickle Test\n\nfrom sklearn import set_config\nset_config(display=\"diagram\")\n\nimport pickle\n\n# save our pipeline\ns1 = pickle.dumps(pipe)\n\n# reload it\ns2 = pickle.loads(s1)\ns2\n\nPipeline(steps=[('preprocess',\n                 ColumnTransformer(transformers=[('pipeline',\n                                                  Pipeline(steps=[('imputer',\n                                                                   SimpleImputerCustom()),\n                                                                  ('scaler',\n                                                                   StandardScaler())]),\n                                                  ['age', 'fare']),\n                                                 ('onehotencoder',\n                                                  OneHotEncoder(handle_unknown='ignore'),\n                                                  ['sex']),\n                                                 ('familyfeaturetransformer',\n                                                  FamilyFeatureTransformer(),\n                                                  ['parch', 'sibsp'])],\n                                   verbose_feature_names_out=False)),\n                ('data...\n                                     kw_args={'transformer': ColumnTransformer(transformers=[('pipeline',\n                                                                                              Pipeline(steps=[('imputer',\n                                                                                                               SimpleImputerCustom()),\n                                                                                                              ('scaler',\n                                                                                                               StandardScaler())]),\n                                                                                              ['age',\n                                                                                               'fare']),\n                                                                                             ('onehotencoder',\n                                                                                              OneHotEncoder(handle_unknown='ignore'),\n                                                                                              ['sex']),\n                                                                                             ('familyfeaturetransformer',\n                                                                                              FamilyFeatureTransformer(),\n                                                                                              ['parch',\n                                                                                               'sibsp'])],\n                                                                               verbose_feature_names_out=False)})),\n                ('rf_estimator', RandomForestClassifier())])Please rerun this cell to show the HTML repr or trust the notebook.PipelinePipeline(steps=[('preprocess',\n                 ColumnTransformer(transformers=[('pipeline',\n                                                  Pipeline(steps=[('imputer',\n                                                                   SimpleImputerCustom()),\n                                                                  ('scaler',\n                                                                   StandardScaler())]),\n                                                  ['age', 'fare']),\n                                                 ('onehotencoder',\n                                                  OneHotEncoder(handle_unknown='ignore'),\n                                                  ['sex']),\n                                                 ('familyfeaturetransformer',\n                                                  FamilyFeatureTransformer(),\n                                                  ['parch', 'sibsp'])],\n                                   verbose_feature_names_out=False)),\n                ('data...\n                                     kw_args={'transformer': ColumnTransformer(transformers=[('pipeline',\n                                                                                              Pipeline(steps=[('imputer',\n                                                                                                               SimpleImputerCustom()),\n                                                                                                              ('scaler',\n                                                                                                               StandardScaler())]),\n                                                                                              ['age',\n                                                                                               'fare']),\n                                                                                             ('onehotencoder',\n                                                                                              OneHotEncoder(handle_unknown='ignore'),\n                                                                                              ['sex']),\n                                                                                             ('familyfeaturetransformer',\n                                                                                              FamilyFeatureTransformer(),\n                                                                                              ['parch',\n                                                                                               'sibsp'])],\n                                                                               verbose_feature_names_out=False)})),\n                ('rf_estimator', RandomForestClassifier())])preprocess: ColumnTransformerColumnTransformer(transformers=[('pipeline',\n                                 Pipeline(steps=[('imputer',\n                                                  SimpleImputerCustom()),\n                                                 ('scaler', StandardScaler())]),\n                                 ['age', 'fare']),\n                                ('onehotencoder',\n                                 OneHotEncoder(handle_unknown='ignore'),\n                                 ['sex']),\n                                ('familyfeaturetransformer',\n                                 FamilyFeatureTransformer(),\n                                 ['parch', 'sibsp'])],\n                  verbose_feature_names_out=False)pipeline['age', 'fare']SimpleImputerCustomSimpleImputerCustom()StandardScalerStandardScaler()onehotencoder['sex']OneHotEncoderOneHotEncoder(handle_unknown='ignore')familyfeaturetransformer['parch', 'sibsp']FamilyFeatureTransformerFamilyFeatureTransformer()FunctionTransformerFunctionTransformer(func=<function get_dataframe at 0x00000161997DF3A0>,\n                    kw_args={'transformer': ColumnTransformer(transformers=[('pipeline',\n                                                                             Pipeline(steps=[('imputer',\n                                                                                              SimpleImputerCustom()),\n                                                                                             ('scaler',\n                                                                                              StandardScaler())]),\n                                                                             ['age',\n                                                                              'fare']),\n                                                                            ('onehotencoder',\n                                                                             OneHotEncoder(handle_unknown='ignore'),\n                                                                             ['sex']),\n                                                                            ('familyfeaturetransformer',\n                                                                             FamilyFeatureTransformer(),\n                                                                             ['parch',\n                                                                              'sibsp'])],\n                                                              verbose_feature_names_out=False)})RandomForestClassifierRandomForestClassifier()"
  },
  {
    "objectID": "posts/2022-03-11-docker-app-logs.html#sample-application",
    "href": "posts/2022-03-11-docker-app-logs.html#sample-application",
    "title": "Docker - Accessing Python Application Logs",
    "section": "Sample Application",
    "text": "Sample Application\nLet us create a simple hello world application that will print “hello world” message to stdout, and also logs them in a logfile. After each message the application sleeps for 5 seconds, and keeps on doing this for 5 mins (300 sec). After this the program exists.\nProject structure of this application is\napp/\n└── src/\n    ├── commons/\n    │   └── logger.py\n    └── hello.py\nWhere * app/ is the project root folder * src/ folder contain the python application code * src/commons/logger.py is the logging module * src/hello.py is the main application\nCode files are provided below\n\n##\n# app/src/commons/logger.py\n\nimport logging\nimport os\n\nlogformat = \"%(levelname)s %(asctime)s - %(message)s\"\nfilename = \"logfile.log\"\n\n# Setting the config of the log object\nlogging.basicConfig(\n    format=logformat,\n    filename=filename,\n    level=logging.INFO,\n)\n\n\n##\n# app/src/hello.py\n\nfrom datetime import datetime\nimport time\nimport commons.logger as logger\n\n\ndef main():\n    # run for about 5 min: 300 sec\n    for i in range(60):\n        now = datetime.now()\n        dt_string = now.strftime(\"%d/%m/%Y %H:%M:%S\")\n\n        # prepare message\n        msg = f\"hello world at {dt_string}\"\n\n        # put message to stdout and logs\n        print(msg)\n        logger.logging.info(msg)\n\n        # sleep for some seconds\n        time.sleep(5)\n\n\nif __name__ == \"__main__\":\n    main()\n\nWhen I run the hello.py file I get the output on the termial with hello world messages like this.\n\n\n\nhelloworld_output\n\n\nWhen we run the application a “logfile.log” will also appear in the project directory containing the logged messages.\n.\n├── app/\n│   └── src/\n│       ├── commons/\n│       │   └── logger.py\n│       └── hello.py\n└── **logfile.log**\nContents of “logfile.log” file will look like this\nINFO 2022-03-11 13:01:56,451 - hello world at 11/03/2022 13:01:56\nINFO 2022-03-11 13:02:01,464 - hello world at 11/03/2022 13:02:01\nINFO 2022-03-11 13:02:06,466 - hello world at 11/03/2022 13:02:06\nINFO 2022-03-11 13:02:11,480 - hello world at 11/03/2022 13:02:11\nINFO 2022-03-11 13:02:16,496 - hello world at 11/03/2022 13:02:16\nAll the code till this point can be found at GitHub repository https://github.com/hassaanbinaslam/snapshots-docker-post-11032022 * Project code files * Project zip file"
  },
  {
    "objectID": "posts/2022-03-14-docker-app-debug.html#sample-application",
    "href": "posts/2022-03-14-docker-app-debug.html#sample-application",
    "title": "Docker - Debugging Python Application",
    "section": "Sample Application",
    "text": "Sample Application\nFor this post I will use a a simple hello world application that will print “hello world” messages to stdout, and also logs them in a logfilelog. After each message the application sleeps for 5 seconds, and keeps on doing this for 5 mins (300 sec). After this the program exists.\nThis application was created as part of the previous blog post Docker - Accessing Python Application Logs. It is a very simple application, and you can find all the code in GitHub repository snapshots-docker-post-11032022 * Project code files * Project zip file\nProject structure of this application is\napp/\n├── src/\n│   ├── commons/\n│   │   └── logger.py\n│   └── hello.py\n└── Dockerfile\nWhere * app/ is the project root folder * src/ folder contain the python application code * src/commons/logger.py is the logging module * src/hello.py is the main application * app/Dockerfile is the Docker image build file\nWhen I run the src/hello.py file from my local machine (Windows 10) I get the output on the termial with hello world messages like this.\n\n\n\napp-run-local\n\n\nA “logfile.log” will also appear in the project directory containing the logged messages.\n.\n├── app/\n│   ├── src/\n│   │   ├── commons/\n│   │   │   └── logger.py\n│   │   └── hello.py\n│   └── Dockerfile\n└── **logfile.log**\nContents of “logfile.log” file will look like this\nINFO 2022-03-11 13:01:56,451 - hello world at 11/03/2022 13:01:56\nINFO 2022-03-11 13:02:01,464 - hello world at 11/03/2022 13:02:01\nINFO 2022-03-11 13:02:06,466 - hello world at 11/03/2022 13:02:06\nINFO 2022-03-11 13:02:11,480 - hello world at 11/03/2022 13:02:11\nINFO 2022-03-11 13:02:16,496 - hello world at 11/03/2022 13:02:16\nWe can build our docker image and run it using commands\ndocker build --tag python-docker .\ndocker run --name helloworld python-docker\nOutput on the terminal will be like this \nNotice the difference in the print message when the application was is locally, and from the docker container. * Local (Win10) message = hello world at 14/03/2022 18:04:02 from Windows * Docker container message = hello world at 14/03/2022 13:12:14 from Linux"
  },
  {
    "objectID": "posts/2022-03-16-lambda-debug.html#sample-application",
    "href": "posts/2022-03-16-lambda-debug.html#sample-application",
    "title": "AWS Lambda - Test and Debug Locally in Visual Studio Code",
    "section": "Sample Application",
    "text": "Sample Application\nFor this post we will use a simple hello world application as our focus is on debugging. We will use AWS SAM CLI to create our application. You can follow the steps provided in tutorial AWS SAM Developer Guide>Getting started with AWS SAM to create this application.\nFrom the provided link (SAM Developer Guide): This application implements a basic API backend. It consists of an Amazon API Gateway endpoint and an AWS Lambda function. When you send a GET request to the API Gateway endpoint, the Lambda function is invoked. This function returns a hello world message.\nThe following diagram shows the components of this application:\n\n\n\nsam-getting-started-hello-world\n\n\nTo initialize a serverless app use command\nsam init\nComplete the SAM initialization setup steps\nPS C:\\MyWorkspace\\gitrepos\\2022-03-16-lambda-debug> sam init\n\nYou can preselect a particular runtime or package type when using the `sam init` experience.\nCall `sam init --help` to learn more.\n\nWhich template source would you like to use?\n        1 - AWS Quick Start Templates\n        2 - Custom Template Location\nChoice: 1\n\nChoose an AWS Quick Start application template\n        1 - Hello World Example\n        2 - Multi-step workflow\n        3 - Serverless API\n        4 - Scheduled task\n        5 - Standalone function\n        6 - Data processing\n        7 - Infrastructure event management\n        8 - Machine Learning\nTemplate: 1\n\n Use the most popular runtime and package type? (Python and zip) [y/N]: y\n\nProject name [sam-app]:\n\nCloning from https://github.com/aws/aws-sam-cli-app-templates (process may take a moment)\n\n    -----------------------\n    Generating application:\n    -----------------------\n    Name: sam-app\n    Runtime: python3.9\n    Architectures: x86_64\n    Dependency Manager: pip\n    Application Template: hello-world\n    Output Directory: .\n\n    Next steps can be found in the README file at ./sam-app/README.md\n\n\n    Commands you can use next\n    =========================\n    [*] Create pipeline: cd sam-app && sam pipeline init --bootstrap\n    [*] Test Function in the Cloud: sam sync --stack-name {stack-name} --watch\nOnce the application is initialized the project structure will look like this\nsam-app/\n   ├── README.md\n   ├── events/\n   │   └── event.json\n   ├── hello_world/\n   │   ├── __init__.py\n   │   ├── app.py            #Contains your AWS Lambda handler logic.\n   │   └── requirements.txt  #Contains any Python dependencies the application requires, used for sam build\n   ├── template.yaml         #Contains the AWS SAM template defining your application's AWS resources.\n   └── tests/\n       └── unit/\n           ├── __init__.py\n           └── test_handler.py\nThere are three especially important files:\n\ntemplate.yaml: Contains the AWS SAM template that defines your application’s AWS resources.\nhello_world/app.py: Contains your actual Lambda handler logic.\nhello_world/requirements.txt: Contains any Python dependencies that the application requires, and is used for sam build.\n\nFollow the instructions from the tutorial to build, test, and deploy the application.\nAll the code used for this post can be obtained from the GitHub repository hassaanbinaslam/2022-03-16-lambda-debug\n\nProject code files\nProject zip file"
  },
  {
    "objectID": "posts/2022-03-16-lambda-debug.html#option-1-debug-through-sam-template",
    "href": "posts/2022-03-16-lambda-debug.html#option-1-debug-through-sam-template",
    "title": "AWS Lambda - Test and Debug Locally in Visual Studio Code",
    "section": "Option 1: Debug through SAM template",
    "text": "Option 1: Debug through SAM template\nFrom VSCode open template.yaml, and go to the resources section of the template that defines serverless resources. Click on the lambda function resource, which in our case is HelloWorldFunction. A tooltip will appear over it saying AWS: Add Debug Configuration. Click it as shown below.\n\n\n\nsam-template-add-debug-config\n\n\nThis will create a new folder in the project with debug launch configuration launch.json.\n\n\n\ndebug-launch-config\n\n\nLet’s add a breakpoint in our lambda handler code hello_world/app.py, and start debugging by clicking the green “play” button in the RUN view. When the debugging sessions starts, the DEBUG CONSOLE panel shows debugging output and displays any values returned by the Lambda function.\n\n\n\nlambda-debug-template"
  },
  {
    "objectID": "posts/2022-03-16-lambda-debug.html#option-2-debug-lambda-directly-from-code",
    "href": "posts/2022-03-16-lambda-debug.html#option-2-debug-lambda-directly-from-code",
    "title": "AWS Lambda - Test and Debug Locally in Visual Studio Code",
    "section": "Option 2: Debug Lambda Directly from Code",
    "text": "Option 2: Debug Lambda Directly from Code\nFrom VSCode open lambda handler code sam-app/hello_world/app.py. A tooltip will appear above the lambda_handler function with options * AWS: Add Debug Configuration * AWS: Edit Debug Configuration\n\n\n\nlambda-debug-tooltip\n\n\nClick on AWS: Add Debug Configuration and it will show two further options * template.yaml:HelloWorldFunction (to debug only the lambda function) * template.yaml:HelloWorldFunction (API Event: HelloWorld) (to debug lambda function along with API gateway)\n\n\n\nlambda-debug-options\n\n\nLet’s select API option this time. It will again create a launch configuration, and now we can debug our code. Click on the green “play” button again to start the debug session with request request coming from API Gateway to Lambda function.\nYou can also edit the debug config visually by selecting the AWS: Edit Debug Configuration, and a side pane will appear from where we can easily edit and update our debug configuration.\n\n\n\nedit-debug-config\n\n\nAll the code used for this post can be obtained from the GitHub repository hassaanbinaslam/2022-03-16-lambda-debug\n\nProject code files\nProject zip file"
  },
  {
    "objectID": "posts/2022-03-28-efs-s3-sync-lambda.html#create-an-s3-bucket",
    "href": "posts/2022-03-28-efs-s3-sync-lambda.html#create-an-s3-bucket",
    "title": "AWS EFS Sync to S3 Using Lambda",
    "section": "Create an S3 bucket",
    "text": "Create an S3 bucket\nLet’s first create an S3 bucket that will contain our data, and this is the bucket we would like to be in sync with EFS. I am naming the bucket mydata-202203. You may name it as you please. Choose a region of your choice and leave the rest of the settings as defaults."
  },
  {
    "objectID": "posts/2022-03-28-efs-s3-sync-lambda.html#create-a-lambda-function",
    "href": "posts/2022-03-28-efs-s3-sync-lambda.html#create-a-lambda-function",
    "title": "AWS EFS Sync to S3 Using Lambda",
    "section": "Create a Lambda function",
    "text": "Create a Lambda function\nNow create a lambda function that will receive event notifications from the S3 bucket, and sync files on efs. I am naming it mydata-sync and our runtime will be Python 3.9. Keep the rest of the settings as default, and create the function."
  },
  {
    "objectID": "posts/2022-03-28-efs-s3-sync-lambda.html#create-s3-event-notifications",
    "href": "posts/2022-03-28-efs-s3-sync-lambda.html#create-s3-event-notifications",
    "title": "AWS EFS Sync to S3 Using Lambda",
    "section": "Create S3 event notifications",
    "text": "Create S3 event notifications\nFrom the bucket, mydata-sync go to Properties. Scroll down to Event notifications and click create. Give any name to the event. I am calling it object-sync. From the provided event types select * s3:ObjectCreated:Put * s3:ObjectRemoved:Delete\nFrom the section Destination select Lambda Function, and from the list choose the lambda function name we created in the last section mydata-sync\nClick Save Changes"
  },
  {
    "objectID": "posts/2022-03-28-efs-s3-sync-lambda.html#test-s3-notifications",
    "href": "posts/2022-03-28-efs-s3-sync-lambda.html#test-s3-notifications",
    "title": "AWS EFS Sync to S3 Using Lambda",
    "section": "Test S3 notifications",
    "text": "Test S3 notifications\nLet’s now test if S3 event notifications are being received by our lambda function. For this update lambda function code and simply print the event received. After updating the lambda function, make sure to deploy it.\n\nimport json\n\ndef lambda_handler(event, context):\n    print(event)\n    return {\n        'statusCode': 200,\n        'body': json.dumps('Hello from Lambda!')\n    }\n\nNow upload some files in our S3 bucket, and it should trigger our lambda function. For testing, I have uploaded an empty test1.txt file in our bucket. Once successfully uploaded I check the Lambda function logs to see if any event is received. For this go to lambda function mydata-sync > Monitor > Logs > View logs in CloudWatch. For the CloudWatch console view the latest log stream. Below is the event I have received in the logs\n{'Records': [{'eventVersion': '2.1', 'eventSource': 'aws:s3', 'awsRegion': 'us-east-1', 'eventTime': '2022-03-28T16:08:00.896Z', 'eventName': 'ObjectCreated:Put', 'userIdentity': {'principalId': 'AWS:AIDA3VIXXJNKIVU6P5NY3'}, 'requestParameters': {'sourceIPAddress': '202.163.113.76'}, 'responseElements': {'x-amz-request-id': '39MD61ZS00SNK2RT', 'x-amz-id-2': 'U+zPUWOrfzTuVi7kbaBONLHoJXKqUICsVqyKBg4yPKYbUV7pQLGc4Z5A2fSIVvDFtSJHC6v29EDJoXhypWsj2wXanUu8YrLocr3z+yK1qoo='}, 's3': {'s3SchemaVersion': '1.0', 'configurationId': 'object-sync', 'bucket': {'name': 'mydata-202203', 'ownerIdentity': {'principalId': 'AYAQOSFZ1VPK'}, 'arn': 'arn:aws:s3:::mydata-202203'}, 'object': {'key': 'test1.txt', 'size': 0, 'eTag': 'd41d8cd98f00b204e9800998ecf8427e', 'sequencer': '006241DD60D67A4556'}}}]}\nlet’s load this event in a dictionary and find some important parameters\n\nevent = {'Records': [{'eventVersion': '2.1', 'eventSource': 'aws:s3', 'awsRegion': 'us-east-1', 'eventTime': '2022-03-28T16:08:00.896Z', 'eventName': 'ObjectCreated:Put', 'userIdentity': {'principalId': 'AWS:AIDA3VIXXJNKIVU6P5NY3'}, 'requestParameters': {'sourceIPAddress': '202.163.113.76'}, 'responseElements': {'x-amz-request-id': '39MD61ZS00SNK2RT', 'x-amz-id-2': 'U+zPUWOrfzTuVi7kbaBONLHoJXKqUICsVqyKBg4yPKYbUV7pQLGc4Z5A2fSIVvDFtSJHC6v29EDJoXhypWsj2wXanUu8YrLocr3z+yK1qoo='}, 's3': {'s3SchemaVersion': '1.0', 'configurationId': 'object-sync', 'bucket': {'name': 'mydata-202203', 'ownerIdentity': {'principalId': 'AYAQOSFZ1VPK'}, 'arn': 'arn:aws:s3:::mydata-202203'}, 'object': {'key': 'test1.txt', 'size': 0, 'eTag': 'd41d8cd98f00b204e9800998ecf8427e', 'sequencer': '006241DD60D67A4556'}}}]}\nevent\n\n{'Records': [{'eventVersion': '2.1',\n   'eventSource': 'aws:s3',\n   'awsRegion': 'us-east-1',\n   'eventTime': '2022-03-28T16:08:00.896Z',\n   'eventName': 'ObjectCreated:Put',\n   'userIdentity': {'principalId': 'AWS:AIDA3VIXXJNKIVU6P5NY3'},\n   'requestParameters': {'sourceIPAddress': '202.163.113.76'},\n   'responseElements': {'x-amz-request-id': '39MD61ZS00SNK2RT',\n    'x-amz-id-2': 'U+zPUWOrfzTuVi7kbaBONLHoJXKqUICsVqyKBg4yPKYbUV7pQLGc4Z5A2fSIVvDFtSJHC6v29EDJoXhypWsj2wXanUu8YrLocr3z+yK1qoo='},\n   's3': {'s3SchemaVersion': '1.0',\n    'configurationId': 'object-sync',\n    'bucket': {'name': 'mydata-202203',\n     'ownerIdentity': {'principalId': 'AYAQOSFZ1VPK'},\n     'arn': 'arn:aws:s3:::mydata-202203'},\n    'object': {'key': 'test1.txt',\n     'size': 0,\n     'eTag': 'd41d8cd98f00b204e9800998ecf8427e',\n     'sequencer': '006241DD60D67A4556'}}}]}\n\n\n\n##\n# event name\nevent['Records'][0]['eventName']\n\n'ObjectCreated:Put'\n\n\n\n##\n# bucket name\nevent['Records'][0]['s3']['bucket']['name']\n\n'mydata-202203'\n\n\n\n##\n# uploaded object key\nevent['Records'][0]['s3']['object']['key']\n\n'test1.txt'\n\n\nAlright, we have seen that we are receiving notifications from S3 bucket so let’s now move on to the next section."
  },
  {
    "objectID": "posts/2022-03-28-efs-s3-sync-lambda.html#create-an-efs",
    "href": "posts/2022-03-28-efs-s3-sync-lambda.html#create-an-efs",
    "title": "AWS EFS Sync to S3 Using Lambda",
    "section": "Create an EFS",
    "text": "Create an EFS\nFrom EFS console give name as mydata-efs. I am using default VPC for this post. Use Regional availability settings. Click Create. Once file system is created, click on Access points and create an access point for this efs to be mounted in other service. For access point use following settings * name = mydata-ap * root dir path = /efs * POSIX User * POSIX UID = 1000 * Group ID = 1000 * Root directory creation permissions * Owner user id = 1000 * Owner group id = 1000 * POSIX permissions = 777\nClick Create.\nHere I have used the root dir path as /efs this means that from this access point my access will be limited to folder /efs. If you want to provide full access to all folders then set to root path to /."
  },
  {
    "objectID": "posts/2022-03-28-efs-s3-sync-lambda.html#note-on-efs-security-group-settings",
    "href": "posts/2022-03-28-efs-s3-sync-lambda.html#note-on-efs-security-group-settings",
    "title": "AWS EFS Sync to S3 Using Lambda",
    "section": "Note on EFS security group settings",
    "text": "Note on EFS security group settings\nIn the last section, I have used a default VPC security group (sg) while creating EFS. default sg allows traffic for all protocols and all ports, both for inbound and outbound traffic. But if you are using a custom security group then make sure that you have an inbound rule for * Type = NFS * Protocol = TCP * Port range = 2049\nOtherwise, you will not be able to access EFS using NFS clients."
  },
  {
    "objectID": "posts/2022-03-28-efs-s3-sync-lambda.html#mount-efs-to-lambda-function",
    "href": "posts/2022-03-28-efs-s3-sync-lambda.html#mount-efs-to-lambda-function",
    "title": "AWS EFS Sync to S3 Using Lambda",
    "section": "Mount EFS to Lambda Function",
    "text": "Mount EFS to Lambda Function\nTo mount an EFS to the Lambda function requires some additional steps.\nFirst add permissions to Lambda function. From lambda function > Configurations > Permissions > Execution role. Click on the execution role to open it in IAM concole. For the selected role attach an additional policy AmazonElasticFileSystemFullAccess.\nSecond, add the lambda to a VPC group in which efs was created. We have created efs in default VPC so let’s add lambda to it. For this from lambda Configurations > VPC click edit. For the next pane select default VPC, all subnets, default VPC security group, and click save.\nNow we can add EFS to lambda. Go to lambda Configurations > File Systems > Add file system. Select the file system mydata-efs and associated access point mydata-ap and local mount point as /mnt/efs. The local mount point is the mounted directory from where we can access our EFS from inside the lambda environment. Click Save"
  },
  {
    "objectID": "posts/2022-03-28-efs-s3-sync-lambda.html#check-efs-mount-point-from-lambda",
    "href": "posts/2022-03-28-efs-s3-sync-lambda.html#check-efs-mount-point-from-lambda",
    "title": "AWS EFS Sync to S3 Using Lambda",
    "section": "Check EFS mount point from Lambda",
    "text": "Check EFS mount point from Lambda\nLet’s verify from lambda that EFS has been mounted and can we access it. So update the lambda code as below and deploy it.\n\nimport json\nimport os\n\ndef lambda_handler(event, context):\n    mount_path = '/mnt/efs'\n    if os.path.exists(mount_path):\n        print(f\"{mount_path} exists\")\n        print(os.listdir('/mnt/efs'))\n    \n    print(event)\n\n    return {\n        'statusCode': 200,\n        'body': json.dumps('Hello from Lambda!')\n    }\n\nNow test this code using a test event S3 Put. For this go to lambda Test > Create new event > Template (s3-put). ‘S3 Put’ test event is similar to the one we saw in the last section. We can use this request template to simulate the event received from S3 bucket. Once the test is successfully executed, check the log output.\nSTART RequestId: 2e307a14-f373-46d5-b763-594d5f406ae6 Version: $LATEST\n/mnt/efs exists\n[]\n{'Records': [{'eventVersion': '2.0', 'eventSource': 'aws:s3', 'awsRegion': 'us-east-1', 'eventTime': '1970-01-01T00:00:00.000Z', 'eventName': 'ObjectCreated:Put', 'userIdentity': {'principalId': 'EXAMPLE'}, 'requestParameters': {'sourceIPAddress': '127.0.0.1'}, 'responseElements': {'x-amz-request-id': 'EXAMPLE123456789', 'x-amz-id-2': 'EXAMPLE123/5678abcdefghijklambdaisawesome/mnopqrstuvwxyzABCDEFGH'}, 's3': {'s3SchemaVersion': '1.0', 'configurationId': 'testConfigRule', 'bucket': {'name': 'example-bucket', 'ownerIdentity': {'principalId': 'EXAMPLE'}, 'arn': 'arn:aws:s3:::example-bucket'}, 'object': {'key': 'test%2Fkey', 'size': 1024, 'eTag': '0123456789abcdef0123456789abcdef', 'sequencer': '0A1B2C3D4E5F678901'}}}]}\nEND RequestId: 2e307a14-f373-46d5-b763-594d5f406ae6\nREPORT RequestId: 2e307a14-f373-46d5-b763-594d5f406ae6  Duration: 7.02 ms   Billed Duration: 8 ms   Memory Size: 128 MB Max Memory Used: 37 MB  Init Duration: 93.81 ms \nFrom the logs we can see that the mounted EFS directory exists /mnt/efs but currently the folder is empty."
  },
  {
    "objectID": "posts/2022-03-28-efs-s3-sync-lambda.html#configure-vpc-endpoint-for-s3",
    "href": "posts/2022-03-28-efs-s3-sync-lambda.html#configure-vpc-endpoint-for-s3",
    "title": "AWS EFS Sync to S3 Using Lambda",
    "section": "Configure VPC endpoint for S3",
    "text": "Configure VPC endpoint for S3\nTill now we have configured S3 notifications to trigger a lambda function and also mounted EFS to it. Our next step is to process the event received in lambda, and download the file from S3 to EFS. But since our lambda function is configured for a VPC we cannot connect to S3 from it. Even though we can still receive S3 event notification, when we try to connect to S3 to download any file we will get a timeout error. To fix this we will create a VPC endpoint for S3 bucket.\nFor this go to VPC console > Endpoints > Create endpoint, and set the following * name = mydata-ep * service category = aws services * services = com.amazonaws.us-east-1.s3 (Gateway) * vpc = default * route table = default (main route table) * policy = full access\nClick Create endpoint"
  },
  {
    "objectID": "posts/2022-03-28-efs-s3-sync-lambda.html#configure-s3-permissions-for-lambda",
    "href": "posts/2022-03-28-efs-s3-sync-lambda.html#configure-s3-permissions-for-lambda",
    "title": "AWS EFS Sync to S3 Using Lambda",
    "section": "Configure S3 permissions for Lambda",
    "text": "Configure S3 permissions for Lambda\nFor lambda to be able to connect to S3 we also need to give it proper permissions. For this go to Lambda > Configurations > Permissions > Execution Role > click on role name. From the IAM Role console select add permissions, and then select AmazonS3FullAccess"
  },
  {
    "objectID": "posts/2022-03-28-efs-s3-sync-lambda.html#process-s3-event-notifications",
    "href": "posts/2022-03-28-efs-s3-sync-lambda.html#process-s3-event-notifications",
    "title": "AWS EFS Sync to S3 Using Lambda",
    "section": "Process S3 event notifications",
    "text": "Process S3 event notifications\nOur lambda and EFS are ready and we can now process S3 events. Update the lambda code as below to process S3 events. It will download and delete from EFS to keep it in sync with S3 bucket.\n\nimport json\nimport boto3\nimport os\n\ns3 = boto3.client(\"s3\")\n\ndef lambda_handler(event, context):\n\n    event_name = event[\"Records\"][0][\"eventName\"]\n    bucket_name = event[\"Records\"][0][\"s3\"][\"bucket\"][\"name\"]\n    object_key = event[\"Records\"][0][\"s3\"][\"object\"][\"key\"]\n\n    efs_file_name = \"/mnt/efs/\" + object_key\n\n    # S3 put\n    if event_name == \"ObjectCreated:Put\":\n        s3.download_file(bucket_name, object_key, efs_file_name)\n        print(f\"file downloaded: {efs_file_name}\")\n\n    # S3 delete\n    if event_name == \"ObjectRemoved:Delete\":\n        # check if file exists on efs\n        if os.path.exists(efs_file_name):\n            os.remove(efs_file_name)\n            print(f\"file deleted: {efs_file_name}\")\n\n    return {\"statusCode\": 200, \"body\": json.dumps(event)}\n\nWe can test this code using the S3-put test event we used last time. Modify the event for bucket name and object key as below.\n{\n  \"Records\": [\n    {\n      \"eventVersion\": \"2.0\",\n      \"eventSource\": \"aws:s3\",\n      \"awsRegion\": \"us-east-1\",\n      \"eventTime\": \"1970-01-01T00:00:00.000Z\",\n      \"eventName\": \"ObjectCreated:Put\",\n      \"userIdentity\": {\n        \"principalId\": \"EXAMPLE\"\n      },\n      \"requestParameters\": {\n        \"sourceIPAddress\": \"127.0.0.1\"\n      },\n      \"responseElements\": {\n        \"x-amz-request-id\": \"EXAMPLE123456789\",\n        \"x-amz-id-2\": \"EXAMPLE123/5678abcdefghijklambdaisawesome/mnopqrstuvwxyzABCDEFGH\"\n      },\n      \"s3\": {\n        \"s3SchemaVersion\": \"1.0\",\n        \"configurationId\": \"testConfigRule\",\n        \"bucket\": {\n          \"name\": \"mydata-202203\",\n          \"ownerIdentity\": {\n            \"principalId\": \"EXAMPLE\"\n          },\n          \"arn\": \"arn:aws:s3:::example-bucket\"\n        },\n        \"object\": {\n          \"key\": \"test1.txt\",\n          \"size\": 1024,\n          \"eTag\": \"0123456789abcdef0123456789abcdef\",\n          \"sequencer\": \"0A1B2C3D4E5F678901\"\n        }\n      }\n    }\n  ]\n}\nClick test. From the output logs, we can see that our code was able to download the file from S3 bucket and write it on EFS.\nSTART RequestId: 7e9c0dc2-f970-426e-8372-e59b07f5536c Version: $LATEST\nfile downloaded: /mnt/efs/test1.txt\nEND RequestId: 7e9c0dc2-f970-426e-8372-e59b07f5536c\nREPORT RequestId: 7e9c0dc2-f970-426e-8372-e59b07f5536c  Duration: 370.00 ms Billed Duration: 371 ms Memory Size: 128 MB Max Memory Used: 72 MB  Init Duration: 367.68 ms\nNote that if you get any permission errors then it could be due to the mounting path errors. Please do check the access point path and lambda mount path."
  },
  {
    "objectID": "posts/2022-03-28-efs-s3-sync-lambda.html#verify-file-on-efs",
    "href": "posts/2022-03-28-efs-s3-sync-lambda.html#verify-file-on-efs",
    "title": "AWS EFS Sync to S3 Using Lambda",
    "section": "Verify file on EFS",
    "text": "Verify file on EFS\nWe can verify files on EFS by directly mounting them to an EC2 machine and verifying from there. So let’s do that.\nCreate an EC2 machine * AMI = Amazon Linux 2 AMI (HVM) - Kernel 5.10, SSD Volume Type * Intance type = t2.micro (free tier) * Instance details * Network = default VPC * Auto-assign Public IP = Enable * Review and Lanunch > Launch > Proceed without key pair.\nOnce the instance is up and running, click on it and connect using EC2 instance connect option. Create a dir ‘efs’ using the command\nmkdir efs\nIn a separate tab open EFS, and click on the file system we have created. Click Attach. From “Mount via DNS” copy command for NFS client. paste that in EC2 bash terminal\nsudo mount -t nfs4 -o nfsvers=4.1,rsize=1048576,wsize=1048576,hard,timeo=600,retrans=2,noresvport fs-0c9526e2f48ece247.efs.us-east-1.amazonaws.com:/ efs\nOnce successfully mounted verify that the file ‘test1.txt’ exists in EFS. We can also delete the file from S3 and similarly verify from EFS that the file has been removed."
  },
  {
    "objectID": "posts/2022-03-29-efs-s3-datasync.html#create-an-s3-bucket",
    "href": "posts/2022-03-29-efs-s3-datasync.html#create-an-s3-bucket",
    "title": "AWS EFS Sync to S3 Using DataSync",
    "section": "Create an S3 bucket",
    "text": "Create an S3 bucket\nLet’s first create an S3 bucket that will contain our data, and this is the bucket we would like to be in sync with EFS. I am naming the bucket as mydata-202203. You may name it as you please. Choose a region of your choice and leave the rest of the settings as defaults."
  },
  {
    "objectID": "posts/2022-03-29-efs-s3-datasync.html#create-an-efs",
    "href": "posts/2022-03-29-efs-s3-datasync.html#create-an-efs",
    "title": "AWS EFS Sync to S3 Using DataSync",
    "section": "Create an EFS",
    "text": "Create an EFS\nFrom EFS console give name as mydata-efs. I am using default VPC for this post. Use Regional availability settings. Click Create. Once file system is created, click on Access points and create an access point for this efs to be mounted in other service. For access point use following settings * name = mydata-ap * root dir path = / * POSIX User * POSIX UID = 1000 * Group ID = 1000 * Root directory creation permissions * Owner user id = 1000 * Owner group id = 1000 * POSIX permissions = 777\nClick Create."
  },
  {
    "objectID": "posts/2022-03-29-efs-s3-datasync.html#note-on-efs-security-group-settings",
    "href": "posts/2022-03-29-efs-s3-datasync.html#note-on-efs-security-group-settings",
    "title": "AWS EFS Sync to S3 Using DataSync",
    "section": "Note on EFS security group settings",
    "text": "Note on EFS security group settings\nIn the last section, I have used a default VPC security group (sg) while creating EFS. Default sg allows traffic for all protocols and all ports, both inbound and outbound. But if you are using a custom security group then make sure that you have an inbound rule for * Type = NFS * Protocol = TCP * Port range = 2049\nOtherwise, you will not be able to access EFS using NFS clients, and if you find an error similar to the below then it means you need to check the security group settings.\n\n\n\nsecurity-group-error"
  },
  {
    "objectID": "posts/2022-03-29-efs-s3-datasync.html#create-datasync-service-task",
    "href": "posts/2022-03-29-efs-s3-datasync.html#create-datasync-service-task",
    "title": "AWS EFS Sync to S3 Using DataSync",
    "section": "Create DataSync service task",
    "text": "Create DataSync service task\n\nconfigure source location = create a new location\n\nlocation type = Amazon S3\nregion = us-east-1\ns3 bucket = mydata-202203\ns3 storage class = standard\nfolder = [leave empty]\nIAM role = click on auto generate\n\nconfigure destination location = create a new location\n\nlocation type = EFS\nregion = us-east-1\nefs file system = mydata-efs\nmount path = /efs\nsubnet = us-east-1a\nsecurity group = default\n\nconfigure settings\n\ntask name = mydata-datasync\ntask execution configuration\n\nverify data = verify only the data transferred\nset bandwidth limit = use available\n\ndata transfer configuration\n\ndata to scan = entire source location\ntransfer mode = transfer only the data that has changed\nuncheck “keep deleted files”\ncheck “overwrite files”\n\nschedule\n\nfrequency = not scheduled\n\ntask logging\n\ncloudwatch log group = autogenerate\n\n\n\nClick “next”. Review and Launch."
  },
  {
    "objectID": "posts/2022-03-29-efs-s3-datasync.html#test-datasync-service",
    "href": "posts/2022-03-29-efs-s3-datasync.html#test-datasync-service",
    "title": "AWS EFS Sync to S3 Using DataSync",
    "section": "Test DataSync Service",
    "text": "Test DataSync Service\nLet’s test datasync service by manually starting it. If S3 bucket is empty then datasync will throw an exception as below\n\n\n\ns3-empty\n\n\nThis is not an issue. Just place some files (test1.txt in my case) in the bucket and start the datasync service again. If it executes successfully then you will get a message as Execution Status = Success"
  },
  {
    "objectID": "posts/2022-03-29-efs-s3-datasync.html#datasync-can-work-without-internet-gateway-or-vpc-endpoint",
    "href": "posts/2022-03-29-efs-s3-datasync.html#datasync-can-work-without-internet-gateway-or-vpc-endpoint",
    "title": "AWS EFS Sync to S3 Using DataSync",
    "section": "DataSync can work without Internet Gateway or VPC Endpoint",
    "text": "DataSync can work without Internet Gateway or VPC Endpoint\nOne thing I noticed is that DataSync service can work even without the presence of an internet gateway or S3 service endpoint. EFS is VPC bound and S3 is global but DataSync can still communicate with both of them. This was different for Lambda. Once Lambda is configured for a VPC then it is not able to access S3 without an internet gateway or VPC endpoint."
  },
  {
    "objectID": "posts/2022-03-29-efs-s3-datasync.html#verify-efs-by-mounting-it-to-the-ec2-machine",
    "href": "posts/2022-03-29-efs-s3-datasync.html#verify-efs-by-mounting-it-to-the-ec2-machine",
    "title": "AWS EFS Sync to S3 Using DataSync",
    "section": "Verify EFS by mounting it to the EC2 machine",
    "text": "Verify EFS by mounting it to the EC2 machine\nIn the last section, we ran DataSync and it successfully copied files from S3 to EFS. So let’s verify our files from EFS by mounting it to an EC2 instance.\nCreate an EC2 machine * AMI = Amazon Linux 2 AMI (HVM) - Kernel 5.10, SSD Volume Type * Intance type = t2.micro (free tier) * Instance details * Network = default VPC * Auto-assign Public IP = Enable * Review and Lanunch > Launch > Proceed without key pair.\nOnce the instance is up and running, click on it and connect using EC2 instance connect option. Create a dir ‘efs’ using the command\nmkdir efs\nIn a separate tab open EFS, and click on the file system we have created. Click Attach. From “Mount via DNS” copy command for NFS client. paste that in EC2 bash terminal\nsudo mount -t nfs4 -o nfsvers=4.1,rsize=1048576,wsize=1048576,hard,timeo=600,retrans=2,noresvport fs-01acd308743098251.efs.us-east-1.amazonaws.com:/ efs\nOnce successfully mounted, verify that the file ‘test1.txt’ exists in EFS.\n\n\n\nefs-verify"
  },
  {
    "objectID": "posts/2022-03-29-efs-s3-datasync.html#create-lambda-function-to-trigger-datasync-task",
    "href": "posts/2022-03-29-efs-s3-datasync.html#create-lambda-function-to-trigger-datasync-task",
    "title": "AWS EFS Sync to S3 Using DataSync",
    "section": "Create Lambda function to trigger DataSync task",
    "text": "Create Lambda function to trigger DataSync task\nNow let’s create a lambda function that will trigger the datasync task. This function will itself be triggered by an S3 event notification whenever a file is uploaded or deleted.\n\nCreate a lambda function as\nname = datasync-trigger-s3\nruntime = Python 3.9\n\nLeave the rest of the settings as default, update the code as below, and deploy.\nIn the code, we are first filtering the object key for which the event is generated. Then we trigger the datasync task and pass the object key as a filter string. With the filter key provided datasync job will only sync provided object from S3 to EFS.\n\nimport json  \nimport boto3  \nimport os  \n  \nDataSync_task_arn = 'arn:aws:datasync:us-east-1:801598032724:task/task-0c04a4a15668b6b8a'  \nDataSync = boto3.client('datasync')\n      \ndef lambda_handler(event, context):  \n    objectKey = ''  \n    try:  \n        objectKey = event[\"Records\"][0][\"s3\"][\"object\"][\"key\"]  \n    except KeyError:  \n        raise KeyError(\"Received invalid event - unable to locate Object key to upload.\", event)  \n          \n    response = DataSync.start_task_execution(  \n        TaskArn=DataSync_task_arn,  \n        OverrideOptions={  \n            'OverwriteMode' : 'ALWAYS',\n            'PreserveDeletedFiles' : 'REMOVE',\n        },  \n        Includes=[  \n            {  \n                'FilterType': 'SIMPLE_PATTERN',  \n                'Value': '/' + os.path.basename(objectKey)  \n            }  \n        ]  \n    )  \n      \n    print(f\"response= {response}\")\n    return {  \n        'response' : response  \n    } \n\nAdd policy AWSDataSyncFullAccess to this lambda function role otherwise it will not be able to trigger datasync task."
  },
  {
    "objectID": "posts/2022-03-29-efs-s3-datasync.html#configure-s3-bucket-event-notifications",
    "href": "posts/2022-03-29-efs-s3-datasync.html#configure-s3-bucket-event-notifications",
    "title": "AWS EFS Sync to S3 Using DataSync",
    "section": "Configure S3 bucket event notifications",
    "text": "Configure S3 bucket event notifications\nOur lambda function is ready. Now we can enable S3 bucket event notifications as put the lambda function as a target. For this from S3 bucket Properties > Event notifications > Create event notifications\n\nevent name = object-put-delete\nevent type = s3:ObjectCreated:Put, and s3:ObjectRemoved:Delete\ndestination = lambda function (datasync-trigger-s3)\n\nClick Save changes"
  },
  {
    "objectID": "posts/2022-03-29-efs-s3-datasync.html#test-datasync-task-through-s3-events-trigger",
    "href": "posts/2022-03-29-efs-s3-datasync.html#test-datasync-task-through-s3-events-trigger",
    "title": "AWS EFS Sync to S3 Using DataSync",
    "section": "Test DataSync task through S3 events trigger",
    "text": "Test DataSync task through S3 events trigger\nNow let’s test our trigger by placing a new file in S3 bucket. In my case it is ‘test2.txt’. Once file is successfully uploaded we can check the EC2 instance to verify the file presence.\n\n\n\nec2-verify-files\n\n\nWe can also verify that the datasync job was triggered from lambda CloudWatch logs.\nresponse= {'TaskExecutionArn': 'arn:aws:datasync:us-east-1:801598032724:task/task-0c04a4a15668b6b8a/execution/exec-020e456f670ca2419', 'ResponseMetadata': {'RequestId': 'c8166ce4-ef14-415c-beff-09cc7720f4a3', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Wed, 30 Mar 2022 13:27:45 GMT', 'content-type': 'application/x-amz-json-1.1', 'content-length': '123', 'connection': 'keep-alive', 'x-amzn-requestid': 'c8166ce4-ef14-415c-beff-09cc7720f4a3'}, 'RetryAttempts': 0}}\nIn the logs we have task execution id exec-020e456f670ca2419 , and we can use that to verify task’s status from datasync console.\n\n\n\ndatasync-task-status"
  },
  {
    "objectID": "posts/2022-03-29-efs-s3-datasync.html#create-a-lambda-function",
    "href": "posts/2022-03-29-efs-s3-datasync.html#create-a-lambda-function",
    "title": "AWS EFS Sync to S3 Using DataSync",
    "section": "Create a lambda function",
    "text": "Create a lambda function\nLet’s create a new lambda function with the following code. This lambda will invoke the datasync task. Add permissions to this lambda AWSDataSyncFullAccess\n\nfunction name = datasync-trigger-scheduled\nruntime = Python 3.9\n\n\nimport json  \nimport boto3  \nimport os  \n  \nDataSync_task_arn = 'arn:aws:datasync:us-east-1:801598032724:task/task-0c04a4a15668b6b8a'  \nDataSync = boto3.client('datasync')\n      \ndef lambda_handler(event, context):\n    response = DataSync.start_task_execution(  \n        TaskArn=DataSync_task_arn,  \n        OverrideOptions={  \n            'OverwriteMode' : 'ALWAYS',\n            'PreserveDeletedFiles' : 'REMOVE',\n        }\n    )  \n    \n    print(f\"response= {response}\")\n    return {  \n        'response' : response  \n    }"
  },
  {
    "objectID": "posts/2022-03-29-efs-s3-datasync.html#create-eventbridge-event",
    "href": "posts/2022-03-29-efs-s3-datasync.html#create-eventbridge-event",
    "title": "AWS EFS Sync to S3 Using DataSync",
    "section": "Create EventBridge event",
    "text": "Create EventBridge event\nGo to EventBridge Events > Rules > select Create Rule - Define rule details - name = datasync-trigger - event bus = default - rule type = scheduled - Define schedule - Sample event = {} - Schedule Pattern - Rate expression = 5 min - Select Targets - target = Lambda function - function = datasync-trigger-scheduled\nClick Next and Create Rule\nEventBridge will automatically add a policy statement to lambda function (datasync-trigger-scheduled) allowing it to trigger lambda. You can verify the policy from lambda Configurations > Permissions > Resource based policy. If no resource policy exists then you need to manually add a policy to allow EventBridge to invoke it. For this click on Resource based policy > Policy statements > Add permissions. * policy statement = AWS service * service = EventBridge * statement id = eventbridge-1000 (or any unique id) * principal = events.amazonaws.com * source ARN = arn:aws:events:us-east-1:801598032724:rule/datasync-trigger (arn for eventbridge event)"
  },
  {
    "objectID": "posts/2022-03-29-efs-s3-datasync.html#verify-event-and-datasync-task-execution",
    "href": "posts/2022-03-29-efs-s3-datasync.html#verify-event-and-datasync-task-execution",
    "title": "AWS EFS Sync to S3 Using DataSync",
    "section": "Verify event and datasync task execution",
    "text": "Verify event and datasync task execution\n\nWe have configured eventbridge to fire an event after every 5 min we can verify it from eventbrige monitoring tab and its cloudwatch logs.\nLambda function invocations can be verified from its cloudwatch logs\nDatasync task execution status can be verified from its history tab and cloudwatch logs."
  },
  {
    "objectID": "posts/2022-03-31-ml-nomenclature.html#dataset",
    "href": "posts/2022-03-31-ml-nomenclature.html#dataset",
    "title": "Machine Learning Nomenclature",
    "section": "Dataset",
    "text": "Dataset\nThe data we use in ML is usually defined as dataset, and datasets are a collection of data. The dataset contains the features and target to predict.\nIt has other names * data * input data * train and test data"
  },
  {
    "objectID": "posts/2022-03-31-ml-nomenclature.html#instance",
    "href": "posts/2022-03-31-ml-nomenclature.html#instance",
    "title": "Machine Learning Nomenclature",
    "section": "Instance",
    "text": "Instance\nAn instance is a row in the dataset.\nIs has other names * row * observation * sample * (data) point"
  },
  {
    "objectID": "posts/2022-03-31-ml-nomenclature.html#feature",
    "href": "posts/2022-03-31-ml-nomenclature.html#feature",
    "title": "Machine Learning Nomenclature",
    "section": "Feature",
    "text": "Feature\nFeature is a column in the dataset. It is used as an input used for prediction or classification. Features are commonly represented by x variable.\nIt has other names * column * attribute * (input) variable\nFeatures are of two types * Categorical or qualitative * Numerical or quantitative"
  },
  {
    "objectID": "posts/2022-03-31-ml-nomenclature.html#target",
    "href": "posts/2022-03-31-ml-nomenclature.html#target",
    "title": "Machine Learning Nomenclature",
    "section": "Target",
    "text": "Target\nIt is the information a machine learning algorithm learns to predict. Target is commonly represented by y variable.\nIt has other names * label * output"
  },
  {
    "objectID": "posts/2022-03-31-ml-nomenclature.html#labeled-data",
    "href": "posts/2022-03-31-ml-nomenclature.html#labeled-data",
    "title": "Machine Learning Nomenclature",
    "section": "Labeled Data",
    "text": "Labeled Data\nA data that has both the feature and target attributes defined"
  },
  {
    "objectID": "posts/2022-03-31-ml-nomenclature.html#unlabeled-data",
    "href": "posts/2022-03-31-ml-nomenclature.html#unlabeled-data",
    "title": "Machine Learning Nomenclature",
    "section": "Unlabeled Data",
    "text": "Unlabeled Data\nA data that has the features defined but has no target attribute."
  },
  {
    "objectID": "posts/2022-03-31-ml-nomenclature.html#categorical-feature",
    "href": "posts/2022-03-31-ml-nomenclature.html#categorical-feature",
    "title": "Machine Learning Nomenclature",
    "section": "Categorical Feature",
    "text": "Categorical Feature\nA feature that is not measureable and has discrete set of values like gender, family retionships, movie categories etc. We commonly use bar charts and pie graphs for categorical features.\nIt has other names * qualitative feature\nCategorical features are of two types * Nominal * Ordinal"
  },
  {
    "objectID": "posts/2022-03-31-ml-nomenclature.html#nominal-feature",
    "href": "posts/2022-03-31-ml-nomenclature.html#nominal-feature",
    "title": "Machine Learning Nomenclature",
    "section": "Nominal feature",
    "text": "Nominal feature\nNominal (categorical) feature is one that can not be measured and has no order assgined to it e.g. eye colors, gender etc."
  },
  {
    "objectID": "posts/2022-03-31-ml-nomenclature.html#ordinal-feature",
    "href": "posts/2022-03-31-ml-nomenclature.html#ordinal-feature",
    "title": "Machine Learning Nomenclature",
    "section": "Ordinal feature",
    "text": "Ordinal feature\nOrdinal (categorical) feature is one that can not be measured but has some order assgined to it like movie ratings, military ranks etc."
  },
  {
    "objectID": "posts/2022-03-31-ml-nomenclature.html#numerical-feature",
    "href": "posts/2022-03-31-ml-nomenclature.html#numerical-feature",
    "title": "Machine Learning Nomenclature",
    "section": "Numerical feature",
    "text": "Numerical feature\nNumerical features are those that can be measured or counted and have some ascending or descending order assigned to them.\nIt has other names * Continious feature * Quantitative feature\nNumerical features can be of two types * Discrete * Continous"
  },
  {
    "objectID": "posts/2022-03-31-ml-nomenclature.html#discrete-feature",
    "href": "posts/2022-03-31-ml-nomenclature.html#discrete-feature",
    "title": "Machine Learning Nomenclature",
    "section": "Discrete feature",
    "text": "Discrete feature\nDiscrete (numerical) feature is one that has specified values and are usually counted e.g. number of facebook like, number of tickets sold etc."
  },
  {
    "objectID": "posts/2022-03-31-ml-nomenclature.html#continous-feature",
    "href": "posts/2022-03-31-ml-nomenclature.html#continous-feature",
    "title": "Machine Learning Nomenclature",
    "section": "Continous feature",
    "text": "Continous feature\nContinous (numerical) feature is one that can have any value assigned to it, and is usually measured e.g. temperature, wind speed etc.\nData\n├── Categorical / Qualitative\n│   ├── Nominal\n│   └── Ordinal\n└── Numerical / Quantitative\n    ├── Discrete\n    └── Continous"
  },
  {
    "objectID": "posts/2022-03-31-ml-nomenclature.html#classification",
    "href": "posts/2022-03-31-ml-nomenclature.html#classification",
    "title": "Machine Learning Nomenclature",
    "section": "Classification",
    "text": "Classification\nIf the target feature is categorical then the ML task is called classification."
  },
  {
    "objectID": "posts/2022-03-31-ml-nomenclature.html#regression",
    "href": "posts/2022-03-31-ml-nomenclature.html#regression",
    "title": "Machine Learning Nomenclature",
    "section": "Regression",
    "text": "Regression\nIf the target feature is numerical then the ML task is called regression."
  },
  {
    "objectID": "posts/2022-03-31-ml-nomenclature.html#positive-class",
    "href": "posts/2022-03-31-ml-nomenclature.html#positive-class",
    "title": "Machine Learning Nomenclature",
    "section": "Positive class",
    "text": "Positive class\nIn binary classification the output class is usually labelled as positive or negative. The positive class is the thing we are testing for. For example, positive class for an email classifier is ‘spam’, and positive class for a medical test can be ‘tumor’."
  },
  {
    "objectID": "posts/2022-03-31-ml-nomenclature.html#negative-class",
    "href": "posts/2022-03-31-ml-nomenclature.html#negative-class",
    "title": "Machine Learning Nomenclature",
    "section": "Negative class",
    "text": "Negative class\nNegative class is the opposite to positive class. For example, negative class for an email classifier is ‘not spam’, and a negative class for a medical test can be ‘not tumor’."
  },
  {
    "objectID": "posts/2022-03-31-ml-nomenclature.html#true-positive-tp",
    "href": "posts/2022-03-31-ml-nomenclature.html#true-positive-tp",
    "title": "Machine Learning Nomenclature",
    "section": "True positive (TP)",
    "text": "True positive (TP)\nModel correctly predicted the positve class"
  },
  {
    "objectID": "posts/2022-03-31-ml-nomenclature.html#true-negative-tn",
    "href": "posts/2022-03-31-ml-nomenclature.html#true-negative-tn",
    "title": "Machine Learning Nomenclature",
    "section": "True negative (TN)",
    "text": "True negative (TN)\nModel correctly predicted the negative class"
  },
  {
    "objectID": "posts/2022-03-31-ml-nomenclature.html#false-positive-fp",
    "href": "posts/2022-03-31-ml-nomenclature.html#false-positive-fp",
    "title": "Machine Learning Nomenclature",
    "section": "False positive (FP)",
    "text": "False positive (FP)\nModel incorrectly predicted the positive class. Actual class is negative. It has other names * Type I error"
  },
  {
    "objectID": "posts/2022-03-31-ml-nomenclature.html#false-negative-fn",
    "href": "posts/2022-03-31-ml-nomenclature.html#false-negative-fn",
    "title": "Machine Learning Nomenclature",
    "section": "False negative (FN)",
    "text": "False negative (FN)\nModel incorrectly predicted the negative class. Actual class is positive. It has other names * Type II error"
  },
  {
    "objectID": "posts/2022-03-31-ml-nomenclature.html#accuracy",
    "href": "posts/2022-03-31-ml-nomenclature.html#accuracy",
    "title": "Machine Learning Nomenclature",
    "section": "Accuracy",
    "text": "Accuracy\nAccuracy = (TP + TN) / (TP + TN + FP + FN)"
  },
  {
    "objectID": "posts/2022-03-31-ml-nomenclature.html#precision",
    "href": "posts/2022-03-31-ml-nomenclature.html#precision",
    "title": "Machine Learning Nomenclature",
    "section": "Precision",
    "text": "Precision\nIt tells how accurate the positive predictions are.\nPrecision = TP / (TP + FP)\nIt is a good metric when cost of false positives is high."
  },
  {
    "objectID": "posts/2022-03-31-ml-nomenclature.html#true-positive-rate-tpr",
    "href": "posts/2022-03-31-ml-nomenclature.html#true-positive-rate-tpr",
    "title": "Machine Learning Nomenclature",
    "section": "True Positive Rate (TPR)",
    "text": "True Positive Rate (TPR)\nTPR = TP / (TP + FN)\nIt is the probability that an actual positive class will test positive.\nIt has other names * Recall * Sensitivity\nTrue positive is the y-axis in an ROC curve. It is a good metric when cost of false negatives is high."
  },
  {
    "objectID": "posts/2022-03-31-ml-nomenclature.html#false-positive-rate-fpr",
    "href": "posts/2022-03-31-ml-nomenclature.html#false-positive-rate-fpr",
    "title": "Machine Learning Nomenclature",
    "section": "False Positive Rate (FPR)",
    "text": "False Positive Rate (FPR)\nFPR = FP / (FP + TN)\nIt has other names * 1 - specificity\nIt is x-axis on ROC curve."
  },
  {
    "objectID": "posts/2022-03-31-ml-nomenclature.html#roc-curve",
    "href": "posts/2022-03-31-ml-nomenclature.html#roc-curve",
    "title": "Machine Learning Nomenclature",
    "section": "ROC Curve",
    "text": "ROC Curve\nReceiver Operating Characteristic (ROC) is a curve of TPR vs FPR at different classification thresholds."
  },
  {
    "objectID": "posts/2022-03-31-ml-nomenclature.html#true-negative-rate-tnr",
    "href": "posts/2022-03-31-ml-nomenclature.html#true-negative-rate-tnr",
    "title": "Machine Learning Nomenclature",
    "section": "True Negative Rate (TNR)",
    "text": "True Negative Rate (TNR)\nTNR = TN / (TN + FP)\nIt is the probability of a negative class to test negative.\nIt has other names * Specificity"
  },
  {
    "objectID": "posts/2022-04-11-docker-logs-cloudwatch.html#sample-application",
    "href": "posts/2022-04-11-docker-logs-cloudwatch.html#sample-application",
    "title": "Docker - Send Container Logs to AWS CloudWatch",
    "section": "Sample Application",
    "text": "Sample Application\nLet us create a simple hello world application that will print “hello world” message to stdout. After each message the application sleeps for 5 seconds, and keeps on doing this for 5 mins (300 sec). After this the program exists.\nProject structure of this application is\napp/\n└── src/\n    └── hello.py\nWhere * app/ is the project root folder * src/ folder contain the python application code * src/hello.py is the main application\nCode files are provided below\n\n##\n# app/src/hello.py\n\nfrom datetime import datetime\nimport time\n\n\ndef main():\n    # run for about 5 min: 300 sec\n    for i in range(60):\n        now = datetime.now()\n        dt_string = now.strftime(\"%d/%m/%Y %H:%M:%S\")\n\n        # prepare message\n        msg = f\"hello world at {dt_string}\"\n\n        # put message to stdout and logs\n        print(msg)\n\n        # sleep for some seconds\n        time.sleep(5)\n\n\nif __name__ == \"__main__\":\n    main()\n\nWhen I run the hello.py file I get the output on the termial with hello world messages like this.\n\n\n\nhelloworld_output"
  },
  {
    "objectID": "posts/2022-04-11-docker-logs-cloudwatch.html#get-aws-credentials",
    "href": "posts/2022-04-11-docker-logs-cloudwatch.html#get-aws-credentials",
    "title": "Docker - Send Container Logs to AWS CloudWatch",
    "section": "Get AWS Credentials",
    "text": "Get AWS Credentials\nNow that we have our sample application and it’s docker container ready, we can work on pushing the docker logs to AWS CloudWatch. For this we need access credentials to AWS account where we want our logs to be available. We will create a separate account in AWS with CloudWatch access and use it’s credentials with docker daemon. Our steps will be * Create IAM policy with CloudWatch access * Create IAM group with that policy * Create IAM user and add that to this group\n\nCreate IAM Policy\n\nFrom AWS Console go to IAM Console\nSelect Policies, and click ‘Create Policy’\nFrom Create Policy window, select\n\nService = CloudWatch Logs\nActions = CreateLogStream, GetLogRecord, DescribeLogGroups, DescribeLogStreams, GetLogEvents, CreateLogGroup, PutLogEvents\nResources = All\n\n\nAfter giving required permissions, policy summary will be like\n{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Sid\": \"DockerContainerLogs\",\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"logs:CreateLogStream\",\n                \"logs:GetLogRecord\",\n                \"logs:DescribeLogGroups\",\n                \"logs:DescribeLogStreams\",\n                \"logs:GetLogEvents\",\n                \"logs:CreateLogGroup\",\n                \"logs:PutLogEvents\"\n            ],\n            \"Resource\": \"*\"\n        }\n    ]\n}\n\n\nCreate IAM Group and User\n\nFrom IAM console create a new IAM group and give it some appropriate name ‘docker-logs-group’\nAttach the above created policy to that group\nFrom the console create a new IAM user with “Access key - Programmatic access”. Give it some appropriate name ‘docker-logs-user’\nStore access key ID and secret access key\nAdd the user to the group created in last step"
  },
  {
    "objectID": "posts/2022-04-11-docker-logs-cloudwatch.html#configure-aws-credentials-for-docker-daemon",
    "href": "posts/2022-04-11-docker-logs-cloudwatch.html#configure-aws-credentials-for-docker-daemon",
    "title": "Docker - Send Container Logs to AWS CloudWatch",
    "section": "Configure AWS credentials for docker daemon",
    "text": "Configure AWS credentials for docker daemon\nTo configure docker daemon to use AWS access credentials, execute command from the terminal sudo systemctl edit docker. A new window will open for text to edit, and add the following lines to it. Replace my-aws-access-key and my-secret-access-key with your access keys.\n[Service]\nEnvironment=\"AWS_ACCESS_KEY_ID=my-aws-access-key\"\nEnvironment=\"AWS_SECRET_ACCESS_KEY=my-secret-access-key\"\nThis command will update the credentials in file /etc/systemd/system/docker.service.d/override.conf. Verify it using command\n$ cat /etc/systemd/system/docker.service.d/override.conf\n[Service]\nEnvironment=\"AWS_ACCESS_KEY_ID=AKIA3VIXXJNKPUSIOR3Y\"\nEnvironment=\"AWS_SECRET_ACCESS_KEY=XhjlKVkZm1XdXedjgBcfLVM3FBU6zkGU\"\nAfter making changes to Docker daemon we need to restart it. For this * Flush the change with command sudo systemctl daemon-reload * Restart the docker daemon with command sudo systemctl restart docker"
  },
  {
    "objectID": "posts/2022-04-11-docker-logs-cloudwatch.html#run-docker-container-with-awslogs-driver",
    "href": "posts/2022-04-11-docker-logs-cloudwatch.html#run-docker-container-with-awslogs-driver",
    "title": "Docker - Send Container Logs to AWS CloudWatch",
    "section": "Run docker container with awslogs driver",
    "text": "Run docker container with awslogs driver\nWe can now run the docker image with awslogs driver using command\ndocker run \\\n--log-driver=awslogs \\\n--log-opt awslogs-region=us-east-1 \\\n--log-opt awslogs-group=myLogGroup \\\n--log-opt awslogs-create-group=true \\\npython-docker\n\nlog-driver configures the driver to be used for logs. Default driver is ‘json-file’ and awslogs is for CloudWatch\nawslogs-region specifies the region for AWS CloudWatch logs\nawslogs-group specifies the log group for CloudWatch\nawslogs-create-group specifes that if provided log group does not exists on CloudWatch then create one\n\n\n\n\ndocker-images-cmd"
  },
  {
    "objectID": "posts/2022-04-11-docker-logs-cloudwatch.html#verify-logs-from-cloudwatch",
    "href": "posts/2022-04-11-docker-logs-cloudwatch.html#verify-logs-from-cloudwatch",
    "title": "Docker - Send Container Logs to AWS CloudWatch",
    "section": "Verify Logs from CloudWatch",
    "text": "Verify Logs from CloudWatch\nGo to CloudWatch console and select Log Groups and then myLogGroup. You will find the logs generated by docker container.\n\n\n\ndocker-images-cmd\n\n\nAll the code used for this post can be obtained from the GitHub repository hassaanbinaslam/2022-04-11-docker-logs-cloudwatch\n\nProject code files\nProject zip file"
  },
  {
    "objectID": "posts/2022-04-11-docker-logs-cloudwatch.html#error-messages",
    "href": "posts/2022-04-11-docker-logs-cloudwatch.html#error-messages",
    "title": "Docker - Send Container Logs to AWS CloudWatch",
    "section": "Error Messages",
    "text": "Error Messages\nIf docker daemon is not able to find AWS credentails then it will generate an error message similar to pasted below\ndocker: Error response from daemon: failed to initialize logging driver: failed to create Cloudwatch log stream: NoCredentialProviders: no valid providers in chain. Deprecated.\n        For verbose messaging see aws.Config.CredentialsChainVerboseErrors.\nIf you get this message then you need to recheck the credentails passed to docker daemon.\nOne thing I noticed is that on Windows there is no way to pass AWS credentials to docker daemon. People have reported similar issues with docker running on MAC OS. Refer to below link for this discussion\n\nhttps://github.com/docker/for-win/issues/9684\n\n\nOther method to provide AWS credentials to docker daemon\nDocker documentation mentions that AWS credentails can also be set * By configuring the environment variables AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY. I have tried this approach but docker daemon is not able to pick AWS credentials from environment variables * By using AWS credentials file ~/.aws/credentials. I have also tried this approach and it does not work either"
  },
  {
    "objectID": "posts/2022-04-25-bluebook-for-bulldozers.html#environment-details",
    "href": "posts/2022-04-25-bluebook-for-bulldozers.html#environment-details",
    "title": "Kaggle - Blue Book for Bulldozers",
    "section": "Environment Details",
    "text": "Environment Details\n\n\nCode\nfrom platform import python_version\nimport sklearn, numpy, matplotlib, pandas\n\nprint(\"python==\" + python_version())\nprint(\"sklearn==\" + sklearn.__version__)\nprint(\"numpy==\" + numpy.__version__)\nprint(\"pandas==\" + pandas.__version__)\nprint(\"matplotlib==\" + matplotlib.__version__)\n\n\npython==3.8.8\nsklearn==1.0.2\nnumpy==1.20.1\npandas==1.2.3\nmatplotlib==3.5.1\n\n\n\n##\n# Notebook settings\nimport pandas as pd\n\n# display all dataframe columns \npd.set_option('display.max_columns', None)"
  },
  {
    "objectID": "posts/2022-04-25-bluebook-for-bulldozers.html#prepare-the-dataset",
    "href": "posts/2022-04-25-bluebook-for-bulldozers.html#prepare-the-dataset",
    "title": "Kaggle - Blue Book for Bulldozers",
    "section": "Prepare the dataset",
    "text": "Prepare the dataset\n\nDownload the dataset files\n\nTrain.zip and extract Train.csv. This is our training dataset.\nValid.csv. This is our validation dataset.\nTest.csv. This is our test dataset.\n\nThis dataset can be downloaded from the Kaggle competition site, and extracted files should be placed under folder ./datasets/2022-04-35-bluebook-for-bulldozers/. These files are made available with this notebook in the GitHub repository and can be downloaded from there too. If you are using Git then it will not download them from the remote server as they exceed 50MB limit (read more here). For working with large files Git needs an extra extension to work with them called git-lfs.\nFollow the steps from Git-LFS site to install it on the system. To install it directly from the notebook (running on Linux) use these commands\n\n## \n# download and install git-lfs. Uncomment them as execute.\n\n# !curl -s https://packagecloud.io/install/repositories/github/git-lfs/script.rpm.sh | sudo bash\n# !sudo yum install git-lfs -y\n# !git lfs install\n\nOnce git-lfs is installed use the below command to download large files from the remote server (GitHub).\n\n##\n# uncomment them and execute\n\n# git lfs fetch --all\n# git lfs checkout\n\n\n\nTake an initial look at the training data\nLoad the training data and look for the following information * column names * column data types * how much data is missing? * sample data elements\n\n##\n# load the training dataset\ndataset_path = 'datasets/2022-04-35-bluebook-for-bulldozers/'\n\ndf_raw = pd.read_csv(dataset_path+'Train.csv', low_memory=False)\ndf = df_raw.copy()\n\n\n##\n# print training dataset summary information\ndf_info = pd.DataFrame()\ndf_info['sample'] = df.iloc[0]\ndf_info['data_type'] = df.dtypes\ndf_info['percent_missing'] = 100*df.isnull().sum() / len(df)\nprint(f\"Total features: {len(df.columns)}\")\ndf_info.sort_values('percent_missing')\n\n\n\n\n\n  \n    \n      \n      sample\n      data_type\n      percent_missing\n    \n  \n  \n    \n      SalesID\n      1139246\n      int64\n      0.000000\n    \n    \n      state\n      Alabama\n      object\n      0.000000\n    \n    \n      fiProductClassDesc\n      Wheel Loader - 110.0 to 120.0 Horsepower\n      object\n      0.000000\n    \n    \n      fiBaseModel\n      521\n      object\n      0.000000\n    \n    \n      fiModelDesc\n      521D\n      object\n      0.000000\n    \n    \n      ProductGroup\n      WL\n      object\n      0.000000\n    \n    \n      saledate\n      11/16/2006 0:00\n      object\n      0.000000\n    \n    \n      datasource\n      121\n      int64\n      0.000000\n    \n    \n      ModelID\n      3157\n      int64\n      0.000000\n    \n    \n      MachineID\n      999089\n      int64\n      0.000000\n    \n    \n      SalePrice\n      66000\n      int64\n      0.000000\n    \n    \n      YearMade\n      2004\n      int64\n      0.000000\n    \n    \n      ProductGroupDesc\n      Wheel Loader\n      object\n      0.000000\n    \n    \n      Enclosure\n      EROPS w AC\n      object\n      0.081022\n    \n    \n      auctioneerID\n      3.0\n      float64\n      5.019882\n    \n    \n      Hydraulics\n      2 Valve\n      object\n      20.082269\n    \n    \n      fiSecondaryDesc\n      D\n      object\n      34.201558\n    \n    \n      Coupler\n      None or Unspecified\n      object\n      46.662013\n    \n    \n      Forks\n      None or Unspecified\n      object\n      52.115425\n    \n    \n      ProductSize\n      NaN\n      object\n      52.545964\n    \n    \n      Transmission\n      NaN\n      object\n      54.320972\n    \n    \n      Ride_Control\n      None or Unspecified\n      object\n      62.952696\n    \n    \n      MachineHoursCurrentMeter\n      68.0\n      float64\n      64.408850\n    \n    \n      Drive_System\n      NaN\n      object\n      73.982923\n    \n    \n      Ripper\n      NaN\n      object\n      74.038766\n    \n    \n      Undercarriage_Pad_Width\n      NaN\n      object\n      75.102026\n    \n    \n      Thumb\n      NaN\n      object\n      75.247616\n    \n    \n      Stick_Length\n      NaN\n      object\n      75.265067\n    \n    \n      Pattern_Changer\n      NaN\n      object\n      75.265067\n    \n    \n      Grouser_Type\n      NaN\n      object\n      75.281271\n    \n    \n      Track_Type\n      NaN\n      object\n      75.281271\n    \n    \n      Tire_Size\n      None or Unspecified\n      object\n      76.386912\n    \n    \n      Travel_Controls\n      NaN\n      object\n      80.097476\n    \n    \n      Blade_Type\n      NaN\n      object\n      80.097725\n    \n    \n      Turbocharged\n      NaN\n      object\n      80.271985\n    \n    \n      Stick\n      NaN\n      object\n      80.271985\n    \n    \n      Pad_Type\n      NaN\n      object\n      80.271985\n    \n    \n      Backhoe_Mounting\n      NaN\n      object\n      80.387161\n    \n    \n      fiModelDescriptor\n      NaN\n      object\n      82.070676\n    \n    \n      UsageBand\n      Low\n      object\n      82.639078\n    \n    \n      Differential_Type\n      Standard\n      object\n      82.695918\n    \n    \n      Steering_Controls\n      Conventional\n      object\n      82.706388\n    \n    \n      fiModelSeries\n      NaN\n      object\n      85.812901\n    \n    \n      Coupler_System\n      NaN\n      object\n      89.165971\n    \n    \n      Grouser_Tracks\n      NaN\n      object\n      89.189903\n    \n    \n      Hydraulics_Flow\n      NaN\n      object\n      89.189903\n    \n    \n      Scarifier\n      NaN\n      object\n      93.710190\n    \n    \n      Pushblock\n      NaN\n      object\n      93.712932\n    \n    \n      Engine_Horsepower\n      NaN\n      object\n      93.712932\n    \n    \n      Enclosure_Type\n      NaN\n      object\n      93.712932\n    \n    \n      Blade_Width\n      NaN\n      object\n      93.712932\n    \n    \n      Blade_Extension\n      NaN\n      object\n      93.712932\n    \n    \n      Tip_Control\n      NaN\n      object\n      93.712932\n    \n  \n\n\n\n\n\n##\n# print some unique values against each feature\ndef sniff(df, rows=7):\n    \"\"\"\n    For each column return a set of unique values\n    \"\"\"\n    data = {}\n    for col in df.columns:\n        data[col] = df[col].unique()[:rows]\n    \n    return pd.DataFrame.from_dict(data, orient='index').T\n\n\nsniff(df)\n\n\n\n\n\n  \n    \n      \n      SalesID\n      SalePrice\n      MachineID\n      ModelID\n      datasource\n      auctioneerID\n      YearMade\n      MachineHoursCurrentMeter\n      UsageBand\n      saledate\n      fiModelDesc\n      fiBaseModel\n      fiSecondaryDesc\n      fiModelSeries\n      fiModelDescriptor\n      ProductSize\n      fiProductClassDesc\n      state\n      ProductGroup\n      ProductGroupDesc\n      Drive_System\n      Enclosure\n      Forks\n      Pad_Type\n      Ride_Control\n      Stick\n      Transmission\n      Turbocharged\n      Blade_Extension\n      Blade_Width\n      Enclosure_Type\n      Engine_Horsepower\n      Hydraulics\n      Pushblock\n      Ripper\n      Scarifier\n      Tip_Control\n      Tire_Size\n      Coupler\n      Coupler_System\n      Grouser_Tracks\n      Hydraulics_Flow\n      Track_Type\n      Undercarriage_Pad_Width\n      Stick_Length\n      Thumb\n      Pattern_Changer\n      Grouser_Type\n      Backhoe_Mounting\n      Blade_Type\n      Travel_Controls\n      Differential_Type\n      Steering_Controls\n    \n  \n  \n    \n      0\n      1139246\n      66000\n      999089\n      3157\n      121\n      3.0\n      2004\n      68.0\n      Low\n      11/16/2006 0:00\n      521D\n      521\n      D\n      NaN\n      NaN\n      NaN\n      Wheel Loader - 110.0 to 120.0 Horsepower\n      Alabama\n      WL\n      Wheel Loader\n      NaN\n      EROPS w AC\n      None or Unspecified\n      NaN\n      None or Unspecified\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      2 Valve\n      NaN\n      NaN\n      NaN\n      NaN\n      None or Unspecified\n      None or Unspecified\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      Standard\n      Conventional\n    \n    \n      1\n      1139248\n      57000\n      117657\n      77\n      132\n      1.0\n      1996\n      4640.0\n      High\n      3/26/2004 0:00\n      950FII\n      950\n      F\n      II\n      LC\n      Medium\n      Wheel Loader - 150.0 to 175.0 Horsepower\n      North Carolina\n      SSL\n      Skid Steer Loaders\n      Four Wheel Drive\n      OROPS\n      NaN\n      None or Unspecified\n      NaN\n      Extended\n      Powershuttle\n      None or Unspecified\n      Yes\n      None or Unspecified\n      None or Unspecified\n      No\n      Auxiliary\n      None or Unspecified\n      None or Unspecified\n      Yes\n      Sideshift & Tip\n      23.5\n      NaN\n      None or Unspecified\n      None or Unspecified\n      Standard\n      Steel\n      None or Unspecified\n      None or Unspecified\n      None or Unspecified\n      None or Unspecified\n      Double\n      None or Unspecified\n      PAT\n      None or Unspecified\n      NaN\n      NaN\n    \n    \n      2\n      1139249\n      10000\n      434808\n      7009\n      136\n      2.0\n      2001\n      2838.0\n      Medium\n      2/26/2004 0:00\n      226\n      226\n      NaN\n      -6E\n      6\n      Small\n      Skid Steer Loader - 1351.0 to 1601.0 Lb Operat...\n      New York\n      TEX\n      Track Excavators\n      Two Wheel Drive\n      EROPS\n      Yes\n      Reversible\n      No\n      Standard\n      Standard\n      Yes\n      None or Unspecified\n      12'\n      Low Profile\n      Variable\n      NaN\n      Yes\n      Yes\n      None or Unspecified\n      None or Unspecified\n      NaN\n      Manual\n      Yes\n      Yes\n      High Flow\n      Rubber\n      16 inch\n      11' 0\"\n      Hydraulic\n      Yes\n      Triple\n      Yes\n      None or Unspecified\n      Differential Steer\n      Limited Slip\n      Command Control\n    \n    \n      3\n      1139251\n      38500\n      1026470\n      332\n      149\n      11.0\n      2007\n      3486.0\n      NaN\n      5/19/2011 0:00\n      PC120-6E\n      PC120\n      G\n      LC\n      L\n      Large / Medium\n      Hydraulic Excavator, Track - 12.0 to 14.0 Metr...\n      Texas\n      BL\n      Backhoe Loaders\n      No\n      NaN\n      None\n      Street\n      Yes\n      None\n      Powershift\n      None\n      None\n      14'\n      High Profile\n      None\n      Standard\n      None\n      Single Shank\n      None\n      Tip\n      13\"\n      Hydraulic\n      None\n      None\n      None or Unspecified\n      None\n      32 inch\n      15' 9\"\n      Manual\n      No\n      Single\n      None\n      Semi U\n      Lever\n      No Spin\n      Four Wheel Standard\n    \n    \n      4\n      1139253\n      11000\n      1057373\n      17311\n      172\n      4.0\n      1993\n      722.0\n      None\n      7/23/2009 0:00\n      S175\n      S175\n      E\n      -5\n      LT\n      Mini\n      Skid Steer Loader - 1601.0 to 1751.0 Lb Operat...\n      Arizona\n      TTT\n      Track Type Tractors\n      All Wheel Drive\n      EROPS AC\n      None\n      Grouser\n      None\n      None\n      None or Unspecified\n      None\n      None\n      13'\n      None\n      None\n      Base + 1 Function\n      None\n      Multi Shank\n      None\n      None\n      26.5\n      None\n      None\n      None\n      None\n      None\n      28 inch\n      10' 2\"\n      None\n      None\n      None\n      None\n      VPAT\n      Finger Tip\n      Locking\n      Wheel\n    \n    \n      5\n      1139255\n      26500\n      1001274\n      4605\n      None\n      7.0\n      2008\n      508.0\n      None\n      12/18/2008 0:00\n      310G\n      310\n      HAG\n      III\n      CR\n      Large\n      Backhoe Loader - 14.0 to 15.0 Ft Standard Digg...\n      Florida\n      MG\n      Motor Graders\n      None\n      NO ROPS\n      None\n      None\n      None\n      None\n      Hydrostatic\n      None\n      None\n      16'\n      None\n      None\n      Base + 3 Function\n      None\n      None\n      None\n      None\n      29.5\n      None\n      None\n      None\n      None\n      None\n      30 inch\n      10' 6\"\n      None\n      None\n      None\n      None\n      Straight\n      2 Pedal\n      None\n      No\n    \n    \n      6\n      1139256\n      21000\n      772701\n      1937\n      None\n      99.0\n      1000\n      11540.0\n      None\n      8/26/2004 0:00\n      790ELC\n      790\n      B\n      -1\n      SB\n      Compact\n      Hydraulic Excavator, Track - 21.0 to 24.0 Metr...\n      Illinois\n      None\n      None\n      None\n      None or Unspecified\n      None\n      None\n      None\n      None\n      Autoshift\n      None\n      None\n      <12'\n      None\n      None\n      4 Valve\n      None\n      None\n      None\n      None\n      14\"\n      None\n      None\n      None\n      None\n      None\n      22 inch\n      9' 10\"\n      None\n      None\n      None\n      None\n      Angle\n      Pedal\n      None\n      None\n    \n  \n\n\n\n\nFrom this first look at the data, we can see that * data is of three types * numeric * string * datetime * some columns have missing data up to 94% e.g. Tip_Control * missing data is represented as * NaN * None or unspecified * some columns’ data types need to be corrected for example * SaleID, MachineID are represented as integers but they are categorical nominal features meaning each value is discrete and has no relation among them * UsageBand is of type string but is a categorical ordinal feature meaning their values cannot be measured but have some order between them * Tire_size, Stick_length are actual measurements and need to be converted to appropriate units"
  },
  {
    "objectID": "posts/2022-04-25-bluebook-for-bulldozers.html#baseline-model",
    "href": "posts/2022-04-25-bluebook-for-bulldozers.html#baseline-model",
    "title": "Kaggle - Blue Book for Bulldozers",
    "section": "Baseline Model",
    "text": "Baseline Model\nIt is a good idea to create a baseline model early in the data science project as it can help to establish a baseline for * time it takes to train a model * if the baseline model is taking too much time then we may use a smaller set of the training data for further steps * feature importances * it can help us establish a relationship between features and the target * help us remove features that have no relationship with the target sooner * model performance * we can take this model performance as a baseline, and compare it to see how much cleanup and feature engineering steps improve the model performance\nFor the baseline model, we would have to rely on numerical features as they don’t require any preprocessing and can be readily used. Some numerical features have too much missing data so we have to be selective here.\n\n##\n# filter columns that are not string along with their percentage of missing data\nnumerical_features = df_info.loc[df_info.data_type != 'object'].sort_values('percent_missing')\nnumerical_features\n\n\n\n\n\n  \n    \n      \n      sample\n      data_type\n      percent_missing\n    \n  \n  \n    \n      SalesID\n      1139246\n      int64\n      0.000000\n    \n    \n      SalePrice\n      66000\n      int64\n      0.000000\n    \n    \n      MachineID\n      999089\n      int64\n      0.000000\n    \n    \n      ModelID\n      3157\n      int64\n      0.000000\n    \n    \n      datasource\n      121\n      int64\n      0.000000\n    \n    \n      YearMade\n      2004\n      int64\n      0.000000\n    \n    \n      auctioneerID\n      3.0\n      float64\n      5.019882\n    \n    \n      MachineHoursCurrentMeter\n      68.0\n      float64\n      64.408850\n    \n  \n\n\n\n\nFrom these numerical features MachineHoursCurrentMeter has around 64% missing data. Let’s keep this feature as well for our baseline model.\n\n##\n# establish target and baseline features\ntarget = 'SalePrice' # this is the feature we are trying to predict\nbaseline_features = list(numerical_features.index)\nbaseline_features.remove(target) # remove target feature form input variables\nbaseline_features\n\n['SalesID',\n 'MachineID',\n 'ModelID',\n 'datasource',\n 'YearMade',\n 'auctioneerID',\n 'MachineHoursCurrentMeter']\n\n\nWe have established our target and features, and can now train our baseline model. We will use only RandomForrest for this dataset. We have 7 features to learn from so let’s start with n_estimators=70\n\nfrom sklearn.ensemble import RandomForestRegressor\n\nX, y = df[baseline_features], df[target]\nX = X.fillna(0) # replace missing numerical values with 0\n\nrf = RandomForestRegressor(n_estimators=70, oob_score=True, n_jobs=-1, verbose=1)\nrf.fit(X, y)\noob_score = rf.oob_score_\noob_score\n\n[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 4 concurrent workers.\n[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:  1.2min\n[Parallel(n_jobs=-1)]: Done  70 out of  70 | elapsed:  1.9min finished\n\n\n0.7901663917842495\n\n\nBesides the OOB score, we can also interpret our model by looking into the trained model trees’ depth and leaves. If OOB score is same but our trees are smaller with fewer nodes then that model is better and more generalized. Larger trees make the model more complex and less generalized. For this let’s create two more functions.\n\nimport numpy as np\n\ndef rf_n_leaves(rf):\n    \"\"\"\n    return the total number of nodes in all the trees of the forrest.\n    \"\"\"\n    return sum(est.tree_.n_leaves for est in rf.estimators_)\n\ndef rf_m_depth(rf):\n    \"\"\"\n    return a median height of all the trees of the forrest.\n    \"\"\"\n    return np.median([est.tree_.max_depth for est in rf.estimators_])\n\n\n##\n# print model oob_score, number of forrest leaves and median height\nn_leaves = rf_n_leaves(rf)\nm_depth = rf_m_depth(rf)\n\nprint(f\"OOB scrore = {oob_score: .3f} \\nTree leaves = {n_leaves: ,d} \\nMedian depth = {m_depth}\")\n\nOOB scrore =  0.790 \nTree leaves =  16,209,726 \nMedian depth = 53.5\n\n\nOur baseline model has an OOB score of just around 79% which is not bad as a starter. Now let’s also plot the feature importance for this model.\n\ndef plot_feature_importance(feature_importance, feature_names, figsize=(7,7)):\n    \"\"\"\n    plot the feature importances in a bar graph along with feature names.\n    \"\"\"\n    fimp = pd.Series(feature_importance, feature_names)\n    fimp.nlargest(len(fimp)).plot(figsize=figsize, kind='barh').invert_yaxis()\n\n\nfeature_importance = rf.feature_importances_\nfeature_names = X.columns\nplot_feature_importance(feature_importance, feature_names)\n\n\n\n\nFrom this feature importance plot, we can see that * ModelID is the highest predictor of SalePrice. This could be because vehicles belonging to a certain ModelID category could have their SalePrice in the same range. * SalesID and MachineID are coming up next as important features. This is not a good signal as both these features are unique for sale transactions and machine identification. MachineID also has inconsistencies as noted in this kaggle discussion. A model using these features will not be generalized. It would be better if we remove these features altogether otherwise they can affect the model’s performance. * YearMade comes next which also makes sense as older vehicles will have less price compared to the new ones."
  },
  {
    "objectID": "posts/2022-04-25-bluebook-for-bulldozers.html#cleaning-up",
    "href": "posts/2022-04-25-bluebook-for-bulldozers.html#cleaning-up",
    "title": "Kaggle - Blue Book for Bulldozers",
    "section": "Cleaning up",
    "text": "Cleaning up\nIn this section we will remove unimportanct features and fix the data types of remaining features.\n\nRemove ID columns\nAs noted in last section we noted that following ID features can be removed from the dataset. * SalesID * MachineID\n\ndel df['SalesID']\ndel df['MachineID']"
  },
  {
    "objectID": "posts/2022-04-25-bluebook-for-bulldozers.html#fix-data-types-and-data-issues",
    "href": "posts/2022-04-25-bluebook-for-bulldozers.html#fix-data-types-and-data-issues",
    "title": "Kaggle - Blue Book for Bulldozers",
    "section": "Fix data types and data issues",
    "text": "Fix data types and data issues\nLet’s visit each feature from our dataset and check whether we need to fix the data type. Use df_info created in the last section to verify the data types of each feature.\n\nNumerical features\nLet’s first visit the numerical feature.\n\nauctioneerID\nIt has the datatype as float64 but this feature is actually categorical nominal as each ID is discrete and has no relation between them. It should be of type str. So let’s fix that.\n\ndf['auctioneerID'] = df['auctioneerID'].astype(str)\n\n\n\n\nDatetime feature\nLet’s visit DateTime features and correct their data type #### saledate ‘saledate’ is a DateTime feature. So let’s correct its data type.\n\ndf['saledate'] = pd.to_datetime(df['saledate'])\n\n\n\nCategorical features\nLet’s now visit the categorical features.\nFor categorical features, there is no better way than printing the unique values for each column and spend some time analyzing the values. Analyze * if the feature has some missing values * if there are any missing values but are represented by some other value like ‘Unspecified’, ‘None or Unspecified’ * keep a separate sheet with all the features and make notes for each feature like * there are no further actions required. The feature is good for use * need to replace missing values * any other observations * etc.\n\nTransform missing values\nAfter visiting all the features we have found that missing values are represented in multiple ways like * Unspecified * None or Unspecified * None * #NAME? * “”\nSo we would transform and replace all these values with np.nan so they all represent the same thing.\n\n##\n# before transformation. \n# let's use this feature to verify results.\ndf['Hydraulics'].unique()\n\narray(['2 Valve', 'Auxiliary', nan, 'Standard', 'Base + 1 Function',\n       'Base + 3 Function', '4 Valve', '3 Valve', 'Base + 2 Function',\n       'Base + 4 Function', 'None or Unspecified', 'Base + 5 Function',\n       'Base + 6 Function'], dtype=object)\n\n\n\n#collapse-output\ndef normalize_str_values(df):\n    \"\"\"\n    normalize dataframe str values\n    * transform case to lowercase\n    * replace missing values with np.nan\n    \"\"\"\n    for col in df.columns:\n        if df[col].dtype == object: \n            print(f\"normalize column: {col}\")\n            df[col] = df[col].str.lower()\n            df[col] = df[col].fillna(np.nan)\n            df[col] = df[col].replace('unspecified', np.nan)\n            df[col] = df[col].replace('none or unspecified', np.nan)\n            df[col] = df[col].replace('none', np.nan)\n            df[col] = df[col].replace('#name?', np.nan)\n            df[col] = df[col].replace('', np.nan)\n\nnormalize_str_values(df)\n\nnormalize column: auctioneerID\nnormalize column: UsageBand\nnormalize column: fiModelDesc\nnormalize column: fiBaseModel\nnormalize column: fiSecondaryDesc\nnormalize column: fiModelSeries\nnormalize column: fiModelDescriptor\nnormalize column: ProductSize\nnormalize column: fiProductClassDesc\nnormalize column: state\nnormalize column: ProductGroup\nnormalize column: ProductGroupDesc\nnormalize column: Drive_System\nnormalize column: Enclosure\nnormalize column: Forks\nnormalize column: Pad_Type\nnormalize column: Ride_Control\nnormalize column: Stick\nnormalize column: Transmission\nnormalize column: Turbocharged\nnormalize column: Blade_Extension\nnormalize column: Blade_Width\nnormalize column: Enclosure_Type\nnormalize column: Engine_Horsepower\nnormalize column: Hydraulics\nnormalize column: Pushblock\nnormalize column: Ripper\nnormalize column: Scarifier\nnormalize column: Tip_Control\nnormalize column: Tire_Size\nnormalize column: Coupler\nnormalize column: Coupler_System\nnormalize column: Grouser_Tracks\nnormalize column: Hydraulics_Flow\nnormalize column: Track_Type\nnormalize column: Undercarriage_Pad_Width\nnormalize column: Stick_Length\nnormalize column: Thumb\nnormalize column: Pattern_Changer\nnormalize column: Grouser_Type\nnormalize column: Backhoe_Mounting\nnormalize column: Blade_Type\nnormalize column: Travel_Controls\nnormalize column: Differential_Type\nnormalize column: Steering_Controls\n\n\n\n##\n# after transformation.\n# remember that transformation is applied to all string type columns. We are using just one column to verify the results.\ndf['Hydraulics'].unique()\n\narray(['2 valve', 'auxiliary', nan, 'standard', 'base + 1 function',\n       'base + 3 function', '4 valve', '3 valve', 'base + 2 function',\n       'base + 4 function', 'base + 5 function', 'base + 6 function'],\n      dtype=object)\n\n\n\n\nTransform measurements\nSome features are represented as a string but actually they are numerical measurement values. For example * Tire_Size has the size in inches with a symbol attached \" * Undercarriage_Pad_Width has the size in inches with the unit attached inch * Blade_Width has the size in cm with a symbol attached '. It also has values less the 12cm represented as <12' * Stick_Length has values in both feet and inches. We can simply convert them from 19\\'8\" to 19.8 * After the above transformations, their data types should be converted to numeric\nlet’s apply these changes to our dataset.\n\n##\n# before transformation\nfor col in ['Tire_Size', 'Undercarriage_Pad_Width', 'Blade_Width', 'Stick_Length']:\n    print(f\"**{col}**: \", df[col].unique())\n\n**Tire_Size**:  [nan '23.5' '13\"' '26.5' '29.5' '14\"' '20.5' '17.5\"' '15.5\"' '20.5\"'\n '17.5' '7.0\"' '15.5' '23.5\"' '10\"' '23.1\"' '10 inch']\n**Undercarriage_Pad_Width**:  [nan '16 inch' '32 inch' '28 inch' '30 inch' '22 inch' '24 inch' '18 inch'\n '36 inch' '20 inch' '27 inch' '15 inch' '26 inch' '34 inch' '33 inch'\n '14 inch' '31 inch' '25 inch' '31.5 inch']\n**Blade_Width**:  [nan \"12'\" \"14'\" \"13'\" \"16'\" \"<12'\"]\n**Stick_Length**:  [nan '11\\' 0\"' '15\\' 9\"' '10\\' 2\"' '10\\' 6\"' '9\\' 10\"' '10\\' 10\"' '9\\' 6\"'\n '9\\' 7\"' '12\\' 8\"' '8\\' 2\"' '8\\' 6\"' '9\\' 8\"' '12\\' 10\"' '11\\' 10\"'\n '8\\' 10\"' '8\\' 4\"' '12\\' 4\"' '9\\' 5\"' '6\\' 3\"' '14\\' 1\"' '13\\' 7\"'\n '13\\' 10\"' '13\\' 9\"' '7\\' 10\"' '15\\' 4\"' '9\\' 2\"' '24\\' 3\"' '19\\' 8\"']\n\n\n\ndf['Stick_Length'] = df['Stick_Length'].replace(r\"' \", \".\", regex=True)\nfor col in ['Tire_Size', 'Undercarriage_Pad_Width', 'Blade_Width', 'Stick_Length']:\n    df[col] = df[col].str.extract(r'([0-9.]*)', expand=True)\n    df[col] = df[col].replace('', np.nan)\n    df[col] = pd.to_numeric(df[col])\n\n\n##\n# after transformation\nfor col in ['Tire_Size', 'Undercarriage_Pad_Width', 'Blade_Width', 'Stick_Length']:\n    print(f\"**{col}**: \", df[col].unique())\n\n**Tire_Size**:  [ nan 23.5 13.  26.5 29.5 14.  20.5 17.5 15.5  7.  10.  23.1]\n**Undercarriage_Pad_Width**:  [ nan 16.  32.  28.  30.  22.  24.  18.  36.  20.  27.  15.  26.  34.\n 33.  14.  31.  25.  31.5]\n**Blade_Width**:  [nan 12. 14. 13. 16.]\n**Stick_Length**:  [ nan 11.  15.9 10.2 10.6  9.1 10.1  9.6  9.7 12.8  8.2  8.6  9.8 12.1\n 11.1  8.1  8.4 12.4  9.5  6.3 14.1 13.7 13.1 13.9  7.1 15.4  9.2 24.3\n 19.8]"
  },
  {
    "objectID": "posts/2022-04-25-bluebook-for-bulldozers.html#dealing-with-missing-data",
    "href": "posts/2022-04-25-bluebook-for-bulldozers.html#dealing-with-missing-data",
    "title": "Kaggle - Blue Book for Bulldozers",
    "section": "Dealing with missing data",
    "text": "Dealing with missing data\n\nReplace missing numeric values\nFor numerical features, we will follow the following approach to replace missing values * For a column x create a new column x_na where x_na[i] is marked as True if x[i] is missing * Replace the missing values in the x column with a median value\n\ndef fix_missing_num(df, colname):\n    \"\"\"\n    replace missing values with\n    * median value\n    * flag the missing value in a separate *_na column\n    \"\"\"\n    df[colname+'_na'] = pd.isnull(df[colname])\n    df[colname].fillna(df[colname].median(), inplace=True)\n\n\nYearMade\n“YearMade” doesn’t show any missing values but if we look closely at the data we will find that some instances have the value “1000”. The year 1000 is very unlikely for any vehicle to be made in and we can consider these instances as missing values. Let’s do that\n\n##\n# befor transformation\ndf.plot.scatter('YearMade', 'SalePrice')\n\n<AxesSubplot:xlabel='YearMade', ylabel='SalePrice'>\n\n\n\n\n\n\ndf.loc[df.YearMade==1000, 'YearMade'] = np.nan\n\n\n##\n# after transformation\ndf.plot.scatter('YearMade', 'SalePrice')\n\n<AxesSubplot:xlabel='YearMade', ylabel='SalePrice'>\n\n\n\n\n\nThe plot now shows a more clear relationship between ‘YearMade’ and ‘SalePrice’. But the spike in the year 1920 is still concerning. Most probably it is also a recording error when the manufacturing year was not known then it was assigned some lowest available value in the system (similar to the year 1000). Let’s take this assumption that manufacturing years before 1950 are unknown and should be assigned np.nan\n\ndf.loc[df.YearMade<1950, 'YearMade'] = np.nan\n\n\n##\n# after transformation\ndf.plot.scatter('YearMade', 'SalePrice')\n\n<AxesSubplot:xlabel='YearMade', ylabel='SalePrice'>\n\n\n\n\n\nLet’s also replace the missing values with the function created above.\n\nfix_missing_num(df, 'YearMade')\n\n\n\nMachineHoursCurrentMeter\nThe next numerical feature that comes is MachineHoursCurrentMeter. This feature tells us the number of hours a machine has been in use when it was brought to the auction. So older machines are much more likely to have more hours on them as compared to newer machines. There should be a correlation between machine hours and the vehicle in use period (a period between manufacturing and auction). To verify this relationship we first need to find the period in years between manufacturing and auction. We have the ‘YearMade’ that tells us when the vehicle was made. We have the ‘saledate’ which is a DateTime string object but we can use it to find the ‘YearSold’.\n\ndf['YearSold'] = df['saledate'].dt.year\n\n\n##\n# verify that we have correct data\ndf[['saledate', 'YearSold']].head()\n\n\n\n\n\n  \n    \n      \n      saledate\n      YearSold\n    \n  \n  \n    \n      0\n      2006-11-16\n      2006\n    \n    \n      1\n      2004-03-26\n      2004\n    \n    \n      2\n      2004-02-26\n      2004\n    \n    \n      3\n      2011-05-19\n      2011\n    \n    \n      4\n      2009-07-23\n      2009\n    \n  \n\n\n\n\nNow we can use ‘YearMade’ and ‘YearSold’ to find the number of years the vehicle remained in use. Let’s call this new column ‘YearsInUse’\n\ndf['YearsInUse'] = df['YearSold'] - df['YearMade']\n\n\n##\n# verify the results\ndf[['YearsInUse', 'YearSold', 'YearMade']].head()\n\n\n\n\n\n  \n    \n      \n      YearsInUse\n      YearSold\n      YearMade\n    \n  \n  \n    \n      0\n      2.0\n      2006\n      2004.0\n    \n    \n      1\n      8.0\n      2004\n      1996.0\n    \n    \n      2\n      3.0\n      2004\n      2001.0\n    \n    \n      3\n      10.0\n      2011\n      2001.0\n    \n    \n      4\n      2.0\n      2009\n      2007.0\n    \n  \n\n\n\n\nA sold year cannot be less than a manufacturing year. So let’s verify data integrity as well.\n\ndf.loc[df.YearsInUse<0, ['YearsInUse', 'saledate', 'YearSold', 'YearMade']].head()\n\n\n\n\n\n  \n    \n      \n      YearsInUse\n      saledate\n      YearSold\n      YearMade\n    \n  \n  \n    \n      24007\n      -2.0\n      1994-02-11\n      1994\n      1996.0\n    \n    \n      24009\n      -1.0\n      1995-04-18\n      1995\n      1996.0\n    \n    \n      24015\n      -2.0\n      1994-09-20\n      1994\n      1996.0\n    \n    \n      24029\n      -1.0\n      1995-04-28\n      1995\n      1996.0\n    \n    \n      24064\n      -1.0\n      1995-04-28\n      1995\n      1996.0\n    \n  \n\n\n\n\nYearInUse cannot have a negative value and this shows that either ‘YearMade’ or ‘saledate’ is incorrect. We can assume that error can be with ‘YearMade’ as this is an auction dataset and ‘saledate’ will be more reliable. For entries where ‘YearMade’ is greater than ‘YearSold’ we can replace ‘YearMade’ with ‘YearSold’ (better to have ‘YearsInUse’ equal to zero than negative).\n\ndf.loc[df.YearMade>df.YearSold, 'YearMade'] = df.YearSold\n\nLet’s recalculate the ‘YearsInUse’ with corrected data.\n\ndf['YearsInUse'] = df['YearSold'] - df['YearMade']\n\nLet’s verify that the data is consistent and all vehicles have ‘YearMade’ less than their ‘YearSold’\n\ndf.loc[df.YearsInUse<0, ['YearsInUse', 'saledate', 'YearSold', 'YearMade']].head()\n\n\n\n\n\n  \n    \n      \n      YearsInUse\n      saledate\n      YearSold\n      YearMade\n    \n  \n  \n  \n\n\n\n\nWe can now plot the relationship between ‘YearsInUse’ and ‘MachineHoursCurrentMeter’\n\ndf.plot.scatter('YearsInUse', 'MachineHoursCurrentMeter')\n\n<AxesSubplot:xlabel='YearsInUse', ylabel='MachineHoursCurrentMeter'>\n\n\n\n\n\nThis plot shows that there is some relation between a vehicle being in use and its meter hours. As the ‘YearsInUse’ value increases we also see an increase in meter hours, but after around 15 ‘YearsInUse’ the relationship does not hold on and meter hours start dropping to zero. It means that MachineHoursCurrentMeter data has inconsistencies as many vehicles remained in use for multiple years but they also have zero meter readings. This is very unrealistic and vehicles will not be sitting idle for many years till their auction. It could be that the meter reading for them was not known and 0 could have been used for the ‘Unspecified or Unknown’ value.\nLet’s take this assumption and transform ‘MachineHoursCurrentMeter’ to correctly represent that\n\ndf.loc[df.MachineHoursCurrentMeter==0, 'MachineHoursCurrentMeter'] = np.nan\n\nAlso apply our missing values fix on this feature\n\nfix_missing_num(df, 'MachineHoursCurrentMeter')\n\n\n\nTire_Size\nThe next numerical feature is ‘Tire_Size’. We can plot the distribution of tire sizes to find any outliers.\n\ndf['Tire_Size'].hist()\n\n<AxesSubplot:>\n\n\n\n\n\n\n##\n# print tire sizes\nnp.sort(df['Tire_Size'].unique())\n\narray([ 7. , 10. , 13. , 14. , 15.5, 17.5, 20.5, 23.1, 23.5, 26.5, 29.5,\n        nan])\n\n\nThe plot does not show any outliers and data seems consistant, so we can apply our missing values fix on this feature.\n\nfix_missing_num(df, 'Tire_Size')\n\n\n\nStick_Length\nThe Next numerical feature is ‘Stick_Lenght’. Let’s plot the distribution to check for any outliers.\n\nnp.sort(df['Stick_Length'].unique())\n\narray([ 6.3,  7.1,  8.1,  8.2,  8.4,  8.6,  9.1,  9.2,  9.5,  9.6,  9.7,\n        9.8, 10.1, 10.2, 10.6, 11. , 11.1, 12.1, 12.4, 12.8, 13.1, 13.7,\n       13.9, 14.1, 15.4, 15.9, 19.8, 24.3,  nan])\n\n\n\ndf['Stick_Length'].plot.hist()\n\n<AxesSubplot:ylabel='Frequency'>\n\n\n\n\n\nThe above plot shows a normal distribution and no outliers. So we can apply our missing values fix on this feature.\n\nfix_missing_num(df, 'Stick_Length')\n\n\n\nUndercarriage_Pad_Width\nNext numerical feature is ‘Undercarriage_Pad_Width’. Let’s follow the same steps for this feature.\n\nnp.sort(df['Undercarriage_Pad_Width'].unique())\n\narray([14. , 15. , 16. , 18. , 20. , 22. , 24. , 25. , 26. , 27. , 28. ,\n       30. , 31. , 31.5, 32. , 33. , 34. , 36. ,  nan])\n\n\n\ndf['Undercarriage_Pad_Width'].plot.hist()\n\n<AxesSubplot:ylabel='Frequency'>\n\n\n\n\n\nThe distribution for this feature looks fine, and we can apply missing values fix on it.\n\nfix_missing_num(df, 'Undercarriage_Pad_Width')\n\n\n\nBlade_Width\nNext numerical feature in ‘Blade_Width’. Following the same steps as before.\n\nnp.sort(df['Blade_Width'].unique())\n\narray([12., 13., 14., 16., nan])\n\n\n\ndf['Blade_Width'].plot.hist()\n\n<AxesSubplot:ylabel='Frequency'>\n\n\n\n\n\nApply the fix on this feature.\n\nfix_missing_num(df, 'Blade_Width')\n\n\n\n\nReplace missing categorical values\nencoding and checking the importance We will now replace missing values for categorical features in the following way. * We will label encode them. We will treat them as ordinal features and assign them a numeric value * Missing values will automatically be assigned a value, and that will be 0\nSome important discussion points on treating nominal categorical features as ordinal and then encoding them. A more prevalent approach is to one hot encode (OHE) them. The drawback of OHE approach is that it makes the decision trees very unbalanced if the dataset has multiple categorical features with high variance. So instead of applying OHE to all features, we will do it in a two-step approach. First, we will label encode them and train a model on them. After that, we will check their feature importance, and if a feature comes up as an important with a low variance then we will use OHE for it. Otherwise we will leave them with label encoding.\nMore can be read about categorical features encoding from these references * The Mechanics of Machine Learning by Terence Parr and Jeremy Howard section 6.2 * Getting Deeper into Categorical Encodings for Machine Learning * One-Hot Encoding is making your Tree-Based Ensembles worse, here’s why?\nLet’s create some functions to encode our categorical features.\n\nfrom pandas.api.types import is_categorical_dtype, is_string_dtype\n\ndef df_string_to_cat(df):\n    for col in df.columns:\n        if is_string_dtype(df[col]):\n            print(f\"label encoding applied on {col}\")\n            df[col] = df[col].astype('category').cat.as_ordered()\n\ndef df_cat_to_catcode(df):\n    for col in df.columns:\n        if is_categorical_dtype(df[col]):\n            df[col] = df[col].cat.codes + 1\n\nPlease note that Pandas represents np.nan with category code “-1”, and so adding “1” in function df_cat_to_catcode shifts np.nan to 0 and all category codes to be 1 and above.\n\n##\n# before transformation\ndf.head(5).T.head(10)\n\n\n\n\n\n  \n    \n      \n      0\n      1\n      2\n      3\n      4\n    \n  \n  \n    \n      SalePrice\n      66000\n      57000\n      10000\n      38500\n      11000\n    \n    \n      ModelID\n      3157\n      77\n      7009\n      332\n      17311\n    \n    \n      datasource\n      121\n      121\n      121\n      121\n      121\n    \n    \n      auctioneerID\n      3.0\n      3.0\n      3.0\n      3.0\n      3.0\n    \n    \n      YearMade\n      2004.0\n      1996.0\n      2001.0\n      2001.0\n      2007.0\n    \n    \n      MachineHoursCurrentMeter\n      68.0\n      4640.0\n      2838.0\n      3486.0\n      722.0\n    \n    \n      UsageBand\n      low\n      low\n      high\n      high\n      medium\n    \n    \n      saledate\n      2006-11-16 00:00:00\n      2004-03-26 00:00:00\n      2004-02-26 00:00:00\n      2011-05-19 00:00:00\n      2009-07-23 00:00:00\n    \n    \n      fiModelDesc\n      521d\n      950fii\n      226\n      pc120-6e\n      s175\n    \n    \n      fiBaseModel\n      521\n      950\n      226\n      pc120\n      s175\n    \n  \n\n\n\n\n\n#collapse-output\n# apply the cat transformation\ndf_string_to_cat(df)\ndf_cat_to_catcode(df)\n\nlabel encoding applied on auctioneerID\nlabel encoding applied on UsageBand\nlabel encoding applied on fiModelDesc\nlabel encoding applied on fiBaseModel\nlabel encoding applied on fiSecondaryDesc\nlabel encoding applied on fiModelSeries\nlabel encoding applied on fiModelDescriptor\nlabel encoding applied on ProductSize\nlabel encoding applied on fiProductClassDesc\nlabel encoding applied on state\nlabel encoding applied on ProductGroup\nlabel encoding applied on ProductGroupDesc\nlabel encoding applied on Drive_System\nlabel encoding applied on Enclosure\nlabel encoding applied on Forks\nlabel encoding applied on Pad_Type\nlabel encoding applied on Ride_Control\nlabel encoding applied on Stick\nlabel encoding applied on Transmission\nlabel encoding applied on Turbocharged\nlabel encoding applied on Blade_Extension\nlabel encoding applied on Enclosure_Type\nlabel encoding applied on Engine_Horsepower\nlabel encoding applied on Hydraulics\nlabel encoding applied on Pushblock\nlabel encoding applied on Ripper\nlabel encoding applied on Scarifier\nlabel encoding applied on Tip_Control\nlabel encoding applied on Coupler\nlabel encoding applied on Coupler_System\nlabel encoding applied on Grouser_Tracks\nlabel encoding applied on Hydraulics_Flow\nlabel encoding applied on Track_Type\nlabel encoding applied on Thumb\nlabel encoding applied on Pattern_Changer\nlabel encoding applied on Grouser_Type\nlabel encoding applied on Backhoe_Mounting\nlabel encoding applied on Blade_Type\nlabel encoding applied on Travel_Controls\nlabel encoding applied on Differential_Type\nlabel encoding applied on Steering_Controls\n\n\n\n##\n# after transformation\ndf.head(5).T.head(10)\n\n\n\n\n\n  \n    \n      \n      0\n      1\n      2\n      3\n      4\n    \n  \n  \n    \n      SalePrice\n      66000\n      57000\n      10000\n      38500\n      11000\n    \n    \n      ModelID\n      3157\n      77\n      7009\n      332\n      17311\n    \n    \n      datasource\n      121\n      121\n      121\n      121\n      121\n    \n    \n      auctioneerID\n      23\n      23\n      23\n      23\n      23\n    \n    \n      YearMade\n      2004.0\n      1996.0\n      2001.0\n      2001.0\n      2007.0\n    \n    \n      MachineHoursCurrentMeter\n      68.0\n      4640.0\n      2838.0\n      3486.0\n      722.0\n    \n    \n      UsageBand\n      2\n      2\n      1\n      1\n      3\n    \n    \n      saledate\n      2006-11-16 00:00:00\n      2004-03-26 00:00:00\n      2004-02-26 00:00:00\n      2011-05-19 00:00:00\n      2009-07-23 00:00:00\n    \n    \n      fiModelDesc\n      950\n      1725\n      331\n      3674\n      4208\n    \n    \n      fiBaseModel\n      296\n      527\n      110\n      1375\n      1529"
  },
  {
    "objectID": "posts/2022-04-25-bluebook-for-bulldozers.html#preprocessed-dataset",
    "href": "posts/2022-04-25-bluebook-for-bulldozers.html#preprocessed-dataset",
    "title": "Kaggle - Blue Book for Bulldozers",
    "section": "Preprocessed dataset",
    "text": "Preprocessed dataset\nAt this point, all our numerical and categorical features have been preprocessed. There should be no missing values, and all categorical features should have been encoded. Only DateTime columns are remaining to be processed and we will do that in the next section.\nLet’s verify the data using summary information.\n\ndf_info = pd.DataFrame()\ndf_info['sample'] = df.iloc[0]\ndf_info['data_type'] = df.dtypes\ndf_info['percent_missing'] = 100*df.isnull().sum() / len(df)\nprint(f\"Total features: {len(df.columns)}\")\ndf_info.sort_values('percent_missing')\n\nTotal features: 59\n\n\n\n\n\n\n  \n    \n      \n      sample\n      data_type\n      percent_missing\n    \n  \n  \n    \n      SalePrice\n      66000\n      int64\n      0.0\n    \n    \n      Pushblock\n      0\n      int8\n      0.0\n    \n    \n      Ripper\n      0\n      int8\n      0.0\n    \n    \n      Scarifier\n      0\n      int8\n      0.0\n    \n    \n      Tip_Control\n      0\n      int8\n      0.0\n    \n    \n      Tire_Size\n      20.5\n      float64\n      0.0\n    \n    \n      Coupler\n      0\n      int8\n      0.0\n    \n    \n      Coupler_System\n      0\n      int8\n      0.0\n    \n    \n      Grouser_Tracks\n      0\n      int8\n      0.0\n    \n    \n      Hydraulics_Flow\n      0\n      int8\n      0.0\n    \n    \n      Track_Type\n      0\n      int8\n      0.0\n    \n    \n      Undercarriage_Pad_Width\n      28.0\n      float64\n      0.0\n    \n    \n      Stick_Length\n      9.7\n      float64\n      0.0\n    \n    \n      Hydraulics\n      1\n      int8\n      0.0\n    \n    \n      Thumb\n      0\n      int8\n      0.0\n    \n    \n      Grouser_Type\n      0\n      int8\n      0.0\n    \n    \n      Backhoe_Mounting\n      0\n      int8\n      0.0\n    \n    \n      Blade_Type\n      0\n      int8\n      0.0\n    \n    \n      Travel_Controls\n      0\n      int8\n      0.0\n    \n    \n      Differential_Type\n      4\n      int8\n      0.0\n    \n    \n      Steering_Controls\n      2\n      int8\n      0.0\n    \n    \n      YearMade_na\n      False\n      bool\n      0.0\n    \n    \n      YearSold\n      2006\n      int64\n      0.0\n    \n    \n      YearsInUse\n      2.0\n      float64\n      0.0\n    \n    \n      MachineHoursCurrentMeter_na\n      False\n      bool\n      0.0\n    \n    \n      Tire_Size_na\n      True\n      bool\n      0.0\n    \n    \n      Stick_Length_na\n      True\n      bool\n      0.0\n    \n    \n      Pattern_Changer\n      0\n      int8\n      0.0\n    \n    \n      Undercarriage_Pad_Width_na\n      True\n      bool\n      0.0\n    \n    \n      Engine_Horsepower\n      0\n      int8\n      0.0\n    \n    \n      Blade_Width\n      14.0\n      float64\n      0.0\n    \n    \n      ModelID\n      3157\n      int64\n      0.0\n    \n    \n      datasource\n      121\n      int64\n      0.0\n    \n    \n      auctioneerID\n      23\n      int8\n      0.0\n    \n    \n      YearMade\n      2004.0\n      float64\n      0.0\n    \n    \n      MachineHoursCurrentMeter\n      68.0\n      float64\n      0.0\n    \n    \n      UsageBand\n      2\n      int8\n      0.0\n    \n    \n      saledate\n      2006-11-16 00:00:00\n      datetime64[ns]\n      0.0\n    \n    \n      fiModelDesc\n      950\n      int16\n      0.0\n    \n    \n      fiBaseModel\n      296\n      int16\n      0.0\n    \n    \n      fiSecondaryDesc\n      40\n      int16\n      0.0\n    \n    \n      fiModelSeries\n      0\n      int8\n      0.0\n    \n    \n      fiModelDescriptor\n      0\n      int16\n      0.0\n    \n    \n      Enclosure_Type\n      0\n      int8\n      0.0\n    \n    \n      ProductSize\n      0\n      int8\n      0.0\n    \n    \n      state\n      1\n      int8\n      0.0\n    \n    \n      ProductGroup\n      6\n      int8\n      0.0\n    \n    \n      ProductGroupDesc\n      6\n      int8\n      0.0\n    \n    \n      Drive_System\n      0\n      int8\n      0.0\n    \n    \n      Enclosure\n      3\n      int8\n      0.0\n    \n    \n      Forks\n      0\n      int8\n      0.0\n    \n    \n      Pad_Type\n      0\n      int8\n      0.0\n    \n    \n      Ride_Control\n      0\n      int8\n      0.0\n    \n    \n      Stick\n      0\n      int8\n      0.0\n    \n    \n      Transmission\n      0\n      int8\n      0.0\n    \n    \n      Turbocharged\n      0\n      int8\n      0.0\n    \n    \n      Blade_Extension\n      0\n      int8\n      0.0\n    \n    \n      fiProductClassDesc\n      59\n      int8\n      0.0\n    \n    \n      Blade_Width_na\n      True\n      bool\n      0.0\n    \n  \n\n\n\n\nLet’s retrain our base model one more time but this time with all the features except datetime columns to see where we stand in our OOB score. Below is a utility function created to quickly iterate over model training.\n\ndef train_and_plot_model(df, target='SalePrice', drop_features=[], n_estimators=70, plot=True, verbose=1):\n    \"\"\"\n    A utility function to train a RandomForrest model on the provided data, and plot the feature importances.\n    \n    Parameters\n    ----------\n    df: pandas.DataFrame\n        input dataset to be used for training\n    target: str\n        target feature. this is the feature we are trying to predict\n    drop_features: list\n        any features to be dropped before training. Default is empty list.\n    n_estimators: int\n        number of estimators to be used for model training. Default is 50.\n    \"\"\"\n\n    # target = 'SalePrice' # this is the feature we are trying to predict\n    features = list(df.columns)\n\n    # remove target feature and other specified features form the input variables\n    features.remove(target)\n    for f in drop_features:\n        features.remove(f)\n\n    X, y = df[features], df[target]\n\n    rf = RandomForestRegressor(n_estimators, oob_score=True, n_jobs=-1, verbose=verbose)\n    rf.fit(X, y)\n    oob_score = rf.oob_score_\n\n    # get trained model leaves and depth    \n    n_leaves = rf_n_leaves(rf)\n    m_depth = rf_m_depth(rf)\n\n    # print trained model info\n    print(f\"OOB scrore = {oob_score: .3f} \\nTree leaves = {n_leaves: ,d} \\nMedian depth = {m_depth}\")\n\n    # plot trained model feature importance\n    feature_importance = rf.feature_importances_\n    if plot:\n        plot_feature_importance(feature_importance, features, (10,15))\n    \n    # return trained model, feature names, and their importances\n    return (rf, features, feature_importance, oob_score)\n\n\n##\n# keeping n_estimators same as previous base model i.e. 70\n(rf, feature_names, feature_importance, oob_pre) = train_and_plot_model(df, drop_features=['saledate'], n_estimators=70)\n\n[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 4 concurrent workers.\n[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:  2.7min\n[Parallel(n_jobs=-1)]: Done  70 out of  70 | elapsed:  4.2min finished\n\n\nOOB scrore =  0.904 \nTree leaves =  14,660,873 \nMedian depth = 45.0\n\n\n\n\n\nThis is a big improvement in our model performance. Our base model had 0.790 OOB score and now we are at 0.904. Our features count has also increased from 7 to 59, so we can take one more shot at it by increasing the estomators count (n_estimators). Let’s use 150 trees this time (double that last time) to see how much effect it can have on model performance.\n\n(rf, feature_names, feature_importance, oob_pre) = train_and_plot_model(df, drop_features=['saledate'], n_estimators=150, plot=False)\n\n[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 4 concurrent workers.\n[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:  2.8min\n[Parallel(n_jobs=-1)]: Done 150 out of 150 | elapsed:  9.2min finished\n\n\nOOB scrore =  0.906 \nTree leaves =  31,408,663 \nMedian depth = 46.0\n\n\n\n\n\nThough there is only a slight increase in model performance but it took us significantly more time to train the model. So we will keep our estimators low and revisit them during the tuning phase. With “70” estimators our model performance is\nOOB scrore =  0.904\nTree leaves =  14,660,873 \nMedian depth = 45.0\nAt this point, our features have correct data types and their missing values are properly adjusted. We can now focus on some feature engineering aspects. Before moving further let’s also save our dataset till this point so if we make an error we can restart from this checkpoint.\n\n##\n# store preprocessed data as a check point for this state\ndf.to_pickle(dataset_path+'preprocessed.pkl')\n\nWe have used pickle format to preserve data types for saved data.\n\n##\n# load preprocessed data (optional step)\n# df = pd.read_pickle(dataset_path+'preprocessed.pkl')\n# (rf, feature_names, feature_importance, oob_pre) = train_and_plot_model(df, drop_features=['saledate'], n_estimators=70)"
  },
  {
    "objectID": "posts/2022-04-25-bluebook-for-bulldozers.html#feature-engineering",
    "href": "posts/2022-04-25-bluebook-for-bulldozers.html#feature-engineering",
    "title": "Kaggle - Blue Book for Bulldozers",
    "section": "Feature Engineering",
    "text": "Feature Engineering\nFor feature engineering, we will give priority to important features. For this let us again analyze the preprocessed dataset starting from important features to see what can be done against each feature.\n\n##\n# sort the dataframe with important features at the start\ntemp = pd.Series(feature_importance, feature_names)\ncols = temp.nlargest(len(temp)).index\n\nsniff(df[cols], 10)\n\n\n\n\n\n  \n    \n      \n      ProductSize\n      YearsInUse\n      fiBaseModel\n      fiSecondaryDesc\n      YearMade\n      fiProductClassDesc\n      ModelID\n      YearMade_na\n      fiModelDesc\n      Hydraulics_Flow\n      YearSold\n      state\n      Enclosure\n      Hydraulics\n      auctioneerID\n      fiModelSeries\n      fiModelDescriptor\n      MachineHoursCurrentMeter\n      Transmission\n      Pushblock\n      Engine_Horsepower\n      Ripper\n      Drive_System\n      datasource\n      Blade_Type\n      Stick_Length\n      UsageBand\n      Coupler\n      Tire_Size\n      Tire_Size_na\n      Undercarriage_Pad_Width\n      Thumb\n      Grouser_Type\n      Travel_Controls\n      Track_Type\n      Stick_Length_na\n      Forks\n      MachineHoursCurrentMeter_na\n      Ride_Control\n      Undercarriage_Pad_Width_na\n      Tip_Control\n      Differential_Type\n      Pattern_Changer\n      ProductGroup\n      ProductGroupDesc\n      Scarifier\n      Enclosure_Type\n      Stick\n      Steering_Controls\n      Blade_Width\n      Blade_Width_na\n      Pad_Type\n      Blade_Extension\n      Turbocharged\n      Grouser_Tracks\n      Coupler_System\n      Backhoe_Mounting\n    \n  \n  \n    \n      0\n      0\n      2.0\n      296\n      40\n      2004.0\n      59\n      3157\n      False\n      950\n      0\n      2006\n      1\n      3\n      1\n      23\n      0\n      0\n      68.0\n      0\n      0\n      0\n      0\n      0\n      121\n      0\n      9.7\n      2\n      0\n      20.5\n      True\n      28.0\n      0\n      0\n      0\n      0\n      True\n      0\n      False\n      0\n      True\n      0\n      4\n      0\n      6\n      6\n      0\n      0\n      0\n      2\n      14.0\n      True\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      1\n      4\n      8.0\n      527\n      54\n      1996.0\n      62\n      77\n      True\n      1725\n      2\n      2004\n      33\n      5\n      4\n      2\n      97\n      65\n      4640.0\n      5\n      1\n      1\n      3\n      2\n      132\n      5\n      11.0\n      1\n      2\n      23.5\n      False\n      16.0\n      1\n      1\n      3\n      2\n      False\n      1\n      True\n      1\n      False\n      1\n      0\n      2\n      3\n      3\n      1\n      2\n      1\n      0\n      12.0\n      False\n      2\n      1\n      1\n      1\n      1\n      1\n    \n    \n      2\n      6.0\n      3.0\n      110.0\n      0.0\n      2001.0\n      39.0\n      7009.0\n      NaN\n      331.0\n      1.0\n      2011.0\n      32.0\n      1.0\n      0.0\n      13.0\n      44.0\n      20.0\n      2838.0\n      6.0\n      NaN\n      2.0\n      2.0\n      4.0\n      136.0\n      6.0\n      15.9\n      3.0\n      1.0\n      13.0\n      NaN\n      32.0\n      2.0\n      3.0\n      5.0\n      1.0\n      NaN\n      NaN\n      NaN\n      2.0\n      NaN\n      2.0\n      1.0\n      1.0\n      4.0\n      4.0\n      NaN\n      1.0\n      2.0\n      1.0\n      13.0\n      NaN\n      3.0\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      3\n      3.0\n      10.0\n      1375.0\n      56.0\n      2007.0\n      8.0\n      332.0\n      NaN\n      3674.0\n      NaN\n      2009.0\n      44.0\n      0.0\n      11.0\n      4.0\n      102.0\n      64.0\n      3486.0\n      4.0\n      NaN\n      NaN\n      1.0\n      3.0\n      149.0\n      9.0\n      10.2\n      0.0\n      NaN\n      26.5\n      NaN\n      30.0\n      NaN\n      2.0\n      4.0\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      3.0\n      NaN\n      1.0\n      1.0\n      NaN\n      NaN\n      NaN\n      3.0\n      16.0\n      NaN\n      1.0\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      4\n      5.0\n      4.0\n      1529.0\n      47.0\n      1993.0\n      40.0\n      17311.0\n      NaN\n      4208.0\n      NaN\n      2008.0\n      3.0\n      2.0\n      5.0\n      24.0\n      33.0\n      83.0\n      722.0\n      3.0\n      NaN\n      NaN\n      NaN\n      1.0\n      172.0\n      7.0\n      10.6\n      NaN\n      NaN\n      29.5\n      NaN\n      22.0\n      NaN\n      NaN\n      2.0\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      2.0\n      NaN\n      5.0\n      5.0\n      NaN\n      NaN\n      NaN\n      5.0\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      5\n      2.0\n      11.0\n      175.0\n      61.0\n      2008.0\n      2.0\n      4605.0\n      NaN\n      493.0\n      NaN\n      2005.0\n      9.0\n      4.0\n      7.0\n      27.0\n      98.0\n      33.0\n      508.0\n      1.0\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      1.0\n      9.1\n      NaN\n      NaN\n      14.0\n      NaN\n      24.0\n      NaN\n      NaN\n      6.0\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      2.0\n      2.0\n      NaN\n      NaN\n      NaN\n      4.0\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      6\n      1.0\n      1.0\n      419.0\n      20.0\n      1998.0\n      14.0\n      1937.0\n      NaN\n      1453.0\n      NaN\n      2007.0\n      13.0\n      NaN\n      3.0\n      30.0\n      2.0\n      100.0\n      11540.0\n      2.0\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      4.0\n      10.1\n      NaN\n      NaN\n      17.5\n      NaN\n      18.0\n      NaN\n      NaN\n      1.0\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      7\n      NaN\n      7.0\n      243.0\n      105.0\n      1999.0\n      17.0\n      3539.0\n      NaN\n      740.0\n      NaN\n      2010.0\n      37.0\n      NaN\n      2.0\n      26.0\n      73.0\n      128.0\n      4883.0\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      8.0\n      9.6\n      NaN\n      NaN\n      15.5\n      NaN\n      36.0\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      8\n      NaN\n      5.0\n      250.0\n      133.0\n      2003.0\n      68.0\n      36003.0\n      NaN\n      779.0\n      NaN\n      2000.0\n      35.0\n      NaN\n      6.0\n      25.0\n      13.0\n      71.0\n      302.0\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      3.0\n      12.8\n      NaN\n      NaN\n      7.0\n      NaN\n      20.0\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      9\n      NaN\n      14.0\n      540.0\n      129.0\n      1991.0\n      51.0\n      3883.0\n      NaN\n      1771.0\n      NaN\n      2002.0\n      4.0\n      NaN\n      8.0\n      11.0\n      54.0\n      122.0\n      20700.0\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      2.0\n      8.2\n      NaN\n      NaN\n      10.0\n      NaN\n      27.0\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n  \n\n\n\n\nThe above table is sorted based on the importance of each feature. Features at the start have more importance. So let’s visit each feature to see if any feature engineering (FE) can be used to gain more insights from the data.\n\nProductSize, YearsInUse: These features have numbers. Not a candidate for FE.\nfiBaseModel: It is label encoded. Let’s visit this column’s original values to see if any more features can be generated from it.\n\n\ndf_raw['fiBaseModel'].unique()[:50]\n\narray(['521D', '950FII', '226', 'PC120-6E', 'S175', '310G', '790ELC',\n       '416D', '430HAG', '988B', 'D31E', 'PC200LC6', '420D', '214E',\n       '310E', '334', '45NX', '302.5', '580SUPER K', 'JS260', '120G',\n       '966FII', 'EX550STD', '685B', '345BL', '330BL', '873', 'WA250',\n       '750BLT', '303CR', '95ZII', '416', '303.5', 'CTL60', '140G',\n       '307CSB', 'EC210LC', 'MF650', 'RC30', 'EX120-5', '70XT', '772A',\n       '160HNA', '216', '304CR', 'D3CIIIXL', '236', '120C', 'PC228',\n       'SK160LC'], dtype=object)\n\n\n\nfiBaseModel: original values look very random and do not give much information. There are two other columns in the importance list ‘fiModelDesc’, and ‘fiSecondaryDesc’ and from their name they look related to ‘fiBaseModel’. So let’s analyze them together.\n\n\ndf_raw[['fiBaseModel', 'fiModelDesc', 'fiSecondaryDesc']].head(10)\n\n\n\n\n\n  \n    \n      \n      fiBaseModel\n      fiModelDesc\n      fiSecondaryDesc\n    \n  \n  \n    \n      0\n      521\n      521D\n      D\n    \n    \n      1\n      950\n      950FII\n      F\n    \n    \n      2\n      226\n      226\n      NaN\n    \n    \n      3\n      PC120\n      PC120-6E\n      NaN\n    \n    \n      4\n      S175\n      S175\n      NaN\n    \n    \n      5\n      310\n      310G\n      G\n    \n    \n      6\n      790\n      790ELC\n      E\n    \n    \n      7\n      416\n      416D\n      D\n    \n    \n      8\n      430\n      430HAG\n      HAG\n    \n    \n      9\n      988\n      988B\n      B\n    \n  \n\n\n\n\n\nfiBaseModel, fiModelDesc, fiSecondaryDesc: From the above table all these three features are very much related but their values are very random and do not give us much information. So let’s leave them as it is.\nYearMade: It has numbers. Not a candidate for FE.\nfiProductClassDesc. This feature is also encode so let’s visit it’s original values.\n\n\ndf_raw['fiProductClassDesc'].unique()[:15]\n\narray(['Wheel Loader - 110.0 to 120.0 Horsepower',\n       'Wheel Loader - 150.0 to 175.0 Horsepower',\n       'Skid Steer Loader - 1351.0 to 1601.0 Lb Operating Capacity',\n       'Hydraulic Excavator, Track - 12.0 to 14.0 Metric Tons',\n       'Skid Steer Loader - 1601.0 to 1751.0 Lb Operating Capacity',\n       'Backhoe Loader - 14.0 to 15.0 Ft Standard Digging Depth',\n       'Hydraulic Excavator, Track - 21.0 to 24.0 Metric Tons',\n       'Hydraulic Excavator, Track - 3.0 to 4.0 Metric Tons',\n       'Wheel Loader - 350.0 to 500.0 Horsepower',\n       'Track Type Tractor, Dozer - 20.0 to 75.0 Horsepower',\n       'Hydraulic Excavator, Track - 19.0 to 21.0 Metric Tons',\n       'Hydraulic Excavator, Track - 4.0 to 5.0 Metric Tons',\n       'Hydraulic Excavator, Track - 2.0 to 3.0 Metric Tons',\n       'Hydraulic Excavator, Track - 24.0 to 28.0 Metric Tons',\n       'Motorgrader - 45.0 to 130.0 Horsepower'], dtype=object)\n\n\n\nfiProductClassDesc: This feature has text strings and it is also showing that they are not randow but has some pattern in then. They seems to be a good candidate for FE. We will do that in next section.\nModelID, YearSold: These features have numbers. Not a candidate for FE.\nHydraulics_Flow: It is encoded so let’s visit original values first.\n\n\ndf_raw['Hydraulics_Flow'].unique()\n\narray([nan, 'Standard', 'High Flow', 'None or Unspecified'], dtype=object)\n\n\n\nHydraulics_Flow: We label encoded it and it came up as an important feature. Its values are showing low variance so it is a better candidate for one-hot encoding. We will do that in the next section.\nstate: It is encoded so let’s visit its original values.\n\n\nprint(f\"total unique values: {len(df_raw['state'].unique())}\")\ndf_raw['state'].unique()[:15]\n\ntotal unique values: 53\n\n\narray(['Alabama', 'North Carolina', 'New York', 'Texas', 'Arizona',\n       'Florida', 'Illinois', 'Oregon', 'Ohio', 'Arkansas', 'Wisconsin',\n       'Kansas', 'Nevada', 'Iowa', 'Maine'], dtype=object)\n\n\n\nstate: By looking at original values we can see that it is a categorical nominal feature. It can be one hot encoded but since it has high variance (53 unique values) it is better to keep it as label encoded. So leave this feature as it is.\nEnclosure: It is encoded. So let’s check original values\n\n\nprint(f\"total unique values: {len(df_raw['Enclosure'].unique())}\")\ndf_raw['Enclosure'].unique()\n\ntotal unique values: 7\n\n\narray(['EROPS w AC', 'OROPS', 'EROPS', nan, 'EROPS AC', 'NO ROPS',\n       'None or Unspecified'], dtype=object)\n\n\n\nEnclosure: Original values show that it is a categorical feature with good importance and low variance, so it is also suitable for OHE.\nHydraulics: It is also encoded. So let’s check original values\n\n\nprint(f\"total unique values: {len(df_raw['Hydraulics'].unique())}\")\ndf_raw['Hydraulics'].unique()\n\ntotal unique values: 13\n\n\narray(['2 Valve', 'Auxiliary', nan, 'Standard', 'Base + 1 Function',\n       'Base + 3 Function', '4 Valve', '3 Valve', 'Base + 2 Function',\n       'Base + 4 Function', 'None or Unspecified', 'Base + 5 Function',\n       'Base + 6 Function'], dtype=object)\n\n\n\nHydraulics: Now this feature is again categorical, does not have high variance but also has low importance. We can consider it for OHE but since it is coming at the lower end feature importance, it will not have much impact on model performace. So we can skip it for OHE.\nFor the remaining features, importance is not significant enough to be considered for any FE. We can keep them as it is.\nsaledate: We did not use this feature in our last model training. But from the feature importance we can see that features that contain any date information are showing significant importance. So we should also include this feature in our next model.\n\nTo summarize this section, the features that are suitable for any FE are * fiProductClassDesc * Hydraulics_Flow * Enclosure * saledate\n\nfiProductClassDesc\nLet’s check the original values for this feature one more time.\n\ndf_raw['fiProductClassDesc'].head()\n\n0             Wheel Loader - 110.0 to 120.0 Horsepower\n1             Wheel Loader - 150.0 to 175.0 Horsepower\n2    Skid Steer Loader - 1351.0 to 1601.0 Lb Operat...\n3    Hydraulic Excavator, Track - 12.0 to 14.0 Metr...\n4    Skid Steer Loader - 1601.0 to 1751.0 Lb Operat...\nName: fiProductClassDesc, dtype: object\n\n\nThough this feature is named ‘ProductClassDesc’ but by looking at its value we can see that besides class description there is also information on class specification. If we take the first value then * ‘Wheel Loader’ -> this is the class description * ‘110.0 to 120.0 Horsepower’ -> this is class specification\nand even in the class specification we have * 110 -> spec lower limit * 120 -> spec upper limit * ‘Horsepower’ -> spec unit\nUse this information to create new columns\n\n## \n# split the class description\ndf_split = df_raw.fiProductClassDesc.str.split(' - ',expand=True).values\n\n\n##\n# on 0 index we have class description\ndf_split[:,0]\n\narray(['Wheel Loader', 'Wheel Loader', 'Skid Steer Loader', ...,\n       'Hydraulic Excavator, Track', 'Hydraulic Excavator, Track',\n       'Hydraulic Excavator, Track'], dtype=object)\n\n\n\n##\n# on 1 index we have class specification\ndf_split[:,1]\n\narray(['110.0 to 120.0 Horsepower', '150.0 to 175.0 Horsepower',\n       '1351.0 to 1601.0 Lb Operating Capacity', ...,\n       '3.0 to 4.0 Metric Tons', '2.0 to 3.0 Metric Tons',\n       '2.0 to 3.0 Metric Tons'], dtype=object)\n\n\n\n##\n# let's create two new columns for this\ndf['fiProductClassDesc'] = df_split[:,0] \ndf['fiProductClassSpec'] = df_split[:,1]\n\n\n##\n# split class spec further to get limits and units\npattern = r'([0-9.\\+]*)(?: to ([0-9.\\+]*)|\\+) ([a-zA-Z ]*)'\ndf_split = df['fiProductClassSpec'].str.extract(pattern, expand=True).values\ndf_split = pd.DataFrame(df_split, columns=['fiProductClassSpec_lower', 'fiProductClassSpec_upper', 'fiProductClassSpec_units'])\ndf_split.head()\n\n\n\n\n\n  \n    \n      \n      fiProductClassSpec_lower\n      fiProductClassSpec_upper\n      fiProductClassSpec_units\n    \n  \n  \n    \n      0\n      110.0\n      120.0\n      Horsepower\n    \n    \n      1\n      150.0\n      175.0\n      Horsepower\n    \n    \n      2\n      1351.0\n      1601.0\n      Lb Operating Capacity\n    \n    \n      3\n      12.0\n      14.0\n      Metric Tons\n    \n    \n      4\n      1601.0\n      1751.0\n      Lb Operating Capacity\n    \n  \n\n\n\n\n\n##\n# merge new columns to our dataset\ndf = pd.concat([df, df_split], axis=1)\ndel df['fiProductClassSpec'] # class spec is no more required. we have it's sub-features\ndf.head()\n\n\n\n\n\n  \n    \n      \n      SalePrice\n      ModelID\n      datasource\n      auctioneerID\n      YearMade\n      MachineHoursCurrentMeter\n      UsageBand\n      saledate\n      fiModelDesc\n      fiBaseModel\n      fiSecondaryDesc\n      fiModelSeries\n      fiModelDescriptor\n      ProductSize\n      fiProductClassDesc\n      state\n      ProductGroup\n      ProductGroupDesc\n      Drive_System\n      Enclosure\n      Forks\n      Pad_Type\n      Ride_Control\n      Stick\n      Transmission\n      Turbocharged\n      Blade_Extension\n      Blade_Width\n      Enclosure_Type\n      Engine_Horsepower\n      Hydraulics\n      Pushblock\n      Ripper\n      Scarifier\n      Tip_Control\n      Tire_Size\n      Coupler\n      Coupler_System\n      Grouser_Tracks\n      Hydraulics_Flow\n      Track_Type\n      Undercarriage_Pad_Width\n      Stick_Length\n      Thumb\n      Pattern_Changer\n      Grouser_Type\n      Backhoe_Mounting\n      Blade_Type\n      Travel_Controls\n      Differential_Type\n      Steering_Controls\n      YearMade_na\n      YearSold\n      YearsInUse\n      MachineHoursCurrentMeter_na\n      Tire_Size_na\n      Stick_Length_na\n      Undercarriage_Pad_Width_na\n      Blade_Width_na\n      fiProductClassSpec_lower\n      fiProductClassSpec_upper\n      fiProductClassSpec_units\n    \n  \n  \n    \n      0\n      66000\n      3157\n      121\n      23\n      2004.0\n      68.0\n      2\n      2006-11-16\n      950\n      296\n      40\n      0\n      0\n      0\n      Wheel Loader\n      1\n      6\n      6\n      0\n      3\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      14.0\n      0\n      0\n      1\n      0\n      0\n      0\n      0\n      20.5\n      0\n      0\n      0\n      0\n      0\n      28.0\n      9.7\n      0\n      0\n      0\n      0\n      0\n      0\n      4\n      2\n      False\n      2006\n      2.0\n      False\n      True\n      True\n      True\n      True\n      110.0\n      120.0\n      Horsepower\n    \n    \n      1\n      57000\n      77\n      121\n      23\n      1996.0\n      4640.0\n      2\n      2004-03-26\n      1725\n      527\n      54\n      97\n      0\n      4\n      Wheel Loader\n      33\n      6\n      6\n      0\n      3\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      14.0\n      0\n      0\n      1\n      0\n      0\n      0\n      0\n      23.5\n      0\n      0\n      0\n      0\n      0\n      28.0\n      9.7\n      0\n      0\n      0\n      0\n      0\n      0\n      4\n      2\n      False\n      2004\n      8.0\n      False\n      False\n      True\n      True\n      True\n      150.0\n      175.0\n      Horsepower\n    \n    \n      2\n      10000\n      7009\n      121\n      23\n      2001.0\n      2838.0\n      1\n      2004-02-26\n      331\n      110\n      0\n      0\n      0\n      0\n      Skid Steer Loader\n      32\n      3\n      3\n      0\n      5\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      14.0\n      0\n      0\n      4\n      0\n      0\n      0\n      0\n      20.5\n      0\n      0\n      0\n      2\n      0\n      28.0\n      9.7\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      False\n      2004\n      3.0\n      False\n      True\n      True\n      True\n      True\n      1351.0\n      1601.0\n      Lb Operating Capacity\n    \n    \n      3\n      38500\n      332\n      121\n      23\n      2001.0\n      3486.0\n      1\n      2011-05-19\n      3674\n      1375\n      0\n      44\n      0\n      6\n      Hydraulic Excavator, Track\n      44\n      4\n      4\n      0\n      3\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      14.0\n      0\n      0\n      1\n      0\n      0\n      0\n      0\n      20.5\n      0\n      0\n      0\n      0\n      0\n      28.0\n      9.7\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      False\n      2011\n      10.0\n      False\n      True\n      True\n      True\n      True\n      12.0\n      14.0\n      Metric Tons\n    \n    \n      4\n      11000\n      17311\n      121\n      23\n      2007.0\n      722.0\n      3\n      2009-07-23\n      4208\n      1529\n      0\n      0\n      0\n      0\n      Skid Steer Loader\n      32\n      3\n      3\n      0\n      1\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      14.0\n      0\n      0\n      4\n      0\n      0\n      0\n      0\n      20.5\n      0\n      0\n      0\n      2\n      0\n      28.0\n      9.7\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      False\n      2009\n      2.0\n      False\n      True\n      True\n      True\n      True\n      1601.0\n      1751.0\n      Lb Operating Capacity\n    \n  \n\n\n\n\n\n##\n# convert to numerical features\ndf['fiProductClassSpec_lower'] = pd.to_numeric(df['fiProductClassSpec_lower'])\ndf['fiProductClassSpec_upper'] = pd.to_numeric(df['fiProductClassSpec_upper'])\n\n# apply fix for numerical features\nfix_missing_num(df, 'fiProductClassSpec_lower')\nfix_missing_num(df, 'fiProductClassSpec_upper')\n\n# apply fix for categorical features\ndf_string_to_cat(df)\ndf_cat_to_catcode(df)\n\nlabel encoding applied on fiProductClassDesc\nlabel encoding applied on fiProductClassSpec_units\n\n\n\n(rf, feature_names, feature_importance, oob_hydralics) = train_and_plot_model(df, drop_features=['saledate'])\n\n[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 4 concurrent workers.\n[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:  1.6min\n[Parallel(n_jobs=-1)]: Done  70 out of  70 | elapsed:  2.6min finished\n\n\nOOB scrore =  0.905 \nTree leaves =  14,650,747 \nMedian depth = 47.0\n\n\n\n\n\nThere is only a slight increase in OOB score but if we check the feature importance plot both fiProductClassSpec_upper and fiProductClassSpec_lower are showing high importance. We can take this as a positive signal for good features.\n\n\nHydralics_Flow\nWe need to apply one hot encoding (OHE) to this feature. Let’s start by checking unique values for Hydraulics_Flow.\n\ndf['Hydraulics_Flow'].value_counts()\n\n0    357788\n2     42784\n1       553\nName: Hydraulics_Flow, dtype: int64\n\n\nWe have encoded this feature in the preprocessing section. Although we can use this encoded feature for one-hot encoding but we don’t have original labels at this point. It would be better if we use original labels for OHE so that the dummy columns created as a result of that also have proper names with labels. Using encoded dummy column names makes them difficult to understand and follow. let’s use the original dataframe to check the unique values.\n\ndf_raw['Hydraulics_Flow'].value_counts(dropna=False)\n\nNaN                    357763\nStandard                42784\nHigh Flow                 553\nNone or Unspecified        25\nName: Hydraulics_Flow, dtype: int64\n\n\nBefore applying OHE we need to preprocess ‘None or Unspecified’ as they repsent the same as np.nan. So let’s do that.\n\n## \n# get the original values\ndf['Hydraulics_Flow'] = df_raw['Hydraulics_Flow']\ndf['Hydraulics_Flow'] = df['Hydraulics_Flow'].replace('None or Unspecified', np.nan)\n\ndf['Hydraulics_Flow'].value_counts(dropna=False)\n\nNaN          357788\nStandard      42784\nHigh Flow       553\nName: Hydraulics_Flow, dtype: int64\n\n\nLet’s check the first few rows of this column. We will use them to verify our final result.\n\ndf['Hydraulics_Flow'].head()\n\n0         NaN\n1         NaN\n2    Standard\n3         NaN\n4    Standard\nName: Hydraulics_Flow, dtype: object\n\n\nNotice that in the first five rows there are ‘Standard’ values at row index 2 and 4, and the remaining are ‘NaN’ values. We will OHE them in the next step and compare the results to ensure encoding is properly working.\n\nfrom sklearn.preprocessing import OneHotEncoder\n\nonehot_encoder = OneHotEncoder()\nonehot_output = onehot_encoder.fit_transform(df[['Hydraulics_Flow']])\n\n# check the output\nprint(onehot_output[:5].toarray())\n\n[[0. 0. 1.]\n [0. 0. 1.]\n [0. 1. 0.]\n [0. 0. 1.]\n [0. 1. 0.]]\n\n\nThese are same five rows but this time encoded with one-hot values. From the position of ‘1’ appearing in different columns we can deduce that first column is for label ‘High Flow’ and second is for ‘Standard’ and third is for ‘NaN’. It would be easier for us to track these dummy columns if we have proper names on them. So let’s do that.\nWe can get the dummy column names by calling get_feature_names_out() on our encoder.\n\n# name of the columns\nonehot_encoder.get_feature_names_out()\n\narray(['Hydraulics_Flow_High Flow', 'Hydraulics_Flow_Standard',\n       'Hydraulics_Flow_nan'], dtype=object)\n\n\nTo create a dataframe of these dummy variables.\n\ndf_onehot = pd.DataFrame(onehot_output.toarray(), columns=onehot_encoder.get_feature_names_out())\ndf_onehot.head()\n\n\n\n\n\n  \n    \n      \n      Hydraulics_Flow_High Flow\n      Hydraulics_Flow_Standard\n      Hydraulics_Flow_nan\n    \n  \n  \n    \n      0\n      0.0\n      0.0\n      1.0\n    \n    \n      1\n      0.0\n      0.0\n      1.0\n    \n    \n      2\n      0.0\n      1.0\n      0.0\n    \n    \n      3\n      0.0\n      0.0\n      1.0\n    \n    \n      4\n      0.0\n      1.0\n      0.0\n    \n  \n\n\n\n\nAt this point Hydraulics_Flow is OHE so we can drop the original column from the dataset and add these encoded columns.\n\ndel df['Hydraulics_Flow']\n\ndf = pd.concat([df, df_onehot], axis=1) # concat dataframes column wise\ndf.head()\n\n\n\n\n\n  \n    \n      \n      SalePrice\n      ModelID\n      datasource\n      auctioneerID\n      YearMade\n      MachineHoursCurrentMeter\n      UsageBand\n      saledate\n      fiModelDesc\n      fiBaseModel\n      fiSecondaryDesc\n      fiModelSeries\n      fiModelDescriptor\n      ProductSize\n      fiProductClassDesc\n      state\n      ProductGroup\n      ProductGroupDesc\n      Drive_System\n      Enclosure\n      Forks\n      Pad_Type\n      Ride_Control\n      Stick\n      Transmission\n      Turbocharged\n      Blade_Extension\n      Blade_Width\n      Enclosure_Type\n      Engine_Horsepower\n      Hydraulics\n      Pushblock\n      Ripper\n      Scarifier\n      Tip_Control\n      Tire_Size\n      Coupler\n      Coupler_System\n      Grouser_Tracks\n      Track_Type\n      Undercarriage_Pad_Width\n      Stick_Length\n      Thumb\n      Pattern_Changer\n      Grouser_Type\n      Backhoe_Mounting\n      Blade_Type\n      Travel_Controls\n      Differential_Type\n      Steering_Controls\n      YearMade_na\n      YearSold\n      YearsInUse\n      MachineHoursCurrentMeter_na\n      Tire_Size_na\n      Stick_Length_na\n      Undercarriage_Pad_Width_na\n      Blade_Width_na\n      fiProductClassSpec_lower\n      fiProductClassSpec_upper\n      fiProductClassSpec_units\n      fiProductClassSpec_lower_na\n      fiProductClassSpec_upper_na\n      Hydraulics_Flow_High Flow\n      Hydraulics_Flow_Standard\n      Hydraulics_Flow_nan\n    \n  \n  \n    \n      0\n      66000\n      3157\n      121\n      23\n      2004.0\n      68.0\n      2\n      2006-11-16\n      950\n      296\n      40\n      0\n      0\n      0\n      6\n      1\n      6\n      6\n      0\n      3\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      14.0\n      0\n      0\n      1\n      0\n      0\n      0\n      0\n      20.5\n      0\n      0\n      0\n      0\n      28.0\n      9.7\n      0\n      0\n      0\n      0\n      0\n      0\n      4\n      2\n      False\n      2006\n      2.0\n      False\n      True\n      True\n      True\n      True\n      110.0\n      120.0\n      2\n      False\n      False\n      0.0\n      0.0\n      1.0\n    \n    \n      1\n      57000\n      77\n      121\n      23\n      1996.0\n      4640.0\n      2\n      2004-03-26\n      1725\n      527\n      54\n      97\n      0\n      4\n      6\n      33\n      6\n      6\n      0\n      3\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      14.0\n      0\n      0\n      1\n      0\n      0\n      0\n      0\n      23.5\n      0\n      0\n      0\n      0\n      28.0\n      9.7\n      0\n      0\n      0\n      0\n      0\n      0\n      4\n      2\n      False\n      2004\n      8.0\n      False\n      False\n      True\n      True\n      True\n      150.0\n      175.0\n      2\n      False\n      False\n      0.0\n      0.0\n      1.0\n    \n    \n      2\n      10000\n      7009\n      121\n      23\n      2001.0\n      2838.0\n      1\n      2004-02-26\n      331\n      110\n      0\n      0\n      0\n      0\n      4\n      32\n      3\n      3\n      0\n      5\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      14.0\n      0\n      0\n      4\n      0\n      0\n      0\n      0\n      20.5\n      0\n      0\n      0\n      0\n      28.0\n      9.7\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      False\n      2004\n      3.0\n      False\n      True\n      True\n      True\n      True\n      1351.0\n      1601.0\n      3\n      False\n      False\n      0.0\n      1.0\n      0.0\n    \n    \n      3\n      38500\n      332\n      121\n      23\n      2001.0\n      3486.0\n      1\n      2011-05-19\n      3674\n      1375\n      0\n      44\n      0\n      6\n      2\n      44\n      4\n      4\n      0\n      3\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      14.0\n      0\n      0\n      1\n      0\n      0\n      0\n      0\n      20.5\n      0\n      0\n      0\n      0\n      28.0\n      9.7\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      False\n      2011\n      10.0\n      False\n      True\n      True\n      True\n      True\n      12.0\n      14.0\n      4\n      False\n      False\n      0.0\n      0.0\n      1.0\n    \n    \n      4\n      11000\n      17311\n      121\n      23\n      2007.0\n      722.0\n      3\n      2009-07-23\n      4208\n      1529\n      0\n      0\n      0\n      0\n      4\n      32\n      3\n      3\n      0\n      1\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      14.0\n      0\n      0\n      4\n      0\n      0\n      0\n      0\n      20.5\n      0\n      0\n      0\n      0\n      28.0\n      9.7\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      False\n      2009\n      2.0\n      False\n      True\n      True\n      True\n      True\n      1601.0\n      1751.0\n      3\n      False\n      False\n      0.0\n      1.0\n      0.0\n    \n  \n\n\n\n\nLet’s retain our model to check if there is any affect on model performance.\n\n(rf, feature_names, feature_importance, oob_hydralics) = train_and_plot_model(df, drop_features=['saledate'])\n\n[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 4 concurrent workers.\n[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:  1.6min\n[Parallel(n_jobs=-1)]: Done  70 out of  70 | elapsed:  2.6min finished\n\n\nOOB scrore =  0.905 \nTree leaves =  14,650,376 \nMedian depth = 48.0\n\n\n\n\n\nThere is no effect on the model performance but one feature ‘Hydraulics_Flow_nan’ is showing some importance on the plot. The remaining features (‘Hydraulics_Flow_High Flow’ and ‘Hydraulics_Flow_Standard’) do not affect the model’s performance. If it was not of ‘Hydraulics_Flow_nan’ importance we could have skipped OHE for ‘Hydraulics_Flow’.\n\n\nEnclosure\nNext feature is Enclosure, and we will follow the same steps as for last feature to one-hot encode it.\n\n##\n# check value counts\ndf_raw['Enclosure'].value_counts(dropna=False)\n\nOROPS                  173932\nEROPS                  139026\nEROPS w AC              87820\nNaN                       325\nEROPS AC                   17\nNO ROPS                     3\nNone or Unspecified         2\nName: Enclosure, dtype: int64\n\n\nHere ROPS is an abbreviation for Roll Over Protection System and there are multiple variants of this standard * OROPS = Open ROPS * EROPS = Enclosed ROPS * EROPS AC = Enclosed ROPS with Air Conditioning * EROPS w AC = Enclosed ROPS with Air Conditioning. Same as ‘EROPS AC’ * NO ROPS = No ROPS. Same as ‘NaN’ or ‘None or Unspecified’\nYou can read more about ROPS standards here * http://www.miningrops.com.au/ropsintro.html * https://www.youtube.com/watch?v=LZ40O1My8E4&ab_channel=MissouriEarthMovers\nUsing this information we can also preprocess this feature to make its values more consistent.\n\n## \n# get the original values\ndf['Enclosure'] = df_raw['Enclosure']\n\n# change 'None or Unspecified' and 'NO ROPS' to np.nan\ndf['Enclosure'] = df['Enclosure'].replace('None or Unspecified', np.nan)\ndf['Enclosure'] = df['Enclosure'].replace('NO ROPS', np.nan)\n\n# change 'EROPS w AC' to 'EROPS AC'\ndf['Enclosure'] = df['Enclosure'].replace('EROPS w AC', 'EROPS AC')\n\ndf['Enclosure'].value_counts(dropna=False)\n\nOROPS       173932\nEROPS       139026\nEROPS AC     87837\nNaN            330\nName: Enclosure, dtype: int64\n\n\n\n##\n# before OHE\ndf.head()\n\n\n\n\n\n  \n    \n      \n      SalePrice\n      ModelID\n      datasource\n      auctioneerID\n      YearMade\n      MachineHoursCurrentMeter\n      UsageBand\n      saledate\n      fiModelDesc\n      fiBaseModel\n      fiSecondaryDesc\n      fiModelSeries\n      fiModelDescriptor\n      ProductSize\n      fiProductClassDesc\n      state\n      ProductGroup\n      ProductGroupDesc\n      Drive_System\n      Enclosure\n      Forks\n      Pad_Type\n      Ride_Control\n      Stick\n      Transmission\n      Turbocharged\n      Blade_Extension\n      Blade_Width\n      Enclosure_Type\n      Engine_Horsepower\n      Hydraulics\n      Pushblock\n      Ripper\n      Scarifier\n      Tip_Control\n      Tire_Size\n      Coupler\n      Coupler_System\n      Grouser_Tracks\n      Track_Type\n      Undercarriage_Pad_Width\n      Stick_Length\n      Thumb\n      Pattern_Changer\n      Grouser_Type\n      Backhoe_Mounting\n      Blade_Type\n      Travel_Controls\n      Differential_Type\n      Steering_Controls\n      YearMade_na\n      YearSold\n      YearsInUse\n      MachineHoursCurrentMeter_na\n      Tire_Size_na\n      Stick_Length_na\n      Undercarriage_Pad_Width_na\n      Blade_Width_na\n      fiProductClassSpec_lower\n      fiProductClassSpec_upper\n      fiProductClassSpec_units\n      fiProductClassSpec_lower_na\n      fiProductClassSpec_upper_na\n      Hydraulics_Flow_High Flow\n      Hydraulics_Flow_Standard\n      Hydraulics_Flow_nan\n    \n  \n  \n    \n      0\n      66000\n      3157\n      121\n      23\n      2004.0\n      68.0\n      2\n      2006-11-16\n      950\n      296\n      40\n      0\n      0\n      0\n      6\n      1\n      6\n      6\n      0\n      EROPS AC\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      14.0\n      0\n      0\n      1\n      0\n      0\n      0\n      0\n      20.5\n      0\n      0\n      0\n      0\n      28.0\n      9.7\n      0\n      0\n      0\n      0\n      0\n      0\n      4\n      2\n      False\n      2006\n      2.0\n      False\n      True\n      True\n      True\n      True\n      110.0\n      120.0\n      2\n      False\n      False\n      0.0\n      0.0\n      1.0\n    \n    \n      1\n      57000\n      77\n      121\n      23\n      1996.0\n      4640.0\n      2\n      2004-03-26\n      1725\n      527\n      54\n      97\n      0\n      4\n      6\n      33\n      6\n      6\n      0\n      EROPS AC\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      14.0\n      0\n      0\n      1\n      0\n      0\n      0\n      0\n      23.5\n      0\n      0\n      0\n      0\n      28.0\n      9.7\n      0\n      0\n      0\n      0\n      0\n      0\n      4\n      2\n      False\n      2004\n      8.0\n      False\n      False\n      True\n      True\n      True\n      150.0\n      175.0\n      2\n      False\n      False\n      0.0\n      0.0\n      1.0\n    \n    \n      2\n      10000\n      7009\n      121\n      23\n      2001.0\n      2838.0\n      1\n      2004-02-26\n      331\n      110\n      0\n      0\n      0\n      0\n      4\n      32\n      3\n      3\n      0\n      OROPS\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      14.0\n      0\n      0\n      4\n      0\n      0\n      0\n      0\n      20.5\n      0\n      0\n      0\n      0\n      28.0\n      9.7\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      False\n      2004\n      3.0\n      False\n      True\n      True\n      True\n      True\n      1351.0\n      1601.0\n      3\n      False\n      False\n      0.0\n      1.0\n      0.0\n    \n    \n      3\n      38500\n      332\n      121\n      23\n      2001.0\n      3486.0\n      1\n      2011-05-19\n      3674\n      1375\n      0\n      44\n      0\n      6\n      2\n      44\n      4\n      4\n      0\n      EROPS AC\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      14.0\n      0\n      0\n      1\n      0\n      0\n      0\n      0\n      20.5\n      0\n      0\n      0\n      0\n      28.0\n      9.7\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      False\n      2011\n      10.0\n      False\n      True\n      True\n      True\n      True\n      12.0\n      14.0\n      4\n      False\n      False\n      0.0\n      0.0\n      1.0\n    \n    \n      4\n      11000\n      17311\n      121\n      23\n      2007.0\n      722.0\n      3\n      2009-07-23\n      4208\n      1529\n      0\n      0\n      0\n      0\n      4\n      32\n      3\n      3\n      0\n      EROPS\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      14.0\n      0\n      0\n      4\n      0\n      0\n      0\n      0\n      20.5\n      0\n      0\n      0\n      0\n      28.0\n      9.7\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      False\n      2009\n      2.0\n      False\n      True\n      True\n      True\n      True\n      1601.0\n      1751.0\n      3\n      False\n      False\n      0.0\n      1.0\n      0.0\n    \n  \n\n\n\n\n\n##\n# one hot encode 'Enclosure'\nonehot_encoder = OneHotEncoder()\nonehot_output = onehot_encoder.fit_transform(df[['Enclosure']])\n\ndf_onehot = pd.DataFrame(onehot_output.toarray(), columns=onehot_encoder.get_feature_names_out())\ndf_onehot.head()\n\n\n\n\n\n  \n    \n      \n      Enclosure_EROPS\n      Enclosure_EROPS AC\n      Enclosure_OROPS\n      Enclosure_nan\n    \n  \n  \n    \n      0\n      0.0\n      1.0\n      0.0\n      0.0\n    \n    \n      1\n      0.0\n      1.0\n      0.0\n      0.0\n    \n    \n      2\n      0.0\n      0.0\n      1.0\n      0.0\n    \n    \n      3\n      0.0\n      1.0\n      0.0\n      0.0\n    \n    \n      4\n      1.0\n      0.0\n      0.0\n      0.0\n    \n  \n\n\n\n\n\n## \n# drop original column\ndel df['Enclosure']\n\n# add dummy columns to the dataframe\ndf = pd.concat([df, df_onehot], axis=1) # concat dataframes column wise\n\n# after OHE\ndf.head() \n\n\n\n\n\n  \n    \n      \n      SalePrice\n      ModelID\n      datasource\n      auctioneerID\n      YearMade\n      MachineHoursCurrentMeter\n      UsageBand\n      saledate\n      fiModelDesc\n      fiBaseModel\n      fiSecondaryDesc\n      fiModelSeries\n      fiModelDescriptor\n      ProductSize\n      fiProductClassDesc\n      state\n      ProductGroup\n      ProductGroupDesc\n      Drive_System\n      Forks\n      Pad_Type\n      Ride_Control\n      Stick\n      Transmission\n      Turbocharged\n      Blade_Extension\n      Blade_Width\n      Enclosure_Type\n      Engine_Horsepower\n      Hydraulics\n      Pushblock\n      Ripper\n      Scarifier\n      Tip_Control\n      Tire_Size\n      Coupler\n      Coupler_System\n      Grouser_Tracks\n      Track_Type\n      Undercarriage_Pad_Width\n      Stick_Length\n      Thumb\n      Pattern_Changer\n      Grouser_Type\n      Backhoe_Mounting\n      Blade_Type\n      Travel_Controls\n      Differential_Type\n      Steering_Controls\n      YearMade_na\n      YearSold\n      YearsInUse\n      MachineHoursCurrentMeter_na\n      Tire_Size_na\n      Stick_Length_na\n      Undercarriage_Pad_Width_na\n      Blade_Width_na\n      fiProductClassSpec_lower\n      fiProductClassSpec_upper\n      fiProductClassSpec_units\n      fiProductClassSpec_lower_na\n      fiProductClassSpec_upper_na\n      Hydraulics_Flow_High Flow\n      Hydraulics_Flow_Standard\n      Hydraulics_Flow_nan\n      Enclosure_EROPS\n      Enclosure_EROPS AC\n      Enclosure_OROPS\n      Enclosure_nan\n    \n  \n  \n    \n      0\n      66000\n      3157\n      121\n      23\n      2004.0\n      68.0\n      2\n      2006-11-16\n      950\n      296\n      40\n      0\n      0\n      0\n      6\n      1\n      6\n      6\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      14.0\n      0\n      0\n      1\n      0\n      0\n      0\n      0\n      20.5\n      0\n      0\n      0\n      0\n      28.0\n      9.7\n      0\n      0\n      0\n      0\n      0\n      0\n      4\n      2\n      False\n      2006\n      2.0\n      False\n      True\n      True\n      True\n      True\n      110.0\n      120.0\n      2\n      False\n      False\n      0.0\n      0.0\n      1.0\n      0.0\n      1.0\n      0.0\n      0.0\n    \n    \n      1\n      57000\n      77\n      121\n      23\n      1996.0\n      4640.0\n      2\n      2004-03-26\n      1725\n      527\n      54\n      97\n      0\n      4\n      6\n      33\n      6\n      6\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      14.0\n      0\n      0\n      1\n      0\n      0\n      0\n      0\n      23.5\n      0\n      0\n      0\n      0\n      28.0\n      9.7\n      0\n      0\n      0\n      0\n      0\n      0\n      4\n      2\n      False\n      2004\n      8.0\n      False\n      False\n      True\n      True\n      True\n      150.0\n      175.0\n      2\n      False\n      False\n      0.0\n      0.0\n      1.0\n      0.0\n      1.0\n      0.0\n      0.0\n    \n    \n      2\n      10000\n      7009\n      121\n      23\n      2001.0\n      2838.0\n      1\n      2004-02-26\n      331\n      110\n      0\n      0\n      0\n      0\n      4\n      32\n      3\n      3\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      14.0\n      0\n      0\n      4\n      0\n      0\n      0\n      0\n      20.5\n      0\n      0\n      0\n      0\n      28.0\n      9.7\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      False\n      2004\n      3.0\n      False\n      True\n      True\n      True\n      True\n      1351.0\n      1601.0\n      3\n      False\n      False\n      0.0\n      1.0\n      0.0\n      0.0\n      0.0\n      1.0\n      0.0\n    \n    \n      3\n      38500\n      332\n      121\n      23\n      2001.0\n      3486.0\n      1\n      2011-05-19\n      3674\n      1375\n      0\n      44\n      0\n      6\n      2\n      44\n      4\n      4\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      14.0\n      0\n      0\n      1\n      0\n      0\n      0\n      0\n      20.5\n      0\n      0\n      0\n      0\n      28.0\n      9.7\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      False\n      2011\n      10.0\n      False\n      True\n      True\n      True\n      True\n      12.0\n      14.0\n      4\n      False\n      False\n      0.0\n      0.0\n      1.0\n      0.0\n      1.0\n      0.0\n      0.0\n    \n    \n      4\n      11000\n      17311\n      121\n      23\n      2007.0\n      722.0\n      3\n      2009-07-23\n      4208\n      1529\n      0\n      0\n      0\n      0\n      4\n      32\n      3\n      3\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      14.0\n      0\n      0\n      4\n      0\n      0\n      0\n      0\n      20.5\n      0\n      0\n      0\n      0\n      28.0\n      9.7\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      False\n      2009\n      2.0\n      False\n      True\n      True\n      True\n      True\n      1601.0\n      1751.0\n      3\n      False\n      False\n      0.0\n      1.0\n      0.0\n      1.0\n      0.0\n      0.0\n      0.0\n    \n  \n\n\n\n\nLet’s retain our model to check if there is any affect on model performance.\n\n(rf, feature_names, feature_importance, oob_enclosure) = train_and_plot_model(df, drop_features=['saledate'])\n\n[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 4 concurrent workers.\n[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:  2.0min\n[Parallel(n_jobs=-1)]: Done  70 out of  70 | elapsed:  3.6min finished\n\n\nOOB scrore =  0.902 \nTree leaves =  14,668,686 \nMedian depth = 45.0\n\n\n\n\n\nThere is a slight decrease in model performance but one new feature ‘Enclosure_EROPS AC’ is showing very high on the importance plot.\n\n\nsaledate\nWe have already created ‘yearsold’ feature. We can more consequent features from ‘saledate’.\n\ndf[\"salemonth\"] = df['saledate'].dt.month\ndf[\"saleday\"] = df['saledate'].dt.day\ndf[\"saledayofweek\"] = df['saledate'].dt.dayofweek\ndf[\"saledayofyear\"] = df['saledate'].dt.dayofyear\n\n# we can drop the orignal\ndel df['saledate']\n\n\ndf.head()\n\n\n\n\n\n  \n    \n      \n      SalePrice\n      ModelID\n      datasource\n      auctioneerID\n      YearMade\n      MachineHoursCurrentMeter\n      UsageBand\n      fiModelDesc\n      fiBaseModel\n      fiSecondaryDesc\n      fiModelSeries\n      fiModelDescriptor\n      ProductSize\n      fiProductClassDesc\n      state\n      ProductGroup\n      ProductGroupDesc\n      Drive_System\n      Forks\n      Pad_Type\n      Ride_Control\n      Stick\n      Transmission\n      Turbocharged\n      Blade_Extension\n      Blade_Width\n      Enclosure_Type\n      Engine_Horsepower\n      Hydraulics\n      Pushblock\n      Ripper\n      Scarifier\n      Tip_Control\n      Tire_Size\n      Coupler\n      Coupler_System\n      Grouser_Tracks\n      Track_Type\n      Undercarriage_Pad_Width\n      Stick_Length\n      Thumb\n      Pattern_Changer\n      Grouser_Type\n      Backhoe_Mounting\n      Blade_Type\n      Travel_Controls\n      Differential_Type\n      Steering_Controls\n      YearMade_na\n      YearSold\n      YearsInUse\n      MachineHoursCurrentMeter_na\n      Tire_Size_na\n      Stick_Length_na\n      Undercarriage_Pad_Width_na\n      Blade_Width_na\n      fiProductClassSpec_lower\n      fiProductClassSpec_upper\n      fiProductClassSpec_units\n      fiProductClassSpec_lower_na\n      fiProductClassSpec_upper_na\n      Hydraulics_Flow_High Flow\n      Hydraulics_Flow_Standard\n      Hydraulics_Flow_nan\n      Enclosure_EROPS\n      Enclosure_EROPS AC\n      Enclosure_OROPS\n      Enclosure_nan\n      salemonth\n      saleday\n      saledayofweek\n      saledayofyear\n    \n  \n  \n    \n      0\n      66000\n      3157\n      121\n      23\n      2004.0\n      68.0\n      2\n      950\n      296\n      40\n      0\n      0\n      0\n      6\n      1\n      6\n      6\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      14.0\n      0\n      0\n      1\n      0\n      0\n      0\n      0\n      20.5\n      0\n      0\n      0\n      0\n      28.0\n      9.7\n      0\n      0\n      0\n      0\n      0\n      0\n      4\n      2\n      False\n      2006\n      2.0\n      False\n      True\n      True\n      True\n      True\n      110.0\n      120.0\n      2\n      False\n      False\n      0.0\n      0.0\n      1.0\n      0.0\n      1.0\n      0.0\n      0.0\n      11\n      16\n      3\n      320\n    \n    \n      1\n      57000\n      77\n      121\n      23\n      1996.0\n      4640.0\n      2\n      1725\n      527\n      54\n      97\n      0\n      4\n      6\n      33\n      6\n      6\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      14.0\n      0\n      0\n      1\n      0\n      0\n      0\n      0\n      23.5\n      0\n      0\n      0\n      0\n      28.0\n      9.7\n      0\n      0\n      0\n      0\n      0\n      0\n      4\n      2\n      False\n      2004\n      8.0\n      False\n      False\n      True\n      True\n      True\n      150.0\n      175.0\n      2\n      False\n      False\n      0.0\n      0.0\n      1.0\n      0.0\n      1.0\n      0.0\n      0.0\n      3\n      26\n      4\n      86\n    \n    \n      2\n      10000\n      7009\n      121\n      23\n      2001.0\n      2838.0\n      1\n      331\n      110\n      0\n      0\n      0\n      0\n      4\n      32\n      3\n      3\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      14.0\n      0\n      0\n      4\n      0\n      0\n      0\n      0\n      20.5\n      0\n      0\n      0\n      0\n      28.0\n      9.7\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      False\n      2004\n      3.0\n      False\n      True\n      True\n      True\n      True\n      1351.0\n      1601.0\n      3\n      False\n      False\n      0.0\n      1.0\n      0.0\n      0.0\n      0.0\n      1.0\n      0.0\n      2\n      26\n      3\n      57\n    \n    \n      3\n      38500\n      332\n      121\n      23\n      2001.0\n      3486.0\n      1\n      3674\n      1375\n      0\n      44\n      0\n      6\n      2\n      44\n      4\n      4\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      14.0\n      0\n      0\n      1\n      0\n      0\n      0\n      0\n      20.5\n      0\n      0\n      0\n      0\n      28.0\n      9.7\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      False\n      2011\n      10.0\n      False\n      True\n      True\n      True\n      True\n      12.0\n      14.0\n      4\n      False\n      False\n      0.0\n      0.0\n      1.0\n      0.0\n      1.0\n      0.0\n      0.0\n      5\n      19\n      3\n      139\n    \n    \n      4\n      11000\n      17311\n      121\n      23\n      2007.0\n      722.0\n      3\n      4208\n      1529\n      0\n      0\n      0\n      0\n      4\n      32\n      3\n      3\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      14.0\n      0\n      0\n      4\n      0\n      0\n      0\n      0\n      20.5\n      0\n      0\n      0\n      0\n      28.0\n      9.7\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      False\n      2009\n      2.0\n      False\n      True\n      True\n      True\n      True\n      1601.0\n      1751.0\n      3\n      False\n      False\n      0.0\n      1.0\n      0.0\n      1.0\n      0.0\n      0.0\n      0.0\n      7\n      23\n      3\n      204\n    \n  \n\n\n\n\n\n(rf, feature_names, feature_importance, oob_date) = train_and_plot_model(df)\n\n[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 4 concurrent workers.\n[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:  2.4min\n[Parallel(n_jobs=-1)]: Done  70 out of  70 | elapsed:  3.9min finished\n\n\nOOB scrore =  0.907 \nTree leaves =  14,493,456 \nMedian depth = 45.0\n\n\n\n\n\nThere is an increase in model performance and multiple newly created date features are showing good importance on the plot."
  },
  {
    "objectID": "posts/2022-05-14-aws-ml-cert-notes.html",
    "href": "posts/2022-05-14-aws-ml-cert-notes.html",
    "title": "AWS Machine Learning Certification Notes (MLS-C01)",
    "section": "",
    "text": "AWS ML Speciality Badge"
  },
  {
    "objectID": "posts/2022-05-14-aws-ml-cert-notes.html#amazon-comprehend",
    "href": "posts/2022-05-14-aws-ml-cert-notes.html#amazon-comprehend",
    "title": "AWS Machine Learning Certification Notes (MLS-C01)",
    "section": "Amazon Comprehend",
    "text": "Amazon Comprehend\nAmazon Comprehend is a natural-language processing (NLP) service that uses machine learning to uncover valuable insights and connections in text.\nReferences\nhttps://aws.amazon.com/comprehend/"
  },
  {
    "objectID": "posts/2022-05-14-aws-ml-cert-notes.html#amazon-rekognition",
    "href": "posts/2022-05-14-aws-ml-cert-notes.html#amazon-rekognition",
    "title": "AWS Machine Learning Certification Notes (MLS-C01)",
    "section": "Amazon Rekognition",
    "text": "Amazon Rekognition\nAmazon Rekognition offers pre-trained and customizable computer vision (CV) capabilities to extract information and insights from your images and videos.\nReferences\nhttps://aws.amazon.com/rekognition/"
  },
  {
    "objectID": "posts/2022-05-14-aws-ml-cert-notes.html#amazon-polly",
    "href": "posts/2022-05-14-aws-ml-cert-notes.html#amazon-polly",
    "title": "AWS Machine Learning Certification Notes (MLS-C01)",
    "section": "Amazon Polly",
    "text": "Amazon Polly\nAmazon Polly is a service that turns text into lifelike speech, allowing you to create applications that talk, and build entirely new categories of speech-enabled products.\nReferences\nhttps://aws.amazon.com/polly/"
  },
  {
    "objectID": "posts/2022-05-14-aws-ml-cert-notes.html#amazon-lex",
    "href": "posts/2022-05-14-aws-ml-cert-notes.html#amazon-lex",
    "title": "AWS Machine Learning Certification Notes (MLS-C01)",
    "section": "Amazon Lex",
    "text": "Amazon Lex\nAmazon Lex is a fully managed artificial intelligence (AI) service with advanced natural language models to design, build, test, and deploy conversational interfaces in applications (chat bots).\nReferences\nhttps://aws.amazon.com/lex/"
  },
  {
    "objectID": "posts/2022-05-14-aws-ml-cert-notes.html#amazon-transcribe",
    "href": "posts/2022-05-14-aws-ml-cert-notes.html#amazon-transcribe",
    "title": "AWS Machine Learning Certification Notes (MLS-C01)",
    "section": "Amazon Transcribe",
    "text": "Amazon Transcribe\nAmazon Transcribe is an automatic speech recognition service that makes it easy to add speech to text capabilities to any application. Transcribe’s features enable you to ingest audio input, produce easy to read and review transcripts, improve accuracy with customization, and filter content to ensure customer privacy.\nReferences\nhttps://aws.amazon.com/transcribe/"
  },
  {
    "objectID": "posts/2022-05-14-aws-ml-cert-notes.html#latent-dirichlet-allocation-lda-algorithm",
    "href": "posts/2022-05-14-aws-ml-cert-notes.html#latent-dirichlet-allocation-lda-algorithm",
    "title": "AWS Machine Learning Certification Notes (MLS-C01)",
    "section": "Latent Dirichlet Allocation (LDA) Algorithm",
    "text": "Latent Dirichlet Allocation (LDA) Algorithm\n\nIt is a topic modeling technique to generate abstract topics based on word frequency from a set of documents\nIt is similar to unsupervised classification of documents\nIt is useful for automatically organizing, summerizing, understanding and searching large electronic archives. It can help in\n\ndiscovering hidden themes in the collection\nclassifying document into dicoverable themes\norganize/summerize/search the documents\n\n\nReferences\nhttps://towardsdatascience.com/latent-dirichlet-allocation-lda-9d1cd064ffa2"
  },
  {
    "objectID": "posts/2022-05-14-aws-ml-cert-notes.html#multinomial-logistic-regression-algorithm",
    "href": "posts/2022-05-14-aws-ml-cert-notes.html#multinomial-logistic-regression-algorithm",
    "title": "AWS Machine Learning Certification Notes (MLS-C01)",
    "section": "Multinomial Logistic Regression Algorithm",
    "text": "Multinomial Logistic Regression Algorithm\nMultinomial Logistic Regression is an extension of logistic regression (supervised) that allows more than two discrete outcomes (multiclass).\nReferences\nhttps://en.wikipedia.org/wiki/Multinomial_logistic_regression"
  },
  {
    "objectID": "posts/2022-05-14-aws-ml-cert-notes.html#factorization-machines-algorithm",
    "href": "posts/2022-05-14-aws-ml-cert-notes.html#factorization-machines-algorithm",
    "title": "AWS Machine Learning Certification Notes (MLS-C01)",
    "section": "Factorization Machines Algorithm",
    "text": "Factorization Machines Algorithm\nThe Factorization Machines algorithm is a general-purpose supervised learning algorithm that you can use for both classification and regression tasks. It is an extension of a linear model that is designed to capture interactions between features within high dimensional sparse datasets economically. For example, in a click prediction system, the Factorization Machines model can capture click rate patterns observed when ads from a certain ad-category are placed on pages from a certain page-category. Factorization machines are a good choice for tasks dealing with high dimensional sparse datasets, such as click prediction and item recommendation.\nReferences\nhttps://docs.aws.amazon.com/sagemaker/latest/dg/fact-machines.html"
  },
  {
    "objectID": "posts/2022-05-14-aws-ml-cert-notes.html#sequence-to-sequence-seq2seq-algorithm",
    "href": "posts/2022-05-14-aws-ml-cert-notes.html#sequence-to-sequence-seq2seq-algorithm",
    "title": "AWS Machine Learning Certification Notes (MLS-C01)",
    "section": "Sequence to Sequence (seq2seq) Algorithm",
    "text": "Sequence to Sequence (seq2seq) Algorithm\nAmazon SageMaker Sequence to Sequence is a supervised learning algorithm where the input is a sequence of tokens (for example, text, audio) and the output generated is another sequence of tokens. Example applications include * machine translation (input a sentence from one language and predict what that sentence would be in another language) * text summarization (input a longer string of words and predict a shorter string of words that is a summary) * speech-to-text (audio clips converted into output sentences in tokens)\nProblems in this domain have been successfully modeled with deep neural networks that show a significant performance boost over previous methodologies. Amazon SageMaker seq2seq uses Recurrent Neural Networks (RNNs) and Convolutional Neural Network (CNN) models with attention as encoder-decoder architectures.\nReferences\nhttps://docs.aws.amazon.com/sagemaker/latest/dg/seq-2-seq.html"
  },
  {
    "objectID": "posts/2022-05-14-aws-ml-cert-notes.html#term-frequency-inverse-document-frequency-algorithm",
    "href": "posts/2022-05-14-aws-ml-cert-notes.html#term-frequency-inverse-document-frequency-algorithm",
    "title": "AWS Machine Learning Certification Notes (MLS-C01)",
    "section": "Term frequency-inverse document frequency Algorithm",
    "text": "Term frequency-inverse document frequency Algorithm\nTF-IDF (term frequency-inverse document frequency) is a statistical measure that evaluates how relevant a word is to a document in a collection of documents.\nThis is done by multiplying two metrics: how many times a word appears in a document (frequency), and the inverse document frequency of the word across a set of documents.\n\nThe term frequency of a word in a document. There are several ways of calculating this frequency, with the simplest being a raw count of instances a word appears in a document.\nThe inverse document frequency of the word across a set of documents. This means, how common or rare a word is in the entire document set. The closer it is to 0, the more common a word is. This metric can be calculated by taking the total number of documents, dividing it by the number of documents that contain a word, and calculating the logarithm. So, if the word is very common and appears in many documents, this number will approach 0. Otherwise, it will approach 1\n\nIt has many uses, most importantly in automated text analysis, and is very useful for scoring words in machine learning algorithms for Natural Language Processing (NLP).\nReferences\nhttps://monkeylearn.com/blog/what-is-tf-idf"
  },
  {
    "objectID": "posts/2022-05-14-aws-ml-cert-notes.html#blazingtext-algorithm",
    "href": "posts/2022-05-14-aws-ml-cert-notes.html#blazingtext-algorithm",
    "title": "AWS Machine Learning Certification Notes (MLS-C01)",
    "section": "BlazingText Algorithm",
    "text": "BlazingText Algorithm\nThe Amazon SageMaker BlazingText algorithm provides highly optimized implementations of the Word2vec and text classification algorithms. The Word2vec algorithm is useful for many downstream natural language processing (NLP) tasks, such as sentiment analysis, named entity recognition, machine translation, etc. Text classification is an important task for applications that perform web searches, information retrieval, ranking, and document classification.\nThe Word2vec algorithm maps words to high-quality distributed vectors. The resulting vector representation of a word is called a word embedding. Words that are semantically similar correspond to vectors that are close together. That way, word embeddings capture the semantic relationships between words.\nReferences\nhttps://docs.aws.amazon.com/sagemaker/latest/dg/blazingtext.html"
  },
  {
    "objectID": "posts/2022-05-14-aws-ml-cert-notes.html#amazon-sagemaker-batch-transform",
    "href": "posts/2022-05-14-aws-ml-cert-notes.html#amazon-sagemaker-batch-transform",
    "title": "AWS Machine Learning Certification Notes (MLS-C01)",
    "section": "Amazon SageMaker Batch Transform",
    "text": "Amazon SageMaker Batch Transform\nUse batch transform when you need to do the following:\n\nPreprocess datasets to remove noise or bias that interferes with training or inference from your dataset.\nGet inferences from large datasets.\nRun inference when you don’t need a persistent endpoint.\nAssociate input records with inferences to assist the interpretation of results.\n\nReferences\nhttps://docs.aws.amazon.com/sagemaker/latest/dg/batch-transform.html"
  },
  {
    "objectID": "posts/2022-05-14-aws-ml-cert-notes.html#amazon-sagemaker-real-time-inference-hosting-services",
    "href": "posts/2022-05-14-aws-ml-cert-notes.html#amazon-sagemaker-real-time-inference-hosting-services",
    "title": "AWS Machine Learning Certification Notes (MLS-C01)",
    "section": "Amazon SageMaker Real-time inference / Hosting Services",
    "text": "Amazon SageMaker Real-time inference / Hosting Services\nReal-time inference is ideal for inference workloads where you have real-time, interactive, low latency requirements. You can deploy your model to SageMaker hosting services and get an endpoint that can be used for inference. These endpoints are fully managed and support autoscaling.\nReferences\nhttps://docs.aws.amazon.com/sagemaker/latest/dg/realtime-endpoints.html"
  },
  {
    "objectID": "posts/2022-05-14-aws-ml-cert-notes.html#amazon-sagemaker-inference-pipeline",
    "href": "posts/2022-05-14-aws-ml-cert-notes.html#amazon-sagemaker-inference-pipeline",
    "title": "AWS Machine Learning Certification Notes (MLS-C01)",
    "section": "Amazon SageMaker Inference Pipeline",
    "text": "Amazon SageMaker Inference Pipeline\nAn inference pipeline is a Amazon SageMaker model that is composed of a linear sequence of two to fifteen containers that process requests for inferences on data. You use an inference pipeline to define and deploy any combination of pretrained SageMaker built-in algorithms and your own custom algorithms packaged in Docker containers. You can use an inference pipeline to combine preprocessing, predictions, and post-processing data science tasks. Inference pipelines are fully managed.\nWithin an inference pipeline model, SageMaker handles invocations as a sequence of HTTP requests. The first container in the pipeline handles the initial request, then the intermediate response is sent as a request to the second container, and so on, for each container in the pipeline. SageMaker returns the final response to the client.\nWhen you deploy the pipeline model, SageMaker installs and runs all of the containers on each Amazon Elastic Compute Cloud (Amazon EC2) instance in the endpoint or transform job. Feature processing and inferences run with low latency because the containers are co-located on the same EC2 instances.\nReferences\nhttps://docs.aws.amazon.com/sagemaker/latest/dg/inference-pipelines.html"
  },
  {
    "objectID": "posts/2022-05-14-aws-ml-cert-notes.html#amazon-sagemaker-neo",
    "href": "posts/2022-05-14-aws-ml-cert-notes.html#amazon-sagemaker-neo",
    "title": "AWS Machine Learning Certification Notes (MLS-C01)",
    "section": "Amazon SageMaker Neo",
    "text": "Amazon SageMaker Neo\nAmazon SageMaker Neo automatically optimizes machine learning models for inference on cloud instances and edge devices to run faster with no loss in accuracy. You start with a machine learning model already built with DarkNet, Keras, MXNet, PyTorch, TensorFlow, TensorFlow-Lite, ONNX, or XGBoost and trained in Amazon SageMaker or anywhere else. Then you choose your target hardware platform, which can be a SageMaker hosting instance or an edge device based on processors from Ambarella, Apple, ARM, Intel, MediaTek, Nvidia, NXP, Qualcomm, RockChip, Texas Instruments, or Xilinx. With a single click, SageMaker Neo optimizes the trained model and compiles it into an executable. The compiler uses a machine learning model to apply the performance optimizations that extract the best available performance for your model on the cloud instance or edge device. You then deploy the model as a SageMaker endpoint or on supported edge devices and start making predictions.\nReferences\nhttps://aws.amazon.com/sagemaker/neo/"
  },
  {
    "objectID": "posts/2022-05-14-aws-ml-cert-notes.html#lstm-long-short-term-memory",
    "href": "posts/2022-05-14-aws-ml-cert-notes.html#lstm-long-short-term-memory",
    "title": "AWS Machine Learning Certification Notes (MLS-C01)",
    "section": "LSTM / Long Short-Term Memory",
    "text": "LSTM / Long Short-Term Memory\nLSTM is an artificial recurrent neural network (RNN) architecture used in the field of deep learning. Unlike standard feedforward neural networks, LSTM has feedback connections. It can process not only single data points (such as images), but also entire sequences of data (such as speech or video).\nLSTM networks are well-suited to classifying, processing and making predictions based on time series data, since there can be lags of unknown duration between important events in a time series. LSTM is applicable to tasks such as anomaly detection in network traffic or IDSs (intrusion detection systems)\nReferences\nhttps://en.wikipedia.org/wiki/Long_short-term_memory"
  },
  {
    "objectID": "posts/2022-05-14-aws-ml-cert-notes.html#semantic-segmentation-algorithm",
    "href": "posts/2022-05-14-aws-ml-cert-notes.html#semantic-segmentation-algorithm",
    "title": "AWS Machine Learning Certification Notes (MLS-C01)",
    "section": "Semantic Segmentation Algorithm",
    "text": "Semantic Segmentation Algorithm\nThe SageMaker semantic segmentation algorithm provides a fine-grained, pixel-level approach to developing computer vision applications. It tags every pixel in an image with a class label from a predefined set of classes. Tagging is fundamental for understanding scenes, which is critical to an increasing number of computer vision applications, such as self-driving vehicles, medical imaging diagnostics, and robot sensing.\nFor comparison, the SageMaker Image Classification Algorithm is a supervised learning algorithm that analyzes only whole images, classifying them into one of multiple output categories. The Object Detection Algorithm is a supervised learning algorithm that detects and classifies all instances of an object in an image. It indicates the location and scale of each object in the image with a rectangular bounding box.\nBecause the semantic segmentation algorithm classifies every pixel in an image, it also provides information about the shapes of the objects contained in the image. The segmentation output is represented as a grayscale image, called a segmentation mask. A segmentation mask is a grayscale image with the same shape as the input image.\nThe SageMaker semantic segmentation algorithm is built using the MXNet Gluon framework and the Gluon CV toolkit.\nReferences * https://docs.aws.amazon.com/sagemaker/latest/dg/semantic-segmentation.html"
  },
  {
    "objectID": "posts/2022-05-14-aws-ml-cert-notes.html#accuracy",
    "href": "posts/2022-05-14-aws-ml-cert-notes.html#accuracy",
    "title": "AWS Machine Learning Certification Notes (MLS-C01)",
    "section": "Accuracy",
    "text": "Accuracy\nAccuracy measures the fraction of correct predictions. The range is 0 to 1.\nAccuracy = (TP + TN) / (TP + FP + TN + FN)"
  },
  {
    "objectID": "posts/2022-05-14-aws-ml-cert-notes.html#precision",
    "href": "posts/2022-05-14-aws-ml-cert-notes.html#precision",
    "title": "AWS Machine Learning Certification Notes (MLS-C01)",
    "section": "Precision",
    "text": "Precision\nPrecision measures the fraction of actual positives among those examples that are predicted as positive. The range is 0 to 1. This formula tells us that the larger value of FP (False Positives), the lower the Precision.\nPrecision = TP / (TP + FP)\nFor maximun precision there should be no FP. FP are also called Type 1 error."
  },
  {
    "objectID": "posts/2022-05-14-aws-ml-cert-notes.html#recall",
    "href": "posts/2022-05-14-aws-ml-cert-notes.html#recall",
    "title": "AWS Machine Learning Certification Notes (MLS-C01)",
    "section": "Recall",
    "text": "Recall\nThe Recall measures the fraction of actual positives that are predicted as positive. The range is 0 to 1. This formula tells us that the larger value of FN (False Negatives), the lower the Recall.\nRecall = TP / (TP + FN)\nFor maximun recall there should be no FN. FN are also called Type 2 error.\nNote: Precision and Recall are inversely proportional to eachother.\nReferences\nhttps://towardsdatascience.com/model-evaluation-i-precision-and-recall-166ddb257c7b"
  },
  {
    "objectID": "posts/2022-05-14-aws-ml-cert-notes.html#l1-regularization",
    "href": "posts/2022-05-14-aws-ml-cert-notes.html#l1-regularization",
    "title": "AWS Machine Learning Certification Notes (MLS-C01)",
    "section": "L1 regularization",
    "text": "L1 regularization\nL1 regularization, also known as L1 norm or Lasso (in regression problems), combats overfitting by shrinking the parameters towards 0. This makes some features obsolete.\nIt’s a form of feature selection, because when we assign a feature with a 0 weight, we’re multiplying the feature values by 0 which returns 0, eradicating the significance of that feature. If the input features of our model have weights closer to 0, our L1 norm would be sparse. A selection of the input features would have weights equal to zero, and the rest would be non-zero."
  },
  {
    "objectID": "posts/2022-05-14-aws-ml-cert-notes.html#l2-regularization",
    "href": "posts/2022-05-14-aws-ml-cert-notes.html#l2-regularization",
    "title": "AWS Machine Learning Certification Notes (MLS-C01)",
    "section": "L2 regularization",
    "text": "L2 regularization\nL2 regularization, or the L2 norm, or Ridge (in regression problems), combats overfitting by forcing weights to be small, but not making them exactly 0. This regularization returns a non-sparse solution since the weights will be non-zero (although some may be close to 0). A major snag to consider when using L2 regularization is that it’s not robust to outliers.\nReferences\nhttps://neptune.ai/blog/fighting-overfitting-with-l1-or-l2-regularization"
  },
  {
    "objectID": "posts/2022-05-14-aws-ml-cert-notes.html#k-means",
    "href": "posts/2022-05-14-aws-ml-cert-notes.html#k-means",
    "title": "AWS Machine Learning Certification Notes (MLS-C01)",
    "section": "K-means",
    "text": "K-means\nK-means is a clustering algorithm that tries to partition a set of points into K sets (clusters) such that the points in each cluster tend to be near each other. It is unsupervised because the points have no external classification."
  },
  {
    "objectID": "posts/2022-05-14-aws-ml-cert-notes.html#k-nearest-neighbors",
    "href": "posts/2022-05-14-aws-ml-cert-notes.html#k-nearest-neighbors",
    "title": "AWS Machine Learning Certification Notes (MLS-C01)",
    "section": "K-nearest neighbors",
    "text": "K-nearest neighbors\nK-nearest neighbors is a classification (or regression) algorithm that in order to determine the classification of a point, combines the classification of the K nearest points. It is supervised because you are trying to classify a point based on the known classification of other points."
  },
  {
    "objectID": "posts/2022-05-14-aws-ml-cert-notes.html#exam-readiness-aws-certified-machine-learning---specialty",
    "href": "posts/2022-05-14-aws-ml-cert-notes.html#exam-readiness-aws-certified-machine-learning---specialty",
    "title": "AWS Machine Learning Certification Notes (MLS-C01)",
    "section": "Exam Readiness: AWS Certified Machine Learning - Specialty",
    "text": "Exam Readiness: AWS Certified Machine Learning - Specialty\nhttps://explore.skillbuilder.aws/learn/course/27/play/54/exam-readiness-aws-certified-machine-learning-specialty\nThis is overall a very good short course that can help you identify your strengths and weaknesses in each exam domain so you know where to focus when studying for the exam."
  },
  {
    "objectID": "posts/2022-05-14-aws-ml-cert-notes.html#acloudguru-aws-certified-machine-learning---specialty-2020",
    "href": "posts/2022-05-14-aws-ml-cert-notes.html#acloudguru-aws-certified-machine-learning---specialty-2020",
    "title": "AWS Machine Learning Certification Notes (MLS-C01)",
    "section": "ACloudGuru AWS Certified Machine Learning - Specialty 2020",
    "text": "ACloudGuru AWS Certified Machine Learning - Specialty 2020\nhttps://acloudguru.com/course/aws-certified-machine-learning-specialty\nThis is a detailed course on the topics covered in the exam. But this course lacks on “Modeling” domain and hands-on labs. Besides taking this course you should have a good knowledge and working experience in data science and machine learning domain. I already have AI/ML background so it was not an issue for me. Some people have recommended taking Machine Learning, Data Science and Deep Learning with Python from Frank Kane on Udemy if you don’t have an ML background but I am not sure about it’s worth."
  },
  {
    "objectID": "posts/2022-05-17-aws-sagemaker-wrangler-p1.html#data",
    "href": "posts/2022-05-17-aws-sagemaker-wrangler-p1.html#data",
    "title": "Data Preparation with SageMaker Data Wrangler (Part 1)",
    "section": "Data",
    "text": "Data\nMobile operators have historical records on which customers ultimately ended up churning and which continued using the service. We can use this historical information to construct an ML model of one mobile operator’s churn using a process called training. After training the model, we can pass the profile information of an arbitrary customer (the same profile information that we used to train the model) to the model, and have the model predict whether this customer is going to churn. Of course, we expect the model to make mistakes. After all, predicting the future is a tricky business! But we’ll learn how to deal with prediction errors.\nThe dataset we use is publicly available and was mentioned in the book Discovering Knowledge in Data by Daniel T. Larose. It is attributed by the author to the University of California Irvine Repository of Machine Learning Datasets (Jafari-Marandi, R., Denton, J., Idris, A., Smith, B. K., & Keramati, A. (2020)."
  },
  {
    "objectID": "posts/2022-05-17-aws-sagemaker-wrangler-p1.html#preparation",
    "href": "posts/2022-05-17-aws-sagemaker-wrangler-p1.html#preparation",
    "title": "Data Preparation with SageMaker Data Wrangler (Part 1)",
    "section": "Preparation",
    "text": "Preparation\n\n##\n# install aws data wrangler package\n# restart kernel after installation\n# more on this package later in the notebook.\n!pip install -q awswrangler\n\nWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\nWARNING: You are using pip version 22.0.4; however, version 22.1 is available.\nYou should consider upgrading via the '/opt/conda/bin/python -m pip install --upgrade pip' command.\n\n\n\n## import required libraries\nimport pandas as pd\nimport sagemaker\n\nsess = sagemaker.Session()\nprefix = 'myblog/demo-customer-churn'\n\n\n!aws s3 cp s3://sagemaker-sample-files/datasets/tabular/synthetic/churn.txt ./\n\ndownload: s3://sagemaker-sample-files/datasets/tabular/synthetic/churn.txt to ./churn.txt\n\n\n\ndf = pd.read_csv(\"./churn.txt\")\n\n# make 'CustomerID' column from the index\ndf['CustomerID']=df.index\n\npd.set_option(\"display.max_columns\", 500)\ndf.head(10)\n\n\n\n\n\n  \n    \n      \n      State\n      Account Length\n      Area Code\n      Phone\n      Int'l Plan\n      VMail Plan\n      VMail Message\n      Day Mins\n      Day Calls\n      Day Charge\n      Eve Mins\n      Eve Calls\n      Eve Charge\n      Night Mins\n      Night Calls\n      Night Charge\n      Intl Mins\n      Intl Calls\n      Intl Charge\n      CustServ Calls\n      Churn?\n      CustomerID\n    \n  \n  \n    \n      0\n      PA\n      163\n      806\n      403-2562\n      no\n      yes\n      300\n      8.162204\n      3\n      7.579174\n      3.933035\n      4\n      6.508639\n      4.065759\n      100\n      5.111624\n      4.928160\n      6\n      5.673203\n      3\n      True.\n      0\n    \n    \n      1\n      SC\n      15\n      836\n      158-8416\n      yes\n      no\n      0\n      10.018993\n      4\n      4.226289\n      2.325005\n      0\n      9.972592\n      7.141040\n      200\n      6.436188\n      3.221748\n      6\n      2.559749\n      8\n      False.\n      1\n    \n    \n      2\n      MO\n      131\n      777\n      896-6253\n      no\n      yes\n      300\n      4.708490\n      3\n      4.768160\n      4.537466\n      3\n      4.566715\n      5.363235\n      100\n      5.142451\n      7.139023\n      2\n      6.254157\n      4\n      False.\n      2\n    \n    \n      3\n      WY\n      75\n      878\n      817-5729\n      yes\n      yes\n      700\n      1.268734\n      3\n      2.567642\n      2.528748\n      5\n      2.333624\n      3.773586\n      450\n      3.814413\n      2.245779\n      6\n      1.080692\n      6\n      False.\n      3\n    \n    \n      4\n      WY\n      146\n      878\n      450-4942\n      yes\n      no\n      0\n      2.696177\n      3\n      5.908916\n      6.015337\n      3\n      3.670408\n      3.751673\n      250\n      2.796812\n      6.905545\n      4\n      7.134343\n      6\n      True.\n      4\n    \n    \n      5\n      VA\n      83\n      866\n      454-9110\n      no\n      no\n      0\n      3.634776\n      7\n      4.804892\n      6.051944\n      5\n      5.278437\n      2.937880\n      300\n      4.817958\n      4.948816\n      4\n      5.135323\n      5\n      False.\n      5\n    \n    \n      6\n      IN\n      140\n      737\n      331-5751\n      yes\n      no\n      0\n      3.229420\n      4\n      3.165082\n      2.440153\n      8\n      0.264543\n      2.352274\n      300\n      3.869176\n      5.393439\n      4\n      1.784765\n      4\n      False.\n      6\n    \n    \n      7\n      LA\n      54\n      766\n      871-3612\n      no\n      no\n      0\n      0.567920\n      6\n      1.950098\n      4.507027\n      0\n      4.473086\n      0.688785\n      400\n      6.132137\n      5.012747\n      5\n      0.417421\n      8\n      False.\n      7\n    \n    \n      8\n      MO\n      195\n      777\n      249-5723\n      yes\n      no\n      0\n      5.811116\n      6\n      4.331065\n      8.104126\n      2\n      4.475034\n      4.208352\n      250\n      5.974575\n      4.750153\n      7\n      3.320311\n      7\n      True.\n      8\n    \n    \n      9\n      AL\n      104\n      657\n      767-7682\n      yes\n      no\n      0\n      2.714430\n      7\n      5.138669\n      8.529944\n      6\n      3.321121\n      2.342177\n      300\n      4.328966\n      3.433554\n      5\n      5.677058\n      4\n      False.\n      9\n    \n  \n\n\n\n\n\ndf.shape\n\n(5000, 22)\n\n\nBy modern standards, it’s a relatively small dataset, with only 5,000 records, where each record uses 21 attributes to describe the profile of a customer of an unknown US mobile operator. The attributes are:\n\nState: the US state in which the customer resides, indicated by a two-letter abbreviation; for example, OH or NJ\nAccount Length: the number of days that this account has been active\nArea Code: the three-digit area code of the corresponding customer’s phone number\nPhone: the remaining seven-digit phone number\nInt’l Plan: whether the customer has an international calling plan: yes/no\nVMail Plan: whether the customer has a voice mail feature: yes/no\nVMail Message: the average number of voice mail messages per month\nDay Mins: the total number of calling minutes used during the day\nDay Calls: the total number of calls placed during the day\nDay Charge: the billed cost of daytime calls\nEve Mins, Eve Calls, Eve Charge: the billed cost for calls placed during the evening\nNight Mins, Night Calls, Night Charge: the billed cost for calls placed during nighttime\nIntl Mins, Intl Calls, Intl Charge: the billed cost for international calls\nCustServ Calls: the number of calls placed to Customer Service\nChurn?: whether the customer left the service: true/false\n\nThe last attribute, Churn?, is known as the target attribute: the attribute that we want the ML model to predict. Because the target attribute is binary, our model will be performing binary prediction, also known as binary classification.\nWe have our dataset. Now we will split this dataset into three subsets * customer: customer data, and place it as a CSV file on the S3 bucket * account: accounts data, and place it as CSV on the same S3 bucket * utility: utility data, and place it as Glue tables\n\ncustomer_columns = ['CustomerID', 'State', 'Area Code', 'Phone']\naccount_columns = ['CustomerID', 'Account Length', \"Int'l Plan\", 'VMail Plan', 'Churn?']\nutility_columns = ['CustomerID', 'VMail Message', 'Day Mins', 'Day Calls', 'Day Charge', \n                   'Eve Mins', 'Eve Calls', 'Eve Charge', 'Night Mins', 'Night Calls', \n                   'Night Charge', 'Intl Mins', 'Intl Calls', 'Intl Charge', 'CustServ Calls']\n\nWe will use the default bucket associated with our SageMaker session. You may use any other bucket with proper access permissions.\n\nbucket = sess.default_bucket()\nbucket\n\n'sagemaker-us-east-1-801598032724'\n\n\nNext, we will use AWS Data Wrangler Python package (awswrangler) to create an AWS Glue database.\nawswrangler is an open source Python library maintained by AWS team, as is defined as\n\nAn AWS Professional Service open source python initiative that extends the power of Pandas library to AWS connecting DataFrames and AWS data related services. Easy integration with Athena, Glue, Redshift, Timestream, OpenSearch, Neptune, QuickSight, Chime, CloudWatchLogs, DynamoDB, EMR, SecretManager, PostgreSQL, MySQL, SQLServer and S3 (Parquet, CSV, JSON and EXCEL).\n\nYou may read more about this library here * Documentation: https://aws-data-wrangler.readthedocs.io/en/stable/what.html * Github repo: https://github.com/awslabs/aws-data-wrangler\nPlease note that AWS SageMaker session needs some additional AWS Glue permissions to create a database. If you get an error while creating a Glue database in following steps then add those permissions.\nError: AccessDeniedException: An error occurred (AccessDeniedException) when calling the GetDatabase operation: User: arn:aws:sts::801598032724:assumed-role/AmazonSageMaker-ExecutionRole-20220516T161743/SageMaker is not authorized to perform: glue:GetDatabase on resource: arn:aws:glue:us-east-1:801598032724:database/telco_db because no identity-based policy allows the glue:GetDatabase action\nFix: Go to your SageMaker Execution Role and add permission AWSGlueConsoleFullAccess\n\n##\n# define the Glue DB name\ndb_name = 'telco_db'\n\n\n##\nimport awswrangler as wr\n\n# get all the existing Glue db list\ndatabases = wr.catalog.databases()\n\n# print existing db names\nprint(\"*** existing databases ***\\n\")\nprint(databases)\n\n# if our db does not exist then create it\nif db_name not in databases.values:\n    wr.catalog.create_database(db_name, description = 'Demo DB for telco churn dataset')\n    print(\"\\n*** existing + new databases ***\\n\")\n    print(wr.catalog.databases())\nelse:\n    print(f\"Database {db_name} already exists\")\n\n*** existing databases ***\n\n                  Database Description\n0  sagemaker_data_wrangler            \n1     sagemaker_processing            \n\n*** existing + new databases ***\n\n                  Database                      Description\n0  sagemaker_data_wrangler                                 \n1     sagemaker_processing                                 \n2                 telco_db  Demo DB for telco churn dataset\n\n\n\n##\n# in case you want to delete a database using this notebook\n# wr.catalog.delete_database(db_name)\n\nSimilarly you can go to AWS Glue console to see that the new database has been created.\n\n\n\naws-glue-database\n\n\nNow we will place the three data subsets into their respective locations.\n\nsuffix = ['customer_info', 'account_info', 'utility']\nfor i, columns in enumerate([customer_columns, account_columns, utility_columns]):\n    \n    # get the data subset\n    df_tmp = df[columns]\n\n    # prepare filename and output path\n    fname = 'telco_churn_%s' % suffix[i]\n    outputpath = f's3://{bucket}/{prefix}/data/{fname}'\n    \n    print(f\"\\n*** working on {suffix[i]}***\")\n    print(f\"filename: {fname}\")\n    print(f\"output path: {outputpath}\")\n    \n    if i > 1: # for utility\n        wr.s3.to_csv(\n            df=df_tmp,\n            path=outputpath,\n            dataset=True,\n            database=db_name,  # Athena/Glue database\n            table=fname,  # Athena/Glue table\n            index=False,\n            mode='overwrite')\n    else: # for customer and account\n        wr.s3.to_csv(\n            df=df_tmp,\n            path=f'{outputpath}.csv',\n            index=False)\n\n\n*** working on customer_info***\nfilename: telco_churn_customer_info\noutput path: s3://sagemaker-us-east-1-801598032724/myblog/demo-customer-churn/data/telco_churn_customer_info\n\n*** working on account_info***\nfilename: telco_churn_account_info\noutput path: s3://sagemaker-us-east-1-801598032724/myblog/demo-customer-churn/data/telco_churn_account_info\n\n*** working on utility***\nfilename: telco_churn_utility\noutput path: s3://sagemaker-us-east-1-801598032724/myblog/demo-customer-churn/data/telco_churn_utility\n\n\nWe can verify the uploaded data from the S3 bucket.\n\n\n\naws-s3-churn-data\n\n\nSimilarly, from Glue console we can verify that the utility table has been created.\n\n\n\naws-glue-churn-data\n\n\nIf you want to remain within the notebook and do the verification then that can also be done.\n\n##\n# list s3 objects\nwr.s3.list_objects('s3://sagemaker-us-east-1-801598032724/myblog/demo-customer-churn/data/')\n\n['s3://sagemaker-us-east-1-801598032724/myblog/demo-customer-churn/data/telco_churn_account_info.csv',\n 's3://sagemaker-us-east-1-801598032724/myblog/demo-customer-churn/data/telco_churn_customer_info.csv',\n 's3://sagemaker-us-east-1-801598032724/myblog/demo-customer-churn/data/telco_churn_utility/b4003acdf33e48ce989401e92146923c.csv']\n\n\n\n##\n# list glue catalog tables\nwr.catalog.tables()\n\n\n\n\n\n  \n    \n      \n      Database\n      Table\n      Description\n      TableType\n      Columns\n      Partitions\n    \n  \n  \n    \n      0\n      telco_db\n      telco_churn_utility\n      \n      EXTERNAL_TABLE\n      customerid, vmail_message, day_mins, day_calls..."
  },
  {
    "objectID": "posts/2022-05-17-aws-sagemaker-wrangler-p1.html#summary",
    "href": "posts/2022-05-17-aws-sagemaker-wrangler-p1.html#summary",
    "title": "Data Preparation with SageMaker Data Wrangler (Part 1)",
    "section": "Summary",
    "text": "Summary\nAt this point we have our dataset ready in AWS S3 and Glue, and in the next part we will use AWS SageMaker Data Wrangler to import and join this data."
  },
  {
    "objectID": "posts/2022-05-21-cloudwatch-agent-onprem.html#create-iam-roles-and-users-for-use-with-cloudwatch-agent",
    "href": "posts/2022-05-21-cloudwatch-agent-onprem.html#create-iam-roles-and-users-for-use-with-cloudwatch-agent",
    "title": "Collecting metrics and logs from on-premises servers with the CloudWatch agent",
    "section": "Create IAM roles and users for use with CloudWatch agent",
    "text": "Create IAM roles and users for use with CloudWatch agent\nAccess to AWS resources requires permissions. You create an IAM role, an IAM user, or both to grant permissions that the CloudWatch agent needs to write metrics to CloudWatch. * If you’re going to use the agent on Amazon EC2 instances, you should create an IAM role. * f you’re going to use the agent on on-premises servers, you should create an IAM user.\nSince we want to use EC2 machine as an on-premises machine so we will create an IAM user.\nTo create the IAM user necessary for the CloudWatch agent to run on on-premises servers follow these steps 1. Sign in to the AWS Management Console and open the IAM console at https://console.aws.amazon.com/iam/. 2. In the navigation pane on the left, choose Users and then Add users. 3. Enter the user name for the new user. 4. Select Access key - Programmatic access and choose Next: Permissions. 5. Choose Attach existing policies directly. 6. In the list of policies, select the check box next to CloudWatchAgentServerPolicy. If necessary, use the search box to find the policy. 7. Choose Next: Tags. 8. Optionally create tags for the new IAM user, and then choose Next:Review. 9. Confirm that the correct policy is listed, and choose Create user. 10. Next to the name of the new user, choose Show. Copy the access key and secret key to a file so that you can use them when installing the agent. Choose Close."
  },
  {
    "objectID": "posts/2022-05-21-cloudwatch-agent-onprem.html#install-and-configure-aws-cli-on-ubuntu-server",
    "href": "posts/2022-05-21-cloudwatch-agent-onprem.html#install-and-configure-aws-cli-on-ubuntu-server",
    "title": "Collecting metrics and logs from on-premises servers with the CloudWatch agent",
    "section": "Install and configure AWS CLI on Ubuntu server",
    "text": "Install and configure AWS CLI on Ubuntu server\nConnect to the Ubuntu server using any SSH client. We need to first download and install AWS CLI. Follow the below commands to download and install it. For installing AWS CLI on macOS and Windows take help from this post awscli-getting-started-install\n\n1. Download AWS CLI package\ncurl \"https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip\" -o \"awscliv2.zip\"\n\n\n2. Install UNZIP package\nsudo apt install unzip\n\n\n3. Unzip AWSCLI Package\nunzip awscliv2.zip\n\n\n4. Install AWS CLI\nsudo ./aws/install\n5. Verify AWS CLI Installation\naws --version\n\n\n\naws-cli-installation-complete\n\n\n\n\n6. Configure AWS CLI\nMake sure that you use AmazonCloudWatchAgent profile name as this is used by the OnPremise case by default. For more details, you may take help from this post install-CloudWatch-Agent-commandline-fleet\naws configure --profile AmazonCloudWatchAgent\n\n\n\nconfigure-aws-cli\n\n\n\n\n7. Verify credentials in User home directory\ncat /home/ubuntu/.aws/credentials"
  },
  {
    "objectID": "posts/2022-05-21-cloudwatch-agent-onprem.html#install-and-run-the-cloudwatch-agent-on-ubuntu-server",
    "href": "posts/2022-05-21-cloudwatch-agent-onprem.html#install-and-run-the-cloudwatch-agent-on-ubuntu-server",
    "title": "Collecting metrics and logs from on-premises servers with the CloudWatch agent",
    "section": "Install and run the CloudWatch agent on Ubuntu server",
    "text": "Install and run the CloudWatch agent on Ubuntu server\n\n1. Download the agent\nThe following download link is for Ubuntu. For any other OS you can take help from this post for downloaded agent download-cloudwatch-agent-commandline\nwget https://s3.amazonaws.com/amazoncloudwatch-agent/ubuntu/amd64/latest/amazon-cloudwatch-agent.deb\n\n\n2. Install the agent\nsudo dpkg -i -E ./amazon-cloudwatch-agent.deb\n\n\n3. Prepare agent configuration file\nPrepare agent configuration file. This config file will be provided to the agent in the run command. One such sample is provided below. For more details on this config file you may take help from this link create-cloudwatch-agent-configuration-file. Note the path of this config file (agent config) as we will need it in later commands.\n\n// config-cloudwatchagent.json\n{\n    \"agent\": {\n      \"metrics_collection_interval\": 10,\n      \"logfile\": \"/opt/aws/amazon-cloudwatch-agent/logs/amazon-cloudwatch-agent.log\",\n      \"run_as_user\": \"ubuntu\",\n      \"debug\": false\n    },\n    \"metrics\": {\n      \"namespace\": \"myblog/cloudwatchagent/demo\",\n      \"metrics_collected\": {\n        \"cpu\": {\n          \"resources\": [\n            \"*\"\n          ],\n          \"measurement\": [\n            {\"name\": \"cpu_usage_idle\", \"rename\": \"CPU_USAGE_IDLE\", \"unit\": \"Percent\"},\n            {\"name\": \"cpu_usage_nice\", \"unit\": \"Percent\"},\n            \"cpu_usage_guest\",\n            \"cpu_usage_active\"\n            \n          ],\n          \"totalcpu\": true,\n          \"metrics_collection_interval\": 10\n        },\n        \"disk\": {\n          \"resources\": [\n            \"/\",\n            \"/tmp\"\n          ],\n          \"measurement\": [\n            {\"name\": \"free\", \"rename\": \"DISK_FREE\", \"unit\": \"Gigabytes\"},\n            \"total\",\n            \"used\"\n          ],\n           \"ignore_file_system_types\": [\n            \"sysfs\", \"devtmpfs\"\n          ],\n          \"metrics_collection_interval\": 60\n        },\n        \"diskio\": {\n          \"resources\": [\n            \"*\"\n          ],\n          \"measurement\": [\n            \"reads\",\n            \"writes\",\n            \"read_time\",\n            \"write_time\",\n            \"io_time\"\n          ],\n          \"metrics_collection_interval\": 60\n        },\n        \"swap\": {\n          \"measurement\": [\n            \"swap_used\",\n            \"swap_free\",\n            \"swap_used_percent\"\n          ]\n        },\n        \"mem\": {\n          \"measurement\": [\n            \"mem_used\",\n            \"mem_cached\",\n            \"mem_total\"\n          ],\n          \"metrics_collection_interval\": 1\n        },\n        \"net\": {\n          \"resources\": [\n            \"eth0\"\n          ],\n          \"measurement\": [\n            \"bytes_sent\",\n            \"bytes_recv\",\n            \"drop_in\",\n            \"drop_out\"\n          ]\n        },\n        \"netstat\": {\n          \"measurement\": [\n            \"tcp_established\",\n            \"tcp_syn_sent\",\n            \"tcp_close\"\n          ],\n          \"metrics_collection_interval\": 60\n        },\n        \"processes\": {\n          \"measurement\": [\n            \"running\",\n            \"sleeping\",\n            \"dead\"\n          ]\n        }\n      },\n      \"force_flush_interval\" : 30\n    },\n    \"logs\": {\n      \"logs_collected\": {\n        \"files\": {\n          \"collect_list\": [\n            {\n              \"file_path\": \"/opt/aws/amazon-cloudwatch-agent/logs/amazon-cloudwatch-agent.log\",\n              \"log_group_name\": \"myblog/onprem/ubuntu/amazon-cloudwatch-agent\",\n              \"log_stream_name\": \"myblog-cloudwatchagent-demo.log\",\n              \"timezone\": \"UTC\"\n            }\n          ]\n        }\n      },\n      \"log_stream_name\": \"my_log_stream_name\",\n      \"force_flush_interval\" : 15\n    }\n  }\n\nSome important parts of this config file\nlogfile\n\"logfile\": \"/opt/aws/amazon-cloudwatch-agent/logs/amazon-cloudwatch-agent.log\"\nCloudWatch agent log file location on on-premise server is specified by this tag. After running the agent you can check this log file for any exception messages.\nlog_group_name\n\"log_group_name\": \"myblog/onprem/ubuntu/amazon-cloudwatch-agent\"\nAn on-premise logfile is also uploaded to CloudWatch under log-group-name specified by this tag.\nlog_stream_name\n\"log_stream_name\": \"myblog-cloudwatchagent-demo.log\"\nLog stream name of the CloudWatch where logfile log steam will be uploaded.\nnamespace\n\"namespace\": \"myblog/cloudwatchagent/demo\"\nOn CloudWatch console you find the uploaded metrics under the custom namespace specified by this tag. In our case, it is “myblog/cloudwatchagent/demo”\n\n\n4. Update shared configuration file\nFrom the config file 1. Uncomment the [credentails] tag 2. Update shared_credentails_profile name. This is the profile name with which we have configured our AWS CLI ‘AmazonCloudWatchAgent’. If you have used any other name then use that name here. 3. Update shared_credentials_file path. This is the path for AWS user credentails file created by AWS CLI. ‘/home/username/.aws/credentials’ and in our case it is /home/ubuntu/.aws/credentials\nConfiguration file is located at /opt/aws/amazon-cloudwatch-agent/etc/common-config.toml. For more details on this shared configuration file follow this link CloudWatch-Agent-profile-instance-first\nsudo vim /opt/aws/amazon-cloudwatch-agent/etc/common-config.toml\n\n\n\nagent-shared-config\n\n\n\n\n5. Start the agent\nsudo /opt/aws/amazon-cloudwatch-agent/bin/amazon-cloudwatch-agent-ctl -a fetch-config -m onPremise -s -c file:/home/ubuntu/config-cloudwatchagent.json\nMake sure that you provide the correct path to the JSON config file. In our case, it is file:/home/ubuntu/config-cloudwatchagent.json. For more details check this link start-CloudWatch-Agent-on-premise-SSM-onprem\n\n\n\nrun-agent-success\n\n\n\n\n6. Check agent status\nsudo /opt/aws/amazon-cloudwatch-agent/bin/amazon-cloudwatch-agent-ctl -m ec2 -a status\nIf the agent is running you will get status : running otherwise you will get status : stopped\n\n\n\ncheck-agent-status\n\n\n\n\n7. Check agent logs\nThe agent generates a log while it runs. This log includes troubleshooting information. This log is the amazon-cloudwatch-agent.log file. This file is located in /opt/aws/amazon-cloudwatch-agent/logs/amazon-cloudwatch-agent.log on Linux servers. This is the same logfile path we also defined in the JSON config file. If you are using multiple agents on the machine then you can give them separate log file paths using their JSON configurations.\nsudo tail -f /var/log/amazon/amazon-cloudwatch-agent/amazon-cloudwatch-agent.log\nCheck the logs if there is an exception message or not.\n\n\n\nagent-logs\n\n\nPlease note that both the log files are the same. It could be that agent is keeping multiple copies for internal processing.\n/opt/aws/amazon-cloudwatch-agent/logs/amazon-cloudwatch-agent.log\nor\n/var/log/amazon/amazon-cloudwatch-agent/amazon-cloudwatch-agent.log"
  },
  {
    "objectID": "posts/2022-05-21-cloudwatch-agent-onprem.html#check-the-agent-logs-on-aws-cloudwatch-console",
    "href": "posts/2022-05-21-cloudwatch-agent-onprem.html#check-the-agent-logs-on-aws-cloudwatch-console",
    "title": "Collecting metrics and logs from on-premises servers with the CloudWatch agent",
    "section": "Check the agent logs on AWS CloudWatch console",
    "text": "Check the agent logs on AWS CloudWatch console\nAgent logs are also uploaded to CloudWatch console under log group and stream that we mentioned in JSON config file. In our case it is\n\"log_group_name\": \"myblog/onprem/ubuntu/amazon-cloudwatch-agent\"\n\"log_stream_name\": \"myblog-cloudwatchagent-demo.log\"\n\n\n\nagent-log-group"
  },
  {
    "objectID": "posts/2022-05-21-cloudwatch-agent-onprem.html#check-the-machine-metrics-on-cloudwatch-console",
    "href": "posts/2022-05-21-cloudwatch-agent-onprem.html#check-the-machine-metrics-on-cloudwatch-console",
    "title": "Collecting metrics and logs from on-premises servers with the CloudWatch agent",
    "section": "Check the machine metrics on CloudWatch console",
    "text": "Check the machine metrics on CloudWatch console\nNow finally we can check the metrics uploaded by the agent on CloudWatch console under CloudWatch > Metrics > ALL metrics > Custom namespaces\nThe name of the metrics namespace is the same as what we defined in our JSON config file\n\"metrics\": {\n      \"namespace\": \"myblog/cloudwatchagent/demo\"\n\n\n\nagent-metrics"
  },
  {
    "objectID": "posts/2022-05-21-cloudwatch-agent-onprem.html#to-stop-the-cloudwatch-agent-locally-using-the-command-line",
    "href": "posts/2022-05-21-cloudwatch-agent-onprem.html#to-stop-the-cloudwatch-agent-locally-using-the-command-line",
    "title": "Collecting metrics and logs from on-premises servers with the CloudWatch agent",
    "section": "To stop the CloudWatch agent locally using the command line",
    "text": "To stop the CloudWatch agent locally using the command line\nOn a Linux server, enter the following\nsudo /opt/aws/amazon-cloudwatch-agent/bin/amazon-cloudwatch-agent-ctl -m ec2 -a stop"
  },
  {
    "objectID": "posts/2022-05-21-cloudwatch-agent-onprem.html#i-updated-my-agent-configuration-but-dont-see-the-new-metrics-or-logs-in-the-cloudwatch-console",
    "href": "posts/2022-05-21-cloudwatch-agent-onprem.html#i-updated-my-agent-configuration-but-dont-see-the-new-metrics-or-logs-in-the-cloudwatch-console",
    "title": "Collecting metrics and logs from on-premises servers with the CloudWatch agent",
    "section": "I updated my agent configuration but don’t see the new metrics or logs in the CloudWatch console",
    "text": "I updated my agent configuration but don’t see the new metrics or logs in the CloudWatch console\nIf you update your CloudWatch agent configuration file, the next time that you start the agent, you need to use the fetch-config option. For example, if you stored the updated file on the local computer, enter the following command. Replace <configuration-file-path> with the actual config file path.\nsudo /opt/aws/amazon-cloudwatch-agent/bin/amazon-cloudwatch-agent-ctl -a fetch-config -s -m ec2 -c file:<configuration-file-path>"
  },
  {
    "objectID": "posts/2022-05-23-aws-sagemaker-wrangler-p2.html#launch-sagemaker-data-wrangler-flow",
    "href": "posts/2022-05-23-aws-sagemaker-wrangler-p2.html#launch-sagemaker-data-wrangler-flow",
    "title": "Data Preparation with SageMaker Data Wrangler (Part 2)",
    "section": "Launch SageMaker Data Wrangler Flow",
    "text": "Launch SageMaker Data Wrangler Flow\nCreate a new Data Wrangler flow by clicking on the main menu tabs File > New > Data Wrangler Flow.\n\n\n\ndata-wrangler-new-flow\n\n\nOnce launched SageMaker may take a minute to initialize a new flow. The reason for this is SageMaker will launch a separate machine in the background ml.m5.4xlarge with 16vCPU and 64 GiB memory for processing flow files. A flow file is a JSON file that just captures all the steps performed from the Flow UI console. When you execute the flow, the Flow engine parses this file and performs all the steps. Once a new flow file is available, rename it to customer-churn.flow.\n\n\n\ndata-wrangler-flow-ready"
  },
  {
    "objectID": "posts/2022-05-23-aws-sagemaker-wrangler-p2.html#import-data-from-sources",
    "href": "posts/2022-05-23-aws-sagemaker-wrangler-p2.html#import-data-from-sources",
    "title": "Data Preparation with SageMaker Data Wrangler (Part 2)",
    "section": "Import data from sources",
    "text": "Import data from sources\nFirst, we will create a flow to import data (created in the part-1 post) from S3 bucket. For this from the flow UI click on Amazon S3 bucket. From the next window select the bucket name S3://sagemaker-us-east-1-801598032724. In your case, it could be different where you have stored the data. From the UI select the filename “telco_churn_customer_info.csv” and click Import\n\n\n\ncustomer-churn-s3\n\n\nOnce the data is imported repeat the steps for the filename “telco_churn_account_info.csv”. If you are not seeing the “import from S3 bucket” option on the UI then check the flow UI and click on the ‘Import’ tab option. Once both files are imported, your Data Flow tab will look similar to this\n\n\n\ndata-flow-customer-account.png\n\n\nNow that we have imported data from S3, we can now work on importing data from the Athena database. For this from the Flow UI Import tab click on Amazon Athena option. From the next UI select AwsDataCatalog Data catalog option. For Databases drop down select telco_db and in the query pane write the below query.\nselect * from telco_churn_utility\nYou can also preview the data by clicking on the table preview option. Once satisfied with the results click ‘Import’. When asked about the database name write telco_churn_utility\n\n\n\nimport-athena-table.png\n\n\nAt this point, you will find all three tables imported in Data Flow UI. Against each table, a plus sign (+) will appear that you can use to add any transformations you want to apply on each table.\n\n\n\nall-tables-imported.png\n\n\nfor telco_churn_customer_info click on the plus sign and then select ‘Edit’ to change data types.\n\n\n\nedit_customer_info.png\n\n\nWe will add the following transformations * Change Area Code from Long to String * Click Preview * Then click Apply\n\n\n\ntelco_churn_customer_info_edit.png\n\n\nSimilarly for telco_churn_account_info.csv edit data types as * Change Account Length to Long * Change Int’l Plan and VMail Plan to Bool * Click Preview and then click Apply\nFor telco_churn_utility.csv edit data types as * Change custserv_calls to Long * Click Preview and then click Apply\nAt this point, we have imported the data from all three sources and have also properly transformed their column types."
  },
  {
    "objectID": "posts/2022-05-23-aws-sagemaker-wrangler-p2.html#joining-tables",
    "href": "posts/2022-05-23-aws-sagemaker-wrangler-p2.html#joining-tables",
    "title": "Data Preparation with SageMaker Data Wrangler (Part 2)",
    "section": "Joining Tables",
    "text": "Joining Tables\nNow we will join all three tables to get a full dataset. For this from the Flow UI Data flow click on the plus sign next to customer_info data type and this time select ‘Join’. From the new window select account_info as the right dataset and click Configure\n\n\n\njoin-configure.png\n\n\nFrom the next screen select * Join Type = Full Outer * Columns Left = CustomerID * Columns Right = CustomerID * Click Preview and then Add\n\n\n\njoin-preview.png\n\n\nA new join step will appear on the Data Flow UI. Click on the plus sign next to it and repeat the steps for utility table\n\n\n\nfirst-join.png\n\n\n\nJoin Type = Full Outer\nColumns Left = CustomerID_0\nColumns Right = CustomerID\nClick Preview and then Add\n\n\n\n\njoin-second.png"
  },
  {
    "objectID": "posts/2022-05-24-aws-sagemaker-wrangler-p3.html#preview-ml-model-performance-using-quick-model",
    "href": "posts/2022-05-24-aws-sagemaker-wrangler-p3.html#preview-ml-model-performance-using-quick-model",
    "title": "Data Preparation with SageMaker Data Wrangler (Part 3)",
    "section": "Preview ML model performance using Quick Model",
    "text": "Preview ML model performance using Quick Model\nQuick Model is another great feature of SageMaker wrangler with which we can quickly train a Random Forrest Classification model and analyze the importance of features. For this again click on the plus sign against the 2nd Join, and select Add Analysis. Then from the Analysis UI select\n\nAnalysis Type = Quick Model\nAnalysis Name = Quick model\nLabel = Churn?\n\nLabel is our target identifier. Click preview. Data Wrangler will take around a minute to train the model, and will provide a chart with feature importances.\n\n\n\nquick_model.png\n\n\nFrom this feature importance chart, we can see that the day_mins and night_charge features have the highest importance. It also shows that the model has achieved F1 score of 0.841 on the test data. We can take this model as a baseline and work on the important features and model tuning to improve its performance. Click Save to return to the main Data Flow UI."
  },
  {
    "objectID": "posts/2022-05-25-aws-sagemaker-wrangler-p4.html#remove-redundant-columns",
    "href": "posts/2022-05-25-aws-sagemaker-wrangler-p4.html#remove-redundant-columns",
    "title": "Data Preparation with SageMaker Data Wrangler (Part 4)",
    "section": "Remove redundant columns",
    "text": "Remove redundant columns\nWhen we made joins between tables (see part-2) it resulted in some redundant columns CustomerID_* . We will remove them first. For this click on plus sign beside 2nd Join, and select Add Transform. From the next transform UI clink Add Step and then search for transformer Manage Column. Inside Manage Columns transformer select\n\nTransform = Drop Column\nColumns to drop = CustomerID_0, CustomerID_1\n\nClick preview and Add."
  },
  {
    "objectID": "posts/2022-05-25-aws-sagemaker-wrangler-p4.html#remove-features-with-low-predictive-power",
    "href": "posts/2022-05-25-aws-sagemaker-wrangler-p4.html#remove-features-with-low-predictive-power",
    "title": "Data Preparation with SageMaker Data Wrangler (Part 4)",
    "section": "Remove features with low predictive power",
    "text": "Remove features with low predictive power\nIn part-3 we used Quick Model to get the predictive power of features. When we analyze features with low importance we find that Phone is one such feature that does not hold much information for the model. For a model, a phone number is just some random collection of numbers and does not hold any meaning. There are other features with low importance too but they still hold some information for the model. So let’s drop Phone. The steps will be same as in the last part."
  },
  {
    "objectID": "posts/2022-05-25-aws-sagemaker-wrangler-p4.html#transform-feature-values-to-correct-format",
    "href": "posts/2022-05-25-aws-sagemaker-wrangler-p4.html#transform-feature-values-to-correct-format",
    "title": "Data Preparation with SageMaker Data Wrangler (Part 4)",
    "section": "Transform feature values to correct format",
    "text": "Transform feature values to correct format\nChurn? is our target label but its value has an extra ‘.’ at the end. If we remove that symbol then it can easily be converted to a Boolean type. So let’s do that. From the transformers list this time choose Format String and select\n\nTransform = Remove Symbols\nInput Columns = Churn?\nSymbols = .\n\nClick Preview and Add.\n\n\n\nformat-strings.png\n\n\nNow that the data is in the correct format (True/False) we can apply another transformer on it to convert it to Boolean feature. So select PARSE COLUMN AS TYPE transformer and configure\n\nColumn = Churn?\nFrom = String\nTo = Boolean\n\nClick Preview and then Add."
  },
  {
    "objectID": "posts/2022-05-25-aws-sagemaker-wrangler-p4.html#encode-categorical-features",
    "href": "posts/2022-05-25-aws-sagemaker-wrangler-p4.html#encode-categorical-features",
    "title": "Data Preparation with SageMaker Data Wrangler (Part 4)",
    "section": "Encode categorical features",
    "text": "Encode categorical features\nAt this point we have only two columns with String datatype: State and Area Code. If we look at the Area Code it has high variance and little feature importance. It is better to drop this feature. So Add another transformer and drop Area Code. For State we will apply one-hot encoding. So for this select transformer Encode Categorical and configure\n\nTransform = One-hot encode\nInput Columns = State\nOutput style = Columns\n\nLeave the rest of the options as default. Click Preview and Add.\n\n\n\none-hot-encode.png"
  },
  {
    "objectID": "posts/2022-05-25-aws-sagemaker-wrangler-p4.html#move-the-target-label-to-the-start",
    "href": "posts/2022-05-25-aws-sagemaker-wrangler-p4.html#move-the-target-label-to-the-start",
    "title": "Data Preparation with SageMaker Data Wrangler (Part 4)",
    "section": "Move the target label to the start",
    "text": "Move the target label to the start\nSageMaker requires that the target label should be the first column in the dataset. So add another transformer Manage columns and configure\n\nTransform = Move column\nMove Type = Move to start\nColumn to move = Churn?\n\n\n\n\nmove-target.png"
  },
  {
    "objectID": "posts/2022-05-25-aws-sagemaker-wrangler-p4.html#evaluate-model-performance",
    "href": "posts/2022-05-25-aws-sagemaker-wrangler-p4.html#evaluate-model-performance",
    "title": "Data Preparation with SageMaker Data Wrangler (Part 4)",
    "section": "Evaluate model performance",
    "text": "Evaluate model performance\nWe have done some key transformations. We can use Quick Model again to analyze the model performance at this point. We have done a similar analysis in part-3 so let’s do it again and compare the results. From the last transformation step, click plus sign and choose Add Analysis\n\n\n\nquick_model_2.png\n\n\nWe can see from the results that these transformations have a positive impact on the model performance and the F1 score has moved up from 0.841 to 0.861."
  },
  {
    "objectID": "posts/2022-05-26-aws-sagemaker-wrangler-p5.html",
    "href": "posts/2022-05-26-aws-sagemaker-wrangler-p5.html",
    "title": "Data Preparation with SageMaker Data Wrangler (Part 5)",
    "section": "",
    "text": "Enviornment\nThis notebook is prepared with Amazon SageMaker Studio using Python 3 (Data Science) Kernel and ml.t3.medium instance.\n\n\nAbout\nThis is a detailed guide on using AWS SageMaker Data Wrangler service to prepare data for machine learning models. SageMaker Data Wrangler is a multipurpose tool with which you can * import data from multiple sources * explore data with visualizations * apply transformations * export data for ml training\nThis guide is divided into five parts * Part 1: Prepare synthetic data and place it on multiple sources * Part 2: Import data from multiple sources using Data Wrangler * Part 3: Explore data with Data Wrangler visualizations * Part 4: Preprocess data using Data Wrangler * Part 5: Export data for ML training (You are here)\n\n\nPart 5: Export data for ML training\nIt is important to note that the transformations we have used are not applied to the data yet. These transformations need to be executed to get the final transformed data. When we export Data Wrangler flow it generates the code that when executed will perform the data transformations. Data Wrangler supports four export methods: Save to S3, Pipeline, Python Code, and Feature Store. In this post, we will see how to export data to S3 as this is the most common use case.\nOpen the customer-churn-p4.flow file from part-4. On the last step click the plus sign and select Export to > Amazon S3 (via Jupyter Notebook)\n\n\n\nexport_s3.png\n\n\nSagemaker Data Wrangler will auto-generate a Jupyter notebook that will contain all the required code to transform and put data on the S3 bucket.\n\n\n\nexport_notebook.png\n\n\nYou may review the code and make any changes otherwise run it as it is till point (Optional)Next Steps. This is the first time SageMaker will process the data and place the output on S3 bucket. SageMaker may take a couple of minutes to execute all the cells. It is important to note that this notebook will initiate a container running on a separate machine to do all the processing. The machine specs are defined in the notebook as\n# Processing Job Instance count and instance type.\ninstance_count = 2\ninstance_type = \"ml.m5.4xlarge\"\nOnce execution is complete you see the output message containing the S3 bucket location where the final output is stored.\n\n\n\nexport_s3_output.png\n\n\nThe optional part of this notebook also contains code to generate xgboost model on the transformed data. To execute these steps make the following changes in the notebook.\nChange the flag to run the optional steps.\nrun_optional_steps = True\nNext, update the xgboost hyperparameters to train a binary classification model (customer churn or not?).\nhyperparameters = {\n    \"max_depth\":\"5\",\n    \"objective\": \"binary:logistic\",\n    \"num_round\": \"10\",\n}\nExecute the optional steps. Again note that these steps will initiate a container running on a separate machine (“ml.m5.2xlarge”) to do the training work. The training job will take a few minutes to complete and once it is done trained model will be available on the S3 bucket for inference use. This autogenerated notebook customer-churn-p4.ipynb is available on GitHub here.\n\n\nSummary\nIn this last post of the series, we used SageMaker Data Wrangler to auto-generate code to preprocess the data and store the final output on S3 bucket. We also used the same notebook to train an xgboost model on the processed data."
  },
  {
    "objectID": "posts/2022-05-30-storemagic-jupyter-notebook.html#store-a-variable",
    "href": "posts/2022-05-30-storemagic-jupyter-notebook.html#store-a-variable",
    "title": "storemagic - Don’t lose your variables in Jupyter Notebook",
    "section": "Store a variable",
    "text": "Store a variable\nUse %store magic for lightweight persistence. It stores variables, aliases and macros in IPython’s database. Let’s create a variable and then store it using this magic.\n\n##\n# create a variable\nvar_hello = \"hello world!\"\n\n# store this variable\n%store var_hello\n\nStored 'var_hello' (str)\n\n\nNow I am going to intentionally restart the kernel. We can check that our created variable is now gone from the memory.\n\nvar_hello\n\nNameError: name 'var_hello' is not defined\n\n\nBut no worries. We have it stored safely using our magic. So let’s get it back.\n\n## \n# get the variable back from store\n%store -r var_hello\n\nOkay, we have our variable back and (with a sigh of relief) we can use it again.\n\nvar_hello\n\n'hello world!'\n\n\nLet’s create a few more variables and do some more magic with them.\n\n##\n# create variables\nvar_foo = [1,2,3,4]\nvar_bar = {'a':var_hello}\n\n# store multiple variables\n%store var_foo var_bar\n\nStored 'var_foo' (list)\nStored 'var_bar' (dict)"
  },
  {
    "objectID": "posts/2022-05-30-storemagic-jupyter-notebook.html#check-all-stored-varaibles",
    "href": "posts/2022-05-30-storemagic-jupyter-notebook.html#check-all-stored-varaibles",
    "title": "storemagic - Don’t lose your variables in Jupyter Notebook",
    "section": "Check all stored varaibles",
    "text": "Check all stored varaibles\nWe can check all the varaibles stored using the following magic command.\n\n%store\n\nStored variables and their in-db values:\nvar_bar               -> {'a': 'hello world!'}\nvar_foo               -> [1, 2, 3, 4]\nvar_hello             -> 'hello world!'"
  },
  {
    "objectID": "posts/2022-05-30-storemagic-jupyter-notebook.html#remove-a-variable-from-store",
    "href": "posts/2022-05-30-storemagic-jupyter-notebook.html#remove-a-variable-from-store",
    "title": "storemagic - Don’t lose your variables in Jupyter Notebook",
    "section": "Remove a variable from store",
    "text": "Remove a variable from store\nTo remove a variable from our storage is also straight forward. Put its name after %store -d flag\n\n##\n# remove 'var_hello'\n%store -d var_hello\n\n‘var_hello’ is now gone. Forever …\n\n##\n# check the remaining variables in store\n%store\n\nStored variables and their in-db values:\nvar_bar             -> {'a': 'hello world!'}\nvar_foo             -> [1, 2, 3, 4]"
  },
  {
    "objectID": "posts/2022-05-30-storemagic-jupyter-notebook.html#remove-all-variables-from-store",
    "href": "posts/2022-05-30-storemagic-jupyter-notebook.html#remove-all-variables-from-store",
    "title": "storemagic - Don’t lose your variables in Jupyter Notebook",
    "section": "Remove all variables from store",
    "text": "Remove all variables from store\nIf you want to remove all the varaibles from store and start clean then use -z flag\n\n##\n# remove all variables\n%store -z\n\nKaboom! all variables are gone.\n\n##\n# check store\n%store\n\nStored variables and their in-db values:"
  },
  {
    "objectID": "posts/2022-05-30-storemagic-jupyter-notebook.html#reference",
    "href": "posts/2022-05-30-storemagic-jupyter-notebook.html#reference",
    "title": "storemagic - Don’t lose your variables in Jupyter Notebook",
    "section": "Reference",
    "text": "Reference\nCheck the official IPython storemagic documentation here: https://ipython.readthedocs.io/en/stable/config/extensions/storemagic.html"
  },
  {
    "objectID": "posts/2022-06-08-sagemaker-training-overview.html#reading-and-checking-the-data",
    "href": "posts/2022-06-08-sagemaker-training-overview.html#reading-and-checking-the-data",
    "title": "Demystifying Amazon SageMaker Training for scikit-learn Lovers",
    "section": "Reading and Checking the Data",
    "text": "Reading and Checking the Data\nIn this post, we will be using Boston Housing Dataset. This is a small dataset with 506 rows and 14 columns. medv is the target variable which means median value of owner-occupied homes in $1000s. This dataset is also available with this notebook. Let’s read it and see how it looks.\n\nimport pandas as pd\nimport numpy as np\n\ndata_location = \"./datasets/2022-06-08-sagemaker-training-overview/\"\n\ndf = pd.read_csv(data_location + 'housing.csv')\ndf.head()\n\n\n\n\n\n  \n    \n      \n      crim\n      zn\n      indus\n      chas\n      nox\n      age\n      rm\n      dis\n      rad\n      tax\n      ptratio\n      lstat\n      medv\n    \n  \n  \n    \n      0\n      0.00632\n      18.0\n      2.31\n      0\n      0.538\n      6.575\n      65.2\n      4.0900\n      1\n      296.0\n      15.3\n      4.98\n      24.0\n    \n    \n      1\n      0.02731\n      0.0\n      7.07\n      0\n      0.469\n      6.421\n      78.9\n      4.9671\n      2\n      242.0\n      17.8\n      9.14\n      21.6\n    \n    \n      2\n      0.02729\n      0.0\n      7.07\n      0\n      0.469\n      7.185\n      61.1\n      4.9671\n      2\n      242.0\n      17.8\n      4.03\n      34.7\n    \n    \n      3\n      0.03237\n      0.0\n      2.18\n      0\n      0.458\n      6.998\n      45.8\n      6.0622\n      3\n      222.0\n      18.7\n      2.94\n      33.4\n    \n    \n      4\n      0.06905\n      0.0\n      2.18\n      0\n      0.458\n      7.147\n      54.2\n      6.0622\n      3\n      222.0\n      18.7\n      5.33\n      36.2\n    \n  \n\n\n\n\nlet’s quickly check the dimensions of our loaded dataset.\n\ndf.shape\n\n(506, 13)\n\n\nThe good thing about this dataset is that it does not require any preprocessing as all the features are already in numerical format (no categorical features), and also there are no missing values. We can quickly verify these assumptions.\nCheck the feature data types.\n\ndf.dtypes\n\ncrim       float64\nzn         float64\nindus      float64\nchas         int64\nnox        float64\nage        float64\nrm         float64\ndis        float64\nrad          int64\ntax        float64\nptratio    float64\nlstat      float64\nmedv       float64\ndtype: object\n\n\nCheck if there are any missing values in our dataset.\n\ndf.isnull().values.any()\n\nFalse"
  },
  {
    "objectID": "posts/2022-06-08-sagemaker-training-overview.html#preparing-the-data",
    "href": "posts/2022-06-08-sagemaker-training-overview.html#preparing-the-data",
    "title": "Demystifying Amazon SageMaker Training for scikit-learn Lovers",
    "section": "Preparing the Data",
    "text": "Preparing the Data\nAt this point, our data is ready to be used for training but we also need to check the algorithm we want to use for any specific requirements. We have selected Linear Learner so let’s check its documentation: Linear Learner Algorithm\nIn the documentation it says >For training, the linear learner algorithm supports both recordIO-wrapped protobuf and CSV formats. For the application/x-recordio-protobuf input type, only Float32 tensors are supported. For the text/csv input type, the first column is assumed to be the label, which is the target variable for prediction. You can use either File mode or Pipe mode to train linear learner models on data that is formatted as recordIO-wrapped-protobuf or as CSV.\nIt means that we can use CSV format for our training data. It also mentions that the first column in the training dataset should be the target label. So let’s move our target label medv to the first column.\n\ndf = pd.concat([df['medv'], df.drop('medv', axis='columns')], axis='columns')\n\ndf.head()\n\n\n\n\n\n  \n    \n      \n      medv\n      crim\n      zn\n      indus\n      chas\n      nox\n      age\n      rm\n      dis\n      rad\n      tax\n      ptratio\n      lstat\n    \n  \n  \n    \n      0\n      24.0\n      0.00632\n      18.0\n      2.31\n      0\n      0.538\n      6.575\n      65.2\n      4.0900\n      1\n      296.0\n      15.3\n      4.98\n    \n    \n      1\n      21.6\n      0.02731\n      0.0\n      7.07\n      0\n      0.469\n      6.421\n      78.9\n      4.9671\n      2\n      242.0\n      17.8\n      9.14\n    \n    \n      2\n      34.7\n      0.02729\n      0.0\n      7.07\n      0\n      0.469\n      7.185\n      61.1\n      4.9671\n      2\n      242.0\n      17.8\n      4.03\n    \n    \n      3\n      33.4\n      0.03237\n      0.0\n      2.18\n      0\n      0.458\n      6.998\n      45.8\n      6.0622\n      3\n      222.0\n      18.7\n      2.94\n    \n    \n      4\n      36.2\n      0.06905\n      0.0\n      2.18\n      0\n      0.458\n      7.147\n      54.2\n      6.0622\n      3\n      222.0\n      18.7\n      5.33\n    \n  \n\n\n\n\nWe can now proceed with splitting our data into train and validation sets. After splitting we will export it as CSV files so it could be uploaded to S3 in the next section. Before exporting our dataframe into any format we should also check SageMaker general instructions for that format. For this use the link Common Data Formats for Training and check section Using CSV Format. Here it mentions\n\nTo use data in CSV format for training, in the input data channel specification, specify text/csv as the ContentType. Amazon SageMaker requires that a CSV file does not have a header record and that the target variable is in the first column.\n\nWe have already moved our target label to the first column, and while exporting our dataframe to CSV format we should omit column headers as well.\n\nfrom sklearn.model_selection import train_test_split\n\n# out data size is very small so we will use a small test set\ntraining_data, validation_data = train_test_split(df, test_size=0.1, random_state=42)\n\ntraining_data.to_csv(data_location + \"training_data.csv\", index=False, header=False)\nvalidation_data.to_csv(data_location + \"validation_data.csv\", index=False, header=False)\n\nThe next step is to upload this data to S3 bucket, and for this we will take the help of SageMaker Python SDK. There are two Python SDKs (Software Development Kit) available for SageMaker.\n\nAWS SDK for Python (Boto3). It provides low-level access to SageMaker APIs.\nSageMaker Python SDK. It provides a high-level API interface, and you can do more with fewer lines of code. Internally it is calling Boto3 APIs.\n\nWe will be using SageMaker Python SDK for this post, and you will see that it has an interface similar to scikit-learn, and is a more natural choice for Data Scientists. SageMaker Python SDK documentation is super helpful, and it provides many examples to understand the working of its interface. Make sure that you check it out as well sagemaker.readthedocs.io. If you don’t have much time then I would suggest at least read the following sections from the documentation as we will be using them in the coming sections.\n\nInitialize a SageMaker Session\nUpload local file or directory to S3\ndefault_bucket\nCreate an Amazon SageMaker training job\nA generic Estimator to train using any supplied algorithm\n\nSince we are already running this notebook from SageMaker environment, we don’t need to care about credentials and permissions. We can simply start our new session with SageMaker environment using its SDK.\n\nimport sagemaker\n\nsession = sagemaker.Session()\nrole = sagemaker.get_execution_role()\nbucket = session.default_bucket()\nregion = session.boto_region_name\n\nprint(f\"sagemaker.__version__: {sagemaker.__version__}\")\nprint(f\"Session: {session}\")\nprint(f\"Role: {role}\")\nprint(f\"Bucket: {bucket}\")\nprint(f\"Region: {region}\")\n\nsagemaker.__version__: 2.88.1\nSession: <sagemaker.session.Session object at 0x7ff91637cd50>\nRole: arn:aws:iam::801598032724:role/service-role/AmazonSageMaker-ExecutionRole-20220516T161743\nBucket: sagemaker-us-east-1-801598032724\nRegion: us-east-1\n\n\nWhat we have done is * imported the SageMaker Python SDK into our runtime * get a session to work with SageMaker API and other AWS services * get the execution role associated with the user profile. It is the same profile that is available to the user to work from console UI and has AmazonSageMakerFullAccess policy attached to it. * create or get a default bucket to use and return its name. Default bucket name has the format sagemaker-{region}-{account_id}. If it doesn’t exist then our session will automatically create it. You may also use any other bucket in its place given that you have enough permission for reading and writing. * get the region name attached to our session\nNext, we will use this session to upload data to our default bucket.\n\n##\n# You may choose any other prefix for your bucket. All the data related to this post will be under this prefix.\nbucket_prefix = '2022-06-08-sagemaker-training-overview'\n\nLet’s upload our training data first. In the output, we will get the complete path (S3 URI) for our uploaded data.\n\ns3_train_data_path = session.upload_data(\n    path=data_location + \"training_data.csv\",\n    bucket=bucket,\n    key_prefix=bucket_prefix + '/input/training'\n)\n\nprint(s3_train_data_path)\n\ns3://sagemaker-us-east-1-801598032724/2022-06-08-sagemaker-training-overview/input/training/training_data.csv\n\n\nLet’s do the same for our validation data.\n\ns3_validation_data_path =  session.upload_data(\n    path=data_location + \"validation_data.csv\",\n    bucket=bucket,\n    key_prefix=bucket_prefix + '/input/validation_data'\n)\n\nprint(s3_validation_data_path)\n\ns3://sagemaker-us-east-1-801598032724/2022-06-08-sagemaker-training-overview/input/validation_data/validation_data.csv\n\n\nAt this point we have our data available on S3 bucket. We can now proceed to the next step and configure our training job."
  },
  {
    "objectID": "posts/2022-06-08-sagemaker-training-overview.html#finding-the-right-docker-container",
    "href": "posts/2022-06-08-sagemaker-training-overview.html#finding-the-right-docker-container",
    "title": "Demystifying Amazon SageMaker Training for scikit-learn Lovers",
    "section": "Finding the Right Docker Container",
    "text": "Finding the Right Docker Container\nAWS SageMaker built-in algorithms are fully managed containers that can be accessed with one call. Each algorithm has a separate container and is also dependent on the region in which you want to run it. Getting the container URI is not a problem as long as we know about the region and the algorithm framework name. We already have the region name from our SageMaker session. To get the algorithm framework name (for linear learner) visit the AWS Docker Registry Paths page. From this page select your region. In my case it is us-east-1. On the regional docker registry page find the algorithm you want to use; Linear Learner in our case. This will give you the example code and algorithm framework name as shown below.\n\n\n\nlinear-learner-framework-name\n\n\nSo let’s use the provided sample code to get the container URI for our linear learner algorithm.\n\nfrom sagemaker import image_uris\n\nimage_uri = image_uris.retrieve(framework='linear-learner',region=region)\n\nprint(image_uri)\n\n382416733822.dkr.ecr.us-east-1.amazonaws.com/linear-learner:1"
  },
  {
    "objectID": "posts/2022-06-08-sagemaker-training-overview.html#configure-the-estimator-object",
    "href": "posts/2022-06-08-sagemaker-training-overview.html#configure-the-estimator-object",
    "title": "Demystifying Amazon SageMaker Training for scikit-learn Lovers",
    "section": "Configure the Estimator Object",
    "text": "Configure the Estimator Object\nTo configure our estimator we need to fulfill the following requirements. * output path Define the output path where we want to store the trained model artifacts * instance type Since we have a small dataset and not so complex model so a small machine should suffice. ‘ml.m5.large’ will do. It is a general purpose instance with 2vCPU and 8GiB RAM. * hyperparameters For the hyperparameters check the Linear Learner model documentaion. From the documentation, we find that the most important parameters for our problem are * predictor_type: which should be ‘regressor’ in our case * mini_batch_size: default is 1000 which is too large for our small dataset. Let’s use 30 instead.\nIt is also important to note that the Estimator class will automatically provision a separate ml.m5.large machine to start the training run. This machine will be different from the one on which we are running this Jupyter notebook. Once training is complete this new machine will be terminated and we will be billed for only the time we have used it. This approach makes SageMaker very cost effective. We can keep using small less powerful machines for running Jupyter notebooks, and for training and other heavy workloads, we can provision separate machines for short durations and avoid any unnecessary bills.\n\n##\n# define the output path to store trained model artifacts\ns3_output_path = f\"s3://{bucket}/{bucket_prefix}/output/\"\n\nprint(s3_output_path)\n\ns3://sagemaker-us-east-1-801598032724/2022-06-08-sagemaker-training-overview/output/\n\n\n\nfrom sagemaker.estimator import Estimator\n\nll_estimator = Estimator(\n    image_uri = image_uri, # algorithm container\n    role = role, # execution role with necessary permissions\n    instance_count = 1,\n    instance_type = 'ml.m5.large',\n    sagemaker_session = session, # SageMaker API session\n    output_path = s3_output_path, # training artifacts output path\n    hyperparameters = {\n        'predictor_type': 'regressor',\n        'mini_batch_size': 30\n    }\n)\n\nIn the above cell, we have defined the hyperparameters within the Estimator object constructor. There is a second way to pass the hyperparameters to the Estimator object using the ‘set_hyperparameters’ function call. This method can be useful when we have a large number of parameters to set, or when we want to change them in multiple training runs.\nll_estimator.set_hyperparameters(\n    predictor_type='regressor', \n    mini_batch_size=30)\nYou might ask that for our problem even a small ml.t3.medium or ml.c5.large machine would have been sufficient. Why have not we used them? The answer to this is that AWS SageMaker at this time supports a limited number of machine types for training jobs and both of them are not supported. If you configure the Estimator object for these instance types you will get an error shown below\nAn error occurred (ValidationException) when calling the CreateTrainingJob operation: 1 validation error detected: Value 'ml.t3.medium' at 'resourceConfig.instanceType' failed to satisfy constraint: Member must satisfy enum value set: \n\n[ml.p2.xlarge, ml.m5.4xlarge, ml.m4.16xlarge, ml.p4d.24xlarge, ml.g5.2xlarge, ml.c5n.xlarge, ml.p3.16xlarge, ml.m5.large, ml.p2.16xlarge, ml.g5.4xlarge, ml.c4.2xlarge, ml.c5.2xlarge, ml.c4.4xlarge, ml.g5.8xlarge, ml.c5.4xlarge, ml.c5n.18xlarge, ml.g4dn.xlarge, ml.g4dn.12xlarge, ml.c4.8xlarge, ml.g4dn.2xlarge, ml.c5.9xlarge, ml.g4dn.4xlarge, ml.c5.xlarge, ml.g4dn.16xlarge, ml.c4.xlarge, ml.g4dn.8xlarge, ml.g5.xlarge, ml.c5n.2xlarge, ml.g5.12xlarge, ml.g5.24xlarge, ml.c5n.4xlarge, ml.c5.18xlarge, ml.p3dn.24xlarge, ml.g5.48xlarge, ml.g5.16xlarge, ml.p3.2xlarge, ml.m5.xlarge, ml.m4.10xlarge, ml.c5n.9xlarge, ml.m5.12xlarge, ml.m4.xlarge, ml.m5.24xlarge, ml.m4.2xlarge, ml.p2.8xlarge, ml.m5.2xlarge, ml.p3.8xlarge, ml.m4.4xlarge]"
  },
  {
    "objectID": "posts/2022-06-17-sagemaker-endpoint.html#through-sagemaker-console-ui",
    "href": "posts/2022-06-17-sagemaker-endpoint.html#through-sagemaker-console-ui",
    "title": "Serverless Inference with SageMaker Serverless Endpoints",
    "section": "Through SageMaker Console UI",
    "text": "Through SageMaker Console UI\nLet’s first deploy our serverless endpoint through SageMaker console UI. In the next section, we will do the same through SageMaker Python SDK.\nVisit the SageMaker model repository to find the registered Linear Learner model. You can find the repository on the SageMaker Inference > Model page.\n\n\n\nmodel-repo\n\n\nNote the mode name linear-learner-2022-06-16-09-10-17-207 as will need it in later steps.\nClick on the model name and then Create endpoint\n\n\n\ncreate-endpoint\n\n\nThis will take you to configure endpoint page. Here do the following configurations. * Set Endpoint name to 2022-06-17-sagemaker-endpoint-serverless. You may use any other unique string here. * From Attach endpoint configuration select create a new endpoint configuration * From New endpoint configuration > Endpoint configuration set * Endpoint configuration name to config-2022-06-17-sagemaker-endpoint-serverless. You may use any other name here. * Type of endpoint to Serverless * From Production variants click on Add Model and then select the model name we want to deploy. In our case it is linear-learner-2022-06-16-09-10-17-207. Click Save.\n\n\n\nadd-model\n\n\n\nThen Edit the Max Concurrency and set it to 5.\n\n\n\n\nmax-concurrency\n\n\n\nClick Create endpoint configuration\n\n\n\n\nnew-endpoint-config\n\n\n\nClick Create endpoint\n\n\n\n\nendpoint-created\n\n\nIt will take a minute for the created endpoint to become ready.\nWhile we were configuring the concurrency for our endpoint we have given it a value of 5. This is because at this point there is a limit on concurrency per account across all serverless endpoints. The maximum total concurrency for an account is 20, and if you cross this limit you will get an error as shown below.\n\n\n\nserverless-endpoints-concurrency-error"
  },
  {
    "objectID": "posts/2022-06-17-sagemaker-endpoint.html#through-sagemaker-python-sdk",
    "href": "posts/2022-06-17-sagemaker-endpoint.html#through-sagemaker-python-sdk",
    "title": "Serverless Inference with SageMaker Serverless Endpoints",
    "section": "Through SageMaker Python SDK",
    "text": "Through SageMaker Python SDK\nLet’s create another endpoint but using SageMaker SDK. Deploying a model to a serverless endpoint using SDK involves the following steps: * Get session to SageMaker API * Create a serverless endpoint deployment config * Create a reference to a model container * Deploy the model on a serverless endpoint using serverless configuration\nLet’s do it now.\n\n##\n# get a session to sagemaker api\nimport sagemaker\n\nsession = sagemaker.Session()\nrole = sagemaker.get_execution_role()\n\nprint(f\"sagemaker.__version__: {sagemaker.__version__}\")\nprint(f\"Session: {session}\")\nprint(f\"Role: {role}\")\n\nsagemaker.__version__: 2.88.1\nSession: <sagemaker.session.Session object at 0x7feb1853fc10>\nRole: arn:aws:iam::801598032724:role/service-role/AmazonSageMaker-ExecutionRole-20220516T161743\n\n\n\n##\n# define a serverless endpoint configuration\nfrom sagemaker.serverless import ServerlessInferenceConfig\n\nserverless_config = ServerlessInferenceConfig(\n    memory_size_in_mb=1024, max_concurrency=5\n)\n\nNote that here we are only defining the endpoint configuration. It will be created when we will deploy the model. Also, note that we have not passed any configuration name. It will default to the endpoint name. To read more about the serverless inference configuration read the documentation ServerlessInferenceConfig\nI could not find a way to give a name to endpoint configuration from SageMaker SDK. Let me know in the comments if there is a way to do it.\n\n##\n# create a SageMaker model. \n# In our case model is already registered so it will only create a reference to it\nfrom sagemaker.model import Model\n\nll_model = Model(\n    image_uri = '382416733822.dkr.ecr.us-east-1.amazonaws.com/linear-learner', # find it from the SageMaker mode repository\n    name = 'linear-learner-2022-06-16-09-10-17-207',\n    role=role\n)\n\nWhile creating a SageMaker model you need to provide its container URI, name, and role. The role gives necessary permissions to SageMaker to pull the image container from the ECR repository. To read more about the Model read the docs sagemaker.model.Model\n\n##\n# define the endpoint name\nendpoint_name = '2022-06-17-sagemaker-endpoint-serverless-sdk'\n\n\n##\n# deploy the model to serverless endpoint\nll_model.deploy(\n    endpoint_name=endpoint_name,\n    serverless_inference_config=serverless_config,\n)\n\nUsing already existing model: linear-learner-2022-06-16-09-10-17-207\n\n\n-----!\n\n\nIt will take a minute or so for the serverless endpoint to get provisioned. Once it is ready (InService) you will find it on the SageMaker Inference > Endpoints page.\n\n\n\nendpoint-created-sdk\n\n\nmodel.deploy() command will also create the endpoint configuration with same name as endpoint, and it can be found on SageMaker Inference > Endpoint configurations page\n\n\n\nnew-endpoint-config-sdk"
  },
  {
    "objectID": "posts/2022-07-05-aws-linear-learner-apache-mxnet-python.html",
    "href": "posts/2022-07-05-aws-linear-learner-apache-mxnet-python.html",
    "title": "Loading SageMaker Linear Learner Model with Apache MXNet in Python",
    "section": "",
    "text": "About\nYou have trained a model with Amazon SageMaker’s built-in algorithm Linear Learner. You can test this model by deploying it on a SageMaker endpoint. But you want to test this model in your local environment. In this post, we will learn to use Apache MXNet and Gluon API to load the model in a local environment, extract its parameters, and perform predictions.\n\n\nIntroduction\nApache MXNet is a fully featured, flexibly programmable, and ultra-scalable deep learning framework supporting state of the art in deep learning models, including convolutional neural networks (CNNs) and long short-term memory networks (LSTMs). Amazon has selected MXNet as their deep learning framework of choice (see Amazon CTO, Werner Vogels blog post on this). When you train a deep learning model using Amazon SageMaker builtin algorithm then there are high chances that the model has been trained and saved using MXNet framework. If a model has been saved with MXNet then we can use the same library to load that model in a local environment.\nIn my last post Demystifying Amazon SageMaker Training for scikit-learn Lovers, I used SageMaker builtin Linear Learner algorithm to train a model on Boston housing dataset. Once the training was complete the model artifacts were stored on the S3 bucket at the following location\ns3://sagemaker-us-east-1-801598032724/2022-06-08-sagemaker-training-overview/output/linear-learner-2022-06-16-09-04-57-576/output/model.tar.gz\nNote that Amazon Linear Learner is built using a Neural Network and is different from scikit-learn linear regression algorithm. Linear Learner documentation does not provide details on the architecture of this neural network but it does mention that it trains using a distributed implementation of stochastic gradient descent (SGD). We can also specify the hyperparameters such as momentum, learning rate, and the learning rate schedule. Also, note that not all SageMaker built-in models are using deep learning e.g. XGBoost which is based on regression trees. If you have trained xgboost model then to load this model in a local environment you will have to use xgboost library, and the MXNet library will not work for it.\nSince Linear Learner is based on deep learning, We can use MXNet Gluon API to load this model in our local environment and make some predictions.\nThis post assumes that you have already trained a Linear Learner model and its artifacts are available on the S3 bucket. If you have not done so then you may use my another post to train a Linear Learner on the Boston housing dataset.\n\n\nEnvironment\nThis notebook is prepared with Amazon SageMaker Studio using Python 3 (MXNet 1.9 Python 3.8 CPU Optimized) Kernel and ml.t3.medium instance.\n\n\n\nsagemaker-mxnet-container.png\n\n\n\n##\n# AWS CLI version\n!aws --version\n\naws-cli/1.22.42 Python/3.8.10 Linux/4.14.281-212.502.amzn2.x86_64 botocore/1.23.42\n\n\n\n##\n# OS version\n!cat /etc/os-release\n\nNAME=\"Ubuntu\"\nVERSION=\"20.04.3 LTS (Focal Fossa)\"\nID=ubuntu\nID_LIKE=debian\nPRETTY_NAME=\"Ubuntu 20.04.3 LTS\"\nVERSION_ID=\"20.04\"\nHOME_URL=\"https://www.ubuntu.com/\"\nSUPPORT_URL=\"https://help.ubuntu.com/\"\nBUG_REPORT_URL=\"https://bugs.launchpad.net/ubuntu/\"\nPRIVACY_POLICY_URL=\"https://www.ubuntu.com/legal/terms-and-policies/privacy-policy\"\nVERSION_CODENAME=focal\nUBUNTU_CODENAME=focal\n\n\n\n\nLoading a SageMaker Linear Learner model with Apache MXNet in Python\nLet’s initialize SageMaker API session.\n\nimport sagemaker\n\nsession = sagemaker.Session()\nrole = sagemaker.get_execution_role()\nbucket = session.default_bucket()\nregion = session.boto_region_name\n\nprint(f\"sagemaker.__version__: {sagemaker.__version__}\")\nprint(f\"Session: {session}\")\nprint(f\"Role: {role}\")\nprint(f\"Bucket: {bucket}\")\nprint(f\"Region: {region}\")\n\nsagemaker.__version__: 2.73.0\nSession: <sagemaker.session.Session object at 0x7f6b0500a760>\nRole: arn:aws:iam::801598032724:role/service-role/AmazonSageMaker-ExecutionRole-20220516T161743\nBucket: sagemaker-us-east-1-801598032724\nRegion: us-east-1\n\n\nWe have our trained model artifacts available on S3 bucket. Let’s define that bucket path.\n\nmodel_data = \"s3://sagemaker-us-east-1-801598032724/2022-06-08-sagemaker-training-overview/output/linear-learner-2022-06-16-09-04-57-576/output/model.tar.gz\"\n\nWe will use SageMaker SDK to download model artifacts from the S3 bucket to a local directory. Let’s define the local path.\n\nlocal_path = \"datasets/2022-07-05-aws-linear-learner-apache-mxnet-python/\"\n\nDownload the model artifacts.\n\nfrom sagemaker.s3 import S3Downloader\n\nS3Downloader.download(\n    s3_uri=model_data, \n    local_path=local_path, \n    sagemaker_session=session\n)\n\nOnce downloaded, you will find an archive “model.tar.gz” in the local directory. Let’s extract this file.\n\n!tar -xzvf $local_path/model.tar.gz -C $local_path\n\nmodel_algo-1\n\n\nExtracting once will give us a zip file. Let’s unzip it to get the model contents\n\n!unzip $local_path/model_algo-1 -d $local_path\n\nArchive:  datasets/2022-07-05-aws-linear-learner-apache-mxnet-python//model_algo-1\n extracting: datasets/2022-07-05-aws-linear-learner-apache-mxnet-python/additional-params.json  \n extracting: datasets/2022-07-05-aws-linear-learner-apache-mxnet-python/mx-mod-symbol.json  \n extracting: datasets/2022-07-05-aws-linear-learner-apache-mxnet-python/manifest.json  \n extracting: datasets/2022-07-05-aws-linear-learner-apache-mxnet-python/mx-mod-0000.params  \n\n\nExtracted model has two important files. * mx-mod-symbol.json is the JSON file that defines the computational graph for the model * mx-mod-0000.params is a binary file that contains the parameters for the trained model\nSerializing models as JSON files has the benefit that these models can be loaded from other language bindings like C++ or Scala for faster inference or inference in different environments. You can read more about it here: Saving and Loading Gluon Models.\n\nimport mxnet\nimport pprint\n\nfrom mxnet import gluon\nfrom json import load as json_load\nfrom json import dumps as json_dumps\n\nGluon API is a wrapper around low level MXNet API to provide a simple interface for deep learning. You may read more about this API here: mxnet.gluon\nLet’s read model computational graph.\n\nsym_json = json_load(open(f\"{local_path}mx-mod-symbol.json\"))\nsym_json_string = json_dumps(sym_json)\n\n\nfrom pprint import pprint\npprint(sym_json)\n\n{'arg_nodes': [0, 1, 3, 5],\n 'attrs': {'mxnet_version': ['int', 10301]},\n 'heads': [[6, 0, 0]],\n 'node_row_ptr': [0, 1, 2, 3, 4, 5, 6, 7],\n 'nodes': [{'inputs': [], 'name': 'data', 'op': 'null'},\n           {'attrs': {'__shape__': '(12, 1)'},\n            'inputs': [],\n            'name': 'fc0_weight',\n            'op': 'null'},\n           {'inputs': [[0, 0, 0], [1, 0, 0]], 'name': 'dot46', 'op': 'dot'},\n           {'attrs': {'__lr_mult__': '10.0', '__shape__': '(1, 1)'},\n            'inputs': [],\n            'name': 'fc0_bias',\n            'op': 'null'},\n           {'inputs': [[2, 0, 0], [3, 0, 0]],\n            'name': 'broadcast_plus46',\n            'op': 'broadcast_add'},\n           {'inputs': [], 'name': 'out_label', 'op': 'null'},\n           {'inputs': [[4, 0, 0], [5, 0, 0]],\n            'name': 'linearregressionoutput46',\n            'op': 'LinearRegressionOutput'}]}\n\n\n\n##\n# initialize the model graph\nmodel = gluon.nn.SymbolBlock(\n    outputs=mxnet.sym.load_json(sym_json_string), \n    inputs=mxnet.sym.var(\"data\")\n)\n\n/usr/local/lib/python3.8/dist-packages/mxnet/gluon/block.py:1849: UserWarning: Cannot decide type for the following arguments. Consider providing them as input:\n    data: None\n  input_sym_arg_type = in_param.infer_type()[0]\n\n\n\n##\n# load the model parameters\nmodel.load_parameters(f\"{local_path}mx-mod-0000.params\", allow_missing=True)\n\n\n##\n# finally initialize our model\nmodel.initialize()\n\n/usr/local/lib/python3.8/dist-packages/mxnet/gluon/parameter.py:896: UserWarning: Parameter 'fc0_weight' is already initialized, ignoring. Set force_reinit=True to re-initialize.\n  v.initialize(None, ctx, init, force_reinit=force_reinit)\n/usr/local/lib/python3.8/dist-packages/mxnet/gluon/parameter.py:896: UserWarning: Parameter 'fc0_bias' is already initialized, ignoring. Set force_reinit=True to re-initialize.\n  v.initialize(None, ctx, init, force_reinit=force_reinit)\n\n\nAt this point our model is ready in our local environment, and we can use it to make some predictions.\nLet’s prepare an input request. This request is same as used in the model training blog post.\n\ninput_request = [\n    0.00632,\n    18.00,\n    2.310,\n    0,\n    0.5380,\n    6.5750,\n    65.20,\n    4.0900,\n    1,\n    296.0,\n    15.30,\n    4.98,\n]\n\nWe need to convert our request Python list to MXNet array to be used for inference.\n\ninput_request_nd = mxnet.nd.array(input_request)\n\n\nprint(f\"type(input_request): {type(input_request)}\")\nprint(f\"type(input_request_nd): {type(input_request_nd)}\")\n\ntype(input_request): <class 'list'>\ntype(input_request_nd): <class 'mxnet.ndarray.ndarray.NDArray'>\n\n\nLet’s pass our converted request to model for inference.\n\nmodel(input_request_nd)[0].asscalar()\n\nExtension horovod.torch has not been built: /usr/local/lib/python3.8/dist-packages/horovod/torch/mpi_lib/_mpi_lib.cpython-38-x86_64-linux-gnu.so not found\nIf this is not expected, reinstall Horovod with HOROVOD_WITH_PYTORCH=1 to debug the build error.\nWarning! MPI libs are missing, but python applications are still avaiable.\n[2022-07-05 10:53:31.777 mxnet-1-9-cpu-py38-ub-ml-t3-medium-3179f602905714e1b45dfa06b970:222 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\n[2022-07-05 10:53:31.944 mxnet-1-9-cpu-py38-ub-ml-t3-medium-3179f602905714e1b45dfa06b970:222 INFO profiler_config_parser.py:111] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\n\n\n29.986717\n\n\nThat’s it. We have loaded SageMaker built-in model in our local envionment and have done prediction from it. But we can go one step further and explore this model’s trained parameters.\n\nparams = model.collect_params()\nparams\n\n(\n  Parameter fc0_weight (shape=(12, 1), dtype=<class 'numpy.float32'>)\n  Parameter fc0_bias (shape=(1, 1), dtype=<class 'numpy.float32'>)\n  Parameter out_label (shape=(1,), dtype=<class 'numpy.float32'>)\n)\n\n\nLet’s define a function to extract model’s weights and biases\n\ndef extract_weight_and_bias(model):\n    params = model.collect_params()\n    weight = params[\"fc0_weight\"].data().asnumpy()\n    bias = params[\"fc0_bias\"].data()[0].asscalar()\n    return {\"weight\": weight, \"bias\": bias}\n\n\nweight_and_bias = extract_weight_and_bias(model)\nweight_and_bias\n\n{'weight': array([[-1.6160294e-01],\n        [ 5.2438524e-02],\n        [ 1.5013154e-02],\n        [-4.4300285e-01],\n        [-2.0226759e+01],\n        [ 3.2423832e+00],\n        [ 7.3540364e-03],\n        [-1.4330027e+00],\n        [ 2.0710023e-01],\n        [-8.0383439e-03],\n        [-1.0465978e+00],\n        [-5.0012934e-01]], dtype=float32),\n 'bias': 44.62983}\n\n\nThis shows that model has 12 weights, one for each input parameter, and a bias. For linear learner there is no activation function so we can use summation formula to create a prediction using the provided weights and bais.\n\n\n\nnn-summation-formula.jpeg\n\n\n\n##\n# convert the input request to np.array\nimport numpy as np\n\ninput_request = np.array(input_request)\n\n\n##\n# extract weights and biases\nweight = weight_and_bias[\"weight\"]\nbias = weight_and_bias[\"bias\"]\n\nWe have all the ingredients ready. Let’s use them to calcualte the prediction ourselves.\n\n##\n# calculate the final prediction\nnp.sum(input_request.reshape((-1, 1)) * weight) + bias\n\n29.98671686516441"
  },
  {
    "objectID": "posts/2022-07-07-sagemaker-script-mode.html#download-and-preprocess-data",
    "href": "posts/2022-07-07-sagemaker-script-mode.html#download-and-preprocess-data",
    "title": "Deploy Scikit-learn Models to Amazon SageMaker with the SageMaker Python SDK using Script mode",
    "section": "Download and preprocess data",
    "text": "Download and preprocess data\n\n##\n# download dataset\nimport boto3\nimport pandas as pd\nimport numpy as np\n\ns3 = boto3.client(\"s3\")\ns3.download_file(\n    f\"sagemaker-sample-files\", \"datasets/tabular/iris/iris.data\", \"iris.data\"\n)\n\ndf = pd.read_csv(\n    \"iris.data\",\n    header=None,\n    names=[\"sepal_len\", \"sepal_wid\", \"petal_len\", \"petal_wid\", \"class\"],\n)\ndf.head()\n\n\n\n\n\n  \n    \n      \n      sepal_len\n      sepal_wid\n      petal_len\n      petal_wid\n      class\n    \n  \n  \n    \n      0\n      5.1\n      3.5\n      1.4\n      0.2\n      Iris-setosa\n    \n    \n      1\n      4.9\n      3.0\n      1.4\n      0.2\n      Iris-setosa\n    \n    \n      2\n      4.7\n      3.2\n      1.3\n      0.2\n      Iris-setosa\n    \n    \n      3\n      4.6\n      3.1\n      1.5\n      0.2\n      Iris-setosa\n    \n    \n      4\n      5.0\n      3.6\n      1.4\n      0.2\n      Iris-setosa\n    \n  \n\n\n\n\n\n##\n# Convert the three classes from strings to integers in {0,1,2}\ndf[\"class_cat\"] = df[\"class\"].astype(\"category\").cat.codes\ncategories_map = dict(enumerate(df[\"class\"].astype(\"category\").cat.categories))\nprint(categories_map)\ndf.head()\n\n{0: 'Iris-setosa', 1: 'Iris-versicolor', 2: 'Iris-virginica'}\n\n\n\n\n\n\n  \n    \n      \n      sepal_len\n      sepal_wid\n      petal_len\n      petal_wid\n      class\n      class_cat\n    \n  \n  \n    \n      0\n      5.1\n      3.5\n      1.4\n      0.2\n      Iris-setosa\n      0\n    \n    \n      1\n      4.9\n      3.0\n      1.4\n      0.2\n      Iris-setosa\n      0\n    \n    \n      2\n      4.7\n      3.2\n      1.3\n      0.2\n      Iris-setosa\n      0\n    \n    \n      3\n      4.6\n      3.1\n      1.5\n      0.2\n      Iris-setosa\n      0\n    \n    \n      4\n      5.0\n      3.6\n      1.4\n      0.2\n      Iris-setosa\n      0"
  },
  {
    "objectID": "posts/2022-07-07-sagemaker-script-mode.html#prepare-and-store-train-and-test-sets-as-csv-files",
    "href": "posts/2022-07-07-sagemaker-script-mode.html#prepare-and-store-train-and-test-sets-as-csv-files",
    "title": "Deploy Scikit-learn Models to Amazon SageMaker with the SageMaker Python SDK using Script mode",
    "section": "Prepare and store train and test sets as CSV files",
    "text": "Prepare and store train and test sets as CSV files\n\n##\n# split the data into train and test set\nfrom sklearn.model_selection import train_test_split\n\ntrain, test = train_test_split(df, test_size=0.2, random_state=42)\n\nprint(f\"train.shape: {train.shape}\")\nprint(f\"test.shape: {test.shape}\")\n\ntrain.shape: (120, 6)\ntest.shape: (30, 6)\n\n\nWe have our dataset ready. Let’s define a local directory local_path to keep all the files and artifacts related to this post. I will refer to this directory as ‘workspace’.\n\n##\n# `local_path` will be the root directory for this post.\nlocal_path = \"./datasets/2022-07-07-sagemaker-script-mode\"\n\nWe have train and test sets ready. Let’s create two more directories in our workspace and store our data in them.\n\nfrom pathlib import Path\n\n# local paths\nlocal_train_path = local_path + \"/train\"\nlocal_test_path = local_path + \"/test\"\n\n# create local directories\nPath(local_train_path).mkdir(parents=True, exist_ok=True)\nPath(local_test_path).mkdir(parents=True, exist_ok=True)\n\nprint(\"local_train_path: \", local_train_path)\nprint(\"local_test_path: \", local_test_path)\n\n# local file names\nlocal_train_file = local_train_path + \"/train.csv\"\nlocal_test_file = local_test_path + \"/test.csv\"\n\n# write train and test CSV files\ntrain.to_csv(local_train_file, index=False)\ntest.to_csv(local_test_file, index=False)\n\nprint(\"local_train_file: \", local_train_file)\nprint(\"local_test_file: \", local_test_file)\n\nlocal_train_path:  ./datasets/2022-07-07-sagemaker-script-mode/train\nlocal_test_path:  ./datasets/2022-07-07-sagemaker-script-mode/test\nlocal_train_file:  ./datasets/2022-07-07-sagemaker-script-mode/train/train.csv\nlocal_test_file:  ./datasets/2022-07-07-sagemaker-script-mode/test/test.csv"
  },
  {
    "objectID": "posts/2022-07-07-sagemaker-script-mode.html#create-sagemaker-session",
    "href": "posts/2022-07-07-sagemaker-script-mode.html#create-sagemaker-session",
    "title": "Deploy Scikit-learn Models to Amazon SageMaker with the SageMaker Python SDK using Script mode",
    "section": "Create SageMaker session",
    "text": "Create SageMaker session\n\nimport sagemaker\n\nsession = sagemaker.Session()\nrole = sagemaker.get_execution_role()\nbucket = session.default_bucket()\nregion = session.boto_region_name\n\nprint(\"sagemaker.__version__: \", sagemaker.__version__)\nprint(\"Session: \", session)\nprint(\"Role: \", role)\nprint(\"Bucket: \", bucket)\nprint(\"Region: \", region)\n\nsagemaker.__version__:  2.86.2\nSession:  <sagemaker.session.Session object at 0x7f80ad720460>\nRole:  arn:aws:iam::801598032724:role/service-role/AmazonSageMakerServiceCatalogProductsUseRole\nBucket:  sagemaker-us-east-1-801598032724\nRegion:  us-east-1\n\n\nWhat we have done here is * imported the SageMaker Python SDK into our runtime * get a session to work with SageMaker API and other AWS services * get the execution role associated with the user profile. It is the same profile that is available to the user to work from console UI and has AmazonSageMakerFullAccess policy attached to it. * create or get a default bucket to use and return its name. Default bucket name has the format sagemaker-{region}-{account_id}. If it doesn’t exist then our session will automatically create it. You may also use any other bucket in its place given that you have enough permission for reading and writing. * get the region name attached to our session\nNext, we will use this session to upload data to our default bucket."
  },
  {
    "objectID": "posts/2022-07-07-sagemaker-script-mode.html#upload-data-to-amazon-s3-bucket",
    "href": "posts/2022-07-07-sagemaker-script-mode.html#upload-data-to-amazon-s3-bucket",
    "title": "Deploy Scikit-learn Models to Amazon SageMaker with the SageMaker Python SDK using Script mode",
    "section": "Upload data to Amazon S3 bucket",
    "text": "Upload data to Amazon S3 bucket\n\n##\n# You may choose any other prefix for your bucket.\n# All the data related to this post will be under this prefix.\nbucket_prefix = \"2022-07-07-sagemaker-script-mode\"\n\nNow upload the data. In the output, we will get the complete path (S3 URI) for our uploaded data.\n\ns3_train_uri = session.upload_data(local_train_file, key_prefix=bucket_prefix + \"/data\")\ns3_test_uri = session.upload_data(local_test_file, key_prefix=bucket_prefix + \"/data\")\n\nprint(\"s3_train_uri: \", s3_train_uri)\nprint(\"s3_test_uri: \", s3_test_uri)\n\ns3_train_uri:  s3://sagemaker-us-east-1-801598032724/2022-07-07-sagemaker-script-mode/data/train.csv\ns3_test_uri:  s3://sagemaker-us-east-1-801598032724/2022-07-07-sagemaker-script-mode/data/test.csv\n\n\nAt this point, our data preparation step is complete. Train and test CSV files are available on the local system and in our default Amazon S3 bucket."
  },
  {
    "objectID": "posts/2022-07-07-sagemaker-script-mode.html#how-sagemaker-managed-environment-works",
    "href": "posts/2022-07-07-sagemaker-script-mode.html#how-sagemaker-managed-environment-works",
    "title": "Deploy Scikit-learn Models to Amazon SageMaker with the SageMaker Python SDK using Script mode",
    "section": "How SageMaker managed environment works?",
    "text": "How SageMaker managed environment works?\nWhen you send a request to SageMaker API (fit or deploy call) * it spins up new instances with the provided specification * loads the algorithm container * pulls the data from S3 * runs the training code * store the results and trained model artifacts to S3 * terminates the new instances\nAll this happens behind the scenes with a single line of code and is a huge advantage. Spinning up new hardware every time can be good for repeatability and security, but it can add some friction while testing and debugging our code. We can test our code on a small dataset in our local environment with SageMaker local mode and then switch seamlessly to SageMaker managed environment by changing a single line of code."
  },
  {
    "objectID": "posts/2022-07-07-sagemaker-script-mode.html#steps-to-prepare-amazon-sagemaker-local-environment",
    "href": "posts/2022-07-07-sagemaker-script-mode.html#steps-to-prepare-amazon-sagemaker-local-environment",
    "title": "Deploy Scikit-learn Models to Amazon SageMaker with the SageMaker Python SDK using Script mode",
    "section": "Steps to prepare Amazon SageMaker local environment",
    "text": "Steps to prepare Amazon SageMaker local environment\nInstall the following pre-requisites if you want to set up Amazon SageMaker on your local system. 1. Install required Python packages: pip install boto3 sagemaker pandas scikit-learn     pip install 'sagemaker[local]' 2. Docker Desktop installed and running on your computer: docker ps 3. You should have AWS credentials configured on your local machine to be able to pull the docker image from ECR.\n\nInstructions for SageMaker notebook instances\nYou can also set up SageMaker’s local environment in SageMaker notebook instances. Required Python packages and Docker service is already there. You only need to upgrade the sagemaker[local] Python package.\n\n#collapse_output\n# this is required for SageMaker notebook instances\n!pip install 'sagemaker[local]' --upgrade\n\nLooking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\nRequirement already satisfied: sagemaker[local] in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (2.86.2)\nCollecting sagemaker[local]\n  Downloading sagemaker-2.99.0.tar.gz (542 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 542.7/542.7 KB 10.6 MB/s eta 0:00:0000:01\n  Preparing metadata (setup.py) ... done\nRequirement already satisfied: attrs<22,>=20.3.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from sagemaker[local]) (20.3.0)\nRequirement already satisfied: boto3<2.0,>=1.20.21 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from sagemaker[local]) (1.21.42)\nRequirement already satisfied: google-pasta in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from sagemaker[local]) (0.2.0)\nRequirement already satisfied: numpy<2.0,>=1.9.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from sagemaker[local]) (1.20.3)\nRequirement already satisfied: protobuf<4.0,>=3.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from sagemaker[local]) (3.19.1)\nRequirement already satisfied: protobuf3-to-dict<1.0,>=0.1.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from sagemaker[local]) (0.1.5)\nRequirement already satisfied: smdebug_rulesconfig==1.0.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from sagemaker[local]) (1.0.1)\nRequirement already satisfied: importlib-metadata<5.0,>=1.4.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from sagemaker[local]) (4.8.2)\nRequirement already satisfied: packaging>=20.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from sagemaker[local]) (21.3)\nRequirement already satisfied: pandas in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from sagemaker[local]) (1.3.4)\nRequirement already satisfied: pathos in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from sagemaker[local]) (0.2.8)\nRequirement already satisfied: urllib3==1.26.8 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from sagemaker[local]) (1.26.8)\nRequirement already satisfied: docker-compose==1.29.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from sagemaker[local]) (1.29.2)\nRequirement already satisfied: docker~=5.0.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from sagemaker[local]) (5.0.3)\nRequirement already satisfied: PyYAML==5.4.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from sagemaker[local]) (5.4.1)\nRequirement already satisfied: texttable<2,>=0.9.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from docker-compose==1.29.2->sagemaker[local]) (1.6.4)\nRequirement already satisfied: websocket-client<1,>=0.32.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from docker-compose==1.29.2->sagemaker[local]) (0.59.0)\nRequirement already satisfied: docopt<1,>=0.6.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from docker-compose==1.29.2->sagemaker[local]) (0.6.2)\nRequirement already satisfied: jsonschema<4,>=2.5.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from docker-compose==1.29.2->sagemaker[local]) (3.2.0)\nRequirement already satisfied: dockerpty<1,>=0.4.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from docker-compose==1.29.2->sagemaker[local]) (0.4.1)\nRequirement already satisfied: distro<2,>=1.5.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from docker-compose==1.29.2->sagemaker[local]) (1.7.0)\nRequirement already satisfied: python-dotenv<1,>=0.13.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from docker-compose==1.29.2->sagemaker[local]) (0.20.0)\nRequirement already satisfied: requests<3,>=2.20.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from docker-compose==1.29.2->sagemaker[local]) (2.26.0)\nCollecting botocore<1.25.0,>=1.24.42\n  Downloading botocore-1.24.46-py3-none-any.whl (8.7 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 8.7/8.7 MB 34.3 MB/s eta 0:00:00:00:0100:01\nRequirement already satisfied: s3transfer<0.6.0,>=0.5.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from boto3<2.0,>=1.20.21->sagemaker[local]) (0.5.2)\nRequirement already satisfied: jmespath<2.0.0,>=0.7.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from boto3<2.0,>=1.20.21->sagemaker[local]) (0.10.0)\nRequirement already satisfied: zipp>=0.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from importlib-metadata<5.0,>=1.4.0->sagemaker[local]) (3.6.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from packaging>=20.0->sagemaker[local]) (3.0.6)\nRequirement already satisfied: six in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from protobuf3-to-dict<1.0,>=0.1.5->sagemaker[local]) (1.16.0)\nRequirement already satisfied: python-dateutil>=2.7.3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from pandas->sagemaker[local]) (2.8.2)\nRequirement already satisfied: pytz>=2017.3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from pandas->sagemaker[local]) (2021.3)\nRequirement already satisfied: multiprocess>=0.70.12 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from pathos->sagemaker[local]) (0.70.12.2)\nRequirement already satisfied: pox>=0.3.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from pathos->sagemaker[local]) (0.3.0)\nRequirement already satisfied: dill>=0.3.4 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from pathos->sagemaker[local]) (0.3.4)\nRequirement already satisfied: ppft>=1.6.6.4 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from pathos->sagemaker[local]) (1.6.6.4)\nRequirement already satisfied: paramiko>=2.4.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from docker~=5.0.0->sagemaker[local]) (2.10.3)\nRequirement already satisfied: pyrsistent>=0.14.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from jsonschema<4,>=2.5.1->docker-compose==1.29.2->sagemaker[local]) (0.18.0)\nRequirement already satisfied: setuptools in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from jsonschema<4,>=2.5.1->docker-compose==1.29.2->sagemaker[local]) (59.4.0)\nRequirement already satisfied: charset-normalizer~=2.0.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from requests<3,>=2.20.0->docker-compose==1.29.2->sagemaker[local]) (2.0.8)\nRequirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from requests<3,>=2.20.0->docker-compose==1.29.2->sagemaker[local]) (3.1)\nRequirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from requests<3,>=2.20.0->docker-compose==1.29.2->sagemaker[local]) (2021.10.8)\nRequirement already satisfied: pynacl>=1.0.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from paramiko>=2.4.2->docker~=5.0.0->sagemaker[local]) (1.5.0)\nRequirement already satisfied: cryptography>=2.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from paramiko>=2.4.2->docker~=5.0.0->sagemaker[local]) (36.0.0)\nRequirement already satisfied: bcrypt>=3.1.3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from paramiko>=2.4.2->docker~=5.0.0->sagemaker[local]) (3.2.0)\nRequirement already satisfied: cffi>=1.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from bcrypt>=3.1.3->paramiko>=2.4.2->docker~=5.0.0->sagemaker[local]) (1.15.0)\nRequirement already satisfied: pycparser in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from cffi>=1.1->bcrypt>=3.1.3->paramiko>=2.4.2->docker~=5.0.0->sagemaker[local]) (2.21)\nBuilding wheels for collected packages: sagemaker\n  Building wheel for sagemaker (setup.py) ... done\n  Created wheel for sagemaker: filename=sagemaker-2.99.0-py2.py3-none-any.whl size=756462 sha256=309b5159cfb7f5c739c6159b8bf309bfa7ce28d2ca402296e824f3e84bc837c1\n  Stored in directory: /home/ec2-user/.cache/pip/wheels/fc/df/14/14b7871f4cf108cfe8891338510d97e28cfe2da00f37114fcf\nSuccessfully built sagemaker\nInstalling collected packages: botocore, sagemaker\n  Attempting uninstall: botocore\n    Found existing installation: botocore 1.24.19\n    Uninstalling botocore-1.24.19:\n      Successfully uninstalled botocore-1.24.19\n  Attempting uninstall: sagemaker\n    Found existing installation: sagemaker 2.86.2\n    Uninstalling sagemaker-2.86.2:\n      Successfully uninstalled sagemaker-2.86.2\nERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nawscli 1.22.97 requires botocore==1.24.42, but you have botocore 1.24.46 which is incompatible.\naiobotocore 2.0.1 requires botocore<1.22.9,>=1.22.8, but you have botocore 1.24.46 which is incompatible.\nSuccessfully installed botocore-1.24.46 sagemaker-2.99.0\nWARNING: You are using pip version 22.0.4; however, version 22.1.2 is available.\nYou should consider upgrading via the '/home/ec2-user/anaconda3/envs/python3/bin/python -m pip install --upgrade pip' command.\n\n\n\n\nInstructions for SageMaker Studio environment\nNote that SageMaker local mode will not work in SageMaker Studio environment as it does not have docker service installed on the provided instances."
  },
  {
    "objectID": "posts/2022-07-07-sagemaker-script-mode.html#create-sagemaker-local-session",
    "href": "posts/2022-07-07-sagemaker-script-mode.html#create-sagemaker-local-session",
    "title": "Deploy Scikit-learn Models to Amazon SageMaker with the SageMaker Python SDK using Script mode",
    "section": "Create SageMaker local session",
    "text": "Create SageMaker local session\nSageMaker local session is required for working in a local environment. Let’s create it.\n\nfrom sagemaker.local import LocalSession\n\nsession_local = LocalSession()\nsession_local\n\n<sagemaker.local.local_session.LocalSession at 0x7f80ac223910>\n\n\n\n##\n# configure local session\nsession_local.config = {\"local\": {\"local_code\": True}}"
  },
  {
    "objectID": "posts/2022-07-07-sagemaker-script-mode.html#inspecting-sagemaker-sklearn-docker-image",
    "href": "posts/2022-07-07-sagemaker-script-mode.html#inspecting-sagemaker-sklearn-docker-image",
    "title": "Deploy Scikit-learn Models to Amazon SageMaker with the SageMaker Python SDK using Script mode",
    "section": "Inspecting SageMaker SKLearn docker image",
    "text": "Inspecting SageMaker SKLearn docker image\nSince the container was executed in the local environment, we can also inspect the SageMaker SKLearn local image.\n\n!docker images\n\nREPOSITORY                                                            TAG             IMAGE ID       CREATED       SIZE\n683313688378.dkr.ecr.us-east-1.amazonaws.com/sagemaker-scikit-learn   1.0-1-cpu-py3   8a6ea8272ad0   10 days ago   3.7GB\n\n\nLet’s also inspect the docker image. Notice multiple container environment variables and their default values in the output.\n\n#collapse-output\n!docker inspect 8a6ea8272ad0\n\n[\n    {\n        \"Id\": \"sha256:8a6ea8272ad003ec816569b0f879b16c770116584301161565f065aadb99436c\",\n        \"RepoTags\": [\n            \"683313688378.dkr.ecr.us-east-1.amazonaws.com/sagemaker-scikit-learn:1.0-1-cpu-py3\"\n        ],\n        \"RepoDigests\": [\n            \"683313688378.dkr.ecr.us-east-1.amazonaws.com/sagemaker-scikit-learn@sha256:fc8c3a617ff0e436c25f3b64d03e1f485f1d159478c26757f3d1d267fc849445\"\n        ],\n        \"Parent\": \"\",\n        \"Comment\": \"\",\n        \"Created\": \"2022-07-06T18:55:02.854297671Z\",\n        \"Container\": \"11b9a5fec2d61294aee63e549100ed18ceb7aa0de6a4ff198da2f556dfe3ec2f\",\n        \"ContainerConfig\": {\n            \"Hostname\": \"11b9a5fec2d6\",\n            \"Domainname\": \"\",\n            \"User\": \"\",\n            \"AttachStdin\": false,\n            \"AttachStdout\": false,\n            \"AttachStderr\": false,\n            \"ExposedPorts\": {\n                \"8080/tcp\": {}\n            },\n            \"Tty\": false,\n            \"OpenStdin\": false,\n            \"StdinOnce\": false,\n            \"Env\": [\n                \"PATH=/miniconda3/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\",\n                \"PYTHONDONTWRITEBYTECODE=1\",\n                \"PYTHONUNBUFFERED=1\",\n                \"PYTHONIOENCODING=UTF-8\",\n                \"LANG=C.UTF-8\",\n                \"LC_ALL=C.UTF-8\",\n                \"SAGEMAKER_SKLEARN_VERSION=1.0-1\",\n                \"SAGEMAKER_TRAINING_MODULE=sagemaker_sklearn_container.training:main\",\n                \"SAGEMAKER_SERVING_MODULE=sagemaker_sklearn_container.serving:main\",\n                \"SKLEARN_MMS_CONFIG=/home/model-server/config.properties\",\n                \"SM_INPUT=/opt/ml/input\",\n                \"SM_INPUT_TRAINING_CONFIG_FILE=/opt/ml/input/config/hyperparameters.json\",\n                \"SM_INPUT_DATA_CONFIG_FILE=/opt/ml/input/config/inputdataconfig.json\",\n                \"SM_CHECKPOINT_CONFIG_FILE=/opt/ml/input/config/checkpointconfig.json\",\n                \"SM_MODEL_DIR=/opt/ml/model\",\n                \"TEMP=/home/model-server/tmp\"\n            ],\n            \"Cmd\": [\n                \"/bin/sh\",\n                \"-c\",\n                \"#(nop) \",\n                \"LABEL transform_id=9be8b540-703b-4ecd-a127-c37333a0dcec_sagemaker-scikit-learn-1_0\"\n            ],\n            \"Image\": \"sha256:58b15b990d550868caed6f885423deee97a6c7f525c228a043096bf28e775d18\",\n            \"Volumes\": null,\n            \"WorkingDir\": \"\",\n            \"Entrypoint\": null,\n            \"OnBuild\": null,\n            \"Labels\": {\n                \"TRANSFORM_TYPE\": \"Aggregate-1.0\",\n                \"VERSION_SET_NAME\": \"SMFrameworksSKLearn/release-cdk\",\n                \"VERSION_SET_REVISION\": \"6086988568\",\n                \"com.amazonaws.sagemaker.capabilities.accept-bind-to-port\": \"true\",\n                \"com.amazonaws.sagemaker.capabilities.multi-models\": \"true\",\n                \"transform_id\": \"9be8b540-703b-4ecd-a127-c37333a0dcec_sagemaker-scikit-learn-1_0\"\n            }\n        },\n        \"DockerVersion\": \"20.10.15\",\n        \"Author\": \"\",\n        \"Config\": {\n            \"Hostname\": \"\",\n            \"Domainname\": \"\",\n            \"User\": \"\",\n            \"AttachStdin\": false,\n            \"AttachStdout\": false,\n            \"AttachStderr\": false,\n            \"ExposedPorts\": {\n                \"8080/tcp\": {}\n            },\n            \"Tty\": false,\n            \"OpenStdin\": false,\n            \"StdinOnce\": false,\n            \"Env\": [\n                \"PATH=/miniconda3/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\",\n                \"PYTHONDONTWRITEBYTECODE=1\",\n                \"PYTHONUNBUFFERED=1\",\n                \"PYTHONIOENCODING=UTF-8\",\n                \"LANG=C.UTF-8\",\n                \"LC_ALL=C.UTF-8\",\n                \"SAGEMAKER_SKLEARN_VERSION=1.0-1\",\n                \"SAGEMAKER_TRAINING_MODULE=sagemaker_sklearn_container.training:main\",\n                \"SAGEMAKER_SERVING_MODULE=sagemaker_sklearn_container.serving:main\",\n                \"SKLEARN_MMS_CONFIG=/home/model-server/config.properties\",\n                \"SM_INPUT=/opt/ml/input\",\n                \"SM_INPUT_TRAINING_CONFIG_FILE=/opt/ml/input/config/hyperparameters.json\",\n                \"SM_INPUT_DATA_CONFIG_FILE=/opt/ml/input/config/inputdataconfig.json\",\n                \"SM_CHECKPOINT_CONFIG_FILE=/opt/ml/input/config/checkpointconfig.json\",\n                \"SM_MODEL_DIR=/opt/ml/model\",\n                \"TEMP=/home/model-server/tmp\"\n            ],\n            \"Cmd\": [\n                \"bash\"\n            ],\n            \"Image\": \"sha256:58b15b990d550868caed6f885423deee97a6c7f525c228a043096bf28e775d18\",\n            \"Volumes\": null,\n            \"WorkingDir\": \"\",\n            \"Entrypoint\": null,\n            \"OnBuild\": null,\n            \"Labels\": {\n                \"TRANSFORM_TYPE\": \"Aggregate-1.0\",\n                \"VERSION_SET_NAME\": \"SMFrameworksSKLearn/release-cdk\",\n                \"VERSION_SET_REVISION\": \"6086988568\",\n                \"com.amazonaws.sagemaker.capabilities.accept-bind-to-port\": \"true\",\n                \"com.amazonaws.sagemaker.capabilities.multi-models\": \"true\",\n                \"transform_id\": \"9be8b540-703b-4ecd-a127-c37333a0dcec_sagemaker-scikit-learn-1_0\"\n            }\n        },\n        \"Architecture\": \"amd64\",\n        \"Os\": \"linux\",\n        \"Size\": 3699696670,\n        \"VirtualSize\": 3699696670,\n        \"GraphDriver\": {\n            \"Data\": {\n                \"LowerDir\": \"/var/lib/docker/overlay2/01a97258168fa360e9f6aa63ac0c6b2417c0ea0ebe888123edad87eb4a646765/diff:/var/lib/docker/overlay2/3b85b71e8fe52c7a27ae71ed492ff72c7e430cccdeea17046e2a361e8d7fd960/diff:/var/lib/docker/overlay2/7de8e16dd696c868ffd028a3ba1f1a80ef04237b9323229e578bc5e3aa6a29d7/diff:/var/lib/docker/overlay2/5eeb27014ab7ac7a894efdbb166d8a87fb9d4b8b739eccd82546ad6a2b53aa70/diff:/var/lib/docker/overlay2/bbd9a81a7aa5bf4c79e81ecf47670a3f8c098eee9c6682f36f88ec52db8e1946/diff:/var/lib/docker/overlay2/eb0e7f3a5bd45c1d611e4c37ba641d1e978043954312da5908fd4003c41c7e7d/diff:/var/lib/docker/overlay2/3daaedc78711e353befc51544a944ad35954327325d056094f445502bf65ce53/diff:/var/lib/docker/overlay2/9dd41e3edfb9d8f852732a968a7b179ca811e0f9d55614a0b193de753fc6aca0/diff:/var/lib/docker/overlay2/ede189a574c79eebc565041a44ebf8b586247a36a99fe3ff9588b8c940783498/diff:/var/lib/docker/overlay2/6b1d78a9c074a42d78650406b90b7b4f51eb31660a7b1e2dcc6d73cc43d29b6b/diff:/var/lib/docker/overlay2/3e0420f6740f876c9355d526cbdedd9ebde5be94ddf0d93d7dadd4f34cae351b/diff:/var/lib/docker/overlay2/de1a2da7ee1b5d9a1b4e5c3dd1adff213185dde7e1212db96c0435e512f50701/diff:/var/lib/docker/overlay2/bebca69aef394f0553634413c7875eb58228c7e6359a305a7501705e75c2b58b/diff:/var/lib/docker/overlay2/8a410db2a038a175ee6ddfb005383f8776c80b1b1901f5d2feedfc8d837ffa40/diff:/var/lib/docker/overlay2/6f6686a8cb3ccf47b214854717cbe33ba777e0985200e3d7b7f761f99231b274/diff:/var/lib/docker/overlay2/ad8b24fa9173d28a83284e4f31d830f1b3d9fe30a3fcc8cbb37895ec2fded7bf/diff:/var/lib/docker/overlay2/e8b0842f0da5b0dbb5076e350bfe1a70ef291546bbbf207fe1f90ae7ccd64517/diff\",\n                \"MergedDir\": \"/var/lib/docker/overlay2/632d2d4d01646bd8be2ec147edc70eb44f59fb262aa12b217fd560c464edd4cb/merged\",\n                \"UpperDir\": \"/var/lib/docker/overlay2/632d2d4d01646bd8be2ec147edc70eb44f59fb262aa12b217fd560c464edd4cb/diff\",\n                \"WorkDir\": \"/var/lib/docker/overlay2/632d2d4d01646bd8be2ec147edc70eb44f59fb262aa12b217fd560c464edd4cb/work\"\n            },\n            \"Name\": \"overlay2\"\n        },\n        \"RootFS\": {\n            \"Type\": \"layers\",\n            \"Layers\": [\n                \"sha256:1dc52a6b4de8561423dd3ec5a1f7f77f5309fd8cb340f80b8bc3d87fa112003e\",\n                \"sha256:b13a10ce059365d68a2113e9dbcac05b17b51f181615fca6d717a0dcf9ba8ffb\",\n                \"sha256:790d00cf365a312488151b354f0b0ae826be031edffb8a4de6a1fab048774dc7\",\n                \"sha256:323e43c53a1cd5abbd55437588f19da04f716452bc6d05486759b35f3e485390\",\n                \"sha256:c99c9d462af0bac5511ed046178ab0de79b8cdad33cd85246e9f661e098426cd\",\n                \"sha256:4a3a4d9fb4d250b1b64629b23bc0a477a45ee2659a8410d59a31a181dad70002\",\n                \"sha256:27b35f432a27e5e275038e559ebbe1aa7e91447bf417f5da01e3326739ba9366\",\n                \"sha256:ee12325fe0b7e7930b76d9a3dc81fcc37fa51a3267b311d2ed7c38703f193d75\",\n                \"sha256:7ceb40593535cdc07299efa2ce3a2c2267c2fa683161515fd6ab97f733492bf0\",\n                \"sha256:f18dbe0eec054f0aedf54a94aa29dab0d2c0f3d920fb482c99819622b0094f47\",\n                \"sha256:df2a7845ea611463f9f3282ccb45156ba883f40b15013ee49bd0a569301738d8\",\n                \"sha256:bcbd5416b87e3e37e05c22e46cbff2e3503d9caa0ec283a44931dc63e51c8cb7\",\n                \"sha256:5bcbb3ccae766c8a72d98ce494500bfd44c32e5780a1cb153139a4c5c143a8d5\",\n                \"sha256:4ecc8a8ffa902f3ea9bebb8d610e02a32ce1ca94c1a3160a31da98b73c1f55a0\",\n                \"sha256:a7a7b8b26735eb2d137fd0f91b83c73ad48cf2c4b83e9d0cadece410d6e598ba\",\n                \"sha256:ae939a0c9d32674ad6674947853ecfda4ff0530a8137960064448ae5e45fa1c5\",\n                \"sha256:6948f39c8f3cf6ec104734ccd1112fcb4af85a7c26c9c3d43495494b9b799f25\",\n                \"sha256:affd18c8e88f35e75bd02158e0418f3aeb4eec4269a208ede24cc829fa88c850\"\n            ]\n        },\n        \"Metadata\": {\n            \"LastTagTime\": \"0001-01-01T00:00:00Z\"\n        }\n    }\n]"
  },
  {
    "objectID": "posts/2022-07-07-sagemaker-script-mode.html#pass-hyperparameters-to-sklearn-estimator",
    "href": "posts/2022-07-07-sagemaker-script-mode.html#pass-hyperparameters-to-sklearn-estimator",
    "title": "Deploy Scikit-learn Models to Amazon SageMaker with the SageMaker Python SDK using Script mode",
    "section": "Pass hyperparameters to SKLearn estimator",
    "text": "Pass hyperparameters to SKLearn estimator\nLet’s pass some dummy hyperparameters to the estimator and see how it affects the output.\n\n#collapse-output\nsk_estimator = SKLearn(\n    entry_point=script_file,\n    role=role,\n    instance_count=1,\n    instance_type='local',\n    framework_version=\"1.0-1\",\n    hyperparameters={\"dummy_param_1\":\"val1\",\"dummy_param_2\":\"val2\"},\n)\n\nsk_estimator.fit()\n\nCreating kc4ahx6e84-algo-1-8m8ve ... \nCreating kc4ahx6e84-algo-1-8m8ve ... done\nAttaching to kc4ahx6e84-algo-1-8m8ve\nkc4ahx6e84-algo-1-8m8ve | 2022-07-17 15:23:46,385 sagemaker-containers INFO     Imported framework sagemaker_sklearn_container.training\nkc4ahx6e84-algo-1-8m8ve | 2022-07-17 15:23:46,389 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\nkc4ahx6e84-algo-1-8m8ve | 2022-07-17 15:23:46,398 sagemaker_sklearn_container.training INFO     Invoking user training script.\nkc4ahx6e84-algo-1-8m8ve | 2022-07-17 15:23:46,595 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\nkc4ahx6e84-algo-1-8m8ve | 2022-07-17 15:23:46,608 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\nkc4ahx6e84-algo-1-8m8ve | 2022-07-17 15:23:46,621 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\nkc4ahx6e84-algo-1-8m8ve | 2022-07-17 15:23:46,630 sagemaker-training-toolkit INFO     Invoking user script\nkc4ahx6e84-algo-1-8m8ve | \nkc4ahx6e84-algo-1-8m8ve | Training Env:\nkc4ahx6e84-algo-1-8m8ve | \nkc4ahx6e84-algo-1-8m8ve | {\nkc4ahx6e84-algo-1-8m8ve |     \"additional_framework_parameters\": {},\nkc4ahx6e84-algo-1-8m8ve |     \"channel_input_dirs\": {},\nkc4ahx6e84-algo-1-8m8ve |     \"current_host\": \"algo-1-8m8ve\",\nkc4ahx6e84-algo-1-8m8ve |     \"framework_module\": \"sagemaker_sklearn_container.training:main\",\nkc4ahx6e84-algo-1-8m8ve |     \"hosts\": [\nkc4ahx6e84-algo-1-8m8ve |         \"algo-1-8m8ve\"\nkc4ahx6e84-algo-1-8m8ve |     ],\nkc4ahx6e84-algo-1-8m8ve |     \"hyperparameters\": {\nkc4ahx6e84-algo-1-8m8ve |         \"dummy_param_1\": \"val1\",\nkc4ahx6e84-algo-1-8m8ve |         \"dummy_param_2\": \"val2\"\nkc4ahx6e84-algo-1-8m8ve |     },\nkc4ahx6e84-algo-1-8m8ve |     \"input_config_dir\": \"/opt/ml/input/config\",\nkc4ahx6e84-algo-1-8m8ve |     \"input_data_config\": {},\nkc4ahx6e84-algo-1-8m8ve |     \"input_dir\": \"/opt/ml/input\",\nkc4ahx6e84-algo-1-8m8ve |     \"is_master\": true,\nkc4ahx6e84-algo-1-8m8ve |     \"job_name\": \"sagemaker-scikit-learn-2022-07-17-15-23-44-284\",\nkc4ahx6e84-algo-1-8m8ve |     \"log_level\": 20,\nkc4ahx6e84-algo-1-8m8ve |     \"master_hostname\": \"algo-1-8m8ve\",\nkc4ahx6e84-algo-1-8m8ve |     \"model_dir\": \"/opt/ml/model\",\nkc4ahx6e84-algo-1-8m8ve |     \"module_dir\": \"s3://sagemaker-us-east-1-801598032724/sagemaker-scikit-learn-2022-07-17-15-23-44-284/source/sourcedir.tar.gz\",\nkc4ahx6e84-algo-1-8m8ve |     \"module_name\": \"train_and_serve\",\nkc4ahx6e84-algo-1-8m8ve |     \"network_interface_name\": \"eth0\",\nkc4ahx6e84-algo-1-8m8ve |     \"num_cpus\": 2,\nkc4ahx6e84-algo-1-8m8ve |     \"num_gpus\": 0,\nkc4ahx6e84-algo-1-8m8ve |     \"output_data_dir\": \"/opt/ml/output/data\",\nkc4ahx6e84-algo-1-8m8ve |     \"output_dir\": \"/opt/ml/output\",\nkc4ahx6e84-algo-1-8m8ve |     \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\nkc4ahx6e84-algo-1-8m8ve |     \"resource_config\": {\nkc4ahx6e84-algo-1-8m8ve |         \"current_host\": \"algo-1-8m8ve\",\nkc4ahx6e84-algo-1-8m8ve |         \"hosts\": [\nkc4ahx6e84-algo-1-8m8ve |             \"algo-1-8m8ve\"\nkc4ahx6e84-algo-1-8m8ve |         ]\nkc4ahx6e84-algo-1-8m8ve |     },\nkc4ahx6e84-algo-1-8m8ve |     \"user_entry_point\": \"train_and_serve.py\"\nkc4ahx6e84-algo-1-8m8ve | }\nkc4ahx6e84-algo-1-8m8ve | \nkc4ahx6e84-algo-1-8m8ve | Environment variables:\nkc4ahx6e84-algo-1-8m8ve | \nkc4ahx6e84-algo-1-8m8ve | SM_HOSTS=[\"algo-1-8m8ve\"]\nkc4ahx6e84-algo-1-8m8ve | SM_NETWORK_INTERFACE_NAME=eth0\nkc4ahx6e84-algo-1-8m8ve | SM_HPS={\"dummy_param_1\":\"val1\",\"dummy_param_2\":\"val2\"}\nkc4ahx6e84-algo-1-8m8ve | SM_USER_ENTRY_POINT=train_and_serve.py\nkc4ahx6e84-algo-1-8m8ve | SM_FRAMEWORK_PARAMS={}\nkc4ahx6e84-algo-1-8m8ve | SM_RESOURCE_CONFIG={\"current_host\":\"algo-1-8m8ve\",\"hosts\":[\"algo-1-8m8ve\"]}\nkc4ahx6e84-algo-1-8m8ve | SM_INPUT_DATA_CONFIG={}\nkc4ahx6e84-algo-1-8m8ve | SM_OUTPUT_DATA_DIR=/opt/ml/output/data\nkc4ahx6e84-algo-1-8m8ve | SM_CHANNELS=[]\nkc4ahx6e84-algo-1-8m8ve | SM_CURRENT_HOST=algo-1-8m8ve\nkc4ahx6e84-algo-1-8m8ve | SM_MODULE_NAME=train_and_serve\nkc4ahx6e84-algo-1-8m8ve | SM_LOG_LEVEL=20\nkc4ahx6e84-algo-1-8m8ve | SM_FRAMEWORK_MODULE=sagemaker_sklearn_container.training:main\nkc4ahx6e84-algo-1-8m8ve | SM_INPUT_DIR=/opt/ml/input\nkc4ahx6e84-algo-1-8m8ve | SM_INPUT_CONFIG_DIR=/opt/ml/input/config\nkc4ahx6e84-algo-1-8m8ve | SM_OUTPUT_DIR=/opt/ml/output\nkc4ahx6e84-algo-1-8m8ve | SM_NUM_CPUS=2\nkc4ahx6e84-algo-1-8m8ve | SM_NUM_GPUS=0\nkc4ahx6e84-algo-1-8m8ve | SM_MODEL_DIR=/opt/ml/model\nkc4ahx6e84-algo-1-8m8ve | SM_MODULE_DIR=s3://sagemaker-us-east-1-801598032724/sagemaker-scikit-learn-2022-07-17-15-23-44-284/source/sourcedir.tar.gz\nkc4ahx6e84-algo-1-8m8ve | SM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{},\"current_host\":\"algo-1-8m8ve\",\"framework_module\":\"sagemaker_sklearn_container.training:main\",\"hosts\":[\"algo-1-8m8ve\"],\"hyperparameters\":{\"dummy_param_1\":\"val1\",\"dummy_param_2\":\"val2\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"sagemaker-scikit-learn-2022-07-17-15-23-44-284\",\"log_level\":20,\"master_hostname\":\"algo-1-8m8ve\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-801598032724/sagemaker-scikit-learn-2022-07-17-15-23-44-284/source/sourcedir.tar.gz\",\"module_name\":\"train_and_serve\",\"network_interface_name\":\"eth0\",\"num_cpus\":2,\"num_gpus\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1-8m8ve\",\"hosts\":[\"algo-1-8m8ve\"]},\"user_entry_point\":\"train_and_serve.py\"}\nkc4ahx6e84-algo-1-8m8ve | SM_USER_ARGS=[\"--dummy_param_1\",\"val1\",\"--dummy_param_2\",\"val2\"]\nkc4ahx6e84-algo-1-8m8ve | SM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\nkc4ahx6e84-algo-1-8m8ve | SM_HP_DUMMY_PARAM_1=val1\nkc4ahx6e84-algo-1-8m8ve | SM_HP_DUMMY_PARAM_2=val2\nkc4ahx6e84-algo-1-8m8ve | PYTHONPATH=/opt/ml/code:/miniconda3/bin:/miniconda3/lib/python38.zip:/miniconda3/lib/python3.8:/miniconda3/lib/python3.8/lib-dynload:/miniconda3/lib/python3.8/site-packages\nkc4ahx6e84-algo-1-8m8ve | \nkc4ahx6e84-algo-1-8m8ve | Invoking script with the following command:\nkc4ahx6e84-algo-1-8m8ve | \nkc4ahx6e84-algo-1-8m8ve | /miniconda3/bin/python train_and_serve.py --dummy_param_1 val1 --dummy_param_2 val2\nkc4ahx6e84-algo-1-8m8ve | \nkc4ahx6e84-algo-1-8m8ve | \nkc4ahx6e84-algo-1-8m8ve | *** Hello from the SageMaker script mode***\nkc4ahx6e84-algo-1-8m8ve | 2022-07-17 15:23:46,657 sagemaker-containers INFO     Reporting training SUCCESS\nkc4ahx6e84-algo-1-8m8ve exited with code 0\nAborting on container exit...\n===== Job Complete =====\n\n\n\n\n\nsklearn-output-hyperparams\n\n\nFrom the output we can see that our hyperparameters are passed to our training script as command line arguments. This is an important point and we will update our script using this information."
  },
  {
    "objectID": "posts/2022-07-07-sagemaker-script-mode.html#sagemaker-sklearn-container-environment-variables",
    "href": "posts/2022-07-07-sagemaker-script-mode.html#sagemaker-sklearn-container-environment-variables",
    "title": "Deploy Scikit-learn Models to Amazon SageMaker with the SageMaker Python SDK using Script mode",
    "section": "SageMaker SKLearn container environment variables",
    "text": "SageMaker SKLearn container environment variables\nLet’s now discuss some important environment variables we see in the output.\n\nSM_MODULE_DIR\nSM_MODULE_DIR=s3://sagemaker-us-east-1-801598032724/sagemaker-scikit-learn-2022-07-13-13-05-48-675/source/sourcedir.tar.gz\nSM_MODULE_DIR points to a location in the S3 bucket where SageMaker will automatically backup our source code for that particular run. SageMaker will create a separate folder in the default bucket for each new run. The default value is s3://sagemaker-{aws-region}-{aws-id}/{training-job-name}/source/sourcedir.tar.gz\nNote: We have used local_code for the SKLean estimator, then why is the source code backed up on the S3 bucket. Should it not be backed on the local system and bypass S3 altogether in local mode? Well, this should have been the default behavior, but it looks like SageMaker SDK is doing it otherwise, and even with the local mode it is using the S3 bucket for keeping source code. You can read more about this behavior in this issue ticket Model repack always uploads data to S3 bucket regardless of local mode settings\n\n\nSM_MODEL_DIR\nSM_MODEL_DIR=/opt/ml/model\nSM_MODEL_DIR points to a directory located inside the container. When the training job finishes, the container and its file system will be deleted, except for the /opt/ml/model and /opt/ml/output directories. Use /opt/ml/model to save the trained model artifacts. These artifacts are uploaded to S3 for model hosting.\n\n\nSM_OUTPUT_DATA_DIR\nSM_OUTPUT_DIR=/opt/ml/output\nSM_OUTPUT_DIR points to a directory in the container to write output artifacts. Output artifacts may include checkpoints, graphs, and other files to save, not including model artifacts. These artifacts are compressed and uploaded to S3 to the same S3 prefix as the model artifacts.\n\n\nSM_CHANNELS\nSM_CHANNELS='[\"testing\",\"training\"]'\nA channel is a named input source that training algorithms can consume. You can partition your training data into different logical “channels” when you run training. Depending on your problem, some common channel ideas are: “training”, “testing”, “evaluation” or “images” and “labels”. You can read more about the channels from SageMaker API reference Channel\n\n\nSM CHANNEL {channel_name}\nSM_CHANNEL_TRAIN='/opt/ml/input/data/train'\nSM_CHANNEL_TEST='/opt/ml/input/data/test'\nSuppose that you have passed two input channels, ‘train’ and ‘test’, to the Scikit-learn estimator’s fit() method, the following will be set, following the format SM_CHANNEL_[channel_name]: * SM_CHANNEL_TRAIN: it points to the directory in the container that has the train channel data downloaded * SM_CHANNEL_TEST: Same as above, but for the test channel\nNote that the channel names train and test are the conventions. Still, you can use any name here, and the environment variables will be created accordingly. It is important to know that the SageMaker container automatically downloads the data from the provided input channels and makes them available in the respective local directories once it starts executing. The training script can then load the data from the local container directories.\nThere are more environment variables available, and you can read about them from Environment variables"
  },
  {
    "objectID": "posts/2022-08-05-sagemaker-feature-store.html#data-source",
    "href": "posts/2022-08-05-sagemaker-feature-store.html#data-source",
    "title": "Building a Feature Repository with SageMaker Feature Store",
    "section": "Data source",
    "text": "Data source\n[Moro et al., 2014] S. Moro, P. Cortez and P. Rita. A Data-Driven Approach to Predict the Success of Bank Telemarketing. Decision Support Systems, Elsevier, 62:22-31, June 2014"
  },
  {
    "objectID": "posts/2022-08-05-sagemaker-feature-store.html#data-link",
    "href": "posts/2022-08-05-sagemaker-feature-store.html#data-link",
    "title": "Building a Feature Repository with SageMaker Feature Store",
    "section": "Data link",
    "text": "Data link\nUCI Machine Learning Repository: https://archive.ics.uci.edu/ml/datasets/bank+marketing. Dataset has multiple files. we will use bank-additional-full.csv file that has all examples (41188) and 20 inputs, ordered by date."
  },
  {
    "objectID": "posts/2022-08-05-sagemaker-feature-store.html#data-classification-goaltarget",
    "href": "posts/2022-08-05-sagemaker-feature-store.html#data-classification-goaltarget",
    "title": "Building a Feature Repository with SageMaker Feature Store",
    "section": "Data classification goal/target",
    "text": "Data classification goal/target\nThe classification goal is to predict if the client will subscribe (yes/no) a term deposit (variable y)"
  },
  {
    "objectID": "posts/2022-08-05-sagemaker-feature-store.html#data-attributes-information",
    "href": "posts/2022-08-05-sagemaker-feature-store.html#data-attributes-information",
    "title": "Building a Feature Repository with SageMaker Feature Store",
    "section": "Data attributes information",
    "text": "Data attributes information\n\nInput variables\nattributes from bank client data 1. age (numeric) 2. job : type of job (categorical: ‘admin.’, ‘blue-collar’, ‘entrepreneur’, ‘housemaid’, ‘management’, ‘retired’, ‘self-employed’, ‘services’,‘student’, ‘technician’, ‘unemployed’, ‘unknown’) 3. marital : marital status (categorical: ‘divorced’, ‘married’, ‘single’, ‘unknown’; note: ‘divorced’ means divorced or widowed) 4. education (categorical: ‘basic.4y’, ‘basic.6y’, ‘basic.9y’, ‘high.school’, ‘illiterate’,‘professional.course’, ‘university.degree’,‘unknown’) 5. default: has credit in default? (categorical: ‘no’,‘yes’,‘unknown’) 6. housing: has housing loan? (categorical: ‘no’,‘yes’,‘unknown’) 7. loan: has personal loan? (categorical: ‘no’,‘yes’,‘unknown’)\nattributes related with the last contact of the current campaign\n\ncontact: contact communication type (categorical: ‘cellular’,‘telephone’)\nmonth: last contact month of year (categorical: ‘jan’, ‘feb’, ‘mar’, …, ‘nov’, ‘dec’)\nday_of_week: last contact day of the week (categorical: ‘mon’,‘tue’,‘wed’,‘thu’,‘fri’)\nduration: last contact duration, in seconds (numeric). Important note: this attribute highly affects the output target (e.g., if duration=0 then y=‘no’). Yet, the duration is not known before a call is performed. Also, after the end of the call y is obviously known. Thus, this input should only be included for benchmark purposes and should be discarded if the intention is to have a realistic predictive model.\n\nother attributes 12. campaign: number of contacts performed during this campaign and for this client (numeric, includes last contact) 13. pdays: number of days that passed by after the client was last contacted from a previous campaign (numeric; 999 means client was not previously contacted) 14. previous: number of contacts performed before this campaign and for this client (numeric) 15. poutcome: outcome of the previous marketing campaign (categorical: ‘failure’,‘nonexistent’,‘success’)\nsocial and economic context attributes 16. emp.var.rate: employment variation rate - quarterly indicator (numeric) 17. cons.price.idx: consumer price index - monthly indicator (numeric) 18. cons.conf.idx: consumer confidence index - monthly indicator (numeric) 19. euribor3m: euribor 3 month rate - daily indicator (numeric) 20. nr.employed: number of employees - quarterly indicator (numeric)\n\n\nOutput variable (desired target)\n\ny: has the client subscribed a term deposit? (binary: ‘yes’,‘no’)"
  },
  {
    "objectID": "posts/2022-08-05-sagemaker-feature-store.html#considerations-for-creating-a-feature-group",
    "href": "posts/2022-08-05-sagemaker-feature-store.html#considerations-for-creating-a-feature-group",
    "title": "Building a Feature Repository with SageMaker Feature Store",
    "section": "Considerations for creating a feature group",
    "text": "Considerations for creating a feature group\n\nSupported data types in feature group are: string, integral, and fractional\nThere should be a feature that can uniquely identify each row\nThere should be a feature that defines event time (event_time). This feature is required for versioning and time travel. Excepted data types for this feature are string or fractional.\n\nFor String type event time has to be ISO-8601 format in UTC time with the yyyy-MM-dd'T'HH:mm:ssZ or yyyy-MM-dd'T'HH:mm:ss.SSSZ patterns\nFor Fractional type, the values are expected to be in seconds from Unix epoch time with millisecond precision\n\n\nOur dataset does not have a feature that can uniquely identify each row. So let’s create one.\n\n##\n# 'FS_id' defines unique id for each row\ndf['FS_id'] = df.index\n\nSimilarly, we also need to create an event time feature. For this, we will use string type with yyyy-MM-dd'T'HH:mm:ss.SSSZ pattern.\n\nfrom datetime import datetime, timezone, date\n\ndef generate_event_timestamp():\n    # naive datetime representing local time\n    naive_dt = datetime.now()\n    # take timezone into account\n    aware_dt = naive_dt.astimezone()\n    # time in UTC\n    utc_dt = aware_dt.astimezone(timezone.utc)\n    # transform to ISO-8601 format\n    event_time = utc_dt.isoformat(timespec=\"milliseconds\")\n    event_time = event_time.replace(\"+00:00\", \"Z\")\n    return event_time\n\n\ngenerate_event_timestamp()\n\n'2022-08-08T06:06:07.059Z'\n\n\n\n##\n# `FS_event_time` contains event timestamps\ndf['FS_event_time'] = [generate_event_timestamp() for _ in range(len(df))]\n\nLet’s check our dataset with two new features.\n\ndf.head()\n\n\n\n\n\n  \n    \n      \n      age\n      job\n      marital\n      education\n      default\n      housing\n      loan\n      contact\n      month\n      day_of_week\n      duration\n      campaign\n      pdays\n      previous\n      poutcome\n      emp.var.rate\n      cons.price.idx\n      cons.conf.idx\n      euribor3m\n      nr.employed\n      y\n      FS_id\n      FS_event_time\n    \n  \n  \n    \n      0\n      56\n      housemaid\n      married\n      basic.4y\n      no\n      no\n      no\n      telephone\n      may\n      mon\n      261\n      1\n      999\n      0\n      nonexistent\n      1.1\n      93.994\n      -36.4\n      4.857\n      5191.0\n      no\n      0\n      2022-08-08T06:06:07.524Z\n    \n    \n      1\n      57\n      services\n      married\n      high.school\n      unknown\n      no\n      no\n      telephone\n      may\n      mon\n      149\n      1\n      999\n      0\n      nonexistent\n      1.1\n      93.994\n      -36.4\n      4.857\n      5191.0\n      no\n      1\n      2022-08-08T06:06:07.524Z\n    \n    \n      2\n      37\n      services\n      married\n      high.school\n      no\n      yes\n      no\n      telephone\n      may\n      mon\n      226\n      1\n      999\n      0\n      nonexistent\n      1.1\n      93.994\n      -36.4\n      4.857\n      5191.0\n      no\n      2\n      2022-08-08T06:06:07.524Z\n    \n    \n      3\n      40\n      admin.\n      married\n      basic.6y\n      no\n      no\n      no\n      telephone\n      may\n      mon\n      151\n      1\n      999\n      0\n      nonexistent\n      1.1\n      93.994\n      -36.4\n      4.857\n      5191.0\n      no\n      3\n      2022-08-08T06:06:07.524Z\n    \n    \n      4\n      56\n      services\n      married\n      high.school\n      no\n      no\n      yes\n      telephone\n      may\n      mon\n      307\n      1\n      999\n      0\n      nonexistent\n      1.1\n      93.994\n      -36.4\n      4.857\n      5191.0\n      no\n      4\n      2022-08-08T06:06:07.524Z\n    \n  \n\n\n\n\nInitialize SageMaker session.\n\nimport sagemaker\n\nsession = sagemaker.Session()\nrole = sagemaker.get_execution_role()\nbucket = session.default_bucket()\nregion = session.boto_region_name\n\nprint(\"sagemaker.__version__: \", sagemaker.__version__)\nprint(\"Session: \", session)\nprint(\"Role: \", role)\nprint(\"Bucket: \", bucket)\nprint(\"Region: \", region)\n\nsagemaker.__version__:  2.99.0\nSession:  <sagemaker.session.Session object at 0x7fb40934c890>\nRole:  arn:aws:iam::801598032724:role/service-role/AmazonSageMaker-ExecutionRole-20220804T174502\nBucket:  sagemaker-us-east-1-801598032724\nRegion:  us-east-1\n\n\n\n# You may choose any other prefix for your bucket.\n# All the data related to this post will be under this prefix.\nbucket_prefix = \"2022-08-05-sagemaker-feature-store\"\n\nFeature store requires an S3 location for storing the ingested data. Let’s define it as well.\n\nfs_offline_bucket_studio = f\"s3://{bucket}/{bucket_prefix}/fs_offline/studio\"\nfs_offline_bucket_studio\n\n's3://sagemaker-us-east-1-801598032724/2022-08-05-sagemaker-feature-store/fs_offline/studio'"
  },
  {
    "objectID": "posts/2022-08-05-sagemaker-feature-store.html#create-feature-group-from-sagemaker-studio-ide",
    "href": "posts/2022-08-05-sagemaker-feature-store.html#create-feature-group-from-sagemaker-studio-ide",
    "title": "Building a Feature Repository with SageMaker Feature Store",
    "section": "Create feature group from SageMaker studio IDE",
    "text": "Create feature group from SageMaker studio IDE\nLet’s see how we can create a feature group using SageMaker Studio IDE. You don’t need to write any code while creating a feature group using Studio IDE. From the left sidebar, use the SageMaker Resources menu to open the Feature Group pane, and click the create feature group option. This will open a new tab in IDE to create a feature group.\n\n\n\ncreate-feature-group.PNG\n\n\nOn the create feature group tab, define the following settings: - Feature group name : “bank-marketing-studio” - Description (optional) : “The data is related to direct marketing campaigns (phone calls) of a Portuguese banking institution.” - Feature group storage configurations - Enable online store : Check this box. Note that for the online store there is no S3 bucket requirement. - Enable offline store : Check this box too. - Enter S3 location from fs_offline_bucket_studio - IAM Role ARN : Default SageMaker role. - Enable Data Catalog for offline store - Select continue\nOn the next page, you will be asked to specify feature definitions. There are two ways to define them. - Using Table, and manually fill each feature and its type - Using JSON. We will use this option to define the features and their types.\nRemember that the feature group only supports three data types: string, integral, and fractional. So we need to create a mapping between Pandas Dataframe data types and that of a feature store. - “object” -> “String” - “int64” -> “Integral” - “float64” -> “Fractional”\n\n##\n# map DataFrame types to feature group.\ndef get_mapping(dt):\n\n    feature_store_dtype_mapping = {\n        \"object\": \"String\",\n        \"int64\": \"Integral\",\n        \"float64\": \"Fractional\",\n    }\n\n    return feature_store_dtype_mapping[str(dt)]\n\n\n##\n# DataFrame feature data types\ndf.dtypes\n\nage                 int64\njob                object\nmarital            object\neducation          object\ndefault            object\nhousing            object\nloan               object\ncontact            object\nmonth              object\nday_of_week        object\nduration            int64\ncampaign            int64\npdays               int64\nprevious            int64\npoutcome           object\nemp.var.rate      float64\ncons.price.idx    float64\ncons.conf.idx     float64\neuribor3m         float64\nnr.employed       float64\ny                  object\nFS_id               int64\nFS_event_time      object\ndtype: object\n\n\n\n##\n# prepare list of feature names and correct data types\nfeature_names = df.columns.tolist()\nfeature_types = [get_mapping(dt) for dt in df.dtypes]\n\nFeature names allow alphanumeric characters including dashes and underscores. So let’s remove the “.” character from the feature names.\n\n##\n# fix feature names\nfor indx in range(len(feature_names)):\n    feature_names[indx] = feature_names[indx].replace(\".\", \"_\")\n\n\n##\n# corrected feature names\nfeature_names\n\n['age',\n 'job',\n 'marital',\n 'education',\n 'default',\n 'housing',\n 'loan',\n 'contact',\n 'month',\n 'day_of_week',\n 'duration',\n 'campaign',\n 'pdays',\n 'previous',\n 'poutcome',\n 'emp_var_rate',\n 'cons_price_idx',\n 'cons_conf_idx',\n 'euribor3m',\n 'nr_employed',\n 'y',\n 'FS_id',\n 'FS_event_time']\n\n\nNow we are ready to prepare JSON for feature definitions. JSON created should be of the following format.\n[\n    {\n        \"FeatureName\": \"age\",\n        \"FeatureType\": \"Integral\"\n    }\n]\nLet’s prepare it.\n\ndf_features = pd.DataFrame({\"FeatureName\": feature_names, \"FeatureType\": feature_types})\n\nprint(df_features.to_json(orient=\"records\"))\n\n[{\"FeatureName\":\"age\",\"FeatureType\":\"Integral\"},{\"FeatureName\":\"job\",\"FeatureType\":\"String\"},{\"FeatureName\":\"marital\",\"FeatureType\":\"String\"},{\"FeatureName\":\"education\",\"FeatureType\":\"String\"},{\"FeatureName\":\"default\",\"FeatureType\":\"String\"},{\"FeatureName\":\"housing\",\"FeatureType\":\"String\"},{\"FeatureName\":\"loan\",\"FeatureType\":\"String\"},{\"FeatureName\":\"contact\",\"FeatureType\":\"String\"},{\"FeatureName\":\"month\",\"FeatureType\":\"String\"},{\"FeatureName\":\"day_of_week\",\"FeatureType\":\"String\"},{\"FeatureName\":\"duration\",\"FeatureType\":\"Integral\"},{\"FeatureName\":\"campaign\",\"FeatureType\":\"Integral\"},{\"FeatureName\":\"pdays\",\"FeatureType\":\"Integral\"},{\"FeatureName\":\"previous\",\"FeatureType\":\"Integral\"},{\"FeatureName\":\"poutcome\",\"FeatureType\":\"String\"},{\"FeatureName\":\"emp_var_rate\",\"FeatureType\":\"Fractional\"},{\"FeatureName\":\"cons_price_idx\",\"FeatureType\":\"Fractional\"},{\"FeatureName\":\"cons_conf_idx\",\"FeatureType\":\"Fractional\"},{\"FeatureName\":\"euribor3m\",\"FeatureType\":\"Fractional\"},{\"FeatureName\":\"nr_employed\",\"FeatureType\":\"Fractional\"},{\"FeatureName\":\"y\",\"FeatureType\":\"String\"},{\"FeatureName\":\"FS_id\",\"FeatureType\":\"Integral\"},{\"FeatureName\":\"FS_event_time\",\"FeatureType\":\"String\"}]\n\n\nCopy the JSON from the last cell output and past it in feature definition JSON input. Click continue\n\n\n\nfeature-definition.PNG\n\n\nOn the next page, it will ask for the required features. - record identifier feature name : select FS_ID - event type feature name : select FS_event_time\nClick continue and create the feature group.\n\n\n\nfeature-group-created.PNG"
  },
  {
    "objectID": "posts/2022-08-05-sagemaker-feature-store.html#create-feature-group-from-sagemaker-sdk",
    "href": "posts/2022-08-05-sagemaker-feature-store.html#create-feature-group-from-sagemaker-sdk",
    "title": "Building a Feature Repository with SageMaker Feature Store",
    "section": "Create feature group from SageMaker SDK",
    "text": "Create feature group from SageMaker SDK\nWe have seen how we can create a feature group from SageMaker studio IDE. Let’s also see how to create it using SageMaker SDK.\n\n##\n# define a feature group\nfrom sagemaker.feature_store.feature_group import FeatureGroup\n\nfeature_group_name = \"bank-marketing-sdk\"\nfeature_group = FeatureGroup(name=feature_group_name, sagemaker_session=session)\n\nI have created a FeatureGroup, now we need to define its schema (FeatureDefinitions). When I check the SageMaker Python SDK Feature Store APIs reference, I could not find any method to provide FeatureDefinitions to a feature group. But feature store documentation examples amazon_sagemaker_featurestore mention that we can use feature_group.load_feature_definitions() method to load the feature definitions from Pandas dataframe. When I checked the sagemaker-python-sdk GitHub page there is an open issue that says “The documentation does not include the load_feature_definitions() method for the FeatureGroup class”, and is still open.\nTo get more understanding of this method we can check the source code for sagemaker feature group class github.com/aws/sagemaker-python-sdk/blob/master/src/sagemaker/feature_store/feature_group.py. If we check the signature and documentation for this method it says:\ndef load_feature_definitions(\n        self,\n        data_frame: DataFrame,\n    ) -> Sequence[FeatureDefinition]:\n    \n        \"\"\"Load feature definitions from a Pandas DataFrame.\n        Column name is used as feature name. Feature type is inferred from the dtype\n        of the column. Dtype int_, int8, int16, int32, int64, uint8, uint16, uint32\n        and uint64 are mapped to Integral feature type. Dtype float_, float16, float32\n        and float64 are mapped to Fractional feature type. string dtype is mapped to\n        String feature type.\n        No feature definitions will be loaded if the given data_frame contains\n        unsupported dtypes.\n        Args:\n            data_frame (DataFrame):\n        Returns:\n            list of FeatureDefinition\n        \"\"\"\nThat is * It loads feature definitions from a Pandas DataFrame * DataFrame column names are used as feature names * Feature types are inferred from the dtype of columns * Dtype int_, int8, int16, int32, int64, uint8, uint16, uint32 and uint64 are mapped to Integral feature type * Dtype float_, float16, float32 and float64 are mapped to Fractional feature type * Dtype string is mapped to String feature type * No feature definitions will be loaded if the given data_frame contains unsupported dtypes\nIn the last section, we have seen that our dataframe has object data types that are not supported. For backward compatibility reasons, Pandas DataFrame infers columns with strings as object data type. With Pandas 1.0 onwards we can explicitly use string type for such columns.\nLet’s see what happens when we use unsupported data types for feature definition.\n\n##\n# load unsupported feature definitions. This will generate an error.\nfeature_group.load_feature_definitions(data_frame=df)\n\nValueError: Failed to infer Feature type based on dtype object for column job.\n\n\nIt throws an error, “ValueError: Failed to infer Feature type based on dtype object for column job.”\nOkay, let’s convet columns to proper data types.\n\n##\n# list of columns with `object` data type\ndf.select_dtypes(\"object\").columns.tolist()\n\n['job',\n 'marital',\n 'education',\n 'default',\n 'housing',\n 'loan',\n 'contact',\n 'month',\n 'day_of_week',\n 'poutcome',\n 'y',\n 'FS_event_time']\n\n\n\n##\n# covert `object` columns to `string` data type\nfor col in df.select_dtypes(\"object\").columns.tolist():\n    df[col] = df[col].astype(\"string\")\n\nLet’s verify the data types of all columns.\n\ndf.dtypes\n\nage                 int64\njob                string\nmarital            string\neducation          string\ndefault            string\nhousing            string\nloan               string\ncontact            string\nmonth              string\nday_of_week        string\nduration            int64\ncampaign            int64\npdays               int64\nprevious            int64\npoutcome           string\nemp.var.rate      float64\ncons.price.idx    float64\ncons.conf.idx     float64\neuribor3m         float64\nnr.employed       float64\ny                  string\nFS_id               int64\nFS_event_time      string\ndtype: object\n\n\n\ndf.head()\n\n\n\n\n\n  \n    \n      \n      age\n      job\n      marital\n      education\n      default\n      housing\n      loan\n      contact\n      month\n      day_of_week\n      duration\n      campaign\n      pdays\n      previous\n      poutcome\n      emp.var.rate\n      cons.price.idx\n      cons.conf.idx\n      euribor3m\n      nr.employed\n      y\n      FS_id\n      FS_event_time\n    \n  \n  \n    \n      0\n      56\n      housemaid\n      married\n      basic.4y\n      no\n      no\n      no\n      telephone\n      may\n      mon\n      261\n      1\n      999\n      0\n      nonexistent\n      1.1\n      93.994\n      -36.4\n      4.857\n      5191.0\n      no\n      0\n      2022-08-08T06:06:07.524Z\n    \n    \n      1\n      57\n      services\n      married\n      high.school\n      unknown\n      no\n      no\n      telephone\n      may\n      mon\n      149\n      1\n      999\n      0\n      nonexistent\n      1.1\n      93.994\n      -36.4\n      4.857\n      5191.0\n      no\n      1\n      2022-08-08T06:06:07.524Z\n    \n    \n      2\n      37\n      services\n      married\n      high.school\n      no\n      yes\n      no\n      telephone\n      may\n      mon\n      226\n      1\n      999\n      0\n      nonexistent\n      1.1\n      93.994\n      -36.4\n      4.857\n      5191.0\n      no\n      2\n      2022-08-08T06:06:07.524Z\n    \n    \n      3\n      40\n      admin.\n      married\n      basic.6y\n      no\n      no\n      no\n      telephone\n      may\n      mon\n      151\n      1\n      999\n      0\n      nonexistent\n      1.1\n      93.994\n      -36.4\n      4.857\n      5191.0\n      no\n      3\n      2022-08-08T06:06:07.524Z\n    \n    \n      4\n      56\n      services\n      married\n      high.school\n      no\n      no\n      yes\n      telephone\n      may\n      mon\n      307\n      1\n      999\n      0\n      nonexistent\n      1.1\n      93.994\n      -36.4\n      4.857\n      5191.0\n      no\n      4\n      2022-08-08T06:06:07.524Z\n    \n  \n\n\n\n\nLet’s load the feature definitions again.\n\nfeature_group.load_feature_definitions(data_frame=df)\n\n[FeatureDefinition(feature_name='age', feature_type=<FeatureTypeEnum.INTEGRAL: 'Integral'>),\n FeatureDefinition(feature_name='job', feature_type=<FeatureTypeEnum.STRING: 'String'>),\n FeatureDefinition(feature_name='marital', feature_type=<FeatureTypeEnum.STRING: 'String'>),\n FeatureDefinition(feature_name='education', feature_type=<FeatureTypeEnum.STRING: 'String'>),\n FeatureDefinition(feature_name='default', feature_type=<FeatureTypeEnum.STRING: 'String'>),\n FeatureDefinition(feature_name='housing', feature_type=<FeatureTypeEnum.STRING: 'String'>),\n FeatureDefinition(feature_name='loan', feature_type=<FeatureTypeEnum.STRING: 'String'>),\n FeatureDefinition(feature_name='contact', feature_type=<FeatureTypeEnum.STRING: 'String'>),\n FeatureDefinition(feature_name='month', feature_type=<FeatureTypeEnum.STRING: 'String'>),\n FeatureDefinition(feature_name='day_of_week', feature_type=<FeatureTypeEnum.STRING: 'String'>),\n FeatureDefinition(feature_name='duration', feature_type=<FeatureTypeEnum.INTEGRAL: 'Integral'>),\n FeatureDefinition(feature_name='campaign', feature_type=<FeatureTypeEnum.INTEGRAL: 'Integral'>),\n FeatureDefinition(feature_name='pdays', feature_type=<FeatureTypeEnum.INTEGRAL: 'Integral'>),\n FeatureDefinition(feature_name='previous', feature_type=<FeatureTypeEnum.INTEGRAL: 'Integral'>),\n FeatureDefinition(feature_name='poutcome', feature_type=<FeatureTypeEnum.STRING: 'String'>),\n FeatureDefinition(feature_name='emp.var.rate', feature_type=<FeatureTypeEnum.FRACTIONAL: 'Fractional'>),\n FeatureDefinition(feature_name='cons.price.idx', feature_type=<FeatureTypeEnum.FRACTIONAL: 'Fractional'>),\n FeatureDefinition(feature_name='cons.conf.idx', feature_type=<FeatureTypeEnum.FRACTIONAL: 'Fractional'>),\n FeatureDefinition(feature_name='euribor3m', feature_type=<FeatureTypeEnum.FRACTIONAL: 'Fractional'>),\n FeatureDefinition(feature_name='nr.employed', feature_type=<FeatureTypeEnum.FRACTIONAL: 'Fractional'>),\n FeatureDefinition(feature_name='y', feature_type=<FeatureTypeEnum.STRING: 'String'>),\n FeatureDefinition(feature_name='FS_id', feature_type=<FeatureTypeEnum.INTEGRAL: 'Integral'>),\n FeatureDefinition(feature_name='FS_event_time', feature_type=<FeatureTypeEnum.STRING: 'String'>)]\n\n\nWe have defined our feature group and its feature definitions, but it has not been created. To create it we need to call create method on the feature group. For this let’s define s3 URI for our feature store offline data storage.\n\nfs_offline_bucket_sdk = f\"s3://{bucket}/{bucket_prefix}/fs_offline/sdk\"\nfs_offline_bucket_sdk\n\n's3://sagemaker-us-east-1-801598032724/2022-08-05-sagemaker-feature-store/fs_offline/sdk'\n\n\n\n##\n# Now create feature group\nrecord_identifier_name = \"FS_id\"\nevent_time_feature_name = \"FS_event_time\"\ndescription = \"The data is related with direct marketing campaigns (phone calls) of a Portuguese banking institution\"\n\nfeature_group.create(\n    record_identifier_name=record_identifier_name,\n    event_time_feature_name=event_time_feature_name,\n    enable_online_store=True,\n    s3_uri=fs_offline_bucket_sdk,\n    role_arn=role,\n    description=description,\n)\n\nClientError: An error occurred (ValidationException) when calling the CreateFeatureGroup operation: 4 validation errors detected: Value 'emp.var.rate' at 'featureDefinitions.16.member.featureName' failed to satisfy constraint: Member must satisfy regular expression pattern: ^[a-zA-Z0-9]([-_]*[a-zA-Z0-9]){0,63}; Value 'cons.price.idx' at 'featureDefinitions.17.member.featureName' failed to satisfy constraint: Member must satisfy regular expression pattern: ^[a-zA-Z0-9]([-_]*[a-zA-Z0-9]){0,63}; Value 'cons.conf.idx' at 'featureDefinitions.18.member.featureName' failed to satisfy constraint: Member must satisfy regular expression pattern: ^[a-zA-Z0-9]([-_]*[a-zA-Z0-9]){0,63}; Value 'nr.employed' at 'featureDefinitions.20.member.featureName' failed to satisfy constraint: Member must satisfy regular expression pattern: ^[a-zA-Z0-9]([-_]*[a-zA-Z0-9]){0,63}\n\n\nWe got an error as we have not fixed feature names. Error is saying that the feature name should satisfy the regular expression pattern: ^[a-zA-Z0-9]([-_]*[a-zA-Z0-9]){0,63}. Let’s fix our column names.\n\ncol_names = df.columns.tolist()\nfor idx in range(len(col_names)):\n    col_names[idx] = col_names[idx].replace(\".\", \"_\")\n\ndf.columns = col_names\ndf.head()\n\n\n\n\n\n  \n    \n      \n      age\n      job\n      marital\n      education\n      default\n      housing\n      loan\n      contact\n      month\n      day_of_week\n      duration\n      campaign\n      pdays\n      previous\n      poutcome\n      emp_var_rate\n      cons_price_idx\n      cons_conf_idx\n      euribor3m\n      nr_employed\n      y\n      FS_id\n      FS_event_time\n    \n  \n  \n    \n      0\n      56\n      housemaid\n      married\n      basic.4y\n      no\n      no\n      no\n      telephone\n      may\n      mon\n      261\n      1\n      999\n      0\n      nonexistent\n      1.1\n      93.994\n      -36.4\n      4.857\n      5191.0\n      no\n      0\n      2022-08-08T06:06:07.524Z\n    \n    \n      1\n      57\n      services\n      married\n      high.school\n      unknown\n      no\n      no\n      telephone\n      may\n      mon\n      149\n      1\n      999\n      0\n      nonexistent\n      1.1\n      93.994\n      -36.4\n      4.857\n      5191.0\n      no\n      1\n      2022-08-08T06:06:07.524Z\n    \n    \n      2\n      37\n      services\n      married\n      high.school\n      no\n      yes\n      no\n      telephone\n      may\n      mon\n      226\n      1\n      999\n      0\n      nonexistent\n      1.1\n      93.994\n      -36.4\n      4.857\n      5191.0\n      no\n      2\n      2022-08-08T06:06:07.524Z\n    \n    \n      3\n      40\n      admin.\n      married\n      basic.6y\n      no\n      no\n      no\n      telephone\n      may\n      mon\n      151\n      1\n      999\n      0\n      nonexistent\n      1.1\n      93.994\n      -36.4\n      4.857\n      5191.0\n      no\n      3\n      2022-08-08T06:06:07.524Z\n    \n    \n      4\n      56\n      services\n      married\n      high.school\n      no\n      no\n      yes\n      telephone\n      may\n      mon\n      307\n      1\n      999\n      0\n      nonexistent\n      1.1\n      93.994\n      -36.4\n      4.857\n      5191.0\n      no\n      4\n      2022-08-08T06:06:07.524Z\n    \n  \n\n\n\n\nAfter updating feature names, load the feature group definitions again.\n\nfeature_group.load_feature_definitions(data_frame=df)\n\n[FeatureDefinition(feature_name='age', feature_type=<FeatureTypeEnum.INTEGRAL: 'Integral'>),\n FeatureDefinition(feature_name='job', feature_type=<FeatureTypeEnum.STRING: 'String'>),\n FeatureDefinition(feature_name='marital', feature_type=<FeatureTypeEnum.STRING: 'String'>),\n FeatureDefinition(feature_name='education', feature_type=<FeatureTypeEnum.STRING: 'String'>),\n FeatureDefinition(feature_name='default', feature_type=<FeatureTypeEnum.STRING: 'String'>),\n FeatureDefinition(feature_name='housing', feature_type=<FeatureTypeEnum.STRING: 'String'>),\n FeatureDefinition(feature_name='loan', feature_type=<FeatureTypeEnum.STRING: 'String'>),\n FeatureDefinition(feature_name='contact', feature_type=<FeatureTypeEnum.STRING: 'String'>),\n FeatureDefinition(feature_name='month', feature_type=<FeatureTypeEnum.STRING: 'String'>),\n FeatureDefinition(feature_name='day_of_week', feature_type=<FeatureTypeEnum.STRING: 'String'>),\n FeatureDefinition(feature_name='duration', feature_type=<FeatureTypeEnum.INTEGRAL: 'Integral'>),\n FeatureDefinition(feature_name='campaign', feature_type=<FeatureTypeEnum.INTEGRAL: 'Integral'>),\n FeatureDefinition(feature_name='pdays', feature_type=<FeatureTypeEnum.INTEGRAL: 'Integral'>),\n FeatureDefinition(feature_name='previous', feature_type=<FeatureTypeEnum.INTEGRAL: 'Integral'>),\n FeatureDefinition(feature_name='poutcome', feature_type=<FeatureTypeEnum.STRING: 'String'>),\n FeatureDefinition(feature_name='emp_var_rate', feature_type=<FeatureTypeEnum.FRACTIONAL: 'Fractional'>),\n FeatureDefinition(feature_name='cons_price_idx', feature_type=<FeatureTypeEnum.FRACTIONAL: 'Fractional'>),\n FeatureDefinition(feature_name='cons_conf_idx', feature_type=<FeatureTypeEnum.FRACTIONAL: 'Fractional'>),\n FeatureDefinition(feature_name='euribor3m', feature_type=<FeatureTypeEnum.FRACTIONAL: 'Fractional'>),\n FeatureDefinition(feature_name='nr_employed', feature_type=<FeatureTypeEnum.FRACTIONAL: 'Fractional'>),\n FeatureDefinition(feature_name='y', feature_type=<FeatureTypeEnum.STRING: 'String'>),\n FeatureDefinition(feature_name='FS_id', feature_type=<FeatureTypeEnum.INTEGRAL: 'Integral'>),\n FeatureDefinition(feature_name='FS_event_time', feature_type=<FeatureTypeEnum.STRING: 'String'>)]\n\n\nNow create the feature group.\n\n##\n# create feature group\nfeature_group.create(\n    record_identifier_name=record_identifier_name,\n    event_time_feature_name=event_time_feature_name,\n    enable_online_store=True,\n    s3_uri=fs_offline_bucket_sdk,\n    role_arn=role,\n    description=description,\n)\n\n{'FeatureGroupArn': 'arn:aws:sagemaker:us-east-1:801598032724:feature-group/bank-marketing-sdk',\n 'ResponseMetadata': {'RequestId': '5c2afeb1-fa03-442b-a3ee-80b1b0ae1069',\n  'HTTPStatusCode': 200,\n  'HTTPHeaders': {'x-amzn-requestid': '5c2afeb1-fa03-442b-a3ee-80b1b0ae1069',\n   'content-type': 'application/x-amz-json-1.1',\n   'content-length': '95',\n   'date': 'Mon, 08 Aug 2022 06:06:32 GMT'},\n  'RetryAttempts': 0}}\n\n\nFeature group creation is an async method, and you need to wait for its creation before ingesting any data into it. For this you can use feature_group.describe method to get feature store creation status.\n\nfeature_group.describe().get('FeatureGroupStatus')\n\n'Creating'\n\n\nWe can create a wrapper function around this method to wait till the feature group is ready.\n\nimport time\n\ndef wait_for_feature_group_creation_complete(feature_group):\n    status = feature_group.describe().get(\"FeatureGroupStatus\")\n    print(f\"Initial status: {status}\")\n    while status == \"Creating\":\n        print(f\"Waiting for feature group: {feature_group.name} to be created ...\")\n        time.sleep(5)\n        status = feature_group.describe().get(\"FeatureGroupStatus\")\n\n    print(f\"FeatureGroup {feature_group.name} was successfully created.\")\n\n\nwait_for_feature_group_creation_complete(feature_group)\n\nInitial status: Creating\nWaiting for feature group: bank-marketing-sdk to be created ...\nWaiting for feature group: bank-marketing-sdk to be created ...\nWaiting for feature group: bank-marketing-sdk to be created ...\nWaiting for feature group: bank-marketing-sdk to be created ...\nFeatureGroup bank-marketing-sdk was successfully created."
  },
  {
    "objectID": "posts/2022-08-05-sagemaker-feature-store.html#accessing-online-feature-store-from-sdk",
    "href": "posts/2022-08-05-sagemaker-feature-store.html#accessing-online-feature-store-from-sdk",
    "title": "Building a Feature Repository with SageMaker Feature Store",
    "section": "Accessing online feature store from SDK",
    "text": "Accessing online feature store from SDK\nBoto3 SDK sagemaker-featurestore-runtime allows us to interact with the online feature store. These are the available methods:\n\nbatch_get_record()\ncan_paginate()\nclose()\ndelete_record()\nget_paginator()\nget_record()\nget_waiter()\nput_record()\n\nTo read more about them use Boto3 SageMakerFeatureStoreRuntime documentation.\n\nfeaturestore_runtime_client = session.boto_session.client(\n    \"sagemaker-featurestore-runtime\", region_name=region\n)\n\n\n##\n# select any random id to query online store\nsample_feature_id = str(df.sample().index.values[0])\nsample_feature_id\n\n'37156'\n\n\nNow query the online store.\n\n%%timeit\n\nfeaturestore_runtime_client.get_record(FeatureGroupName=feature_group_name, \n                                                        RecordIdentifierValueAsString=sample_feature_id)\n\n8.37 ms ± 238 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n\n\n\nfeature_record = featurestore_runtime_client.get_record(\n    FeatureGroupName=feature_group_name, RecordIdentifierValueAsString=sample_feature_id\n)\n\nfeature_record\n\n{'ResponseMetadata': {'RequestId': '946376b7-7745-4b25-9885-e93ba7a284a5',\n  'HTTPStatusCode': 200,\n  'HTTPHeaders': {'x-amzn-requestid': '946376b7-7745-4b25-9885-e93ba7a284a5',\n   'content-type': 'application/json',\n   'content-length': '1189',\n   'date': 'Mon, 08 Aug 2022 06:08:49 GMT'},\n  'RetryAttempts': 0},\n 'Record': [{'FeatureName': 'age', 'ValueAsString': '39'},\n  {'FeatureName': 'job', 'ValueAsString': 'blue-collar'},\n  {'FeatureName': 'marital', 'ValueAsString': 'married'},\n  {'FeatureName': 'education', 'ValueAsString': 'basic.9y'},\n  {'FeatureName': 'default', 'ValueAsString': 'no'},\n  {'FeatureName': 'housing', 'ValueAsString': 'no'},\n  {'FeatureName': 'loan', 'ValueAsString': 'no'},\n  {'FeatureName': 'contact', 'ValueAsString': 'cellular'},\n  {'FeatureName': 'month', 'ValueAsString': 'aug'},\n  {'FeatureName': 'day_of_week', 'ValueAsString': 'wed'},\n  {'FeatureName': 'duration', 'ValueAsString': '394'},\n  {'FeatureName': 'campaign', 'ValueAsString': '1'},\n  {'FeatureName': 'pdays', 'ValueAsString': '999'},\n  {'FeatureName': 'previous', 'ValueAsString': '0'},\n  {'FeatureName': 'poutcome', 'ValueAsString': 'nonexistent'},\n  {'FeatureName': 'emp_var_rate', 'ValueAsString': '-2.9'},\n  {'FeatureName': 'cons_price_idx', 'ValueAsString': '92.201'},\n  {'FeatureName': 'cons_conf_idx', 'ValueAsString': '-31.4'},\n  {'FeatureName': 'euribor3m', 'ValueAsString': '0.884'},\n  {'FeatureName': 'nr_employed', 'ValueAsString': '5076.2'},\n  {'FeatureName': 'y', 'ValueAsString': 'yes'},\n  {'FeatureName': 'FS_id', 'ValueAsString': '37156'},\n  {'FeatureName': 'FS_event_time',\n   'ValueAsString': '2022-08-08T06:06:07.834Z'}]}"
  },
  {
    "objectID": "posts/2022-08-05-sagemaker-feature-store.html#accessing-offline-feature-store-from-sdk",
    "href": "posts/2022-08-05-sagemaker-feature-store.html#accessing-offline-feature-store-from-sdk",
    "title": "Building a Feature Repository with SageMaker Feature Store",
    "section": "Accessing offline feature store from SDK",
    "text": "Accessing offline feature store from SDK\nLet’s query the offline store to get the same data. For offline feature storage, SageMaker stages the data in S3 bucket and creates AWS Data Catalog on it. This catalog is registered in AWS Athena and we can use Athena APIs to query offline store.\n\nquery = feature_group.athena_query()\nquery\n\nAthenaQuery(catalog='AwsDataCatalog', database='sagemaker_featurestore', table_name='bank-marketing-sdk-1659938792', sagemaker_session=<sagemaker.session.Session object at 0x7fb40934c890>, _current_query_execution_id=None, _result_bucket=None, _result_file_prefix=None)\n\n\n\ntable_name = query.table_name\ntable_name\n\n'bank-marketing-sdk-1659938792'\n\n\n\nquery_string = f'SELECT * FROM \"{table_name}\" WHERE FS_id = {sample_feature_id}'\nquery_string\n\n'SELECT * FROM \"bank-marketing-sdk-1659938792\" WHERE FS_id = 37156'\n\n\n\n%%timeit\nquery.run(query_string=query_string,output_location=f's3://{bucket}/{bucket_prefix}/query_results/')\nquery.wait()\n\n5.21 s ± 29.4 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n\n\nNotice that the offline store has taken a much longer time to return the results compared to the online store.\n\ndataset = query.as_dataframe()\ndataset.head()\n\n\n\n\n\n  \n    \n      \n      age\n      job\n      marital\n      education\n      default\n      housing\n      loan\n      contact\n      month\n      day_of_week\n      duration\n      campaign\n      pdays\n      previous\n      poutcome\n      emp_var_rate\n      cons_price_idx\n      cons_conf_idx\n      euribor3m\n      nr_employed\n      y\n      fs_id\n      fs_event_time\n      write_time\n      api_invocation_time\n      is_deleted\n    \n  \n  \n  \n\n\n\n\nData in an online store becomes available immediately but it can take a few minutes to become available in an offline store. That is why we have not received any data in the last cell. Let’s run the same query again after a few minutes.\n\n##\n# run query again\nquery.run(query_string=query_string,output_location=f's3://{bucket}/{bucket_prefix}/query_results/')\nquery.wait()\n\n# get query response\ndataset = query.as_dataframe()\ndataset.head()\n\n\n\n\n\n  \n    \n      \n      age\n      job\n      marital\n      education\n      default\n      housing\n      loan\n      contact\n      month\n      day_of_week\n      duration\n      campaign\n      pdays\n      previous\n      poutcome\n      emp_var_rate\n      cons_price_idx\n      cons_conf_idx\n      euribor3m\n      nr_employed\n      y\n      fs_id\n      fs_event_time\n      write_time\n      api_invocation_time\n      is_deleted\n    \n  \n  \n    \n      0\n      39\n      blue-collar\n      married\n      basic.9y\n      no\n      no\n      no\n      cellular\n      aug\n      wed\n      394\n      1\n      999\n      0\n      nonexistent\n      -2.9\n      92.201\n      -31.4\n      0.884\n      5076.2\n      yes\n      37156\n      2022-08-08T06:06:07.834Z\n      2022-08-08 06:13:03.665\n      2022-08-08 06:07:43.000\n      False"
  },
  {
    "objectID": "posts/2022-08-05-sagemaker-feature-store.html#accessing-offline-store-from-athena",
    "href": "posts/2022-08-05-sagemaker-feature-store.html#accessing-offline-store-from-athena",
    "title": "Building a Feature Repository with SageMaker Feature Store",
    "section": "Accessing offline store from Athena",
    "text": "Accessing offline store from Athena\nThis time lets query the offline feature store directly from AWS Athena service.\n\n\n\nfeature_store_athena.PNG"
  },
  {
    "objectID": "posts/2022-08-10-sagemaker-fastai-classifier.html",
    "href": "posts/2022-08-10-sagemaker-fastai-classifier.html",
    "title": "Train an Image Classifier using Fastai (Deep Dive Analysis)",
    "section": "",
    "text": "Introduction\nIn this notebook, we will build an image classifier to categorize images for different types of balls, including cricket, tennis, basketball, and soccer. Data is collected from the DuckDuckGo search engine. Unlike a getting started tutorial, we will take an alternate approach. Our focus is NOT to quickly train a model/prototype with fastai library. It may take only 5 lines of code to prepare an image classifier using this library. Instead, let’s aim to understand each line and function involved in the process. We have a lot to uncover, so let’s get started.\n\n\nEnvironment\nThis notebook is prepared using Amazon SageMaker studio’s Python 3 (PyTorch 1.10 Python 3.8 GPU Optimized) kernel running on the ml.t3.medium instance. I have also tried other images, including Data Science and Data Science 2.0, but fastai does not work smoothly with them. ml.t3.medium does not have any GPU attached to it but somehow Python 3 (PyTorch 1.10 Python 3.8 CPU Optimized) kernel was giving slow performance compare to GPU optimized.\nYou may run this notebook on any other system without any issue.\n\n!aws --version\n\naws-cli/1.22.68 Python/3.8.10 Linux/4.14.287-215.504.amzn2.x86_64 botocore/1.24.13\n\n\n\n!python3 --version\n\nPython 3.8.10\n\n\n\n\nFastai setup\nIntall fastai library. I have used IPython magic cell %%capture to capture and discard output of this cell. You may read more about this magic command from Capturing Output With %%capture\n\n%%capture\n!pip install -U fastai\n\nNext, I will install the library “duckduckgo_search” that can be used to “Search for words, documents, images, news, maps and text translation using the DuckDuckGo.com search engine.”\n\n%%capture\n!pip install -Uqq duckduckgo_search\n\nChecking the version of the installed fastai library.\n\nimport fastai\n\nfastai.__version__\n\n'2.7.9'\n\n\n\n\nPrepare data set\nWe will download training data images from DuckDuckGo search engine. The following function uses the search engine to “query” for an image and returns searched image URLs.\n\n##\n# Step: Define a function to search images and return URLs\n# Use 'duckduckgo' search engine to find image URLs\n\nfrom duckduckgo_search import ddg_images\nfrom fastcore.all import *\n\ndef search_images(term, max_images=200):\n    print(f\"Searching for '{term}'\")\n    return L(ddg_images(term, max_results=max_images)).itemgot('image')\n\nNow let’s define the search strings to download images for tennis, cricket, soccer, and basketball balls.\n\n##\n# Step: Define the search strings to find images on the Internet\n\nsearches = {\n    \"tennis\": \"tennis ball photo\",\n    \"cricket\": \"cricket hard ball photo\",\n    \"soccer\": \"soccer ball photo\",\n    \"basketball\": \"basketball ball photos\",\n}\n\nsearches\n\n{'tennis': 'tennis ball photo',\n 'cricket': 'cricket hard ball photo',\n 'soccer': 'soccer ball photo',\n 'basketball': 'basketball ball photos'}\n\n\nSearch and get the URL for a tennis ball image.\n\n##\n# Step: Search an image URL for tennis balls\n\nurls = search_images(searches['tennis'], max_images=1)\nurls[0]\n\nSearching for 'tennis ball photo'\n\n\n'https://www.thoughtco.com/thmb/rC73Tl0nBlYStXTVXrCRAnhPaq8=/3888x2592/filters:fill(auto,1)/tennis-ball-on-tennis-court-125847528-58db9de83df78c5162dba2ee.jpg'\n\n\nDefine a local path to store all the downloaded images.\n\n##\n# Step: Define a local path that will be root directory for this project\n# All the artifacts related to this post will be stored under this folder\n \nlocal_path = \"./datasets/2022-08-10-sagemaker-fastai-classifier\"\n\nDownload a sample image and view it.\n\n##\n# Step: Download an image using its URL, and show it as a thumbnail\n\nfrom fastai.vision.all import *\nfrom fastdownload import download_url\n\ndest = f'{local_path}/sample/tennis_ball.jpg'\ndownload_url(urls[0], dest, show_progress=True)\n\nim = Image.open(dest)\nim.to_thumb(256,256)\n\n/opt/conda/lib/python3.8/site-packages/torchvision/io/image.py:11: UserWarning: Failed to load image Python extension: \n  warn(f\"Failed to load image Python extension: {e}\")\n\n\n\n\n\n\n\n    \n      \n      100.57% [1015808/1010004 00:00<00:00]\n    \n    \n\n\n\n\n\nWe have now downloaded a sample image from the Internet using the DuckDuckGo search engine and shown it as a thumbnail. For this, we have also used a few functions. So let’s deconstruct them to understand them better.\nThe first function we have used is download_url. Let’s check its documentation.\n\n#collapse_output\n??download_url\n\n\nSignature: download_url(url, dest=None, timeout=None, show_progress=True)\nSource:   \ndef download_url(url, dest=None, timeout=None, show_progress=True):\n    \"Download `url` to `dest` and show progress\"\n    pbar = progress_bar([])\n    def progress(count=1, bsize=1, tsize=None):\n        pbar.total = tsize\n        pbar.update(count*bsize)\n    return urlsave(url, dest, reporthook=progress if show_progress else None, timeout=timeout)\nFile:      /opt/conda/lib/python3.8/site-packages/fastdownload/core.py\nType:      function\n\n\n\n\nThis tells us that download_url function is from fastdownload library. This library is also published by the fastai team. Its purpose is “Easily download, verify, and extract archives”. You may read more about this library from its documentation site fastdownload.fast.ai\nThe next function we have used is Image.open. Let’s check its documentation too.\n\n#collapse_output\n?Image.open\n\n\nSignature: Image.open(fp, mode='r', formats=None)\nDocstring:\nOpens and identifies the given image file.\nThis is a lazy operation; this function identifies the file, but\nthe file remains open and the actual image data is not read from\nthe file until you try to process the data (or call the\n:py:meth:`~PIL.Image.Image.load` method).  See\n:py:func:`~PIL.Image.new`. See :ref:`file-handling`.\n:param fp: A filename (string), pathlib.Path object or a file object.\n   The file object must implement ``file.read``,\n   ``file.seek``, and ``file.tell`` methods,\n   and be opened in binary mode.\n:param mode: The mode.  If given, this argument must be \"r\".\n:param formats: A list or tuple of formats to attempt to load the file in.\n   This can be used to restrict the set of formats checked.\n   Pass ``None`` to try all supported formats. You can print the set of\n   available formats by running ``python3 -m PIL`` or using\n   the :py:func:`PIL.features.pilinfo` function.\n:returns: An :py:class:`~PIL.Image.Image` object.\n:exception FileNotFoundError: If the file cannot be found.\n:exception PIL.UnidentifiedImageError: If the image cannot be opened and\n   identified.\n:exception ValueError: If the ``mode`` is not \"r\", or if a ``StringIO``\n   instance is used for ``fp``.\n:exception TypeError: If ``formats`` is not ``None``, a list or a tuple.\nFile:      /opt/conda/lib/python3.8/site-packages/PIL/Image.py\nType:      function\n\n\n\n\nIt tells us that this function is from Python Pillow library and is used to open and read an image file. Refer to this library documentation for more information python-pillow.org\nThe next function that we used is Image.to_thumb. Let’s check its documentation.\n\n#collapse_output\n??Image.Image.to_thumb\n\n\nSignature: Image.Image.to_thumb(self: 'Image.Image', h, w=None)\nSource:   \n@patch\ndef to_thumb(self:Image.Image, h, w=None):\n    \"Same as `thumbnail`, but uses a copy\"\n    if w is None: w=h\n    im = self.copy()\n    im.thumbnail((w,h))\n    return im\nFile:      /opt/conda/lib/python3.8/site-packages/fastai/vision/core.py\nType:      function\n\n\n\n\nSo this function is actually from the fastai library, and its docstring tells us that it is the same as the Pillow library thumbnail function but uses a copy of the image.\nLet’s check Pillow thumbnail function documentation as well.\n\n#collapse_output\n?Image.Image.thumbnail\n\n\nSignature: Image.Image.thumbnail(self, size, resample=3, reducing_gap=2.0)\nDocstring:\nMake this image into a thumbnail.  This method modifies the\nimage to contain a thumbnail version of itself, no larger than\nthe given size.  This method calculates an appropriate thumbnail\nsize to preserve the aspect of the image, calls the\n:py:meth:`~PIL.Image.Image.draft` method to configure the file reader\n(where applicable), and finally resizes the image.\nNote that this function modifies the :py:class:`~PIL.Image.Image`\nobject in place.  If you need to use the full resolution image as well,\napply this method to a :py:meth:`~PIL.Image.Image.copy` of the original\nimage.\n:param size: Requested size.\n:param resample: Optional resampling filter.  This can be one\n   of :py:data:`PIL.Image.NEAREST`, :py:data:`PIL.Image.BOX`,\n   :py:data:`PIL.Image.BILINEAR`, :py:data:`PIL.Image.HAMMING`,\n   :py:data:`PIL.Image.BICUBIC` or :py:data:`PIL.Image.LANCZOS`.\n   If omitted, it defaults to :py:data:`PIL.Image.BICUBIC`.\n   (was :py:data:`PIL.Image.NEAREST` prior to version 2.5.0).\n   See: :ref:`concept-filters`.\n:param reducing_gap: Apply optimization by resizing the image\n   in two steps. First, reducing the image by integer times\n   using :py:meth:`~PIL.Image.Image.reduce` or\n   :py:meth:`~PIL.Image.Image.draft` for JPEG images.\n   Second, resizing using regular resampling. The last step\n   changes size no less than by ``reducing_gap`` times.\n   ``reducing_gap`` may be None (no first step is performed)\n   or should be greater than 1.0. The bigger ``reducing_gap``,\n   the closer the result to the fair resampling.\n   The smaller ``reducing_gap``, the faster resizing.\n   With ``reducing_gap`` greater or equal to 3.0, the result is\n   indistinguishable from fair resampling in most cases.\n   The default value is 2.0 (very close to fair resampling\n   while still being faster in many cases).\n:returns: None\nFile:      /opt/conda/lib/python3.8/site-packages/PIL/Image.py\nType:      function\n\n\n\n\nSummary of the functions used till now\nFunctions we have seen till now are summarized in this section.\n\ndownload_url: Download url to dest and show progress. This function is from module fastdownload\\core.py. Fastdownload is a separate library from the fastai team, and its use is to “Easily download, verify, and extract archives”.\n\nDocumentation: fastdownload.fast.ai\nSource code: fastai/fastdownload\n\nImage.open: Opens and identifies the given image file. This is a lazy operation; this function identifies the file, but the file remains open, and the actual image data is not read from the file until you try to process the data. this function is from pil\\image.py or Python Pillow Image library. Fastai installs this library for us. Fastai vision module internally loads Pillow for us. It has defined wrapper functions to make it easier to use this library in machine learning work.\n\nCheck fastai dependencies: settings.ini It contains external dependent libraries that fastai installs for us. Some important libraries from that list include\n\nfastdownload\nfastcore\nfastprogress\nmatplotlib\npandas\npillow\nscikit-learn\npytorch\n\nPillow image library documentation: https://pillow.readthedocs.io/en/stable/index.html\nImage.open documentation link: PIL.Image.open\n\nImage.to_thumb: Same as thumbnail, but uses a copy. This function is from fastai\\vision\\core.py module. It is a wrapper function around the PIL thumbnail function.\n\nSource code: 07_vision.core.ipynb\n\nImage.thumbnail: Make this image into a thumbnail. This method modifies the image to contain a thumbnail version of itself, no larger than the given size. This method calculates an appropriate thumbnail size to preserve the image’s aspect, calls the draft() method to configure the file reader (where applicable), and finally resizes the image. Note that this function modifies the Image object in place. If you need to use the full resolution image, apply this method to a copy() of the original image. This function is from pil\\image.py\n\nDocumentation: PIL.Image.Image.thumbnail\n\n\nSummary of the steps performed till now\n\nDefined a function to search images and return their URLs\nDefined the search strings to find images on the Internet\nDownloaded an image using its URL and showed it as a thumbnail\n\nLet’s proceed with our work. First, define a filesystem “images” path where files from the search engine will be downloaded.\n\n##\n# Step: Define a folder path where downloaded images will be stored\n\npath = Path(f'{local_path}/images')\n\nWe have used a Path class so let’s check its documentation.\n\n#collapse_output\n?Path\n\n\nInit signature: Path(*args, **kwargs)\nDocstring:     \nPurePath subclass that can make system calls.\nPath represents a filesystem path but unlike PurePath, also offers\nmethods to do system calls on path objects. Depending on your system,\ninstantiating a Path will return either a PosixPath or a WindowsPath\nobject. You can also instantiate a PosixPath or WindowsPath directly,\nbut cannot instantiate a WindowsPath on a POSIX system or vice versa.\nFile:           /opt/conda/lib/python3.8/pathlib.py\nType:           type\nSubclasses:     PosixPath, WindowsPath\n\n\n\n\nFrom the documentation, we find that the Path class is from Python standard library module pathlib. Python’s official documentation page for this module is pathlib.html. We have not loaded this module, so it begs the question, who loaded it for us? When I checked the fastai code repository, I found a file named fastai/imports.py. This file has many imports defined in it, and fastai is loading it for us behind the scenes. This imports.py file is loaded in many core fastai modules (data, vision, tabular, etc.). A typical loading sequence is as follows\n\nfrom fast.vision.all import * loads fastai.vision.core.py\n\nsource code for fast.vision.all\n\nfastai.vision.core.py loads fastai.torch_basics.py\n\nsource code for fastai.vision.core.py\n\nfastai.torch_basics.py loads fastai.imports.py\n\nsource code for fastai.torch_basics.py\n\nfastai.imports.py loads from pathlib import Path\n\nsource code for fastai.imports.py\n\n\nOkay, we have understood how Path library is loaded implicitly for us. So let’s continue with our work and download images using the search strings to the Path folder.\n\n##\n# Step: Use each search string to search and download images\n\nfor key, value in searches.items():\n    dest = (path/key)\n    dest.mkdir(exist_ok=True, parents=True)\n    download_images(dest, urls=search_images(value))\n    resize_images(path/key, max_size=400, dest=path/key)\n\n/opt/conda/lib/python3.8/site-packages/PIL/Image.py:945: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n  warnings.warn(\n/opt/conda/lib/python3.8/site-packages/PIL/Image.py:945: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n  warnings.warn(\n/opt/conda/lib/python3.8/site-packages/PIL/Image.py:945: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n  warnings.warn(\n/opt/conda/lib/python3.8/site-packages/PIL/Image.py:945: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n  warnings.warn(\n\n\nIn the last cell, we have introduced another function resize_images. So let’s check its docs too.\n\n#collapse_output\n?resize_images\n\n\nSignature:\nresize_images(\n    path,\n    max_workers=2,\n    max_size=None,\n    recurse=False,\n    dest=Path('.'),\n    n_channels=3,\n    ext=None,\n    img_format=None,\n    resample=2,\n    resume=None,\n    **kwargs,\n)\nDocstring: Resize files on path recursively to dest to max_size\nFile:      /opt/conda/lib/python3.8/site-packages/fastai/vision/utils.py\nType:      function\n\n\n\n\nIt tells us that this function is from the fastai vision module. Its purpose is to resize images to a given MAX size and store them in a destination folder. If any image size exceeds the max size, this function will resize it to a given size. Otherwise (smaller than max size), the image is left as it is.\nSo our images are now downloaded. Let’s check them.\n\n##\n# Step: Check the downloaded image files\n\nfns = get_image_files(path)\nfns\n\n(#757) [Path('datasets/2022-08-10-sagemaker-fastai-classifier/images/tennis/ee4bb413-b78d-4f2a-8e41-3f32bbad79a2.jpeg'),Path('datasets/2022-08-10-sagemaker-fastai-classifier/images/tennis/647709da-50e5-4344-8cd5-9895448d47dc.jpg'),Path('datasets/2022-08-10-sagemaker-fastai-classifier/images/tennis/8a9378aa-6c9d-4456-8a8e-19ddc1a00354.jpeg'),Path('datasets/2022-08-10-sagemaker-fastai-classifier/images/tennis/84fd1455-607d-40c9-9454-8d06c18c8eab.jpg'),Path('datasets/2022-08-10-sagemaker-fastai-classifier/images/tennis/20711509-3b5e-4b06-93e1-2013a1d40279.jpeg'),Path('datasets/2022-08-10-sagemaker-fastai-classifier/images/tennis/ae3b15ca-748b-4fdd-868d-e602bf37c79e.jpg'),Path('datasets/2022-08-10-sagemaker-fastai-classifier/images/tennis/df19cf58-1da4-4dbc-8c9c-0224c6ffa32b.jpg'),Path('datasets/2022-08-10-sagemaker-fastai-classifier/images/tennis/5775d55d-1c61-4477-aab5-0dd0feb9c5ba.jpg'),Path('datasets/2022-08-10-sagemaker-fastai-classifier/images/tennis/31dfd24b-35ca-43fa-8cb9-5227b6ee9dac.jpg'),Path('datasets/2022-08-10-sagemaker-fastai-classifier/images/tennis/bf9ec4b8-f237-4030-b77b-ca02fcbf96c2.jpg')...]\n\n\nChecking the documentation for get_image_files\n\n#collapse_output\n??get_image_files\n\n\nSignature: get_image_files(path, recurse=True, folders=None)\nSource:   \ndef get_image_files(path, recurse=True, folders=None):\n    \"Get image files in `path` recursively, only in `folders`, if specified.\"\n    return get_files(path, extensions=image_extensions, recurse=recurse, folders=folders)\nFile:      /opt/conda/lib/python3.8/site-packages/fastai/data/transforms.py\nType:      function\n\n\n\n\nIt tells us that this function is from the fastai data module. It gets image files in the directory recursively. Internally it is calling another function get_files. Let’s check it as well.\n\n#collapse_output\n??get_files\n\n\nSignature:\nget_files(\n    path,\n    extensions=None,\n    recurse=True,\n    folders=None,\n    followlinks=True,\n)\nSource:   \ndef get_files(path, extensions=None, recurse=True, folders=None, followlinks=True):\n    \"Get all the files in `path` with optional `extensions`, optionally with `recurse`, only in `folders`, if specified.\"\n    path = Path(path)\n    folders=L(folders)\n    extensions = setify(extensions)\n    extensions = {e.lower() for e in extensions}\n    if recurse:\n        res = []\n        for i,(p,d,f) in enumerate(os.walk(path, followlinks=followlinks)): # returns (dirpath, dirnames, filenames)\n            if len(folders) !=0 and i==0: d[:] = [o for o in d if o in folders]\n            else:                         d[:] = [o for o in d if not o.startswith('.')]\n            if len(folders) !=0 and i==0 and '.' not in folders: continue\n            res += _get_files(p, f, extensions)\n    else:\n        f = [o.name for o in os.scandir(path) if o.is_file()]\n        res = _get_files(path, f, extensions)\n    return L(res)\nFile:      /opt/conda/lib/python3.8/site-packages/fastai/data/transforms.py\nType:      function\n\n\n\n\nThis tells us that this function is also from the fastai data module. It reads the files from the folder and returns their paths as “L” class objects. L class is new to us so let’s check what this class is about.\n\n#collapse_output\n?L\n\n\nInit signature: L(items=None, *rest, use_list=False, match=None)\nDocstring:      Behaves like a list of `items` but can also index with list of indices or masks\nFile:           /opt/conda/lib/python3.8/site-packages/fastcore/foundation.py\nType:           _L_Meta\nSubclasses:     TfmdLists, MultiCategory, LabeledBBox\n\n\n\n\nIt tells us that L class is from a separate library fastcore, also released by the fastai team. The purpose of this library is defined as Python supercharged for the fastai library. An important takeaway from this class documentation is that it extends the Python list functionality and calls it the L class. You may read more on this library from fastcore.fast.ai\n\n##\n# Step: Verify the downloaded images\n# If any image is corrupt then remove it.\nfailed = verify_images(fns)\nfailed.map(Path.unlink)\nlen(failed)\n\n3\n\n\nA new function, verify_images is used in the above cell. First, let’s check its documentation.\n\n#collapse_output\n?verify_images\n\n\nSignature: verify_images(fns)\nDocstring: Find images in `fns` that can't be opened\nFile:      /opt/conda/lib/python3.8/site-packages/fastai/vision/utils.py\nType:      function\n\n\n\n\nThis tells us that verify_images is from fastai vision module, and it simply returns images (Paths) that cannot be opened. So we removed these corrupt image files to make our data clean.\n\n##\n# Step: print count of images downloaded against each search string\n\nfor search in searches:\n    fns = get_image_files(path/search)\n    print(search, \"images count: \", len(fns))\n\ntennis images count:  189\ncricket images count:  185\nsoccer images count:  193\nbasketball images count:  190\n\n\nSummary of the functions used till now\n\nPath: Is from Python standard library pathlib module. fastai loads this module for us\n\nDocumentation: pathlib.html\n\nresize_images: Is from fastai library vision module. Its purpose is to resize images to a given MAX size and store them in a destination folder\n\nDocumentation: vision.utils.html#resize_image\nSouce code: 09b_vision.utils.ipynb\n\nget_image_files: Is from fastai library data module. It returns a list (L class) of image file paths. Internally it calls get_files function\n\nDocumentation: data.transforms.html#get_image_files\nSouce code: 05_data.transforms.ipynb\n\nget_files: This is also from fastai library data module. It returns L class list of file paths\n\nDocumentation: data.transforms.html#get_files\nSouce code: 05_data.transforms.ipynb\n\nL class object: Is from fastcore library. It extends Python list object features\n\nDocumentation: fastcore.fast.ai/#l\nSouce code: 02_foundation.ipynb\n\nverify_images: Is from fastai library vision module. It verifies images and returns paths of images that cannot be opened\n\nDocumentation: vision.utils.html#verify_images\nSource code: 09b_vision.utils.ipynb\n\n\nSummary of the steps performed till now\n\nUse each search string to search and download images to a local folder. Resize the downloaded images to a given size.\nGet the downloaded images file paths as a list of type L\nVerify the images and remove any corrupted image\n\n\n\nCreate a data block\nWe have our training data (images) downloaded in a folder. So let’s continue with our work and create a DataBlock on them.\n\n##\n# Step: Create a data block\n\nballs = DataBlock(\n    blocks=(ImageBlock, CategoryBlock), \n    get_items=get_image_files, \n    splitter=RandomSplitter(valid_pct=0.2, seed=42),\n    get_y=parent_label,\n    item_tfms=[Resize(128, method='squish')]\n)\n\nA “DataBlock” is like a package or a pipeline with instructions that tell the type of data we are dealing with and how we want to process it. It is like a blueprint that defines how we want to process our data. If you are coming from scikit-learn world, you may think of it as a sklearn Pipeline and ColumnTransformer. Let’s check this class documentation for a better understanding.\n\n#collapse_output\n?DataBlock\n\n\nInit signature:\nDataBlock(\n    blocks: 'list' = None,\n    dl_type: 'TfmdDL' = None,\n    getters: 'list' = None,\n    n_inp: 'int' = None,\n    item_tfms: 'list' = None,\n    batch_tfms: 'list' = None,\n    *,\n    get_items=None,\n    splitter=None,\n    get_y=None,\n    get_x=None,\n)\nDocstring:      Generic container to quickly build `Datasets` and `DataLoaders`.\nFile:           /opt/conda/lib/python3.8/site-packages/fastai/data/block.py\nType:           type\nSubclasses:     \n\n\n\n\nIt tells us that it is from fastai data module. On its usage, it says it is a Generic container to build DataLoaders. Let’s deconstruct the arguments we have passed to this class to understand them.\nThe first argument that we have passed is the blocks.\nblocks=(ImageBlock, CategoryBlock)\nblocks are themselves predefined domain-specific containers (or pipelines) with default transformations defined for common use cases. For example, CategoryBlock is for “single-label categorical targets”. And the default transformations for this type of data are\n\nEncode categories like one-hot-encoding\nsort them\nfill empty values\n\nSimilary, many other built-in blocks are available in the fastai library for various data types and domains. All these blocks are like pipelines with default transformations defined (or simply generic containers with a set of instructions). So, by blocks=(ImageBlock, CategoryBlock), we are defining a blueprint and saying that for X and y of our data, treat them with ImageBlock and CategoryBlock, respectively. Here X is our training data (or images), and y is our labels i.e. tennis ball, cricket ball, etc.\nThe next argument in DataBlock is get_items\nget_items=get_image_files\nIt tells the blueprint on how to get the data items. Data items in our case are images, and we are telling it to use “get_image_files” function to get the image paths.\nThen we pass the argument “splitter”\nsplitter=RandomSplitter(valid_pct=0.2, seed=42),\nHere we are telling the DataBlock how to split the data.\nNote that each DataBlock blueprint requires four things: the types of your input/labels (or blocks), and at least two functions: get_items and splitter.\nWe have passed a class RandomSplitter for a splitting strategy. Let’s check this class doc for more clarity.\n\n#collapse_output\n?RandomSplitter\n\n\nSignature: RandomSplitter(valid_pct=0.2, seed=None)\nDocstring: Create function that splits `items` between train/val with `valid_pct` randomly.\nFile:      /opt/conda/lib/python3.8/site-packages/fastai/data/transforms.py\nType:      function\n\n\n\n\nThis class is also from the fastai data module, and it randomly splits the data into two sets: train and validation.\nIn the DataBlock constructor, we also passed an argument get_y that defined how to get the labels. Note that we have not defined anything for X, so the library will automatically take the data found in get_items as X. for get_y we have passed a class parent_label. Let’s check its documentation to understand what it is for.\nget_y=parent_label\n\n#collapse_output\n?parent_label\n\n\nSignature: parent_label(o)\nDocstring: Label `item` with the parent folder name.\nFile:      /opt/conda/lib/python3.8/site-packages/fastai/data/transforms.py\nType:      function\n\n\n\n\nIt tells that it is another class from the fastai data module and returns labels derived from the parent folder name. Meaning that all my cricket ball images are placed in a folder name cricket, so this class will automatically label all the images under it as cricket.\nThe last argument we have passed is item_tfms (item transformations). This argument defines if we want to apply any other transformation besides those described in blocks on each data point. Here we are telling it to use the Resize method on each item (from X). What does this function do? Let’s check the docs.\nitem_tfms=[Resize(128, method='squish')]\n\n#collapse_output\n??Resize\n\n\nInit signature:\nResize(\n    self,\n    size: 'int | tuple',\n    method: 'ResizeMethod' = 'crop',\n    pad_mode: 'PadMode' = 'reflection',\n    resamples=(2, 0),\n    **kwargs,\n)\nDocstring:      A transform that before_call its state at each `__call__`\nSource:        \nclass Resize(RandTransform):\n    split_idx,mode,mode_mask,order = None,BILINEAR,NEAREST,1\n    \"Resize image to `size` using `method`\"\n    def __init__(self, \n        size:int|tuple, # Size to resize to, duplicated if one value is specified\n        method:ResizeMethod=ResizeMethod.Crop, # A `ResizeMethod`\n        pad_mode:PadMode=PadMode.Reflection, # A `PadMode`\n        resamples=(BILINEAR, NEAREST), # Pillow `Image` resamples mode, resamples[1] for mask\n        **kwargs\n    ):\n        size = _process_sz(size)\n        store_attr()\n        super().__init__(**kwargs)\n        self.mode,self.mode_mask = resamples\n    def before_call(self, \n        b, \n        split_idx:int # Index of the train/valid dataset\n    ):\n        if self.method==ResizeMethod.Squish: return\n        self.pcts = (0.5,0.5) if split_idx else (random.random(),random.random())\n    def encodes(self, x:Image.Image|TensorBBox|TensorPoint):\n        orig_sz = _get_sz(x)\n        if self.method==ResizeMethod.Squish:\n            return x.crop_pad(orig_sz, fastuple(0,0), orig_sz=orig_sz, pad_mode=self.pad_mode,\n                   resize_mode=self.mode_mask if isinstance(x,PILMask) else self.mode, resize_to=self.size)\n        w,h = orig_sz\n        op = (operator.lt,operator.gt)[self.method==ResizeMethod.Pad]\n        m = w/self.size[0] if op(w/self.size[0],h/self.size[1]) else h/self.size[1]\n        cp_sz = (int(m*self.size[0]),int(m*self.size[1]))\n        tl = fastuple(int(self.pcts[0]*(w-cp_sz[0])), int(self.pcts[1]*(h-cp_sz[1])))\n        return x.crop_pad(cp_sz, tl, orig_sz=orig_sz, pad_mode=self.pad_mode,\n                   resize_mode=self.mode_mask if isinstance(x,PILMask) else self.mode, resize_to=self.size)\nFile:           /opt/conda/lib/python3.8/site-packages/fastai/vision/augment.py\nType:           _TfmMeta\nSubclasses:     \n\n\n\n\nIt tells us that this class is from the fastai vision module, but the docstring is not very helpful. Overall the impression is that this function internally uses the Pillow library and helps resize images. It utilizes multiple techniques to make all images of the same size like padding, cropping, reflection, squish, etc. Here we want all the pictures of the exact same size. Previously we have seen a similar function while downloading images (resize_images), which applied to the images max size.\n\n\nCreate a data loader and show batch\nIn the last section, we created a blueprint that defines the type of data we are dealing with and some transformations for it. We call it DataBlock. But a DataBlock is just a set of instructions as it does not point to any data. When we group a DataBlock with the actual data, we get a Dataset. But in machine learning workloads, we commonly deal with batches of data from the same dataset. For this, we have an iterator class over “Dataset” that creates batches from the given dataset for us. We call it DataLoaders. Both these concepts come from Pytorch on which fastai has its roots. So let’s first learn about these new concepts directly from Pytorch documentation.\n\nCode for processing data samples can get messy and hard to maintain; we ideally want our dataset code to be decoupled from our model training code for better readability and modularity. PyTorch provides two data primitives: torch.utils.data.DataLoader and torch.utils.data.Dataset that allow you to use pre-loaded datasets as well as your own data. Dataset stores the samples and their corresponding labels, and DataLoader wraps an iterable around the Dataset to enable easy access to the samples.\n\nRead more about it from Pytorch documentation data_tutorial.html\nWhat fastai provides is that it builds on these Pytorch constructs and extends their functionality. You can say that while working with Pytorch fastai team found many repeated steps for everyday tasks in their machine learning work. So they created higher-level functions in fastai that do many of these repeated and common tasks for us (though internally, it is still using Pytorch). This way, we developers can achieve more with fewer lines of code in fastai. So using fastai, we don’t need to create a Dataset and then a Dataloader. Instead, we can get a Dataloader by pointing our DataBlock to data and asking it to return a DataLoader. In the next cell, we are doing that.\n\n##\n# Step: Create a DataLoader\n\ndls = balls.dataloaders(path)\n\nTo complete the picture, let’s also check the documentation of Datasets from the fastai library.\n\n#collapse-output\n?Datasets\n\n\nInit signature:\nDatasets(\n    items: 'list' = None,\n    tfms: 'list | Pipeline' = None,\n    tls: 'TfmdLists' = None,\n    n_inp: 'int' = None,\n    dl_type=None,\n    *,\n    use_list: 'bool' = None,\n    do_setup: 'bool' = True,\n    split_idx: 'int' = None,\n    train_setup: 'bool' = True,\n    splits: 'list' = None,\n    types=None,\n    verbose: 'bool' = False,\n)\nDocstring:      A dataset that creates a tuple from each `tfms`\nFile:           /opt/conda/lib/python3.8/site-packages/fastai/data/core.py\nType:           type\nSubclasses:     \n\n\n\n\nIt tells us that it is defined in fastai data module.\n\nDocumentation: data.core.html#datasets\nSource code: 03_data.core.ipynb\n\nLet’s do the same for DataLoaders and check its documentation.\n\n#collapse-output\n?DataLoaders\n\n\nInit signature: DataLoaders(*loaders, path: 'str | Path' = '.', device=None)\nDocstring:      Basic wrapper around several `DataLoader`s.\nFile:           /opt/conda/lib/python3.8/site-packages/fastai/data/core.py\nType:           type\nSubclasses:     ImageDataLoaders, SegmentationDataLoaders\n\n\n\n\nIt tells us that DataLoaders class is also defined in fastai library data module.\n\nDocumentation: data.core.html#dataloaders\nSource code: 03_data.core.ipynb\n\nImportant notes from DataLoaders documentation:\n\nDataLoaders.train: Training DataLoader\nDataLoaders.valid: Validation DataLoader\nDataLoaders.train_ds: Training Dataset\nDataLoaders.valid_ds: Validation Dataset\n\nWe can also check the data loader object’s data type to confirm its class name and origin.\n\n##\n# dataloader object type\ntype(dls)\n\nfastai.data.core.DataLoaders\n\n\n\n##\n# check types for dataloaders.train and dataloaders.valid\nprint(type(dls.train))\nprint(type(dls.valid))\n\n<class 'fastai.data.core.TfmdDL'>\n<class 'fastai.data.core.TfmdDL'>\n\n\nData type for dls.train and dls.valid are of a different class. Let’s check the documentation for TfmdDL to get more clarity.\n\n#collapse-output\n??TfmdDL\n\n\nInit signature:\nTfmdDL(\n    dataset,\n    bs: 'int' = 64,\n    shuffle: 'bool' = False,\n    num_workers: 'int' = None,\n    verbose: 'bool' = False,\n    do_setup: 'bool' = True,\n    *,\n    pin_memory=False,\n    timeout=0,\n    batch_size=None,\n    drop_last=False,\n    indexed=None,\n    n=None,\n    device=None,\n    persistent_workers=False,\n    pin_memory_device='',\n    wif=None,\n    before_iter=None,\n    after_item=None,\n    before_batch=None,\n    after_batch=None,\n    after_iter=None,\n    create_batches=None,\n    create_item=None,\n    create_batch=None,\n    retain=None,\n    get_idxs=None,\n    sample=None,\n    shuffle_fn=None,\n    do_batch=None,\n)\nSource:        \nclass TfmdDL(DataLoader):\n    \"Transformed `DataLoader`\"\n    def __init__(self, \n        dataset, # Map- or iterable-style dataset from which to load the data\n        bs:int=64, # Size of batch\n        shuffle:bool=False, # Whether to shuffle data\n        num_workers:int=None, # Number of CPU cores to use in parallel (default: All available up to 16)\n        verbose:bool=False, # Whether to print verbose logs\n        do_setup:bool=True, # Whether to run `setup()` for batch transform(s)\n        **kwargs\n    ):\n        if num_workers is None: num_workers = min(16, defaults.cpus)\n        for nm in _batch_tfms: kwargs[nm] = Pipeline(kwargs.get(nm,None))\n        super().__init__(dataset, bs=bs, shuffle=shuffle, num_workers=num_workers, **kwargs)\n        if do_setup:\n            for nm in _batch_tfms:\n                pv(f\"Setting up {nm}: {kwargs[nm]}\", verbose)\n                kwargs[nm].setup(self)\n    def _one_pass(self):\n        b = self.do_batch([self.do_item(None)])\n        if self.device is not None: b = to_device(b, self.device)\n        its = self.after_batch(b)\n        self._n_inp = 1 if not isinstance(its, (list,tuple)) or len(its)==1 else len(its)-1\n        self._types = explode_types(its)\n    def _retain_dl(self,b):\n        if not getattr(self, '_types', None): self._one_pass()\n        return retain_types(b, typs=self._types)\n    @delegates(DataLoader.new)\n    def new(self, \n        dataset=None, # Map- or iterable-style dataset from which to load the data\n        cls=None, # Class of the newly created `DataLoader` object\n        **kwargs\n    ):\n        res = super().new(dataset, cls, do_setup=False, **kwargs)\n        if not hasattr(self, '_n_inp') or not hasattr(self, '_types'):\n            try:\n                self._one_pass()\n                res._n_inp,res._types = self._n_inp,self._types\n            except Exception as e: \n                print(\"Could not do one pass in your dataloader, there is something wrong in it. Please see the stack trace below:\")\n                raise\n        else: res._n_inp,res._types = self._n_inp,self._types\n        return res\n    def before_iter(self):\n        super().before_iter()\n        split_idx = getattr(self.dataset, 'split_idx', None)\n        for nm in _batch_tfms:\n            f = getattr(self,nm)\n            if isinstance(f,Pipeline): f.split_idx=split_idx\n    def decode(self, \n        b # Batch to decode\n    ):\n        return to_cpu(self.after_batch.decode(self._retain_dl(b)))\n    def decode_batch(self, \n        b, # Batch to decode\n        max_n:int=9, # Maximum number of items to decode\n        full:bool=True # Whether to decode all transforms. If `False`, decode up to the point the item knows how to show itself\n    ): \n        return self._decode_batch(self.decode(b), max_n, full)\n    def _decode_batch(self, b, max_n=9, full=True):\n        f = self.after_item.decode\n        f1 = self.before_batch.decode\n        f = compose(f1, f, partial(getcallable(self.dataset,'decode'), full = full))\n        return L(batch_to_samples(b, max_n=max_n)).map(f)\n    def _pre_show_batch(self, b, max_n=9):\n        \"Decode `b` to be ready for `show_batch`\"\n        b = self.decode(b)\n        if hasattr(b, 'show'): return b,None,None\n        its = self._decode_batch(b, max_n, full=False)\n        if not is_listy(b): b,its = [b],L((o,) for o in its)\n        return detuplify(b[:self.n_inp]),detuplify(b[self.n_inp:]),its\n    def show_batch(self,\n        b=None, # Batch to show\n        max_n:int=9, # Maximum number of items to show\n        ctxs=None, # List of `ctx` objects to show data. Could be matplotlib axis, DataFrame etc\n        show:bool=True, # Whether to display data\n        unique:bool=False, # Whether to show only one \n        **kwargs\n    ):\n        \"Show `max_n` input(s) and target(s) from the batch.\"\n        if unique:\n            old_get_idxs = self.get_idxs\n            self.get_idxs = lambda: Inf.zeros\n        if b is None: b = self.one_batch()\n        if not show: return self._pre_show_batch(b, max_n=max_n)\n        show_batch(*self._pre_show_batch(b, max_n=max_n), ctxs=ctxs, max_n=max_n, **kwargs)\n        if unique: self.get_idxs = old_get_idxs\n    def show_results(self, \n        b, # Batch to show results for\n        out, # Predicted output from model for the batch\n        max_n:int=9, # Maximum number of items to show\n        ctxs=None, # List of `ctx` objects to show data. Could be matplotlib axis, DataFrame etc\n        show:bool=True, # Whether to display data\n        **kwargs\n    ):\n        \"Show `max_n` results with input(s), target(s) and prediction(s).\"\n        x,y,its = self.show_batch(b, max_n=max_n, show=False)\n        b_out = type(b)(b[:self.n_inp] + (tuple(out) if is_listy(out) else (out,)))\n        x1,y1,outs = self.show_batch(b_out, max_n=max_n, show=False)\n        res = (x,x1,None,None) if its is None else (x, y, its, outs.itemgot(slice(self.n_inp,None)))\n        if not show: return res\n        show_results(*res, ctxs=ctxs, max_n=max_n, **kwargs)\n    @property\n    def n_inp(self) -> int:\n        \"Number of elements in `Datasets` or `TfmdDL` tuple to be considered part of input.\"\n        if hasattr(self.dataset, 'n_inp'): return self.dataset.n_inp\n        if not hasattr(self, '_n_inp'): self._one_pass()\n        return self._n_inp\n    def to(self, \n        device # Device to put `DataLoader` and transforms\n    ):\n        self.device = device\n        for tfm in self.after_batch.fs:\n            for a in L(getattr(tfm, 'parameters', None)): setattr(tfm, a, getattr(tfm, a).to(device))\n        return self\nFile:           /opt/conda/lib/python3.8/site-packages/fastai/data/core.py\nType:           type\nSubclasses:     TabDataLoader, WeightedDL, PartialDL\n\n\n\n\nIt tells us that it is a class derived from Dataloader and calls it Transformed DataLoader. It is defined in fastai data module. Its purpose is defined as\n\nA TfmdDL is a DataLoader that creates Pipeline from a list of Transforms for the callbacks after_item, before_batch and after_batch. As a result, it can decode or show a processed batch\n\n\nDocumentation: data.core.html#tfmddl\nSource code: 03_data.core.ipynb\n\nTo clarify, DataLoaders and DataLoader are two separate classes.\n\nour object “dls” is of DataLoaders origin. This class is a wrapper around several DataLoaders. We have checked this class doc before\ndls attributes “dls.train” and “dls.valid” are of DataLoader origin. TfmdDL (transformed data loader) is one of its kind, and we have seen its docs in the last cell.\n\nLet’s also visit the documentation for the DataLoader class from which TfmdDL is derived.\n\n#collapse-output\n?DataLoader\n\n\nInit signature:\nDataLoader(\n    dataset=None,\n    bs=None,\n    num_workers=0,\n    pin_memory=False,\n    timeout=0,\n    batch_size=None,\n    shuffle=False,\n    drop_last=False,\n    indexed=None,\n    n=None,\n    device=None,\n    persistent_workers=False,\n    pin_memory_device='',\n    *,\n    wif=None,\n    before_iter=None,\n    after_item=None,\n    before_batch=None,\n    after_batch=None,\n    after_iter=None,\n    create_batches=None,\n    create_item=None,\n    create_batch=None,\n    retain=None,\n    get_idxs=None,\n    sample=None,\n    shuffle_fn=None,\n    do_batch=None,\n)\nDocstring:      API compatible with PyTorch DataLoader, with a lot more callbacks and flexibility\nFile:           /opt/conda/lib/python3.8/site-packages/fastai/data/load.py\nType:           type\nSubclasses:     TfmdDL\n\n\n\n\nIt tells us that it is an extension to PyTorch DataLoader with more flexibility (or functionality). This class is defined in fastai data module.\n\nDocumentation: data.load.html#dataloader\nSource code: 02_data.load.ipynb\n\nLet us continue with our work and visualize a small batch from our validation set.\n\n##\n# Step: Show a small batch from validation set\ndls.valid.show_batch(max_n=9, nrows=3)\n\n\n\n\n\n##\n# Step: Show a small batch from training set\ndls.train.show_batch(max_n=9, nrows=3)\n\n\n\n\nshow_batch is a very convenient function from fastai with which you can quickly verify a sample from the training dataset. You can also check that all the images are of the same size and have been appropriately labeled. Let’s quickly review the docs for this function.\n\n#collapse-output\n?TfmdDL.show_batch\n\n\nSignature:\nTfmdDL.show_batch(\n    self,\n    b=None,\n    max_n: 'int' = 9,\n    ctxs=None,\n    show: 'bool' = True,\n    unique: 'bool' = False,\n    **kwargs,\n)\nDocstring: Show `b` (defaults to `one_batch`), a list of lists of pipeline outputs (i.e. output of a `DataLoader`)\nFile:      /opt/conda/lib/python3.8/site-packages/fastai/data/core.py\nType:      function\n\n\n\n\nSummary of the functions used till now\n\nDataBlock: Is from fastai data module. It is a generic container to build DataLoaders\n\nDocumentation: data.block.html#datablock\nSource code: 06_data.block.ipynb\n\nDataBlock.dataloaders(Path): to create a DataLoaders object from DataBlock\nDataLoaders: Is from fastai library data module. It is a basic wrapper around several DataLoaders\n\nDocumentation: data.core.html#dataloaders\nSource code: 03_data.core.ipynb\n\nDataLoader: Is an extension to PyTorch DataLoader with more flexibility (or functionality). This class is defined in fastai data module.\n\nDocumentation: data.load.html#dataloader\nSource code: 02_data.load.ipynb\n\nTfmdDL: A transformed DataLoader\n\nDocumentation: data.core.html#tfmddl\nSource code: 03_data.core.ipynb\n\n\nSummary of the steps till now\n\nCreated a DataBlock object\nCreated a DataLoaders object\nViewed a small batch using show_batch function\n\n\n\nTrain an image classifier\nAt this point, our data is ready for training. Let’s train an image classifier on this data.\n\n##\n# Step: Train an image classifier\nlearn = vision_learner(dls, resnet18, metrics=error_rate)\nlearn.fine_tune(3)\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      1.900133\n      0.625038\n      0.192053\n      00:26\n    \n  \n\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      0.623933\n      0.692287\n      0.192053\n      00:37\n    \n    \n      1\n      0.440623\n      0.726005\n      0.192053\n      00:36\n    \n    \n      2\n      0.325829\n      0.649467\n      0.192053\n      00:36\n    \n  \n\n\n\nIn the last cell, we have performed two steps:\n\nCreated a vision learner using a vision_learner() method\nThen used the learner object fine_tune() method to train a model\n\nLet’s deconstruct these two steps. First, check the documentation on the vision_learner function.\n\n#collapse-output\n?vision_learner\n\n\nSignature:\nvision_learner(\n    dls,\n    arch,\n    normalize=True,\n    n_out=None,\n    pretrained=True,\n    loss_func=None,\n    opt_func=<function Adam at 0x7ff9747445e0>,\n    lr=0.001,\n    splitter=None,\n    cbs=None,\n    metrics=None,\n    path=None,\n    model_dir='models',\n    wd=None,\n    wd_bn_bias=False,\n    train_bn=True,\n    moms=(0.95, 0.85, 0.95),\n    cut=None,\n    init=<function kaiming_normal_ at 0x7ff9912824c0>,\n    custom_head=None,\n    concat_pool=True,\n    pool=True,\n    lin_ftrs=None,\n    ps=0.5,\n    first_bn=True,\n    bn_final=False,\n    lin_first=False,\n    y_range=None,\n    *,\n    n_in=3,\n)\nDocstring: Build a vision learner from `dls` and `arch`\nFile:      /opt/conda/lib/python3.8/site-packages/fastai/vision/learner.py\nType:      function\n\n\n\n\nIt tells us that this method is from the fastai vision module, and the docstring says that its purpose is to “Build a vision learner from dls and arch”. We are getting limited information here. So let’s check the documentation pages vision.learner.html#learner-convenience-functions. From the documentation page, we get the following information\n\nIt is a “convenience function” for Learner class objects\nfastai “vision learner” module purpose is defined as “All the functions necessary to build Learner suitable for transfer learning in computer vision”\n\nThe most essential functions of this module are vision_learner and unet_learner.\nThey will help you define a Learner using a pre-trained model.\n\nArguments passed to this function\n\ndls refers to DataLoaders object\narch refers to model architecture to use\n\n\nSo basically, vision_learner is a helper function for Learner class to create its object. This means we need to understand the Learner concept to complete the picture.\n\n#collapse-output\n?Learner\n\n\nInit signature:\nLearner(\n    dls,\n    model: 'callable',\n    loss_func: 'callable | None' = None,\n    opt_func=<function Adam at 0x7ff9747445e0>,\n    lr=0.001,\n    splitter: 'callable' = <function trainable_params at 0x7ff9774ea790>,\n    cbs=None,\n    metrics=None,\n    path=None,\n    model_dir='models',\n    wd=None,\n    wd_bn_bias=False,\n    train_bn=True,\n    moms=(0.95, 0.85, 0.95),\n    default_cbs: 'bool' = True,\n)\nDocstring:      Group together a `model`, some `dls` and a `loss_func` to handle training\nFile:           /opt/conda/lib/python3.8/site-packages/fastai/learner.py\nType:           type\nSubclasses:     \n\n\n\n\nIt tells us that a Learner “Group together a model, some dls and a loss_func to handle training”. It is also a container that combines data, model, and a loss function into a single pipeline; we get this pipeline using a helper function vision_learner. Let’s also check the type of our “learn” object.\n\ntype(learn)\n\nfastai.learner.Learner\n\n\nOkay, the concept of vision_learner is clear. Let’s also discuss the arguments we have passed to it.\nlearn = vision_learner(dls, resnet18, metrics=error_rate)\n\ndls we already know that it refers to DataLoaders object\nresnet18 we know it is the arch or model architecture we want to use. But from which library/class are we using this object?\nerror_rate we know that it refers to the error metric we want to use for training. But again, from which library/class are we using this object?\n\nLet’s check resnet18 first.\n\n#collapse-output\n?resnet18\n\n\nSignature: resnet18(pretrained: bool = False, progress: bool = True, **kwargs: Any) -> torchvision.models.resnet.ResNet\nDocstring:\nResNet-18 model from\n`\"Deep Residual Learning for Image Recognition\" <https://arxiv.org/pdf/1512.03385.pdf>`_.\nArgs:\n    pretrained (bool): If True, returns a model pre-trained on ImageNet\n    progress (bool): If True, displays a progress bar of the download to stderr\nFile:      /opt/conda/lib/python3.8/site-packages/torchvision/models/resnet.py\nType:      function\n\n\n\n\nIt tells us that this model architecture is defined in torchvision library or PyTorch vision module. From the docstring, we get that it is a pre-trained “ResNet-18 model for Deep Residual Learning for Image Recognition”.\nLet’s check the next argument, error_rate.\n\n#collapse-output\n?error_rate\n\n\nSignature: error_rate(inp, targ, axis=-1)\nDocstring: 1 - `accuracy`\nFile:      /opt/conda/lib/python3.8/site-packages/fastai/metrics.py\nType:      function\n\n\n\n\nIt tells us that error_rate is defined in fastai library metrics module.\nNow let’s go back to the image classifier training step and comprehend the second line.\nlearn.fine_tune(3)\n\n#collapse-output\n?Learner.fine_tune\n\n\nSignature:\nLearner.fine_tune(\n    self: 'Learner',\n    epochs,\n    base_lr=0.002,\n    freeze_epochs=1,\n    lr_mult=100,\n    pct_start=0.3,\n    div=5.0,\n    *,\n    lr_max=None,\n    div_final=100000.0,\n    wd=None,\n    moms=None,\n    cbs=None,\n    reset_opt=False,\n    start_epoch=0,\n)\nDocstring: Fine tune with `Learner.freeze` for `freeze_epochs`, then with `Learner.unfreeze` for `epochs`, using discriminative LR.\nFile:      /opt/conda/lib/python3.8/site-packages/fastai/callback/schedule.py\nType:      function\n\n\n\n\nIt tells us that it is from fastai callback module, and its purpose is defined as “Fine tune with Learner.freeze for freeze_epochs, then with Learner.unfreeze for epochs, using discriminative LR.”\nSummary of the functions used till now\n\nvision_learner: Is a helper function from the fastai Learner class. It is used to conveniently create a learner object\n\nDocumentation: vision.learner.html#vision_learner\nSource code: 21_vision.learner.ipynb\n\nLearner: Is a fastai class that combines data, model, and error function into a single pipeline.\n\nDocumentation: learner.html#learner\nSource code: 21_vision.learner.ipynb\n\nresnet18: Is a Pytorch pre-trained model with ResNet-18 architecture for image recognition.\n\nDocumentation: pytorch.org/vision\n\nerror_rate: Is a fastai metric function\nLearner.fine_tune: Is a Learner class function that trains a model\n\nDocumentation: callback.schedule.html#learner.fine_tune\nSource code: 14_callback.schedule.ipynb\n\n\nSummary of the steps till now\n\nCreate an image classifier Learner object\nTrain a model using Learner.fine_tune method\n\n\n\nInterpret the trained model\nIn the last section, we trained an image classifier that can distinguish between 4 different ball images: cricket, tennis, soccer, and basketball. But how good is our model? A confusion matrix is the best tool to interpret a model for classification problems. It visualizes and summarizes the performance of a classification algorithm. Let’s do it next.\n\n##\n# Step: Interpret the model using confusion matrix\n\ninterp = ClassificationInterpretation.from_learner(learn)\ninterp.plot_confusion_matrix()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAgain, we would like to know more about class ClassificationInterpretation as to which library it belongs. For this, let’s check the documentation.\n\n?ClassificationInterpretation\n\n\nInit signature:\nClassificationInterpretation(\n    learn: 'Learner',\n    dl: 'DataLoader',\n    losses: 'TensorBase',\n    act=None,\n)\nDocstring:      Interpretation methods for classification models.\nFile:           /opt/conda/lib/python3.8/site-packages/fastai/interpret.py\nType:           type\nSubclasses:     \n\n\n\n\nIt tells us that this method is from the fastai interpret module. Its purpose is defined as “Interpretation methods for classification models”. We can check the module documentation interpret.html#interpretation to get more details. There it says\n\nInterpretation is a helper base class for exploring predictions from trained models. It can be inherited for task specific interpretation classes, such as ClassificationInterpretation. Interpretation is memory efficient and should be able to process any sized dataset, provided the hardware could train the same model.\n\nAnd to get an Interpretation object we are Interpretation.from_learner method. Its purpose is to “Construct interpretation object from a learner”. Read more about this method from docs interpret.html#interpretation.from_learner\nThere are many useful functions in fastai Interpretation class, and I would encourage you to check them from the documentation. Once such method is plot_top_losses. Its purpose is “Show k largest(/smallest) preds and losses” from the validation set and plot the data points where our model has the high loss score. Usually, a model makes a wrong prediction when it has low confidence (or the model can be very confident in a wrong prediction). So this function can be valuable in understanding a model’s performance.\nLet’s use this function next. You may read more about this function from docs interpret.html#interpretation.plot_top_losses\n\n##\n# Step: Plot top losses\n\ninterp.plot_top_losses(9, nrows=9)\n\n\n\n\n\n\n\n\n\n\n\nLet’s analyze the results from the last cell. It outputs model performance as “Prediction/Actual/Loss/Confidence”\n\nPrediction: What label has our model predicted for a given image\nActual: What label does our validation data have for that image\nLoss: A quantitative measure of how wrong was the model prediction. Making a wrong prediction with high confidence gets a high score\nConfidence: The confidence/probability that the model thinks a given image has the predicted label\n\nNow let’s check the output images.\n\nFirst image gets the highest loss. Our model thinks (very confidently) that this image is a tennis ball. But in our validation set, it is a cricket ball with green color. This makes sense because our model has learned that green balls are tennis balls.\nThe second image model again thinks it is a tennis ball. It seems right as it looks like a tennis ball to the human eye. We can remove this image from validation if we are unsure about it.\nIn the third image, our model thinks it is a cricket ball though it is a soccer ball. What do we learn from this? Our model has not seen soccer ball images before that have green and red colors in it. So we may include more soccer ball images with that color.\n\nSimilarly, we can analyze the remaining results and take action accordingly.\n\n\nExport and load a trained model\nSo we have trained our image classifier and are happy with the results. The next phase in ML work is to copy/transfer this artifact to a server where it can be hosted for inference work. Remember, a trained model consists of the architecture and the trained parameters. So to store a model, we need to save both these parts, and no information is lost. For this, we can use Learner.export method to keep the model in a pickle binary format.\n\nYou can learn more about Learner.export from documentation learner.html#learner.export\nSource code: 13a_learner.ipynb\nTo read about pickle Python object serialization module check the official docs pickle.html\n\n\n##\n# Step: Define export file name and path\n\nexport_file = local_path + \"/export_model.pkl\"\nexport_file\n\n'./datasets/2022-08-10-sagemaker-fastai-classifier/export_model.pkl'\n\n\n\n##\n# Step: Export the model\n\nlearn.export(export_file)\n\nCheck the local directory contents if the model pickle file is present.\n\n##\n# Step: Check local directory for .pkl file\n\np = Path(local_path)\np.ls(file_exts=\".pkl\")\n\n(#1) [Path('datasets/2022-08-10-sagemaker-fastai-classifier/export_model.pkl')]\n\n\nNow let’s load the exported model again using load_learner function. Read its docs learner.html#load_learner\n\n##\n# Step: Load a model\n\nlearn_inf = load_learner(export_file)\n\nRecall that we downloaded a sample tennis ball image at the start of this notebook. Let’s use that to make a prediction on the loaded model.\n\n##\n# Step: Sample tennis ball image\n\nim = Image.open(dest)\nim.to_thumb(256,256)\n\n\n\n\n\n##\n# Step: Make a prediction\n\nlearn_inf.predict(dest)\n\n\n\n\n\n\n\n\n('tennis',\n TensorBase(3),\n TensorBase([1.0414e-05, 1.6653e-05, 2.1281e-06, 9.9997e-01]))\n\n\nIt has returned three items:\n\nThe predicted category in the format we provided the labels\nThe index of the predicted category “3”. Indexes start from 0\nAnd the probabilities of each category\n\nIf you want to get a list of all the categories, then you can access DataLoaders attribute of the Learner.\n\n##\n# Step: Get a list of all categories from Learner\n\nlearn_inf.dls.vocab\n\n['basketball', 'cricket', 'soccer', 'tennis']"
  },
  {
    "objectID": "posts/2022-10-10-pytorch-linear-regression.html#introduction",
    "href": "posts/2022-10-10-pytorch-linear-regression.html#introduction",
    "title": "Linear Regression with PyTorch",
    "section": "Introduction",
    "text": "Introduction\nIn this notebook, we will train a linear regression model using PyTorch. Given below is the summary of the steps followed in this notebook.\n\nCreate a synthetic linear dataset\nSplit the data into Train and Validation datasets. Then convert them into mini-batches using PyTorch DataLoader class\nCreate a Linear Neural Net model configuration, an SGD optimizer, and a loss function\nCreate a pipeline that will train the model on given data and update the weights based on the loss\nView the training results using TensorBoard graphical view"
  },
  {
    "objectID": "posts/2022-10-10-pytorch-linear-regression.html#environment",
    "href": "posts/2022-10-10-pytorch-linear-regression.html#environment",
    "title": "Linear Regression with PyTorch",
    "section": "Environment",
    "text": "Environment\nThis notebook is prepared with Google Colab.\n\n\nCode\nfrom platform import python_version\nimport sklearn, numpy, matplotlib, pandas, torch\n\nprint(\"python==\" + python_version())\nprint(\"sklearn==\" + sklearn.__version__)\nprint(\"numpy==\" + numpy.__version__)\nprint(\"torch==\" + torch.__version__)\nprint(\"matplotlib==\" + matplotlib.__version__)\n\n\npython==3.7.14\nsklearn==1.0.2\nnumpy==1.21.6\ntorch==1.12.1+cu113\nmatplotlib==3.2.2"
  },
  {
    "objectID": "posts/2022-10-10-pytorch-linear-regression.html#credits",
    "href": "posts/2022-10-10-pytorch-linear-regression.html#credits",
    "title": "Linear Regression with PyTorch",
    "section": "Credits",
    "text": "Credits\nThis notebook takes inspiration from the book “Deep Learning with PyTorch Step-by-Step” by “Daniel Voigt Godoy”. You can get the book from its website: pytorchstepbystep. In addition, the GitHub repository for this book has valuable notebooks and can be used independently: github.com/dvgodoy/PyTorchStepByStep. Parts of the code you see in this notebook are taken from chapter 2 notebook of the same book."
  },
  {
    "objectID": "posts/2022-10-10-pytorch-linear-regression.html#generate-synthetic-linear-data",
    "href": "posts/2022-10-10-pytorch-linear-regression.html#generate-synthetic-linear-data",
    "title": "Linear Regression with PyTorch",
    "section": "Generate synthetic linear data",
    "text": "Generate synthetic linear data\nIn this section, we will generate some data representing a line using equation “y = mx + b”. y = mx + b is the slope intercept form of writing the equation of a straight line. In the equation ‘b’ is the point where the line intersects the ‘y axis’ and ‘m’ denotes the slope of the line. If you want to read more about this equation then follow this post: cuemath.com/geometry/y-mx-b\n\nimport numpy as np\nnp.random.seed(0)\n\ndef generate_linear_data(n_data_points=100, true_m=1, true_b=1):\n    \"\"\"\n    Generate linear data using equation: y = mx + b + e\n    where 'e' is some random noise added\n    \"\"\"\n    x = np.random.rand(n_data_points, 1)\n    y = true_m * x + true_b + (.1 * np.random.randn(n_data_points, 1))\n    return x, y\n\n# Let's generate 100 data points\nn_data_points = 100\ntrue_m = 2 # this is 'm' from slope-intercept line equation\ntrue_b = 1 # this is 'b' from slope-intercept line equation\nx, y = generate_linear_data(n_data_points, true_m, true_b)\n\nLet’s plot our generated data to see how it looks.\n\nimport matplotlib.pyplot as plt\n\nfig, ax = plt.subplots(1, 1, figsize=(6, 6))\nax.scatter(x, y)\nax.set_xlabel('x')\nax.set_ylabel('y')\nax.set_title('Generated Full Dataset')\nplt.show()"
  },
  {
    "objectID": "posts/2022-10-10-pytorch-linear-regression.html#load-generated-data-into-pytorch-dataset-and-dataloader-class",
    "href": "posts/2022-10-10-pytorch-linear-regression.html#load-generated-data-into-pytorch-dataset-and-dataloader-class",
    "title": "Linear Regression with PyTorch",
    "section": "Load generated data into PyTorch Dataset and DataLoader class",
    "text": "Load generated data into PyTorch Dataset and DataLoader class\nIn this section, we will load our data in PyTorch helper classes Dataset and DataLoader. PyTorch documentation defines them as: [see basics/data_tutorial]\n\nCode for processing data samples can get messy and hard to maintain; we ideally want our dataset code to be decoupled from our model training code for better readability and modularity. PyTorch provides two data primitives: torch.utils.data.DataLoader and torch.utils.data.Dataset that allow you to use pre-loaded datasets as well as your own data. Dataset stores the samples and their corresponding labels, and DataLoader wraps an iterable around the Dataset to enable easy access to the samples.\n\nFor this, we first need to convert NumPy data arrays to PyTorch tensors.\n\n## \n# Convert NumPy array to PyTorch tensors\nimport torch\n\nx_tensor = torch.as_tensor(x).float()\ny_tensor = torch.as_tensor(y).float()\n\nNow load the tensors into Dataset and DataLoader class. PyTorch Dataset is a helper class that converts data and labels into a list of tuples. DataLoader is another helper class to create batches from Dataset tuples. batch_size means the number of tuples we want in a single batch. We have used 16 here since our data is small. So each fetch from DataLoader will give us a list of 16 tuples.\n\n## \n# Load tensors into Dataset and DataLoader\nfrom torch.utils.data import DataLoader, TensorDataset, random_split\n\ndataset = TensorDataset(x_tensor, y_tensor)\n\n# Performs the 80-20% train-valid split\nratio = .8\nn_total = len(dataset)\nn_train = int(n_total * ratio)\nn_val = n_total - n_train\n\ntrain_data, val_data = random_split(dataset, [n_train, n_val])\n\n# Builds a loader of each set\n# Use batch_size = 16 as data size is small\ntrain_loader = DataLoader(\n    dataset=train_data,\n    batch_size=16,\n    shuffle=True\n)\n\nval_loader = DataLoader(dataset=val_data, batch_size=16)\n\nWe have our DataLoaders ready for training and validation set. DataLoader objects are iterators, and let’s extract data from them to plot.\n\n## \n# Visualize training and validation data\n\n# extrat train and validation sets from DataLoader as a list of tuples\ntrain_data_list = list(iter(train_data))\nval_data_list = list(iter(val_data))\n\n# get data and labels (x, y) from extracted tuples list\nx_train = [e[0].numpy() for e in train_data_list]\ny_train = [e[1].numpy() for e in train_data_list]\n\nx_val = [e[0].numpy() for e in val_data_list]\ny_val = [e[1].numpy() for e in val_data_list]\n\n# plot the data\nfigure, axes = plt.subplots(1, 3, figsize=(10,5))\nfigure.suptitle('Train and Validation Dataset')\n\naxes[0].set_title('Training Data')\naxes[0].scatter(x_train, y_train)\n\naxes[1].set_title('Validation Data')\naxes[1].scatter(x_val, y_val)\n\naxes[2].set_title('Combined Data')\naxes[2].scatter(x_train, y_train)\naxes[2].scatter(x_val, y_val)\nplt.show()"
  },
  {
    "objectID": "posts/2022-10-10-pytorch-linear-regression.html#create-model-configuration",
    "href": "posts/2022-10-10-pytorch-linear-regression.html#create-model-configuration",
    "title": "Linear Regression with PyTorch",
    "section": "Create model configuration",
    "text": "Create model configuration\nIn this section, we will configure the linear model for training, define a loss function, and an optimizer to update the weights.\n\n## \n# Model configuration\nimport torch.nn as nn\nimport torch.optim as optim\n\ntorch.manual_seed(0)\n\n# check gpu availability\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\n\n# Sets learning rate\nlr = 0.1\n\n# Now we can create a model and send it at once to the device\nmodel = nn.Linear(1, 1).to(device)\n\n# Defines a SGD optimizer to update the parameters (now retrieved directly from the model)\noptimizer = optim.SGD(model.parameters(), lr=lr)\n\n# Defines a MSE loss function\nloss_fn = nn.MSELoss(reduction='mean')\n\nWe have initialized a model with default weights. Let’s view them. Note that “weight” denotes m and “bias” denotes b from our line equation. At this point they are very random but once we have trained our model they will be much closer to true_m and true_b which we used to generate the data.\n\n## \n# model weights before training\nprint(model.state_dict())\n\nOrderedDict([('weight', tensor([[-0.0075]])), ('bias', tensor([0.5364]))])"
  },
  {
    "objectID": "posts/2022-10-10-pytorch-linear-regression.html#define-training-validation-and-mini-batch-processing-pipeline",
    "href": "posts/2022-10-10-pytorch-linear-regression.html#define-training-validation-and-mini-batch-processing-pipeline",
    "title": "Linear Regression with PyTorch",
    "section": "Define training, validation and mini-batch processing pipeline",
    "text": "Define training, validation and mini-batch processing pipeline\nIn this section, we will define our pipelines for training and validation.\n\nTraining pipeline is usually called “training step” which includes the following steps\n\nComputes our model’s predicted output - the forward pass\nComputes the loss\nComputes gradients i.e., find the direction and scale to update the weight to reduce the loss\nUpdates parameters using gradients and the learning rate\n\nValidation pipeline is usually called the “validation step” which includes the following steps\n\nComputes our model’s predicted output - the forward pass\nComputes the loss\n\n\nNote that during validation, we are only concerned about the loss, i.e., how well our model performs on the validation dataset. Therefore, we don’t use it to calculate the gradients.\nLet’s configure our training pipeline steps in a helper function.\n\n## \n# Training pipeline - training step\n\n# helper function for training\ndef make_train_step_fn(model, loss_fn, optimizer):\n    # Builds function that performs a step in the train loop\n    def perform_train_step_fn(x, y):\n        # Sets model to TRAIN mode\n        model.train() \n        \n        # Step 1 - Computes our model's predicted output - forward pass\n        yhat = model(x)\n        # Step 2 - Computes the loss\n        loss = loss_fn(yhat, y)\n        # Step 3 - Computes gradients\n        loss.backward()\n        # Step 4 - Updates parameters using gradients and the learning rate\n        optimizer.step()\n        optimizer.zero_grad()\n        \n        # Returns the loss\n        return loss.item()\n    \n    # Returns the function that will be called inside the train loop\n    return perform_train_step_fn\n\n# Creates the train_step function for our model, loss function and optimizer\ntrain_step_fn = make_train_step_fn(model, loss_fn, optimizer)\n\nLet’s now configure our validation pipeline steps in a helper function.\n\n## \n# Validation pipeline - validation step.\n\n# helper function for validation\ndef make_val_step_fn(model, loss_fn):\n    # Builds function that performs a step in the validation loop\n    def perform_val_step_fn(x, y):\n        # Sets model to EVAL mode\n        model.eval()\n        \n        # Step 1 - Computes our model's predicted output - forward pass\n        yhat = model(x)\n        # Step 2 - Computes the loss\n        loss = loss_fn(yhat, y)\n        # There is no need to compute Steps 3 and 4, since we don't update parameters during evaluation\n        return loss.item()\n    \n    return perform_val_step_fn\n\n# Creates the val_step function for our model and loss function\nval_step_fn = make_val_step_fn(model, loss_fn)\n\nNow let’s define the steps to process a single minibatch in a helper function. For a mini-batch processing, we want to\n\nGet the next batch of data and labels (x, y) from the DataLoader iterator\nPerform a step on the batch. A step can be either training or validation\nCompute the average batch loss\n\n\n## \n# Helper function for minibatch processing\ndef mini_batch(device, data_loader, step_fn):\n    mini_batch_losses = []\n    for x_batch, y_batch in data_loader:\n        x_batch = x_batch.to(device)\n        y_batch = y_batch.to(device)\n\n        mini_batch_loss = step_fn(x_batch, y_batch)\n        mini_batch_losses.append(mini_batch_loss)\n\n    loss = np.mean(mini_batch_losses)\n    return loss"
  },
  {
    "objectID": "posts/2022-10-10-pytorch-linear-regression.html#configure-tensorboard-to-visualize-loss-logs",
    "href": "posts/2022-10-10-pytorch-linear-regression.html#configure-tensorboard-to-visualize-loss-logs",
    "title": "Linear Regression with PyTorch",
    "section": "Configure TensorBoard to visualize loss logs",
    "text": "Configure TensorBoard to visualize loss logs\nIn this section we will configure TensorBoard to track and visualize training and validation loss.\n\nfrom torch.utils.tensorboard import SummaryWriter\nimport datetime\n\n# Creates a Summary Writer to interface with TensorBoard\ntimestamp = datetime.datetime.utcnow().strftime('%Y-%m-%d-%H.%M.%S')\nwriter = SummaryWriter(f'runs/simple_linear_regression/{timestamp}')\n\n# Fetches a single mini-batch so we can use add_graph\nx_sample, y_sample = next(iter(train_loader))\nwriter.add_graph(model, x_sample.to(device))"
  },
  {
    "objectID": "posts/2022-10-10-pytorch-linear-regression.html#execute-model-training-and-validation-pipeline",
    "href": "posts/2022-10-10-pytorch-linear-regression.html#execute-model-training-and-validation-pipeline",
    "title": "Linear Regression with PyTorch",
    "section": "Execute model training and validation pipeline",
    "text": "Execute model training and validation pipeline\nNow we are ready to execute our training pipeline. We will train our model for 200 epochs. An epoch is one cycle when the model has seen all the training data to compute loss, and we want our model to do it 200 times (200 epochs).\n\n#collapse-output\n# Execute pipeline with training and validation steps\nn_epochs = 200\n\nlosses = []\nval_losses = []\n\nfor epoch in range(n_epochs):\n    # training step\n    loss = mini_batch(device, train_loader, train_step_fn)\n    losses.append(loss)\n    \n    # validation step\n    # no gradients in validation!\n    with torch.no_grad():\n        val_loss = mini_batch(device, val_loader, val_step_fn)\n        val_losses.append(val_loss)\n    \n    # Records both losses for each epoch under the main tag \"loss\"\n    writer.add_scalars(main_tag='loss',\n                       tag_scalar_dict={'training': loss, 'validation': val_loss},\n                       global_step=epoch)\n    \n    print(f\"epoch: {epoch:3}, train loss: {loss:.5f}, valid loss: {val_loss:.5f}\")\n\n# Closes the writer\nwriter.close()\n\nepoch:   0, train loss: 1.17219, valid loss: 0.23260\nepoch:   1, train loss: 0.20757, valid loss: 0.11046\nepoch:   2, train loss: 0.13150, valid loss: 0.09627\nepoch:   3, train loss: 0.11224, valid loss: 0.08610\nepoch:   4, train loss: 0.09926, valid loss: 0.07644\nepoch:   5, train loss: 0.08905, valid loss: 0.06739\nepoch:   6, train loss: 0.07843, valid loss: 0.06012\nepoch:   7, train loss: 0.06852, valid loss: 0.05478\nepoch:   8, train loss: 0.06251, valid loss: 0.04898\nepoch:   9, train loss: 0.05494, valid loss: 0.04451\nepoch:  10, train loss: 0.04878, valid loss: 0.04031\nepoch:  11, train loss: 0.04447, valid loss: 0.03616\nepoch:  12, train loss: 0.03953, valid loss: 0.03173\nepoch:  13, train loss: 0.03562, valid loss: 0.02867\nepoch:  14, train loss: 0.03217, valid loss: 0.02642\nepoch:  15, train loss: 0.02985, valid loss: 0.02395\nepoch:  16, train loss: 0.02688, valid loss: 0.02295\nepoch:  17, train loss: 0.02487, valid loss: 0.02103\nepoch:  18, train loss: 0.02344, valid loss: 0.01901\nepoch:  19, train loss: 0.02130, valid loss: 0.01751\nepoch:  20, train loss: 0.01974, valid loss: 0.01663\nepoch:  21, train loss: 0.01851, valid loss: 0.01555\nepoch:  22, train loss: 0.01764, valid loss: 0.01392\nepoch:  23, train loss: 0.01664, valid loss: 0.01410\nepoch:  24, train loss: 0.01585, valid loss: 0.01355\nepoch:  25, train loss: 0.01513, valid loss: 0.01283\nepoch:  26, train loss: 0.01448, valid loss: 0.01217\nepoch:  27, train loss: 0.01387, valid loss: 0.01166\nepoch:  28, train loss: 0.01328, valid loss: 0.01125\nepoch:  29, train loss: 0.01293, valid loss: 0.01146\nepoch:  30, train loss: 0.01253, valid loss: 0.01059\nepoch:  31, train loss: 0.01227, valid loss: 0.01087\nepoch:  32, train loss: 0.01199, valid loss: 0.01042\nepoch:  33, train loss: 0.01176, valid loss: 0.00927\nepoch:  34, train loss: 0.01160, valid loss: 0.00912\nepoch:  35, train loss: 0.01164, valid loss: 0.00951\nepoch:  36, train loss: 0.01119, valid loss: 0.00945\nepoch:  37, train loss: 0.01131, valid loss: 0.00946\nepoch:  38, train loss: 0.01095, valid loss: 0.00918\nepoch:  39, train loss: 0.01084, valid loss: 0.00888\nepoch:  40, train loss: 0.01099, valid loss: 0.00943\nepoch:  41, train loss: 0.01072, valid loss: 0.00863\nepoch:  42, train loss: 0.01063, valid loss: 0.00899\nepoch:  43, train loss: 0.01063, valid loss: 0.00843\nepoch:  44, train loss: 0.01063, valid loss: 0.00809\nepoch:  45, train loss: 0.01066, valid loss: 0.00779\nepoch:  46, train loss: 0.01046, valid loss: 0.00831\nepoch:  47, train loss: 0.01051, valid loss: 0.00780\nepoch:  48, train loss: 0.01043, valid loss: 0.00790\nepoch:  49, train loss: 0.01035, valid loss: 0.00813\nepoch:  50, train loss: 0.01048, valid loss: 0.00796\nepoch:  51, train loss: 0.01032, valid loss: 0.00859\nepoch:  52, train loss: 0.01065, valid loss: 0.00835\nepoch:  53, train loss: 0.01059, valid loss: 0.00790\nepoch:  54, train loss: 0.01039, valid loss: 0.00782\nepoch:  55, train loss: 0.01033, valid loss: 0.00821\nepoch:  56, train loss: 0.01027, valid loss: 0.00832\nepoch:  57, train loss: 0.01024, valid loss: 0.00856\nepoch:  58, train loss: 0.01034, valid loss: 0.00853\nepoch:  59, train loss: 0.01027, valid loss: 0.00846\nepoch:  60, train loss: 0.01019, valid loss: 0.00783\nepoch:  61, train loss: 0.01051, valid loss: 0.00831\nepoch:  62, train loss: 0.01032, valid loss: 0.00790\nepoch:  63, train loss: 0.01022, valid loss: 0.00815\nepoch:  64, train loss: 0.01040, valid loss: 0.00749\nepoch:  65, train loss: 0.01020, valid loss: 0.00768\nepoch:  66, train loss: 0.01025, valid loss: 0.00783\nepoch:  67, train loss: 0.01040, valid loss: 0.00861\nepoch:  68, train loss: 0.01025, valid loss: 0.00764\nepoch:  69, train loss: 0.01018, valid loss: 0.00818\nepoch:  70, train loss: 0.01030, valid loss: 0.00771\nepoch:  71, train loss: 0.01033, valid loss: 0.00809\nepoch:  72, train loss: 0.01034, valid loss: 0.00747\nepoch:  73, train loss: 0.01032, valid loss: 0.00855\nepoch:  74, train loss: 0.01023, valid loss: 0.00852\nepoch:  75, train loss: 0.01019, valid loss: 0.00785\nepoch:  76, train loss: 0.01027, valid loss: 0.00751\nepoch:  77, train loss: 0.01027, valid loss: 0.00742\nepoch:  78, train loss: 0.01031, valid loss: 0.00723\nepoch:  79, train loss: 0.01021, valid loss: 0.00816\nepoch:  80, train loss: 0.01023, valid loss: 0.00829\nepoch:  81, train loss: 0.01028, valid loss: 0.00803\nepoch:  82, train loss: 0.01040, valid loss: 0.00823\nepoch:  83, train loss: 0.01024, valid loss: 0.00815\nepoch:  84, train loss: 0.01020, valid loss: 0.00818\nepoch:  85, train loss: 0.01024, valid loss: 0.00835\nepoch:  86, train loss: 0.01029, valid loss: 0.00797\nepoch:  87, train loss: 0.01021, valid loss: 0.00776\nepoch:  88, train loss: 0.01025, valid loss: 0.00859\nepoch:  89, train loss: 0.01021, valid loss: 0.00840\nepoch:  90, train loss: 0.01026, valid loss: 0.00804\nepoch:  91, train loss: 0.01020, valid loss: 0.00803\nepoch:  92, train loss: 0.01031, valid loss: 0.00883\nepoch:  93, train loss: 0.01028, valid loss: 0.00808\nepoch:  94, train loss: 0.01034, valid loss: 0.00778\nepoch:  95, train loss: 0.01023, valid loss: 0.00797\nepoch:  96, train loss: 0.01019, valid loss: 0.00826\nepoch:  97, train loss: 0.01033, valid loss: 0.00747\nepoch:  98, train loss: 0.01022, valid loss: 0.00785\nepoch:  99, train loss: 0.01021, valid loss: 0.00778\nepoch: 100, train loss: 0.01025, valid loss: 0.00782\nepoch: 101, train loss: 0.01022, valid loss: 0.00807\nepoch: 102, train loss: 0.01032, valid loss: 0.00796\nepoch: 103, train loss: 0.01017, valid loss: 0.00770\nepoch: 104, train loss: 0.01019, valid loss: 0.00778\nepoch: 105, train loss: 0.01017, valid loss: 0.00776\nepoch: 106, train loss: 0.01018, valid loss: 0.00766\nepoch: 107, train loss: 0.01027, valid loss: 0.00823\nepoch: 108, train loss: 0.01021, valid loss: 0.00783\nepoch: 109, train loss: 0.01037, valid loss: 0.00753\nepoch: 110, train loss: 0.01017, valid loss: 0.00747\nepoch: 111, train loss: 0.01045, valid loss: 0.00805\nepoch: 112, train loss: 0.01020, valid loss: 0.00815\nepoch: 113, train loss: 0.01027, valid loss: 0.00811\nepoch: 114, train loss: 0.01016, valid loss: 0.00790\nepoch: 115, train loss: 0.01016, valid loss: 0.00776\nepoch: 116, train loss: 0.01018, valid loss: 0.00758\nepoch: 117, train loss: 0.01020, valid loss: 0.00743\nepoch: 118, train loss: 0.01021, valid loss: 0.00791\nepoch: 119, train loss: 0.01032, valid loss: 0.00731\nepoch: 120, train loss: 0.01019, valid loss: 0.00788\nepoch: 121, train loss: 0.01025, valid loss: 0.00819\nepoch: 122, train loss: 0.01039, valid loss: 0.00786\nepoch: 123, train loss: 0.01032, valid loss: 0.00791\nepoch: 124, train loss: 0.01026, valid loss: 0.00745\nepoch: 125, train loss: 0.01021, valid loss: 0.00786\nepoch: 126, train loss: 0.01026, valid loss: 0.00747\nepoch: 127, train loss: 0.01028, valid loss: 0.00794\nepoch: 128, train loss: 0.01037, valid loss: 0.00768\nepoch: 129, train loss: 0.01029, valid loss: 0.00775\nepoch: 130, train loss: 0.01027, valid loss: 0.00805\nepoch: 131, train loss: 0.01019, valid loss: 0.00828\nepoch: 132, train loss: 0.01024, valid loss: 0.00804\nepoch: 133, train loss: 0.01033, valid loss: 0.00801\nepoch: 134, train loss: 0.01022, valid loss: 0.00773\nepoch: 135, train loss: 0.01034, valid loss: 0.00868\nepoch: 136, train loss: 0.01031, valid loss: 0.00792\nepoch: 137, train loss: 0.01045, valid loss: 0.00862\nepoch: 138, train loss: 0.01031, valid loss: 0.00853\nepoch: 139, train loss: 0.01034, valid loss: 0.00832\nepoch: 140, train loss: 0.01022, valid loss: 0.00793\nepoch: 141, train loss: 0.01019, valid loss: 0.00754\nepoch: 142, train loss: 0.01017, valid loss: 0.00781\nepoch: 143, train loss: 0.01025, valid loss: 0.00809\nepoch: 144, train loss: 0.01022, valid loss: 0.00810\nepoch: 145, train loss: 0.01020, valid loss: 0.00822\nepoch: 146, train loss: 0.01016, valid loss: 0.00778\nepoch: 147, train loss: 0.01042, valid loss: 0.00790\nepoch: 148, train loss: 0.01027, valid loss: 0.00781\nepoch: 149, train loss: 0.01032, valid loss: 0.00742\nepoch: 150, train loss: 0.01018, valid loss: 0.00779\nepoch: 151, train loss: 0.01032, valid loss: 0.00830\nepoch: 152, train loss: 0.01028, valid loss: 0.00748\nepoch: 153, train loss: 0.01045, valid loss: 0.00763\nepoch: 154, train loss: 0.01025, valid loss: 0.00754\nepoch: 155, train loss: 0.01020, valid loss: 0.00739\nepoch: 156, train loss: 0.01022, valid loss: 0.00768\nepoch: 157, train loss: 0.01021, valid loss: 0.00727\nepoch: 158, train loss: 0.01021, valid loss: 0.00834\nepoch: 159, train loss: 0.01026, valid loss: 0.00809\nepoch: 160, train loss: 0.01027, valid loss: 0.00814\nepoch: 161, train loss: 0.01041, valid loss: 0.00773\nepoch: 162, train loss: 0.01028, valid loss: 0.00737\nepoch: 163, train loss: 0.01017, valid loss: 0.00785\nepoch: 164, train loss: 0.01015, valid loss: 0.00795\nepoch: 165, train loss: 0.01021, valid loss: 0.00808\nepoch: 166, train loss: 0.01023, valid loss: 0.00769\nepoch: 167, train loss: 0.01027, valid loss: 0.00792\nepoch: 168, train loss: 0.01031, valid loss: 0.00753\nepoch: 169, train loss: 0.01026, valid loss: 0.00753\nepoch: 170, train loss: 0.01020, valid loss: 0.00774\nepoch: 171, train loss: 0.01027, valid loss: 0.00768\nepoch: 172, train loss: 0.01025, valid loss: 0.00806\nepoch: 173, train loss: 0.01019, valid loss: 0.00826\nepoch: 174, train loss: 0.01034, valid loss: 0.00841\nepoch: 175, train loss: 0.01025, valid loss: 0.00751\nepoch: 176, train loss: 0.01025, valid loss: 0.00740\nepoch: 177, train loss: 0.01026, valid loss: 0.00800\nepoch: 178, train loss: 0.01045, valid loss: 0.00810\nepoch: 179, train loss: 0.01028, valid loss: 0.00799\nepoch: 180, train loss: 0.01039, valid loss: 0.00827\nepoch: 181, train loss: 0.01020, valid loss: 0.00768\nepoch: 182, train loss: 0.01031, valid loss: 0.00794\nepoch: 183, train loss: 0.01027, valid loss: 0.00806\nepoch: 184, train loss: 0.01019, valid loss: 0.00821\nepoch: 185, train loss: 0.01035, valid loss: 0.00847\nepoch: 186, train loss: 0.01026, valid loss: 0.00766\nepoch: 187, train loss: 0.01032, valid loss: 0.00747\nepoch: 188, train loss: 0.01025, valid loss: 0.00788\nepoch: 189, train loss: 0.01025, valid loss: 0.00774\nepoch: 190, train loss: 0.01027, valid loss: 0.00853\nepoch: 191, train loss: 0.01024, valid loss: 0.00778\nepoch: 192, train loss: 0.01026, valid loss: 0.00717\nepoch: 193, train loss: 0.01019, valid loss: 0.00781\nepoch: 194, train loss: 0.01017, valid loss: 0.00754\nepoch: 195, train loss: 0.01024, valid loss: 0.00798\nepoch: 196, train loss: 0.01019, valid loss: 0.00760\nepoch: 197, train loss: 0.01025, valid loss: 0.00762\nepoch: 198, train loss: 0.01025, valid loss: 0.00773\nepoch: 199, train loss: 0.01018, valid loss: 0.00771\n\n\nThat is it! We have trained our linear model and can view its learned weight and bias. Note that after training, they are much closer to true_m and true_b, which we used to generate the data. We can say that our model has learned the inherent pattern from within the data.\n\n## \n# model weights after training\nprint(model.state_dict())\n\nOrderedDict([('weight', tensor([[1.9914]])), ('bias', tensor([1.0314]))])"
  },
  {
    "objectID": "posts/2022-10-10-pytorch-linear-regression.html#visualize-loss-logs-from-tensorboard",
    "href": "posts/2022-10-10-pytorch-linear-regression.html#visualize-loss-logs-from-tensorboard",
    "title": "Linear Regression with PyTorch",
    "section": "Visualize loss logs from TensorBoard",
    "text": "Visualize loss logs from TensorBoard\nLet’s view how our training and validation loss has progressed throughout the training using TensorBoard.\n\nf'runs/simple_linear_regression/{timestamp}'\n\n'runs/simple_linear_regression/2022-10-10-12.02.43'\n\n\n\n%load_ext tensorboard\n%tensorboard --logdir runs\n\n\n\n\n\n\n\ntensorboard.PNG"
  },
  {
    "objectID": "posts/2022-10-11-pytorch-two-class-logistic-regression.html#introduction",
    "href": "posts/2022-10-11-pytorch-two-class-logistic-regression.html#introduction",
    "title": "Two Class (Binary) Logistic Regression in Pytorch",
    "section": "Introduction",
    "text": "Introduction\nIn this notebook, we will train a logistic regression model using PyTorch. Given below is the summary of the steps followed in this notebook.\n\nCreate a synthetic binary class dataset\nSplit the data into Train and Validation datasets. Then convert them into mini-batches using PyTorch DataLoader class\nCreate a Neural Net model configuration, an SGD optimizer, and a loss function\nCreate a pipeline that will train the model on given data and update the weights based on the loss\nCompare the results with a scikit-learn logistic regression model"
  },
  {
    "objectID": "posts/2022-10-11-pytorch-two-class-logistic-regression.html#environment",
    "href": "posts/2022-10-11-pytorch-two-class-logistic-regression.html#environment",
    "title": "Two Class (Binary) Logistic Regression in Pytorch",
    "section": "Environment",
    "text": "Environment\nThis notebook is prepared with Google Colab.\n\n\nCode\nfrom platform import python_version\nimport sklearn, numpy, matplotlib, pandas, torch\n\nprint(\"python==\" + python_version())\nprint(\"sklearn==\" + sklearn.__version__)\nprint(\"numpy==\" + numpy.__version__)\nprint(\"torch==\" + torch.__version__)\nprint(\"matplotlib==\" + matplotlib.__version__)\n\n\npython==3.7.14\nsklearn==1.0.2\nnumpy==1.21.6\ntorch==1.12.1+cu113\nmatplotlib==3.2.2"
  },
  {
    "objectID": "posts/2022-10-11-pytorch-two-class-logistic-regression.html#credits",
    "href": "posts/2022-10-11-pytorch-two-class-logistic-regression.html#credits",
    "title": "Two Class (Binary) Logistic Regression in Pytorch",
    "section": "Credits",
    "text": "Credits\nThis notebook takes inspiration from the book “Deep Learning with PyTorch Step-by-Step” by “Daniel Voigt Godoy”. You can get the book from its website: pytorchstepbystep. In addition, the GitHub repository for this book has valuable notebooks and can be used independently: github.com/dvgodoy/PyTorchStepByStep. Parts of the code you see in this notebook are taken from chapter 3 notebook of the same book."
  },
  {
    "objectID": "posts/2022-10-11-pytorch-two-class-logistic-regression.html#generate-synthetic-data",
    "href": "posts/2022-10-11-pytorch-two-class-logistic-regression.html#generate-synthetic-data",
    "title": "Two Class (Binary) Logistic Regression in Pytorch",
    "section": "Generate synthetic data",
    "text": "Generate synthetic data\nIn this section, we will generate some data representing two interleaving half-circles using sklearn.datasets.make_moons. The purpose of make_moons function is defined as\n\nMake two interleaving half circles. A simple toy dataset to visualize clustering and classification algorithms … It generates 2d binary classification datasets that are challenging to certain algorithms (e.g. centroid-based clustering or linear classification), including optional Gaussian noise.\n\n\n## \n# Synthetic data generation\nfrom sklearn.datasets import make_moons\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\n\nX, y = make_moons(n_samples=100, noise=0.3, random_state=0)\n\n# split data into train-validation sets using 80-20 ratio\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=.2, random_state=13)\n\n# standardize data\nsc = StandardScaler()\nsc.fit(X_train)\n\nX_train = sc.transform(X_train)\nX_val = sc.transform(X_val)\n\nLet’s view the first ten elements of the generated data. Note that X_train has two features (2 columns), and y_train has 0,1 classes as labels.\n\nX_train[:10], y_train[:10]\n\n(array([[-0.59635346, -0.51713419],\n        [ 0.3937561 , -1.35813138],\n        [ 1.33167696, -1.16636502],\n        [-1.52208256, -0.33314461],\n        [-1.20280449,  0.64649722],\n        [-0.65443973,  0.48658224],\n        [ 1.00612744, -1.81018492],\n        [-0.28996374, -1.5477782 ],\n        [ 0.03349394, -0.65113935],\n        [-0.94744907,  0.76650095]]), array([1, 1, 1, 0, 0, 1, 1, 1, 1, 0]))\n\n\nLet’s plot our generated data to see how it looks.\n\nimport matplotlib.pyplot as plt\n\nfigure, axes = plt.subplots(1, 3, figsize=(15,5))\nfigure.suptitle('Train and Validation Dataset')\n\naxes[0].set_title('Training Data')\naxes[0].scatter(X_train[:, 0], X_train[:, 1], c=y_train)\n\naxes[1].set_title('Validation Data')\naxes[1].scatter(X_val[:, 0], X_val[:, 1], c=y_val)\n\naxes[2].set_title('Combined Data')\naxes[2].scatter(X_train[:, 0], X_train[:, 1], c=y_train)\naxes[2].scatter(X_val[:, 0], X_val[:, 1], c=y_val)\nplt.show()"
  },
  {
    "objectID": "posts/2022-10-11-pytorch-two-class-logistic-regression.html#load-generated-data-into-pytorch-dataset-and-dataloader-class",
    "href": "posts/2022-10-11-pytorch-two-class-logistic-regression.html#load-generated-data-into-pytorch-dataset-and-dataloader-class",
    "title": "Two Class (Binary) Logistic Regression in Pytorch",
    "section": "Load generated data into PyTorch Dataset and DataLoader class",
    "text": "Load generated data into PyTorch Dataset and DataLoader class\nIn this section, we will load our data in PyTorch helper classes Dataset and DataLoader. PyTorch documentation defines them as: [see basics/data_tutorial]\n\nCode for processing data samples can get messy and hard to maintain; we ideally want our dataset code to be decoupled from our model training code for better readability and modularity. PyTorch provides two data primitives: torch.utils.data.DataLoader and torch.utils.data.Dataset that allow you to use pre-loaded datasets as well as your own data. Dataset stores the samples and their corresponding labels, and DataLoader wraps an iterable around the Dataset to enable easy access to the samples.\n\nFor this, we first need to convert NumPy data arrays to PyTorch tensors.\n\nimport torch\n\n# Builds tensors from numpy arrays\nx_train_tensor = torch.as_tensor(X_train).float()\ny_train_tensor = torch.as_tensor(y_train.reshape(-1, 1)).float()\n\nx_val_tensor = torch.as_tensor(X_val).float()\ny_val_tensor = torch.as_tensor(y_val.reshape(-1, 1)).float()\n\nNow load the tensors into Dataset and DataLoader class. PyTorch Dataset is a helper class that converts data and labels into a list of tuples. DataLoader is another helper class to create batches from Dataset tuples. batch_size means the number of tuples we want in a single batch. We have used 16 here since our data is small. So each fetch from DataLoader will give us a list of 16 tuples.\n\n## \n# Load tensors into Dataset and DataLoader\nfrom torch.utils.data import DataLoader, TensorDataset\n\n# Builds dataset containing ALL data points\ntrain_dataset = TensorDataset(x_train_tensor, y_train_tensor)\nval_dataset = TensorDataset(x_val_tensor, y_val_tensor)\n\n# Builds a loader of each set\ntrain_loader = DataLoader(dataset=train_dataset, batch_size=16, shuffle=True)\nval_loader = DataLoader(dataset=val_dataset, batch_size=16)"
  },
  {
    "objectID": "posts/2022-10-11-pytorch-two-class-logistic-regression.html#define-a-class-to-implement-training-validation-and-mini-batch-processing-pipeline",
    "href": "posts/2022-10-11-pytorch-two-class-logistic-regression.html#define-a-class-to-implement-training-validation-and-mini-batch-processing-pipeline",
    "title": "Two Class (Binary) Logistic Regression in Pytorch",
    "section": "Define a class to implement training, validation, and mini-batch processing pipeline",
    "text": "Define a class to implement training, validation, and mini-batch processing pipeline\nIn this section we will implement a class that encapsulates all the usual steps required in training a PyTorch model. This way we can focus more on the model architecture and performance, and less concerned about the boilerplate training loop. Important parts of this class are\n\n__init__: Class constructor to define the main actors in a training cycle including model, optimizer, loss function, training and validation DataLoaders\n_make_train_step_fn: Training pipeline is usually called “training step” which includes the following steps\n\nCompute our model’s predicted output - the forward pass\nCompute the loss\nCompute gradients i.e., find the direction and scale to update the weights to reduce the loss\nUpdate weight parameters using gradients and the learning rate\n\n_make_val_step_fn: Validation pipeline is usually called the “validation step” which includes the following steps\n\nCompute our model’s predicted output - the forward pass\nCompute the loss\nNote that during validation, we are only concerned about the loss, i.e., how well our model performs on the validation dataset. Therefore, we don’t use it to calculate the gradients.\n\n_mini_batch: It defines the steps to process a single minibatch in a helper function. For a mini-batch processing, we want to\n\nGet the next batch of data and labels (x, y) from the DataLoader iterator\nPerform a step on the batch. A step can be either training or validation\nCompute the average batch loss\n\ntrain: Execute training and validation steps for given number of epoch\npredict: Make a prediction from model on provided data\n\n\nimport numpy as np\nimport datetime\nfrom torch.utils.tensorboard import SummaryWriter\n\nclass DeepLearningPipeline(object):\n    def __init__(self, model, loss_fn, optimizer):\n        # Here we define the attributes of our class\n        \n        # We start by storing the arguments as attributes \n        # to use them later\n        self.model = model\n        self.loss_fn = loss_fn\n        self.optimizer = optimizer\n        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n        # Let's send the model to the specified device right away\n        self.model.to(self.device)\n\n        # These attributes are defined here, but since they are\n        # not informed at the moment of creation, we keep them None\n        self.train_loader = None\n        self.val_loader = None\n        self.writer = None\n        \n        # These attributes are going to be computed internally\n        self.losses = []\n        self.val_losses = []\n        self.total_epochs = 0\n\n        # Creates the train_step function for our model, \n        # loss function and optimizer\n        # Note: there are NO ARGS there! It makes use of the class\n        # attributes directly\n        self.train_step_fn = self._make_train_step_fn()\n        # Creates the val_step function for our model and loss\n        self.val_step_fn = self._make_val_step_fn()\n\n    def set_loaders(self, train_loader, val_loader=None):\n        # This method allows the user to define which train_loader (and val_loader, optionally) to use\n        # Both loaders are then assigned to attributes of the class\n        # So they can be referred to later\n        self.train_loader = train_loader\n        self.val_loader = val_loader\n\n    def _make_train_step_fn(self):\n        # This method does not need ARGS... it can refer to\n        # the attributes: self.model, self.loss_fn and self.optimizer\n        \n        # Builds function that performs a step in the train loop\n        def perform_train_step_fn(x, y):\n            # Sets model to TRAIN mode\n            self.model.train()\n\n            # Step 1 - Computes our model's predicted output - forward pass\n            yhat = self.model(x)\n            # Step 2 - Computes the loss\n            loss = self.loss_fn(yhat, y)\n            # Step 3 - Computes gradients for both \"a\" and \"b\" parameters\n            loss.backward()\n            # Step 4 - Updates parameters using gradients and the learning rate\n            self.optimizer.step()\n            self.optimizer.zero_grad()\n\n            # Returns the loss\n            return loss.item()\n\n        # Returns the function that will be called inside the train loop\n        return perform_train_step_fn\n    \n    def _make_val_step_fn(self):\n        # Builds function that performs a step in the validation loop\n        def perform_val_step_fn(x, y):\n            # Sets model to EVAL mode\n            self.model.eval()\n\n            # Step 1 - Computes our model's predicted output - forward pass\n            yhat = self.model(x)\n            # Step 2 - Computes the loss\n            loss = self.loss_fn(yhat, y)\n            # There is no need to compute Steps 3 and 4, \n            # since we don't update parameters during evaluation\n            return loss.item()\n\n        return perform_val_step_fn\n            \n    def _mini_batch(self, validation=False):\n        # The mini-batch can be used with both loaders\n        # The argument `validation`defines which loader and \n        # corresponding step function is going to be used\n        if validation:\n            data_loader = self.val_loader\n            step_fn = self.val_step_fn\n        else:\n            data_loader = self.train_loader\n            step_fn = self.train_step_fn\n\n        if data_loader is None:\n            return None\n            \n        # Once the data loader and step function, this is the \n        # same mini-batch loop we had before\n        mini_batch_losses = []\n        for x_batch, y_batch in data_loader:\n            x_batch = x_batch.to(self.device)\n            y_batch = y_batch.to(self.device)\n\n            mini_batch_loss = step_fn(x_batch, y_batch)\n            mini_batch_losses.append(mini_batch_loss)\n\n        loss = np.mean(mini_batch_losses)\n        return loss\n\n    def set_seed(self, seed=42):\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False    \n        torch.manual_seed(seed)\n        np.random.seed(seed)\n    \n    def train(self, n_epochs, seed=42):\n        # To ensure reproducibility of the training process\n        self.set_seed(seed)\n\n        for epoch in range(n_epochs):\n            # Keeps track of the numbers of epochs\n            # by updating the corresponding attribute\n            self.total_epochs += 1\n\n            # inner loop\n            # Performs training using mini-batches\n            loss = self._mini_batch(validation=False)\n            self.losses.append(loss)\n\n            # VALIDATION\n            # no gradients in validation!\n            with torch.no_grad():\n                # Performs evaluation using mini-batches\n                val_loss = self._mini_batch(validation=True)\n                self.val_losses.append(val_loss)\n\n            # If a SummaryWriter has been set...\n            if self.writer:\n                scalars = {'training': loss}\n                if val_loss is not None:\n                    scalars.update({'validation': val_loss})\n                # Records both losses for each epoch under the main tag \"loss\"\n                self.writer.add_scalars(main_tag='loss',\n                                        tag_scalar_dict=scalars,\n                                        global_step=epoch)\n\n        if self.writer:\n            # Closes the writer\n            self.writer.close()\n\n    def predict(self, x):\n        # Set is to evaluation mode for predictions\n        self.model.eval() \n        # Takes aNumpy input and make it a float tensor\n        x_tensor = torch.as_tensor(x).float()\n        # Send input to device and uses model for prediction\n        y_hat_tensor = self.model(x_tensor.to(self.device))\n        # Set it back to train mode\n        self.model.train()\n        # Detaches it, brings it to CPU and back to Numpy\n        return y_hat_tensor.detach().cpu().numpy()\n\n    def plot_losses(self):\n        fig = plt.figure(figsize=(10, 4))\n        plt.plot(self.losses, label='Training Loss', c='b')\n        plt.plot(self.val_losses, label='Validation Loss', c='r')\n        plt.yscale('log')\n        plt.xlabel('Epochs')\n        plt.ylabel('Loss')\n        plt.legend()\n        plt.tight_layout()\n        return fig"
  },
  {
    "objectID": "posts/2022-10-11-pytorch-two-class-logistic-regression.html#create-model-configuration",
    "href": "posts/2022-10-11-pytorch-two-class-logistic-regression.html#create-model-configuration",
    "title": "Two Class (Binary) Logistic Regression in Pytorch",
    "section": "Create model configuration",
    "text": "Create model configuration\nIn this section, we will configure the model for training, define a loss function, and an optimizer to update the weights.\nThere are two ways in which we can define our logistic classifier.\nFirst Approach: A model with a single linear layer and no activation function at the end. In this case, the output from the model will not be probabilities, and the loss function we use is nn.BCEWithLogitsLoss. This way, our model is similar to a linear regression model but with a different loss function. ‘BCEWithLogitsLoss’ is a variant of Binary Cross Entropy loss function (nn.BCELoss) and is defined as ‘numerically more stable’ [see docs torch.nn.BCEWithLogitsLoss.html]\n\nThis loss combines a Sigmoid layer and the BCELoss in one single class. This version is more numerically stable than using a plain Sigmoid followed by a BCELoss as, by combining the operations into one layer, we take advantage of the log-sum-exp trick for numerical stability.\n\nIn this approach, we train a model without ‘Sigmoid’ layer. But at the time of classification, we pass the output (called as logits) from the model to ‘Sigmoid’ function to get class probabilities.\nSecond Approach: Here we have an activation function (nn.Sigmoid) after the Linear layer. In this case, we have probabilities as an output. The loss function we use in this case is torch.nn.BCELoss.\nLet’s try both these approaches.\n\nLogistic regression model without Sigmoid layer\n\n##\n# Logistic model configuration without Sigmoid layer\nimport torch.nn as nn\nimport torch.optim as optim\n\nlr = 0.1\n\ntorch.manual_seed(42)\nmodel_1 = nn.Sequential()\nmodel_1.add_module('linear', nn.Linear(2, 1))\n\n# Defines a SGD optimizer to update the parameters\noptimizer_1 = optim.SGD(model_1.parameters(), lr=lr)\n\n# Defines a BCE loss function\nloss_fn_1 = nn.BCEWithLogitsLoss()\n\nNow let’s train our model for 100 epochs.\n\nn_epochs = 100\n\ndlp_1 = DeepLearningPipeline(model_1, loss_fn_1, optimizer_1)\ndlp_1.set_loaders(train_loader, val_loader)\ndlp_1.train(n_epochs)\n\nLet’s see how our training and validation loss looks like.\n\nfig = dlp_1.plot_losses()\n\n\n\n\nLet’s also print the weights learned by our model. Note that there are two weights in the linear layer, as there were two features (or columns) for our X_train data.\n\nprint(model_1.state_dict())\n\nOrderedDict([('linear.weight', tensor([[ 1.1806, -1.8693]])), ('linear.bias', tensor([-0.0591]))])\n\n\nLet’s also create a confusion matrix for our validation data. Note that here we have used torch.sigmoid to convert the output from the model into probabilities.\n\nfrom sklearn.metrics import confusion_matrix\n\nlogits_val = dlp_1.predict(X_val)\nlogits_val_tensor = torch.from_numpy(logits_val)\nprobabilities_val = torch.sigmoid(logits_val_tensor).squeeze()\ncm_thresh50 = confusion_matrix(y_val, (probabilities_val >= 0.5))\ncm_thresh50\n\narray([[ 7,  2],\n       [ 1, 10]])\n\n\nLet’s also print the output from model for five validation data points. Again, remember that the output is logits, not probabilities.\n\nlogits_val[:5]\n\narray([[-0.37522304],\n       [ 0.7390274 ],\n       [-2.5800889 ],\n       [-0.93623203],\n       [-1.6819004 ]], dtype=float32)\n\n\n\n\nLogistic regression model with Sigmoid layer\nNow let’s again create our model, but this time ‘Sigmoid’ layer is attached at the end.\n\n##\n# Logistic model configuration with Sigmoid layer\nmodel_2 = nn.Sequential()\nmodel_2.add_module('linear', nn.Linear(2, 1))\nmodel_2.add_module('sigmoid', nn.Sigmoid())\n\n# Defines a SGD optimizer to update the parameters\noptimizer_2 = optim.SGD(model_2.parameters(), lr=lr)\n\n# Defines a BCE loss function\nloss_fn_2 = nn.BCELoss(reduction='mean')\n\n\nn_epochs = 100\n\ndlp_2 = DeepLearningPipeline(model_2, loss_fn_2, optimizer_2)\ndlp_2.set_loaders(train_loader, val_loader)\ndlp_2.train(n_epochs)\n\n\nfig = dlp_2.plot_losses()\n\n\n\n\nLet’s print the learned weights. They are slightly different from the last model.\n\nprint(model_2.state_dict())\n\nOrderedDict([('linear.weight', tensor([[ 1.1794, -1.8716]])), ('linear.bias', tensor([-0.0604]))])\n\n\nLet’s also create a confusion matrix for comparison. Note that the results are same as from the first model.\n\nfrom sklearn.metrics import confusion_matrix\n\nprobabilities_val = dlp_2.predict(X_val).squeeze()\ncm_thresh50 = confusion_matrix(y_val, (probabilities_val >= 0.5))\ncm_thresh50\n\narray([[ 7,  2],\n       [ 1, 10]])\n\n\nLet’s also print the model output for five validation data points. Each output shows the probability of a point belonging to class 0 or 1. The points with a probability greater than 0.5 are put into class 1, and the remaining are placed into class 0. So from the below output\n\n0.4070907 –> class 0\n0.6768992 –> class 1\n0.0704094 –> class 0\n\n\nprobabilities_val[:5]\n\narray([0.4070907 , 0.67689914, 0.07040942, 0.28118652, 0.15702702],\n      dtype=float32)\n\n\n\n\nComparison with Scikit-learn LogisticRegression model\nLet’s also compare our model with sklearn logistic regression and see how our neural net model compares to it.\n\n##\n# Comparison with sklearn logistic regression\n\nfrom sklearn.linear_model import LogisticRegression\nlogreg = LogisticRegression()\nlogreg.fit(X_train, y_train)\n\nLogisticRegression()\n\n\nLet’s see the weights learned by the model.\n\nlogreg.coef_ , logreg.intercept_\n\n(array([[ 1.03955962, -1.60876812]]), array([-0.05857131]))\n\n\nLet’s create a confusion matrix for comparison. Note that the results are identical.\n\npredictions = logreg.predict(X_val)\nprobabilities_val = logreg.predict_proba(X_val)\ncm_thresh50 = confusion_matrix(y_val, predictions)\ncm_thresh50\n\narray([[ 7,  2],\n       [ 1, 10]])\n\n\nLet’s also print the output from model for five validation data points. Again, remember the output is probabilities, and the sum of each row is 1.\nNote that the scikit-learn model output has two columns. Both show the probabilities of a point for class 0 or 1 (left to right). PyTorch model only outputs the probability of a point belonging to class 1 (right column). Since the sum of probabilities is equal to 1, we can find the other class probability (for binary classifiers only) if we have one class probability.\n\nprobabilities_val[:5]\n\narray([[0.58380158, 0.41619842],\n       [0.34949557, 0.65050443],\n       [0.90582984, 0.09417016],\n       [0.69290104, 0.30709896],\n       [0.81695121, 0.18304879]])"
  },
  {
    "objectID": "posts/2022-10-11-pytorch-two-class-logistic-regression.html#why-classification-is-called-logistic-regression",
    "href": "posts/2022-10-11-pytorch-two-class-logistic-regression.html#why-classification-is-called-logistic-regression",
    "title": "Two Class (Binary) Logistic Regression in Pytorch",
    "section": "Why classification is called logistic regression",
    "text": "Why classification is called logistic regression\nWhat classification has to do with regression, and why do we call it logistic regression? Remember that our binary classifier model has a linear layer with a logistic function at the end. A logistic function is an S-shaped curve function wiki/Logistic_function.\n\n\n\nlogistic_model.png\n\n\nImage taken from PyTorchStepByStep GitHub repository.\nWhen doing linear regression, we are trying to find a line that best fits our data points. We measure our error by finding the distance between each data point and our fitted line. If the distance between points and the line is minimum, we say we have found the best line that fits our data. Otherwise, we wiggle it up and down slowly till our errors are minimum.\nIn the case of logistic regression, we are also fitting a line on our data such that it can separate them into distinct categories. Points on one side of the line belong to class A, and points on the other side belong to class B. We usually call this line a decision boundary.\nHow do we measure our error in this case? We can measure error by counting how many points we have correctly classified. But just using a count is a very rough measurement. Because there can be multiple angles on which we can place a line in data and still be able to classify the points by the exact count. There should be a better way to tell us that a particular line angle is better than all others. For this, we use probabilities from a sigmoid (logistic) function. This helps us to capture the errors in a better way.\nIf a data point is on one side of the line (decision boundary) but is further away from it, we say it has a high probability of being in class A. If a point is close to the line, we say it has a low probability of being in class A. And we can extend this logic for points on the other side of the line. If a data point is on the other side of the decision boundary but close to the line, we give it a low probability of being in class B. And if a point is on the other side but farther away, we give it a high probability of being in class B.\n\nDuring training, we are trying to find a line that maximizes the certainty of data points for being in their correct classes.\n\nLet’s create a plot to see how our decision boundary looks like for our trained models.\n\n\nCode\ndef plot_decision_boundary(model, X_train, X_val):\n    x_min, x_max = -2, 2\n    y_min, y_max = -2, 2\n    h = .02\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n\n    # Check if model is PyTorch model\n    if type(model) == torch.nn.modules.container.Sequential:\n        # get predictions\n        logits = model(torch.as_tensor(np.c_[xx.ravel(), yy.ravel()]).float())\n        \n        # check if model's last layer is Linear, then apply Sigmoid on predictions to get probabilities\n        if type(model[-1]) == torch.nn.modules.linear.Linear:\n            logits_proba = torch.sigmoid(logits).squeeze()\n        else:\n            logits_proba = logits.clone()\n        \n        logits = logits.detach().cpu().numpy().reshape(xx.shape)\n        logits_proba = logits_proba.detach().cpu().numpy().reshape(xx.shape)\n    else: # Sklean model\n        logits = logreg.predict_proba(np.c_[xx.ravel(), yy.ravel()])\n        logits = logits[:,1]\n        logits = logits.reshape(xx.shape)\n        logits_proba = logits\n\n    fig = plt.figure(figsize=(14,7))\n    fig.suptitle('Plot Model Decision Boundary')\n\n    # Plot 1\n    ax1 = plt.subplot(1,3,1)\n    ax1.set_title('In 2D Plane')\n    # ax1.scatter(X_train[:, 0], X_train[:, 1], c=y_train)\n    ax1.scatter(X_val[:, 0], X_val[:, 1], c=y_val)\n    ax1.contour(xx, yy, logits_proba, levels=[0.5], cmap=\"Greys\", vmin=0, vmax=1)\n\n    # Plot 2\n    ax2 = plt.subplot(1,3,2, projection='3d')\n    ax2.set_title('In 3D Plane - View 1')\n    # ax2.scatter(X_train[:, 0], X_train[:, 1], c=y_train)\n    ax2.scatter(X_val[:, 0], X_val[:, 1], c=y_val)\n    ax2.plot_surface(xx, yy, logits, color='lightgreen')\n    ax2.view_init(10, 10)\n\n    # Plot 3\n    ax3 = plt.subplot(1,3,3, projection='3d')\n    ax3.set_title('3D Plane - View 2')\n    # ax3.scatter(X_train[:, 0], X_train[:, 1], c=y_train)\n    ax3.scatter(X_val[:, 0], X_val[:, 1], c=y_val)\n    ax3.plot_surface(xx, yy, logits, color='lightgreen')\n    \n    plt.show()\n\n\n\nDecision boundary for logistic regression model without Sigmoid\nLet’s view the decision boundary for the first model we built. That model has only a Linear layer and no Sigmoid function at the end. Notice that the decision boundary is a straight line (or a plane in 3D) that cuts the data in a way that maximizes the certainty of data for being in one class or the other.\n\nplot_decision_boundary(model_1, X_train, X_val)\n\n\n\n\n\n\nDecision boundary of a logistic regression model with Sigmoid\nLet’s draw the decision boundary for our classification model with Sigmoid as the last layer.\nNote that it is not a line this time when we view the decision boundary in 3D space. The 3D plane is more like an S-shaped curve. Because now our model output is probabilities coming out of a logistic function. Think of it as an extra step that is now added to the output of the first model (without sigmoid). The points on the extreme right side of the first model linear plane are given the lowest probabilities (almost zero). And as we move to the left, the possibilities gradually increase with 0.5 at the middle and highest when we reach the extreme left (almost 1).\n\nplot_decision_boundary(model_2, X_train, X_val)\n\n\n\n\n\n\nDecision boundary of sklearn logistic regression model\nLet’s now plot the same for sklearn model. The plot is similar to the last model.\n\nplot_decision_boundary(logreg, X_train, X_val)"
  },
  {
    "objectID": "posts/2022-10-11-pytorch-two-class-logistic-regression.html#can-we-do-better",
    "href": "posts/2022-10-11-pytorch-two-class-logistic-regression.html#can-we-do-better",
    "title": "Two Class (Binary) Logistic Regression in Pytorch",
    "section": "Can we do better?",
    "text": "Can we do better?\nWe have seen that our models’ decision boundaries are linear (a line), and on the validation set they misclassify 3 points (two purple dots below the line and one yellow dot above the line). What if we can bend our decision boundary a little? Will it be able to better capture the classes in validation data? Let’s test it.\nWe introduce non-linearity to our decision boundary by placing a non-linear activation function between our neural net layers. Let’s create another model but this time use two hidden layers with a non-linear activation function (ReLU) in between them.\n\n##\n# Logistic model configuration with two hidden layers and ReLU in between\n# No Sigmoid layer at the end\nmodel_3 = nn.Sequential()\nmodel_3.add_module('linear1', nn.Linear(2, 10))\nmodel_3.add_module('activation1', nn.ReLU())\nmodel_3.add_module('linear2', nn.Linear(10, 1))\n\n# Defines a SGD optimizer to update the parameters\noptimizer_3 = optim.SGD(model_3.parameters(), lr=lr)\n\n# Defines a BCE loss function\nloss_fn_3 = nn.BCEWithLogitsLoss()\n\nTrain the model for 100 epochs.\n\nn_epochs = 100\n\ndlp_3 = DeepLearningPipeline(model_3, loss_fn_3, optimizer_3)\ndlp_3.set_loaders(train_loader, val_loader)\ndlp_3.train(n_epochs)\n\nLet’s view model’s performance.\n\nfig = dlp_3.plot_losses()\n\n\n\n\nLet’s print the learned model weights. They are many now.\n\nprint(model_3.state_dict())\n\nOrderedDict([('linear1.weight', tensor([[-0.6459,  0.2700],\n        [-1.0339, -0.3841],\n        [ 0.2376, -0.9286],\n        [-0.2675, -0.0527],\n        [ 0.5548,  0.7580],\n        [ 0.9121, -0.9306],\n        [-0.4046,  1.0968],\n        [ 0.2245, -0.0907],\n        [ 0.0439, -0.4252],\n        [-0.2865,  0.9936]])), ('linear1.bias', tensor([-0.3062, -0.6352, -0.4185, -0.5086,  0.2363, -0.1485,  0.2418,  0.3736,\n        -0.7792,  0.4397])), ('linear2.weight', tensor([[-0.5564, -1.0559,  0.6160,  0.1544, -0.4395,  1.0762, -0.9444,  0.2832,\n         -0.2357, -0.8370]])), ('linear2.bias', tensor([0.7808]))])\n\n\nLet’s create a confusion matrix. Notice that our error has slightly improved, and the model misclassifies only two purple data point from the validation set.\n\nfrom sklearn.metrics import confusion_matrix\n\nlogits_val_3 = dlp_3.predict(X_val)\nlogits_val_tensor_3 = torch.from_numpy(logits_val_3)\nprobabilities_val = torch.sigmoid(logits_val_tensor_3).squeeze()\ncm_thresh50 = confusion_matrix(y_val, (probabilities_val >= 0.5))\ncm_thresh50\n\narray([[ 7,  2],\n       [ 0, 11]])\n\n\nLet’s plot the model’s decision boundary. First, notice that it is not linear and has a bend. This effect is due to placing a non-linear activation function after a linear layer. And, it has correctly captured a yellow point and barely missed purple points.\n\nplot_decision_boundary(model_3, X_train, X_val)\n\n\n\n\n\n##\n# Confusion matrix plot\nfrom sklearn.metrics import ConfusionMatrixDisplay\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm_thresh50, display_labels=[0,1])\ndisp.plot()\nplt.show()"
  },
  {
    "objectID": "posts/2022-10-18-pytorch-mnist-convolutional-neural-networks.html#introduction",
    "href": "posts/2022-10-18-pytorch-mnist-convolutional-neural-networks.html#introduction",
    "title": "Convolutional Neural Networks Filters and Feature Maps with PyTorch",
    "section": "Introduction",
    "text": "Introduction\nWe will train a LeNet-5 CNN model with PyTorch on the MNIST dataset in this notebook. Given below is the summary of the steps followed in this notebook.\n\nDownload the MNIST dataset. Split the data into Train and Validation datasets. Then convert them into mini-batches using PyTorch DataLoader class\nCreate a Neural Net model configuration, an SGD optimizer, and a loss function\nCreate a pipeline that will train the model on given data and update the weights based on the loss\nVisualize filters and feature maps from the trained model"
  },
  {
    "objectID": "posts/2022-10-18-pytorch-mnist-convolutional-neural-networks.html#environment",
    "href": "posts/2022-10-18-pytorch-mnist-convolutional-neural-networks.html#environment",
    "title": "Convolutional Neural Networks Filters and Feature Maps with PyTorch",
    "section": "Environment",
    "text": "Environment\nThis notebook is prepared with Google Colab.\n\n\nCode\nfrom platform import python_version\nimport numpy, matplotlib, pandas, torch\n\nprint(\"python==\" + python_version())\nprint(\"numpy==\" + numpy.__version__)\nprint(\"torch==\" + torch.__version__)\nprint(\"matplotlib==\" + matplotlib.__version__)\n\n\npython==3.7.15\nnumpy==1.21.6\ntorch==1.12.1+cu113\nmatplotlib==3.2.2"
  },
  {
    "objectID": "posts/2022-10-18-pytorch-mnist-convolutional-neural-networks.html#credits",
    "href": "posts/2022-10-18-pytorch-mnist-convolutional-neural-networks.html#credits",
    "title": "Convolutional Neural Networks Filters and Feature Maps with PyTorch",
    "section": "Credits",
    "text": "Credits\nThis notebook takes inspiration and ideas from the following two sources.\n\nThe outstanding book “Deep Learning with PyTorch Step-by-Step” by “Daniel Voigt Godoy”. You can get the book from its website: pytorchstepbystep. In addition, the GitHub repository for this book has valuable notebooks and can be used independently: github.com/dvgodoy/PyTorchStepByStep. Parts of the code you see in this notebook are taken from chapter 5 notebook of the same book.\nA great post by “Eugenia Anello” with the title Visualizing the Feature Maps and Filters by Convolutional Neural Networks. You can click the title to reach the post on the medium.com platform. Eugenia Anello can also be found on her linkedin profile."
  },
  {
    "objectID": "posts/2022-10-18-pytorch-mnist-convolutional-neural-networks.html#download-mnist-dataset",
    "href": "posts/2022-10-18-pytorch-mnist-convolutional-neural-networks.html#download-mnist-dataset",
    "title": "Convolutional Neural Networks Filters and Feature Maps with PyTorch",
    "section": "Download MNIST Dataset",
    "text": "Download MNIST Dataset\nMNIST dataset can be downloaded easily from PyTorch built-in datasets provided under torchvision.datasets. In this section, we will download it, split it into train and test datasets, and then convert it into PyTorch tensors.\n\nRead more about the PyTorch MNIST dataset here\ntorchvision.transforms.Compose is like a container to hold a list of transformations you intend to apply. Read more about it here\ntorchvision.transforms.ToTensor converts a PIL Image or numpy.ndarray to tensor. It converts a PIL Image or numpy.ndarray (H x W x C) in the range [0, 255] to a torch.FloatTensor of shape (C x H x W) in the range [0.0, 1.0]. Here C=Channel, H=Height, W=Width. Read more about this transformation here\n\n\n#collapse-output\nimport torchvision\nimport numpy as np\n\ntrain_dataset = torchvision.datasets.MNIST('classifier_data', train=True, download=True)\ntest_dataset  = torchvision.datasets.MNIST('classifier_data', train=False, download=True)\n\ntransform = torchvision.transforms.Compose([\n    torchvision.transforms.ToTensor()\n])\n\ntrain_dataset.transform=transform\ntest_dataset.transform=transform\n\nprint(f\"Total training images: {len(train_dataset)}\")\nprint(f\"Shape of an image: {np.shape(train_dataset.data[7])}\")\nprint(f\"Values of an image: \\n{train_dataset.data[7]}\")\n\nTotal training images: 60000\nShape of an image: torch.Size([28, 28])\nValues of an image: \ntensor([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  38,  43, 105,\n         255, 253, 253, 253, 253, 253, 174,   6,   0,   0,   0,   0,   0,   0],\n        [  0,   0,   0,   0,   0,   0,   0,   0,   0,  43, 139, 224, 226, 252,\n         253, 252, 252, 252, 252, 252, 252, 158,  14,   0,   0,   0,   0,   0],\n        [  0,   0,   0,   0,   0,   0,   0,   0,   0, 178, 252, 252, 252, 252,\n         253, 252, 252, 252, 252, 252, 252, 252,  59,   0,   0,   0,   0,   0],\n        [  0,   0,   0,   0,   0,   0,   0,   0,   0, 109, 252, 252, 230, 132,\n         133, 132, 132, 189, 252, 252, 252, 252,  59,   0,   0,   0,   0,   0],\n        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   4,  29,  29,  24,   0,\n           0,   0,   0,  14, 226, 252, 252, 172,   7,   0,   0,   0,   0,   0],\n        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n           0,   0,   0,  85, 243, 252, 252, 144,   0,   0,   0,   0,   0,   0],\n        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n           0,   0,  88, 189, 252, 252, 252,  14,   0,   0,   0,   0,   0,   0],\n        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n          91, 212, 247, 252, 252, 252, 204,   9,   0,   0,   0,   0,   0,   0],\n        [  0,   0,   0,   0,   0,   0,   0,   0,   0,  32, 125, 193, 193, 193,\n         253, 252, 252, 252, 238, 102,  28,   0,   0,   0,   0,   0,   0,   0],\n        [  0,   0,   0,   0,   0,   0,   0,   0,  45, 222, 252, 252, 252, 252,\n         253, 252, 252, 252, 177,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n        [  0,   0,   0,   0,   0,   0,   0,   0,  45, 223, 253, 253, 253, 253,\n         255, 253, 253, 253, 253,  74,   0,   0,   0,   0,   0,   0,   0,   0],\n        [  0,   0,   0,   0,   0,   0,   0,   0,   0,  31, 123,  52,  44,  44,\n          44,  44, 143, 252, 252,  74,   0,   0,   0,   0,   0,   0,   0,   0],\n        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n           0,   0,  15, 252, 252,  74,   0,   0,   0,   0,   0,   0,   0,   0],\n        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n           0,   0,  86, 252, 252,  74,   0,   0,   0,   0,   0,   0,   0,   0],\n        [  0,   0,   0,   0,   0,   0,   5,  75,   9,   0,   0,   0,   0,   0,\n           0,  98, 242, 252, 252,  74,   0,   0,   0,   0,   0,   0,   0,   0],\n        [  0,   0,   0,   0,   0,  61, 183, 252,  29,   0,   0,   0,   0,  18,\n          92, 239, 252, 252, 243,  65,   0,   0,   0,   0,   0,   0,   0,   0],\n        [  0,   0,   0,   0,   0, 208, 252, 252, 147, 134, 134, 134, 134, 203,\n         253, 252, 252, 188,  83,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n        [  0,   0,   0,   0,   0, 208, 252, 252, 252, 252, 252, 252, 252, 252,\n         253, 230, 153,   8,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n        [  0,   0,   0,   0,   0,  49, 157, 252, 252, 252, 252, 252, 217, 207,\n         146,  45,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n        [  0,   0,   0,   0,   0,   0,   7, 103, 235, 252, 172, 103,  24,   0,\n           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0]],\n       dtype=torch.uint8)\n\n\nFrom the above cell output, there are 60,000 training images. The shape of each image is 28 x 28, which means it is a 2D matrix. We have also printed the values of one image, but they don’t make much sense unless we view them as an image. So let’s do that.\n\nimport matplotlib.pyplot as plt\n\nplt.imshow(train_dataset.data[7], cmap='gray')\n\n<matplotlib.image.AxesImage at 0x7feae1775310>"
  },
  {
    "objectID": "posts/2022-10-18-pytorch-mnist-convolutional-neural-networks.html#load-generated-data-into-pytorch-dataset-and-dataloader-class",
    "href": "posts/2022-10-18-pytorch-mnist-convolutional-neural-networks.html#load-generated-data-into-pytorch-dataset-and-dataloader-class",
    "title": "Convolutional Neural Networks Filters and Feature Maps with PyTorch",
    "section": "Load generated data into PyTorch Dataset and DataLoader class",
    "text": "Load generated data into PyTorch Dataset and DataLoader class\nNow let’s load our data into Dataset and DataLoader classes. PyTorch Dataset is a helper class that converts data and labels into a list of tuples. DataLoader is another helper class to create batches from Dataset tuples. batch_size means the number of tuples we want in a single batch. We have used 128 here, so each fetch from DataLoader will give us a list of 128 tuples.\n\nimport torch\nfrom torch.utils.data import Dataset, DataLoader, random_split\n\ntrain_size=len(train_dataset)\n\n# Randomly split the data into non-overlapping train and validation set\n# train size = 70% and validation size = 30%\ntrain_data, val_data = random_split(train_dataset, [int(train_size*0.7), int(train_size - train_size*0.7)])\n\nbatch_size=128\n\n# Load data into DataLoader class\ntrain_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size)\nvalid_loader = torch.utils.data.DataLoader(val_data, batch_size=batch_size)\n\nprint(f\"Batches in Train Loader: {len(train_loader)}\")\nprint(f\"Batches in Valid Loader: {len(valid_loader)}\")\n\nprint(f\"Examples in Train Loader: {len(train_loader.sampler)}\")\nprint(f\"Examples in Valid Loader: {len(valid_loader.sampler)}\")\n\nBatches in Train Loader: 469\nBatches in Valid Loader: 141\nExamples in Train Loader: 60000\nExamples in Valid Loader: 18000\n\n\n\n##\n# Helper function to plot images from DataLoader\ndef plot_images(images, targets, n_plot=30):\n    n_rows = n_plot // 10 + ((n_plot % 10) > 0)\n    fig, axes = plt.subplots(n_rows, 10, figsize=(15, 1.5 * n_rows))\n    axes = np.atleast_2d(axes)\n\n    for i, (image, target) in enumerate(zip(images[:n_plot], targets[:n_plot])):\n        row, col = i // 10, i % 10    \n        ax = axes[row, col]\n        ax.set_title('#{} - Label:{}'.format(i, target), {'size': 12})\n        # plot filter channel in grayscale\n        ax.imshow(image.squeeze(), cmap='gray', vmin=0, vmax=1)\n\n    for ax in axes.flat:\n        ax.set_xticks([])\n        ax.set_yticks([])\n        ax.label_outer()\n\n    plt.tight_layout()\n    return fig\n\n## Code taken from https://github.com/dvgodoy/PyTorchStepByStep/blob/master/plots/chapter5.py\n\nLet’s plot some dataset images along with their labels from a batch.\n\nimages, labels = next(iter(train_loader))\nfig = plot_images(images, labels, n_plot=40)"
  },
  {
    "objectID": "posts/2022-10-18-pytorch-mnist-convolutional-neural-networks.html#define-a-class-to-implement-training-validation-and-mini-batch-processing-pipeline",
    "href": "posts/2022-10-18-pytorch-mnist-convolutional-neural-networks.html#define-a-class-to-implement-training-validation-and-mini-batch-processing-pipeline",
    "title": "Convolutional Neural Networks Filters and Feature Maps with PyTorch",
    "section": "Define a class to implement training, validation, and mini-batch processing pipeline",
    "text": "Define a class to implement training, validation, and mini-batch processing pipeline\nIn this section we will implement a class that encapsulates all the usual steps required in training a PyTorch model. This way we can focus more on the model architecture and performance, and less concerned about the boilerplate training loop. Important parts of this class are\n\n__init__: Class constructor to define the main actors in a training cycle including model, optimizer, loss function, training and validation DataLoaders\n_make_train_step_fn: Training pipeline is usually called “training step” which includes the following steps\n\nCompute our model’s predicted output - the forward pass\nCompute the loss\nCompute gradients i.e., find the direction and scale to update the weights to reduce the loss\nUpdate weight parameters using gradients and the learning rate\n\n_make_val_step_fn: Validation pipeline is usually called the “validation step” which includes the following steps\n\nCompute our model’s predicted output - the forward pass\nCompute the loss\nNote that during validation, we are only concerned about the loss, i.e., how well our model performs on the validation dataset. Therefore, we don’t use it to calculate the gradients.\n\n_mini_batch: It defines the steps to process a single minibatch in a helper function. For a mini-batch processing, we want to\n\nGet the next batch of data and labels (x, y) from the DataLoader iterator\nPerform a step on the batch. A step can be either training or validation\nCompute the average batch loss\n\ntrain: Execute training and validation steps for given number of epoch\npredict: Make a prediction from model on provided data\n\n\n\nCode\nclass DeepLearningPipeline(object):\n    def __init__(self, model, loss_fn, optimizer):\n        # Here we define the attributes of our class\n        \n        # We start by storing the arguments as attributes \n        # to use them later\n        self.model = model\n        self.loss_fn = loss_fn\n        self.optimizer = optimizer\n        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n        # Let's send the model to the specified device right away\n        self.model.to(self.device)\n\n        # These attributes are defined here, but since they are\n        # not informed at the moment of creation, we keep them None\n        self.train_loader = None\n        self.val_loader = None\n        self.writer = None\n        \n        # These attributes are going to be computed internally\n        self.losses = []\n        self.val_losses = []\n        self.total_epochs = 0\n\n        # Creates the train_step function for our model, \n        # loss function and optimizer\n        # Note: there are NO ARGS there! It makes use of the class\n        # attributes directly\n        self.train_step_fn = self._make_train_step_fn()\n        # Creates the val_step function for our model and loss\n        self.val_step_fn = self._make_val_step_fn()\n\n    def set_loaders(self, train_loader, val_loader=None):\n        # This method allows the user to define which train_loader (and val_loader, optionally) to use\n        # Both loaders are then assigned to attributes of the class\n        # So they can be referred to later\n        self.train_loader = train_loader\n        self.val_loader = val_loader\n\n    def _make_train_step_fn(self):\n        # This method does not need ARGS... it can refer to\n        # the attributes: self.model, self.loss_fn and self.optimizer\n        \n        # Builds function that performs a step in the train loop\n        def perform_train_step_fn(x, y):\n            # Sets model to TRAIN mode\n            self.model.train()\n\n            # Step 1 - Computes our model's predicted output - forward pass\n            yhat = self.model(x)\n            # Step 2 - Computes the loss\n            loss = self.loss_fn(yhat, y)\n            # Step 3 - Computes gradients for both \"a\" and \"b\" parameters\n            loss.backward()\n            # Step 4 - Updates parameters using gradients and the learning rate\n            self.optimizer.step()\n            self.optimizer.zero_grad()\n\n            # Returns the loss\n            return loss.item()\n\n        # Returns the function that will be called inside the train loop\n        return perform_train_step_fn\n    \n    def _make_val_step_fn(self):\n        # Builds function that performs a step in the validation loop\n        def perform_val_step_fn(x, y):\n            # Sets model to EVAL mode\n            self.model.eval()\n\n            # Step 1 - Computes our model's predicted output - forward pass\n            yhat = self.model(x)\n            # Step 2 - Computes the loss\n            loss = self.loss_fn(yhat, y)\n            # There is no need to compute Steps 3 and 4, \n            # since we don't update parameters during evaluation\n            return loss.item()\n\n        return perform_val_step_fn\n            \n    def _mini_batch(self, validation=False):\n        # The mini-batch can be used with both loaders\n        # The argument `validation`defines which loader and \n        # corresponding step function is going to be used\n        if validation:\n            data_loader = self.val_loader\n            step_fn = self.val_step_fn\n        else:\n            data_loader = self.train_loader\n            step_fn = self.train_step_fn\n\n        if data_loader is None:\n            return None\n            \n        # Once the data loader and step function, this is the \n        # same mini-batch loop we had before\n        mini_batch_losses = []\n        for x_batch, y_batch in data_loader:\n            x_batch = x_batch.to(self.device)\n            y_batch = y_batch.to(self.device)\n\n            mini_batch_loss = step_fn(x_batch, y_batch)\n            mini_batch_losses.append(mini_batch_loss)\n\n        loss = np.mean(mini_batch_losses)\n        return loss\n\n    def set_seed(self, seed=42):\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False    \n        torch.manual_seed(seed)\n        np.random.seed(seed)\n    \n    def train(self, n_epochs, seed=42):\n        # To ensure reproducibility of the training process\n        self.set_seed(seed)\n\n        for epoch in range(n_epochs):\n            # Keeps track of the numbers of epochs\n            # by updating the corresponding attribute\n            self.total_epochs += 1\n\n            # inner loop\n            # Performs training using mini-batches\n            loss = self._mini_batch(validation=False)\n            self.losses.append(loss)\n\n            # VALIDATION\n            # no gradients in validation!\n            with torch.no_grad():\n                # Performs evaluation using mini-batches\n                val_loss = self._mini_batch(validation=True)\n                self.val_losses.append(val_loss)\n\n            # If a SummaryWriter has been set...\n            if self.writer:\n                scalars = {'training': loss}\n                if val_loss is not None:\n                    scalars.update({'validation': val_loss})\n                # Records both losses for each epoch under the main tag \"loss\"\n                self.writer.add_scalars(main_tag='loss',\n                                        tag_scalar_dict=scalars,\n                                        global_step=epoch)\n            \n            print(f\"epoch: {epoch:3}, train loss: {loss:.5f}, valid loss: {val_loss:.5f}\")\n\n        if self.writer:\n            # Closes the writer\n            self.writer.close()\n\n    def predict(self, x):\n        # Set is to evaluation mode for predictions\n        self.model.eval() \n        # Takes aNumpy input and make it a float tensor\n        x_tensor = torch.as_tensor(x).float()\n        # Send input to device and uses model for prediction\n        y_hat_tensor = self.model(x_tensor.to(self.device))\n        # Set it back to train mode\n        self.model.train()\n        # Detaches it, brings it to CPU and back to Numpy\n        return y_hat_tensor.detach().cpu().numpy()\n\n    def plot_losses(self):\n        fig = plt.figure(figsize=(10, 4))\n        plt.plot(self.losses, label='Training Loss', c='b')\n        plt.plot(self.val_losses, label='Validation Loss', c='r')\n        plt.yscale('log')\n        plt.xlabel('Epochs')\n        plt.ylabel('Loss')\n        plt.legend()\n        plt.tight_layout()\n        return fig"
  },
  {
    "objectID": "posts/2022-10-18-pytorch-mnist-convolutional-neural-networks.html#create-lenet-5-model-configuration",
    "href": "posts/2022-10-18-pytorch-mnist-convolutional-neural-networks.html#create-lenet-5-model-configuration",
    "title": "Convolutional Neural Networks Filters and Feature Maps with PyTorch",
    "section": "Create LeNet-5 model configuration",
    "text": "Create LeNet-5 model configuration\n\n\n\narchitecture_lenet.png\n\n\nSource: Generated using Alexander Lenail’s NN-SVG and adapted by the author [“Daniel Voigt Godoy”]. For more details, see LeCun, Y., et al (1998). Gradient-based learning applied to document recognition. Proceedings of the IEEE,86(11), 2278–2324\nImage taken from dvgodoy/PyTorchStepByStep\n\nimport torch.nn as nn\n\nlenet = nn.Sequential()\n\n# Featurizer\n# Block 1: 1@28x28 -> 6@28x28 -> 6@14x14\nlenet.add_module('C1', nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5, padding=2))\nlenet.add_module('func1', nn.ReLU())\nlenet.add_module('S2', nn.MaxPool2d(kernel_size=2))\n# Block 2: 6@14x14 -> 16@10x10 -> 16@5x5\nlenet.add_module('C3', nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5))\nlenet.add_module('func2', nn.ReLU())\nlenet.add_module('S4', nn.MaxPool2d(kernel_size=2))\n# Block 3: 16@5x5 -> 120@1x1\nlenet.add_module('C5', nn.Conv2d(in_channels=16, out_channels=120, kernel_size=5))\nlenet.add_module('func2', nn.ReLU())\n# Flattening\nlenet.add_module('flatten', nn.Flatten())\n\n# Classification\n# Hidden Layer\nlenet.add_module('F6', nn.Linear(in_features=120, out_features=84))\nlenet.add_module('func3', nn.ReLU())\n# Output Layer\nlenet.add_module('OUTPUT', nn.Linear(in_features=84, out_features=10))\n\nNow let’s create optimizer and a loss function.\n\nimport torch.optim as optim\n\nlr = 0.003\ntorch.manual_seed(42)\n\nmodel = lenet\n\n# Defines a SGD optimizer to update the parameters\noptimizer = optim.SGD(model.parameters(), lr=lr)\n\n# Defines a BCE loss function\nloss_fn = nn.CrossEntropyLoss()\n\nNow let’s train our model for 20 epochs.\n\nn_epochs = 20\n\ndlp = DeepLearningPipeline(model, loss_fn, optimizer)\ndlp.set_loaders(train_loader, valid_loader)\ndlp.train(n_epochs)\n\nepoch:   0, train loss: 2.29349, valid loss: 2.28632\nepoch:   1, train loss: 2.27442, valid loss: 2.25871\nepoch:   2, train loss: 2.22077, valid loss: 2.15503\nepoch:   3, train loss: 1.85172, valid loss: 1.27419\nepoch:   4, train loss: 0.82061, valid loss: 0.59110\nepoch:   5, train loss: 0.50765, valid loss: 0.45872\nepoch:   6, train loss: 0.41486, valid loss: 0.39295\nepoch:   7, train loss: 0.36466, valid loss: 0.35130\nepoch:   8, train loss: 0.33108, valid loss: 0.32108\nepoch:   9, train loss: 0.30512, valid loss: 0.29665\nepoch:  10, train loss: 0.28347, valid loss: 0.27550\nepoch:  11, train loss: 0.26473, valid loss: 0.25711\nepoch:  12, train loss: 0.24807, valid loss: 0.24056\nepoch:  13, train loss: 0.23314, valid loss: 0.22590\nepoch:  14, train loss: 0.21989, valid loss: 0.21282\nepoch:  15, train loss: 0.20804, valid loss: 0.20097\nepoch:  16, train loss: 0.19743, valid loss: 0.19028\nepoch:  17, train loss: 0.18791, valid loss: 0.18065\nepoch:  18, train loss: 0.17929, valid loss: 0.17199\nepoch:  19, train loss: 0.17142, valid loss: 0.16408\n\n\nLet’s see how our training and validation loss looks like.\n\nfig = dlp.plot_losses()"
  },
  {
    "objectID": "posts/2022-10-18-pytorch-mnist-convolutional-neural-networks.html#visualize-model-filters",
    "href": "posts/2022-10-18-pytorch-mnist-convolutional-neural-networks.html#visualize-model-filters",
    "title": "Convolutional Neural Networks Filters and Feature Maps with PyTorch",
    "section": "Visualize model filters",
    "text": "Visualize model filters\nNeural network convolutional layers are a stack of square matrices. We repeatedly apply these matrices or filters on images, and the output of this operation is called convolutions. These convolutions act as intermediate (or new) datasets generated from the images (a kind of dynamic feature engineering). We try to learn from these convolutions and calculate a loss for them. Initially, the value for these filters is randomly selected. If the loss is high, we try slowly changing values or weights for these filters. Changing filter values also create changes in outputs or convolutions. If a convolution results in a slight decrease in loss, we take it as a good sign and try to move in that direction (make more similar changes). If we try these steps repeatedly, we might find good convolutions (or good weights in the filter, as they are the source of convolutions). By “good weights”, we mean that the final loss is significantly lower than a random value.\nThese convolutional filters are created in PyTorch using class nn.Conv2d. A filter can have one matrix or a stack of matrices under it. Matrices under a filter are sometimes called kernels, but I will stick to the filter and matrix terms to avoid confusion.\nSo If we look at the LeNet-5 model configuration, we will find that we created three convolutional filters or nn.Conv2d layers. Let’s print the dimension of these layers.\n\nmodel_weights = [] \nconv_layers = [] \nmodel_children = list(model.children())\n\n# counter to keep count of the conv layers\ncounter = 0 \n# append all the conv layers and their respective weights to the list\nfor i in range(len(model_children)):\n    if type(model_children[i]) == nn.Conv2d:\n        counter += 1\n        model_weights.append(model_children[i].weight)\n        conv_layers.append(model_children[i])\n\nprint(f\"Total convolutional layers: {counter}\")\n\nfor weight, conv in zip(model_weights, conv_layers):\n    print(f\"CONV: {conv} ====> SHAPE: {weight.shape}\")\n\nTotal convolutional layers: 3\nCONV: Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2)) ====> SHAPE: torch.Size([6, 1, 5, 5])\nCONV: Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1)) ====> SHAPE: torch.Size([16, 6, 5, 5])\nCONV: Conv2d(16, 120, kernel_size=(5, 5), stride=(1, 1)) ====> SHAPE: torch.Size([120, 16, 5, 5])\n\n\nThe output of the above cell tells us that\n\nThere are three convolutional layers\nFirst layer has a dimension of [6, 1, 5, 5]. It means we have 6 filters in this layer. Each filter has 1 matrix of dimension 5x5 under it.\nSecond layer has a dimension of [16, 6, 5, 5]. It means we have 16 filters. Each filter has 6 matrices of size 5x5 under it.\nThird layer has a dimension of [120, 16, 5, 5]. It means we have 120 filters. Each filter has 16 matrices of size 5x5 under it.\n\nNote that the learned weights from these matrices are stored in model_weights list, which we will visualize in the next section.\n\n##\n# Helper function to visualize filters\ndef visualize_filters(layer, n_plots=30):\n    # Get layer dimensions. e.g. `[6, 1, 5, 5]` where\n    # filters=6, kernels=1, kernel_height=5, kernel_weight=5\n    filters, kernels, kernel_height, kernel_weight = layer.shape\n    # total plots = total number of matrices present in a layer. \n    # Each matrix weights can be plotted as an image\n    total_plots = filters * kernels\n    # total_plots can be too many. So let's create an upper limit on them as 'MAX_PLOTS'\n    MAX_PLOTS = min(n_plots, total_plots)\n    # number of columns for our plots 'MAX_COL_PLOTS'\n    MAX_COL_PLOTS = 10\n    # number of rows for our plots 'MAX_ROW_PLOTS'\n    MAX_ROW_PLOTS = max(MAX_PLOTS // MAX_COL_PLOTS, MAX_COL_PLOTS)\n\n    # specify some size of each plot image\n    plt.figure(figsize=(20, 2.5*MAX_ROW_PLOTS)) # width, height\n    plt.tight_layout()\n    \n    plot_count = 1\n    # interate filters\n    for f, filter in enumerate(layer):\n        # iterate kernels under each filter\n        for k, kernel in enumerate(filter):\n            # plot a single kernel or a matrix\n            plt.subplot(MAX_ROW_PLOTS, MAX_COL_PLOTS, plot_count)\n            plt.imshow(kernel[:, :].detach().cpu().numpy(), cmap='gray')\n            plt.title('#F:{} - K:{}'.format(f, k), {'size': 12})\n            plt.axis('off')\n            plot_count += 1\n            \n            # terminate on `MAX_PLOTS` \n            if plot_count > MAX_PLOTS:\n                return plt.show()\n    \n    plt.show()\n\n\nVisualize weights for first ‘Conv2d’ layer\n\nvisualize_filters(model_weights[0], 50)\n\n\n\n\n\n\nVisualize weights for second ‘Conv2d’ layer\n\nvisualize_filters(model_weights[1], 50)\n\n\n\n\n\n\nVisualize weights for third ‘Conv2d’ layer\n\nvisualize_filters(model_weights[2], 50)\n\n\n\n\n\n\nWhat do these filter images tell us?\nThese learned filter (2d matrix) images seem very random. How can these filters create an output (convolution or intermediate dataset) that can help our model to learn and classify an image to its correct class? Filters usually learn to find edges and curves from images. When these filters are applied to images, they amplify certain aspects of these images, like edges, curves, lines, or other patterns.\nLooking just at the filters does not give us much information. So in the next section, we will visualize the outputs (or convolutions) produced by these filters."
  },
  {
    "objectID": "posts/2022-10-18-pytorch-mnist-convolutional-neural-networks.html#visualize-feature-maps-for-convolutional-layers",
    "href": "posts/2022-10-18-pytorch-mnist-convolutional-neural-networks.html#visualize-feature-maps-for-convolutional-layers",
    "title": "Convolutional Neural Networks Filters and Feature Maps with PyTorch",
    "section": "Visualize feature maps for convolutional layers",
    "text": "Visualize feature maps for convolutional layers\nThe output produced by a neural net layer is called its feature map. These layers can be convolutional, flattening, or linear (fully connected). For example, in our LeNet-5 model, we have three convolutional layers, and each of these layers produces a feature map. In this section, we will visualize them.\n\nDefine a hook\nA hook is simply a function we can give to our model to execute after its forward or backward pass. While attaching (or registering) a hook to a model, we provide the layer name on which we want to connect it. A hook function takes three arguments.\n\na layer or a model\na tensor representing the inputs to the layer or model\na tensor representing the outputs from the layer or model\n\nLet’s define a function that will serve as our hook.\n\n##\n# 'activation' is a dictionary to store the output from the layer\n# It should be defined outside the function, otherwise we will not be able to access it.\nactivation = {}\ndef get_activation(name):\n    def hook(model, input, output):\n        activation[name] = output.detach()\n    return hook\n\nLet’s also define a function to visualize the feature maps.\n\n##\n# Helper funtion to visualize the feature maps\ndef visualize_feature_map(layer, n_plots=30, cmap='gray', repeats=5, figsize=(5, 5)):\n    # get feature map values and store them as 'act'\n    act = layer.squeeze()\n\n    # if feature map has three dimension\n    if len(act.shape) == 3:\n        total_plots, plot_width, plot_height = act.shape\n\n        # total_plots can be too many so let's create an upper limit on them as 'MAX_PLOT'\n        MAX_PLOT = min(total_plots, n_plots)\n        # number of columns for our plots as 'MAX_COL_PLOTS'\n        MAX_COL_PLOTS = 6\n        # number of rows for our plots as 'MAX_ROW_PLOTs'\n        MAX_ROW_PLOTs = max(MAX_PLOT // MAX_COL_PLOTS, MAX_COL_PLOTS)\n\n        # specify some size for each plot image\n        plt.figure(figsize=(20, 3.5*MAX_ROW_PLOTs)) # width, height\n        plt.tight_layout()\n\n        plot_count = 0\n        for i in range(MAX_ROW_PLOTs):\n            for j in range(MAX_COL_PLOTS):\n                plt.subplot(MAX_ROW_PLOTs, MAX_COL_PLOTS, plot_count+1)\n                plt.imshow(act[plot_count].detach().cpu().numpy(), cmap=cmap)\n                plt.title('#R:{} - C:{}'.format(i, j), {'size': 12})\n                plt.axis('off')\n                plot_count += 1\n\n                # terminate if plot_count reaches MAX_PLOT\n                if plot_count >= MAX_PLOT:\n                    return plt.show()\n    else: # len(act.shape) == 3\n    # if feature map has two dimension\n        arr_r = np.repeat(act.reshape(1,-1), repeats=repeats, axis=0)\n        plt.figure(figsize=figsize) # width, height\n        plt.tight_layout()\n        plt.imshow(arr_r, cmap=cmap)\n        plt.axis('off')\n\n        return plt.show()\n\n\n\nFeature map from first ‘Conv2d’ layer\nLet’s register our hook to first convolutional layer and visualize its feature map.\n\n##\n# empty 'activation' as a precaution\nactivation = {}\n\n# first 'Conv2d' layer is named as 'C1'\nhandle = model.C1.register_forward_hook(get_activation('C1'))\n\n# take any dataset image. image '7' is for three number\ndata, label = train_dataset[7]\ndata.unsqueeze_(0)\noutput = model(data)\n\n# remove hook\nhandle.remove()\n\nprint(f\"Dimensions for C1 feature map: {activation['C1'].squeeze().shape}\")\n\nDimensions for C1 feature map: torch.Size([6, 28, 28])\n\n\n\n##\n# visualize the feature map\n# dimensions: 6@28x28\nvisualize_feature_map(activation['C1'])\n\n\n\n\n\n\nFeature map from second ‘Conv2d’ layer\nLet’s register our hook to second convolutional layer and visualize its feature map.\n\n##\n# empty 'activation' as a precaution\nactivation = {}\n\n# second 'Conv2d' layer is named as 'C3'\nhandle = model.C3.register_forward_hook(get_activation('C3'))\n\n# take any dataset image. image '7' is for three number\ndata, label = train_dataset[7]\ndata.unsqueeze_(0)\noutput = model(data)\n\n# remove hook\nhandle.remove()\n\nprint(f\"Dimensions for C3 feature map: {activation['C3'].squeeze().shape}\")\n\nDimensions for C3 feature map: torch.Size([16, 10, 10])\n\n\n\n##\n# visualize the feature map\n# dimensions: 16@10x10\nvisualize_feature_map(activation['C3'])\n\n\n\n\n\n\nFeature maps from the first and second ‘Conv2d’ layers together\nFeature maps from the first layer show that they are sharper. Feature maps from the second layer show that they are more spread out or convolved. By spreading out or blurring effect, it seems like only the most significant features remain in the output, and the rest slowly disappear. For example, in the case of ‘3’ in the second feature map, only the horizontal edge signal remains, and any other signal gets dissolved.\nTo get more intuition of what is happening here, let’s visualize both feature maps together and for multiple images.\n\n##\n# Visualize feature maps for C1 and C3 together\n# Visualize them for first 5 train images\nfor i in range(5):\n    # just a separator.\n    print(f\"{'*'*30} IMAGE {i} {'*'*30}\")\n\n    # empty 'activation' as a precaution\n    activation = {}\n\n    # create hooks for C1 and C3\n    handle1 = model.C1.register_forward_hook(get_activation('C1'))\n    handle2 = model.C3.register_forward_hook(get_activation('C3'))\n    \n    data, _ = train_dataset[i]\n    data.unsqueeze_(0)\n    output = model(data)\n\n    # remove hooks\n    handle1.remove()\n    handle2.remove()\n\n    # visualize feature maps\n    # I have chaged the colors of output to sharpen the differences\n    visualize_feature_map(activation['C1'], cmap='viridis')\n    visualize_feature_map(activation['C3'], cmap='viridis')\n\n****************************** IMAGE 0 ******************************\n\n\n\n\n\n\n\n\n****************************** IMAGE 1 ******************************\n\n\n\n\n\n\n\n\n****************************** IMAGE 2 ******************************\n\n\n\n\n\n\n\n\n****************************** IMAGE 3 ******************************\n\n\n\n\n\n\n\n\n****************************** IMAGE 4 ******************************\n\n\n\n\n\n\n\n\n\n\nFeature map from third ‘Conv2d’ layer\nLet’s register our hook to third convolutional layer and visualize its feature map.\n\n##\n# empty 'activation' as a precaution\nactivation = {}\n\n# third 'Conv2d' layer is named as 'C5'\nhandle = model.C5.register_forward_hook(get_activation('C5'))\n\n# take any dataset image. image '7' is for three number\ndata, label = train_dataset[7]\ndata.unsqueeze_(0)\noutput = model(data)\n\n# remove hook\nhandle.remove()\n\nprint(f\"Dimensions for C5 feature map: {activation['C5'].squeeze().shape}\")\n\nDimensions for C5 feature map: torch.Size([120])\n\n\n\n##\n# visualize the feature map\n# dimensions: 120@1x1\nvisualize_feature_map(activation['C5'], figsize=(20,15))\n\n\n\n\n\n\nFeature map from third ‘Conv2d’ layer for multiple images\nThe output from the third ‘Conv2d’ layer C5 looks like a signature bar code that the model learns to associate with a particular target class. Let’s visualize more of them for a set of 1, 3, and 5 images together.\n\n##\n# create a collection of 1, 3, and 5 images\nbucket = {'1':[], '3':[], '5':[]}\n\n# iterate through the dataset till we have 5 images for each class\ncount_1, count_3, count_5 = 0,0,0\nfor idx in range(100):\n    _, label = train_dataset[idx]\n\n    if label == 1 and count_1 < 5:\n        bucket[str(label)].append(idx)\n        count_1 +=1\n    elif label == 3 and count_3 < 5:\n        bucket[str(label)].append(idx)\n        count_3 +=1\n    elif label == 5 and count_5 < 5:\n        bucket[str(label)].append(idx)\n        count_5 +=1\n\n    if count_1 + count_3 + count_5 == 15:\n        break\n\n\n##\n# visualize feature maps for a set of images from class 1, 3 and 5\nfor key in bucket:\n    print(f\"{'*'*30} LAYER C5, LABEL {key} {'*'*30}\")\n\n    for i in range(len(bucket[key])):\n        activation = {}\n        \n        # attach hook\n        handle = model.C5.register_forward_hook(get_activation('C5'))\n        idx = bucket[key][i]\n        data, label = train_dataset[idx]\n        data.unsqueeze_(0)\n        output = model(data)\n        \n        # remove hook\n        handle.remove()\n        \n        # visualize feature map\n        # i have changed the output colormap to sharpen the differences\n        visualize_feature_map(activation['C5'], cmap='hsv', figsize=(20,15))\n\n****************************** LAYER C5, LABEL 1 ******************************\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n****************************** LAYER C5, LABEL 3 ******************************\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n****************************** LAYER C5, LABEL 5 ******************************"
  },
  {
    "objectID": "posts/2022-10-18-pytorch-mnist-convolutional-neural-networks.html#visualize-feature-maps-for-a-classifier-or-hidden-layer",
    "href": "posts/2022-10-18-pytorch-mnist-convolutional-neural-networks.html#visualize-feature-maps-for-a-classifier-or-hidden-layer",
    "title": "Convolutional Neural Networks Filters and Feature Maps with PyTorch",
    "section": "Visualize feature maps for a classifier or hidden layer",
    "text": "Visualize feature maps for a classifier or hidden layer\nWe have two hidden layers or ‘nn.Linear’ in the classification part of our model. They are also called fully connected layers. Let’s visualize the feature maps for them.\n\nVisualize feature map for first ‘Linear’ layer\nLet’s visualize the feature maps for the first hidden layer.\n\n# visualize differences for images 1,3 and 5\nfor key in bucket:\n    print(f\"{'*'*30} LAYER F6, LABEL {key} {'*'*30}\")\n    for i in range(len(bucket[key])):\n\n        activation = {}\n        handle1 = model.F6.register_forward_hook(get_activation('F6'))\n        idx = bucket[key][i]\n        data, label = train_dataset[idx]\n        \n        data.unsqueeze_(0)\n        output = model(data)\n        handle1.remove()\n        \n        visualize_feature_map(activation['F6'], cmap='hsv', figsize=(20,15))\n\n****************************** LAYER F6, LABEL 1 ******************************\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n****************************** LAYER F6, LABEL 3 ******************************\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n****************************** LAYER F6, LABEL 5 ******************************\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVisualize feature map for second ‘Linear’ layer\nLet’s visualize the feature maps for the second hidden layer.\n\n# visualize differences for images 1,3 and 5\nfor key in bucket:\n    print(f\"{'*'*30} LAYER OUTPUT, LABEL {key} {'*'*30}\")\n    for i in range(len(bucket[key])):\n\n        activation = {}\n        handle1 = model.OUTPUT.register_forward_hook(get_activation('OUTPUT'))\n        idx = bucket[key][i]\n        data, label = train_dataset[idx]\n        \n        data.unsqueeze_(0)\n        output = model(data)\n        handle1.remove()\n        \n        visualize_feature_map(activation['OUTPUT'], repeats=1,cmap='hsv', figsize=(10,5))\n\n****************************** LAYER OUTPUT, LABEL 1 ******************************\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n****************************** LAYER OUTPUT, LABEL 3 ******************************\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n****************************** LAYER OUTPUT, LABEL 5 ******************************\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhat do these OUTPUT feature maps tell us?\nWe can see that output feature maps of the same numbers are very similar. They show that our model has learned some hidden patterns that distinguish these images. For example\n\nFor label 1, all output feature maps have a RED bar at the extreme left and a BLUE bar at the right side\nFor label 3, there are two RED bars around the middle and a light BLUE bar running along them\nFor label 5, RED bars are in the middle and at the extreme right side.\n\nIf we look at the first feature map from label 5, it is a bit different. A kind of outlier from the rest of label 5 feature maps. What could be the reason for this?\nLet’s plot these label 5 images to check their actual shape.\n\n##\n# label 5 image indexes\nbucket['5']\n\n[0, 11, 35, 47, 65]\n\n\n\n##\n# Plot label 5 images\nplt.figure(figsize=(20, 10))\nfor i, idx in enumerate(bucket['5']):\n    plt.subplot(1, 5, i+1)\n    plt.imshow(train_dataset.data[idx], cmap='gray')\n    plt.title(f\"#Index:{idx} \")\n    plt.axis('off')\n\n\n\n\nNow we can see the reason for the output feature map of the first label ‘5’ image being quite different from the rest. The first image from the left side is slightly different from the rest. So it is a bit weird number five image, and that weirdness got reflected in the feature map generated by it."
  },
  {
    "objectID": "posts/2022-10-23-pytorch-vanishing-gradients-deep-neural-networks.html#credits",
    "href": "posts/2022-10-23-pytorch-vanishing-gradients-deep-neural-networks.html#credits",
    "title": "Detect Vanishing Gradients in Deep Neural Networks by Plotting Gradient Distributions",
    "section": "Credits",
    "text": "Credits\nThis notebook takes inspiration and ideas from the following sources.\n\nA great post from MATLAB company MathWorks with the same title: Detect Vanishing Gradients in Deep Neural Networks by Plotting Gradient Distributions. That post is written for the MATLAB audience, and I have tried translating its ideas for Python and PyTorch community.\nThe outstanding book “Deep Learning with PyTorch Step-by-Step” by “Daniel Voigt Godoy”. You can get the book from its website: pytorchstepbystep. In addition, the GitHub repository for this book has valuable notebooks and can be used independently: github.com/dvgodoy/PyTorchStepByStep. Parts of the code you see in this notebook are taken from chapter 3 notebook of the same book.\nUniversity of Amsterdam (UvA) Deep Learning Course series. uvadlc.github.io. Their lecture on “activation functions and gradients” discusses the same topic. Here is the link: tutorial3/Activation_Functions.html. The course is outstanding, and lectures and notebooks are also openly shared. From the course site “This course is taught in the MSc program in Artificial Intelligence of the University of Amsterdam. In this course we study the theory of deep learning, namely of modern, multi-layered neural networks trained on big data.”"
  },
  {
    "objectID": "posts/2022-10-23-pytorch-vanishing-gradients-deep-neural-networks.html#environment",
    "href": "posts/2022-10-23-pytorch-vanishing-gradients-deep-neural-networks.html#environment",
    "title": "Detect Vanishing Gradients in Deep Neural Networks by Plotting Gradient Distributions",
    "section": "Environment",
    "text": "Environment\nThis notebook is prepared with Google Colab.\n\n\nCode\nfrom platform import python_version\nimport numpy, matplotlib, pandas, torch, seaborn\n\nprint(\"python==\" + python_version())\nprint(\"numpy==\" + numpy.__version__)\nprint(\"torch==\" + torch.__version__)\nprint(\"matplotlib==\" + matplotlib.__version__)\nprint(\"seaborn==\" + seaborn.__version__)\n\n\npython==3.7.15\nnumpy==1.21.6\ntorch==1.12.1+cu113\nmatplotlib==3.2.2\nseaborn==0.11.2"
  },
  {
    "objectID": "posts/2022-10-23-pytorch-vanishing-gradients-deep-neural-networks.html#introduction",
    "href": "posts/2022-10-23-pytorch-vanishing-gradients-deep-neural-networks.html#introduction",
    "title": "Detect Vanishing Gradients in Deep Neural Networks by Plotting Gradient Distributions",
    "section": "Introduction",
    "text": "Introduction\nMathWorks post explains vanishing gradients problem in deep neural networks really well, and I am sharing a passage from it.\n\nA common problem in deep network training is vanishing gradients. Deep learning training algorithms aim to minimize the loss by adjusting the network’s learnable parameters (i.e., weights) during training. Gradient-based training algorithms determine the level of adjustment using the gradients of the loss function with respect to the current learnable parameters. The gradient computation uses the propagated gradients from the previous layers for earlier layers (i.e., from the output layer to the input layer). Therefore, when a network contains activation functions that always produce gradient values less than 1 (e.g., Sigmoid), the value of the gradients can become increasingly small as the updating algorithm moves toward the initial layers. As a result, early layers in the network can receive a vanishingly small gradient, and therefore, the network is unable to learn. However, if the gradient of the activation function is always greater than or equal to 1 (e.g., ReLU), the gradients can flow through the network, reducing the chance of vanishing gradients.\n\nLet’s understand it better by visualizing the gradients produced by Sigmoid and ReLU.\n\nCompare activation functions\nIn this section we will compare the properties of two popular activation functions: Sigmoid and ReLU.\n\nSigmoid\nSigmoid function is normally used to refer specifically to the logistic function, also called the logistic sigmoid function. It is defined as\n\\[\nSigmoid(x) =  \\frac{\\mathrm{1} }{\\mathrm{1} + e^{-x} }\n\\]\n\n\nReLU\nReLU function is defined as\n\\[ Relu(x) = max(0,x)\n\\]\n\n\n\nVisualising activation functions and their gradients\nLet’s plot both these functions’ outputs and visualize their gradients. In the next cell, I have created two PyTorch classes that define Sigmoid and ReLU activations.\n\n\nCode\nimport torch.nn as nn\n\n# A class representing Sigmoid activation function\nclass SigmoidAct(nn.Module):\n    def __init__(self):\n        super().__init__()\n        pass\n\n    def forward(self, x):\n        return 1 / (1 + torch.exp(-x))\n\n# A class representing ReLU activation function\nclass ReluAct(nn.Module):\n    def __init__(self):\n        super().__init__()\n        pass\n\n    def forward(self, x):\n        return x * (x > 0).float()\n\n# initialize sigmoid activation function\nsigmoid_fn = SigmoidAct()\n# initialize relu activation function\nrelu_fn = ReluAct()\n\n\nI have defined a helper function to calculate the gradients for these activation functions.\n\n\nCode\nimport matplotlib.pyplot as plt\n\n# A helper function to computes the gradients of an activation function at specified positions.\ndef get_grads(act_fn, x):\n    x = x.clone().requires_grad_()  # Mark the input as tensor for which we want to store gradients\n    out = act_fn(x)\n    out.sum().backward()  # Summing results in an equal gradient flow to each element in x\n    return x.grad  # Accessing the gradients of x by \"x.grad\"\n\n# A helper function to plot the activation function and its gradient\ndef vis_act_fn(act_fn, fn_name, ax, x):\n    # Run activation function\n    y = act_fn(x)\n    y_grads = get_grads(act_fn, x)\n    # Push x, y and gradients back to cpu for plotting\n    x, y, y_grads = x.cpu().numpy(), y.cpu().numpy(), y_grads.cpu().numpy()\n    # Plotting\n    ax.plot(x, y, linewidth=2, label=\"Activation function\")\n    ax.plot(x, y_grads, linewidth=2, label=\"Gradient\")\n    ax.set_title(fn_name)\n    ax.legend()\n    ax.set_ylim(-1.5, x.max())\n\n\nlet’s plot the gradients.\n\n\nCode\nimport torch\n\nx = torch.linspace(-5, 5, 1000)\n\nfig, ax = plt.subplots(1, 2, figsize=(15, 7))\nvis_act_fn(sigmoid_fn, 'sigmoid', ax[0], x)\nvis_act_fn(relu_fn, 'relu', ax[1], x)\nfig.subplots_adjust(hspace=0.3)\nplt.show()\n\n\n\n\n\nWe can take the following explanations from the above plots.\n\nThe sigmoid output is bounded, as it remains between 0 and 1. The gradients are the highest when the input is close to zero and diminishes as the input value moves away from it. Notice that the sigmoid gradient curve is less than 1 for the entire range; therefore, a network containing sigmoid activation functions can suffer from a vanishing gradients problem.\nThe output value for ReLU is not bounded. It keeps on increasing on the positive side. And for positive output values, the gradients do not diminish but remain constant at 1. As the gradient is not decreasing, it reduces the chances of the vanishing gradients problem. However, the gradient is zero for negative values, and this state is sometimes referred to as dead ReLU. It means that if ReLU ends up in this situation, it is improbable that it will recover from it.\n\nWhy will it not recover? Because the gradient from the activation function is zero for negative inputs, it will also not update the weights during the backward pass, thus leaving the weights in that perpetual state.\n\n\nIn the coming sections, we will build networks and try to visualize how gradients flow between different layers and the effect of activation functions on them."
  },
  {
    "objectID": "posts/2022-10-23-pytorch-vanishing-gradients-deep-neural-networks.html#download-mnist-dataset",
    "href": "posts/2022-10-23-pytorch-vanishing-gradients-deep-neural-networks.html#download-mnist-dataset",
    "title": "Detect Vanishing Gradients in Deep Neural Networks by Plotting Gradient Distributions",
    "section": "Download MNIST dataset",
    "text": "Download MNIST dataset\nMNIST dataset can be downloaded easily from PyTorch built-in datasets provided under torchvision.datasets. In this section, we will download it, split it into train and test datasets, and then convert them into PyTorch tensors.\n\nRead more about the PyTorch MNIST dataset here\ntorchvision.transforms.Compose is like a container to hold a list of transformations you intend to apply. Read more about it here\ntorchvision.transforms.ToTensor converts a PIL Image or numpy.ndarray to tensor. It converts a PIL Image or numpy.ndarray (H x W x C) in the range [0, 255] to a torch.FloatTensor of shape (C x H x W) in the range [0.0, 1.0]. Here C=Channel, H=Height, W=Width. Read more about this transformation here\n\n\n#collapse-output\nimport torchvision\nimport numpy as np\n\ntrain_dataset = torchvision.datasets.MNIST('classifier_data', train=True, download=True)\ntest_dataset  = torchvision.datasets.MNIST('classifier_data', train=False, download=True)\n\ntransform = torchvision.transforms.Compose([\n    torchvision.transforms.ToTensor()\n])\n\ntrain_dataset.transform=transform\ntest_dataset.transform=transform\n\nprint(f\"Total training images: {len(train_dataset)}\")\nprint(f\"Shape of an image: {np.shape(train_dataset.data[7])}\")\n\nDownloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\nDownloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to classifier_data/MNIST/raw/train-images-idx3-ubyte.gz\n\n\n\n\n\nExtracting classifier_data/MNIST/raw/train-images-idx3-ubyte.gz to classifier_data/MNIST/raw\n\nDownloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\nDownloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to classifier_data/MNIST/raw/train-labels-idx1-ubyte.gz\n\n\n\n\n\nExtracting classifier_data/MNIST/raw/train-labels-idx1-ubyte.gz to classifier_data/MNIST/raw\n\nDownloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\nDownloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to classifier_data/MNIST/raw/t10k-images-idx3-ubyte.gz\n\n\n\n\n\nExtracting classifier_data/MNIST/raw/t10k-images-idx3-ubyte.gz to classifier_data/MNIST/raw\n\nDownloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\nDownloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to classifier_data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n\n\n\n\n\nExtracting classifier_data/MNIST/raw/t10k-labels-idx1-ubyte.gz to classifier_data/MNIST/raw\n\nTotal training images: 60000\nShape of an image: torch.Size([28, 28])\n\n\nFrom the above cell output, there are 60,000 training images. The shape of each image is 28 x 28, which means it is a 2D matrix.\n\n##\n# plot a single image\nimport matplotlib.pyplot as plt\n\nplt.imshow(train_dataset.data[7], cmap='gray')\n\n<matplotlib.image.AxesImage at 0x7fb77b73b590>"
  },
  {
    "objectID": "posts/2022-10-23-pytorch-vanishing-gradients-deep-neural-networks.html#load-generated-data-into-pytorch-dataset-and-dataloader-class",
    "href": "posts/2022-10-23-pytorch-vanishing-gradients-deep-neural-networks.html#load-generated-data-into-pytorch-dataset-and-dataloader-class",
    "title": "Detect Vanishing Gradients in Deep Neural Networks by Plotting Gradient Distributions",
    "section": "Load generated data into PyTorch Dataset and DataLoader class",
    "text": "Load generated data into PyTorch Dataset and DataLoader class\nNow let’s load our data into Dataset and DataLoader classes. PyTorch Dataset is a helper class that converts data and labels into a list of tuples. DataLoader is another helper class to create batches from Dataset tuples. batch_size means the number of tuples we want in a single batch. We have used 128 here, so each fetch from DataLoader will give us a list of 128 tuples.\n\nimport torch\nfrom torch.utils.data import Dataset, DataLoader, random_split\n\ntrain_size=len(train_dataset)\n\n# Randomly split the data into non-overlapping train and validation set\n# train size = 70% and validation size = 30%\ntrain_data, val_data = random_split(train_dataset, [int(train_size*0.7), int(train_size - train_size*0.7)])\n\nbatch_size=128\n\n# Load data into DataLoader class\ntrain_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size)\nvalid_loader = torch.utils.data.DataLoader(val_data, batch_size=batch_size)\n\nprint(f\"Batches in Train Loader: {len(train_loader)}\")\nprint(f\"Batches in Valid Loader: {len(valid_loader)}\")\n\nprint(f\"Examples in Train Loader: {len(train_loader.sampler)}\")\nprint(f\"Examples in Valid Loader: {len(valid_loader.sampler)}\")\n\nBatches in Train Loader: 469\nBatches in Valid Loader: 141\nExamples in Train Loader: 60000\nExamples in Valid Loader: 18000"
  },
  {
    "objectID": "posts/2022-10-23-pytorch-vanishing-gradients-deep-neural-networks.html#define-a-class-to-implement-training-validation-and-mini-batch-processing-pipeline",
    "href": "posts/2022-10-23-pytorch-vanishing-gradients-deep-neural-networks.html#define-a-class-to-implement-training-validation-and-mini-batch-processing-pipeline",
    "title": "Detect Vanishing Gradients in Deep Neural Networks by Plotting Gradient Distributions",
    "section": "Define a class to implement training, validation, and mini-batch processing pipeline",
    "text": "Define a class to implement training, validation, and mini-batch processing pipeline\nIn this section we will implement a class that encapsulates all the usual steps required in training a PyTorch model. This way we can focus more on the model architecture and performance, and less concerned about the boilerplate training loop. Important parts of this class are\n\n__init__: Class constructor to define the main actors in a training cycle including model, optimizer, loss function, training and validation DataLoaders\n_make_train_step_fn: Training pipeline is usually called “training step” which includes the following steps\n\nCompute our model’s predicted output - the forward pass\nCompute the loss\nCompute gradients i.e., find the direction and scale to update the weights to reduce the loss\nUpdate weight parameters using gradients and the learning rate\n\n_make_val_step_fn: Validation pipeline is usually called the “validation step” which includes the following steps\n\nCompute our model’s predicted output - the forward pass\nCompute the loss\nNote that during validation, we are only concerned about the loss, i.e., how well our model performs on the validation dataset. Therefore, we don’t use it to calculate the gradients.\n\n_mini_batch: It defines the steps to process a single minibatch in a helper function. For a mini-batch processing, we want to\n\nGet the next batch of data and labels (x, y) from the DataLoader iterator\nPerform a step on the batch. A step can be either training or validation\nCompute the average batch loss\n\ntrain: Execute training and validation steps for given number of epoch\npredict: Make a prediction from model on provided data\n\n\n\nCode\nclass DeepLearningPipeline(object):\n    def __init__(self, model, loss_fn, optimizer):\n        # Here we define the attributes of our class\n        \n        # We start by storing the arguments as attributes \n        # to use them later\n        self.model = model\n        self.loss_fn = loss_fn\n        self.optimizer = optimizer\n        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n        # Let's send the model to the specified device right away\n        self.model.to(self.device)\n\n        # These attributes are defined here, but since they are\n        # not informed at the moment of creation, we keep them None\n        self.train_loader = None\n        self.val_loader = None\n        self.writer = None\n        \n        # These attributes are going to be computed internally\n        self.losses = []\n        self.val_losses = []\n        self.total_epochs = 0\n        self.grad = []\n\n        # Creates the train_step function for our model, \n        # loss function and optimizer\n        # Note: there are NO ARGS there! It makes use of the class\n        # attributes directly\n        self.train_step_fn = self._make_train_step_fn()\n        # Creates the val_step function for our model and loss\n        self.val_step_fn = self._make_val_step_fn()\n\n    def set_loaders(self, train_loader, val_loader=None):\n        # This method allows the user to define which train_loader (and val_loader, optionally) to use\n        # Both loaders are then assigned to attributes of the class\n        # So they can be referred to later\n        self.train_loader = train_loader\n        self.val_loader = val_loader\n\n    def _make_train_step_fn(self):\n        # This method does not need ARGS... it can refer to\n        # the attributes: self.model, self.loss_fn and self.optimizer\n        \n        # Builds function that performs a step in the train loop\n        def perform_train_step_fn(x, y):\n            # Sets model to TRAIN mode\n            self.model.train()\n\n            # Step 1 - Computes our model's predicted output - forward pass\n            yhat = self.model(x)\n            # Step 2 - Computes the loss\n            loss = self.loss_fn(yhat, y)\n            # Step 3 - Computes gradients for both \"a\" and \"b\" parameters\n            loss.backward()\n            # Step 4 - Updates parameters using gradients and the learning rate\n            self.optimizer.step()\n            self.optimizer.zero_grad()\n\n            # Returns the loss\n            return loss.item()\n\n        # Returns the function that will be called inside the train loop\n        return perform_train_step_fn\n    \n    def _make_val_step_fn(self):\n        # Builds function that performs a step in the validation loop\n        def perform_val_step_fn(x, y):\n            # Sets model to EVAL mode\n            self.model.eval()\n\n            # Step 1 - Computes our model's predicted output - forward pass\n            yhat = self.model(x)\n            # Step 2 - Computes the loss\n            loss = self.loss_fn(yhat, y)\n            # There is no need to compute Steps 3 and 4, \n            # since we don't update parameters during evaluation\n            return loss.item()\n\n        return perform_val_step_fn\n            \n    def _mini_batch(self, validation=False):\n        # The mini-batch can be used with both loaders\n        # The argument `validation`defines which loader and \n        # corresponding step function is going to be used\n        if validation:\n            data_loader = self.val_loader\n            step_fn = self.val_step_fn\n        else:\n            data_loader = self.train_loader\n            step_fn = self.train_step_fn\n\n        if data_loader is None:\n            return None\n            \n        # Once the data loader and step function, this is the \n        # same mini-batch loop we had before\n        mini_batch_losses = []\n        for x_batch, y_batch in data_loader:\n            x_batch = x_batch.to(self.device)\n            y_batch = y_batch.to(self.device)\n\n            mini_batch_loss = step_fn(x_batch, y_batch)\n            mini_batch_losses.append(mini_batch_loss)\n\n        loss = np.mean(mini_batch_losses)\n        return loss\n\n    def set_seed(self, seed=42):\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False    \n        torch.manual_seed(seed)\n        np.random.seed(seed)\n    \n    def train(self, n_epochs, seed=42):\n        # To ensure reproducibility of the training process\n        self.set_seed(seed)\n\n        for epoch in range(n_epochs):\n            # Keeps track of the numbers of epochs\n            # by updating the corresponding attribute\n            self.total_epochs += 1\n\n            # inner loop\n            # Performs training using mini-batches\n            loss = self._mini_batch(validation=False)\n            self.losses.append(loss)\n\n            ##########################\n            # get grad at the end of each epoch\n            imgs, labels = next(iter(self.train_loader))\n            imgs, labels = imgs.to(device), labels.to(device)\n\n            # Pass one batch through the network, and calculate the gradients for the weights\n            self.model.zero_grad()\n            preds = self.model(imgs)\n            loss = torch.nn.functional.cross_entropy(preds, labels)\n            loss.backward()\n            # We limit our visualization to the weight parameters and exclude the bias to reduce the number of plots\n            grads = {\n                name: params.grad.data.view(-1).cpu().clone().numpy()\n                for name, params in self.model.named_parameters()\n                if \"weight\" in name\n            }\n            self.model.zero_grad()\n            self.grad.append(grads)\n            ##########################\n\n            # VALIDATION\n            # no gradients in validation!\n            with torch.no_grad():\n                # Performs evaluation using mini-batches\n                val_loss = self._mini_batch(validation=True)\n                self.val_losses.append(val_loss)\n\n            # If a SummaryWriter has been set...\n            if self.writer:\n                scalars = {'training': loss}\n                if val_loss is not None:\n                    scalars.update({'validation': val_loss})\n                # Records both losses for each epoch under the main tag \"loss\"\n                self.writer.add_scalars(main_tag='loss',\n                                        tag_scalar_dict=scalars,\n                                        global_step=epoch)\n            \n            print(f\"epoch: {epoch:3}, train loss: {loss:.5f}, valid loss: {val_loss:.5f}\")\n\n        if self.writer:\n            # Closes the writer\n            self.writer.close()\n\n    def predict(self, x):\n        # Set is to evaluation mode for predictions\n        self.model.eval() \n        # Takes aNumpy input and make it a float tensor\n        x_tensor = torch.as_tensor(x).float()\n        # Send input to device and uses model for prediction\n        y_hat_tensor = self.model(x_tensor.to(self.device))\n        # Set it back to train mode\n        self.model.train()\n        # Detaches it, brings it to CPU and back to Numpy\n        return y_hat_tensor.detach().cpu().numpy()\n\n    def plot_losses(self):\n        fig = plt.figure(figsize=(10, 4))\n        plt.plot(self.losses, label='Training Loss', c='b')\n        plt.plot(self.val_losses, label='Validation Loss', c='r')\n        plt.yscale('log')\n        plt.xlabel('Epochs')\n        plt.ylabel('Loss')\n        plt.legend()\n        plt.tight_layout()\n        return fig"
  },
  {
    "objectID": "posts/2022-10-23-pytorch-vanishing-gradients-deep-neural-networks.html#create-a-model-with-sigmoid-activations",
    "href": "posts/2022-10-23-pytorch-vanishing-gradients-deep-neural-networks.html#create-a-model-with-sigmoid-activations",
    "title": "Detect Vanishing Gradients in Deep Neural Networks by Plotting Gradient Distributions",
    "section": "Create a model with sigmoid activations",
    "text": "Create a model with sigmoid activations\nLet’s define a fully connected 4 layers model with only sigmoid activations.\n\nimport torch.nn as nn\n\nSigmoidNet = nn.Sequential()\nSigmoidNet.add_module(\"F\", nn.Flatten())\nSigmoidNet.add_module(\"L1\", nn.Linear(28*28, 10, bias=False))\nSigmoidNet.add_module(\"S1\", nn.Sigmoid())\nSigmoidNet.add_module(\"L2\", nn.Linear(10, 10, bias=False))\nSigmoidNet.add_module(\"S2\", nn.Sigmoid())\nSigmoidNet.add_module(\"L3\", nn.Linear(10, 10, bias=False))\nSigmoidNet.add_module(\"S3\", nn.Sigmoid())\nSigmoidNet.add_module(\"L4\", nn.Linear(10, 10, bias=False))\n\nPrint model’s summary.\n\ntorch.manual_seed(42)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\n#collapse-output\nfrom torchsummary import summary\n\nmodel_sigmoid = SigmoidNet\nmodel_sigmoid = model_sigmoid.to(device)\n\nsummary(model_sigmoid, (1, 28*28))\n\n----------------------------------------------------------------\n        Layer (type)               Output Shape         Param #\n================================================================\n           Flatten-1                  [-1, 784]               0\n            Linear-2                   [-1, 10]           7,840\n           Sigmoid-3                   [-1, 10]               0\n            Linear-4                   [-1, 10]             100\n           Sigmoid-5                   [-1, 10]               0\n            Linear-6                   [-1, 10]             100\n           Sigmoid-7                   [-1, 10]               0\n            Linear-8                   [-1, 10]             100\n================================================================\nTotal params: 8,140\nTrainable params: 8,140\nNon-trainable params: 0\n----------------------------------------------------------------\nInput size (MB): 0.00\nForward/backward pass size (MB): 0.01\nParams size (MB): 0.03\nEstimated Total Size (MB): 0.04\n----------------------------------------------------------------\n\n\nCreate an optimizer and a loss function.\n\nimport torch.optim as optim\n\n# learning rate\nlr = 0.001\n\n# Defines a SGD optimizer to update the parameters\noptimizer_sigmoid = optim.SGD(model_sigmoid.parameters(), lr=lr)\n\n# Defines a BCE loss function\nloss_fn = nn.CrossEntropyLoss()\n\nTrain our model for 15 epochs.\n\n#collapse-output\nn_epochs = 15\n\ndlp_sigmoid = DeepLearningPipeline(model_sigmoid, loss_fn, optimizer_sigmoid)\ndlp_sigmoid.set_loaders(train_loader, valid_loader)\ndlp_sigmoid.train(n_epochs)\n\nepoch:   0, train loss: 2.38803, valid loss: 2.34602\nepoch:   1, train loss: 2.37269, valid loss: 2.33644\nepoch:   2, train loss: 2.36004, valid loss: 2.32891\nepoch:   3, train loss: 2.34957, valid loss: 2.32299\nepoch:   4, train loss: 2.34085, valid loss: 2.31831\nepoch:   5, train loss: 2.33357, valid loss: 2.31463\nepoch:   6, train loss: 2.32746, valid loss: 2.31172\nepoch:   7, train loss: 2.32232, valid loss: 2.30943\nepoch:   8, train loss: 2.31798, valid loss: 2.30762\nepoch:   9, train loss: 2.31431, valid loss: 2.30620\nepoch:  10, train loss: 2.31119, valid loss: 2.30508\nepoch:  11, train loss: 2.30853, valid loss: 2.30420\nepoch:  12, train loss: 2.30627, valid loss: 2.30351\nepoch:  13, train loss: 2.30433, valid loss: 2.30297\nepoch:  14, train loss: 2.30266, valid loss: 2.30254\n\n\nLet’s see how our training and validation loss looks like.\n\nfig = dlp_sigmoid.plot_losses()"
  },
  {
    "objectID": "posts/2022-10-23-pytorch-vanishing-gradients-deep-neural-networks.html#create-a-model-with-relu-activations",
    "href": "posts/2022-10-23-pytorch-vanishing-gradients-deep-neural-networks.html#create-a-model-with-relu-activations",
    "title": "Detect Vanishing Gradients in Deep Neural Networks by Plotting Gradient Distributions",
    "section": "Create a model with ReLU activations",
    "text": "Create a model with ReLU activations\nThis time let’s define the same model with ReLU activation functions.\n\nReluNet = nn.Sequential()\nReluNet.add_module(\"F\", nn.Flatten())\nReluNet.add_module(\"L1\", nn.Linear(28*28, 10, bias=False))\nReluNet.add_module(\"S1\", nn.ReLU())\nReluNet.add_module(\"L2\", nn.Linear(10, 10, bias=False))\nReluNet.add_module(\"S2\", nn.ReLU())\nReluNet.add_module(\"L3\", nn.Linear(10, 10, bias=False))\nReluNet.add_module(\"S3\", nn.ReLU())\nReluNet.add_module(\"L4\", nn.Linear(10, 10, bias=False))\n\nPrint the model’s summary.\n\n#collapse-output\nmodel_relu = ReluNet\nmodel_relu = model_relu.to(device)\n\nsummary(model_relu, (1, 28*28))\n\n----------------------------------------------------------------\n        Layer (type)               Output Shape         Param #\n================================================================\n           Flatten-1                  [-1, 784]               0\n            Linear-2                   [-1, 10]           7,840\n              ReLU-3                   [-1, 10]               0\n            Linear-4                   [-1, 10]             100\n              ReLU-5                   [-1, 10]               0\n            Linear-6                   [-1, 10]             100\n              ReLU-7                   [-1, 10]               0\n            Linear-8                   [-1, 10]             100\n================================================================\nTotal params: 8,140\nTrainable params: 8,140\nNon-trainable params: 0\n----------------------------------------------------------------\nInput size (MB): 0.00\nForward/backward pass size (MB): 0.01\nParams size (MB): 0.03\nEstimated Total Size (MB): 0.04\n----------------------------------------------------------------\n\n\nCreate an optimizer and a loss function.\n\nlr = 0.001\n\n# Defines a SGD optimizer to update the parameters\noptimizer_relu = optim.SGD(model_relu.parameters(), lr=lr)\n\n# Defines a BCE loss function\nloss_fn = nn.CrossEntropyLoss()\n\nTrain the model for 15 epochs.\n\n#collapse-output\nn_epochs = 15\n\ndlp_relu = DeepLearningPipeline(model_relu, loss_fn, optimizer_relu)\ndlp_relu.set_loaders(train_loader, valid_loader)\ndlp_relu.train(n_epochs)\n\nepoch:   0, train loss: 2.30268, valid loss: 2.30202\nepoch:   1, train loss: 2.30229, valid loss: 2.30165\nepoch:   2, train loss: 2.30193, valid loss: 2.30122\nepoch:   3, train loss: 2.30147, valid loss: 2.30069\nepoch:   4, train loss: 2.30086, valid loss: 2.29998\nepoch:   5, train loss: 2.30012, valid loss: 2.29905\nepoch:   6, train loss: 2.29906, valid loss: 2.29793\nepoch:   7, train loss: 2.29775, valid loss: 2.29667\nepoch:   8, train loss: 2.29621, valid loss: 2.29525\nepoch:   9, train loss: 2.29440, valid loss: 2.29363\nepoch:  10, train loss: 2.29227, valid loss: 2.29176\nepoch:  11, train loss: 2.28972, valid loss: 2.28957\nepoch:  12, train loss: 2.28673, valid loss: 2.28703\nepoch:  13, train loss: 2.28323, valid loss: 2.28408\nepoch:  14, train loss: 2.27912, valid loss: 2.28062\n\n\nNow plot the model losses.\n\nfig = dlp_relu.plot_losses()"
  },
  {
    "objectID": "posts/2022-10-23-pytorch-vanishing-gradients-deep-neural-networks.html#gradients-for-a-model-with-sigmoid-activations",
    "href": "posts/2022-10-23-pytorch-vanishing-gradients-deep-neural-networks.html#gradients-for-a-model-with-sigmoid-activations",
    "title": "Detect Vanishing Gradients in Deep Neural Networks by Plotting Gradient Distributions",
    "section": "Gradients for a model with Sigmoid activations",
    "text": "Gradients for a model with Sigmoid activations\nLet’s create a helper function that will plot the gradients for all the weights from each epoch. Note that our models have 4 layers, with L1 being the input layer and L4 being the output layer. Information flows from L1 to L4 during the forward pass. During the backward pass, gradients are calculated from the output layer (L4) and move toward the input layer (L1).\n\n\nCode\nimport seaborn as sns\n\ndef plot_gradients(grads, epoch=0):\n    \"\"\"\n    Args:\n        net: Object of class BaseNetwork\n        color: Color in which we want to visualize the histogram (for easier separation of activation functions)\n    \"\"\"\n    grads = grads\n\n    # Plotting\n    columns = len(grads)\n    fig, ax = plt.subplots(1, columns, figsize=(columns * 3.5, 2.5))\n    fig_index = 0\n    for key in grads:\n        key_ax = ax[fig_index % columns]\n        sns.histplot(data=grads[key], bins=30, ax=key_ax, kde=True)\n        key_ax.set_title(str(key))\n        key_ax.set_xlabel(\"Grad magnitude\", fontsize=11)\n        fig_index += 1\n    fig.suptitle(\n        f\"Epoch: {epoch}\", fontsize=16, y=1.05\n    )\n    fig.subplots_adjust(wspace=0.45)\n    plt.show()\n    plt.close()\n\n\nPlot the gradients for model with sigmoid activations.\n\nfor i in range(len(dlp_sigmoid.grad)):\n    plot_gradients(dlp_sigmoid.grad[i], i)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhat do we get from these plots?\n\nIssues\nIn this section, we will discuss the issues we can identify from these gradient plots for our SigmoidNet\nIssue 1: Look closely at how the Grad magnitude scale changes from L4 to L1 in an epoch. It is diminishing at an exponential rate. This tells us that the gradient is very high at the output layer but diminishes till it reaches L1.\nIssue 2: Gradient is also spread out and not smoother. This is a bad sign because it shows that different areas of the weight layer produce gradients in different ranges. For example, from L4 plot in the last epoch, we can see that the gradients are making clusters around -0.02, 0, and 0.02. We can also find that the gradient mean is not centered around 0. Either it is on the left of 0, or right, or has multiple peaks.\n\n\nPossible explanations\nIssue 1 Reason: Our network is facing Diminishing Gradient problem. I have pasted below another plot for Sigmoid. Image source\n\n\n\nsigmoid.png\n\n\nFrom this plot, we can see that the highest gradient produced by sigmoid is 0.25. So during backpropagation, when we calculate derivatives for deeper layers (L1, L2), there is a chain reaction where smaller and smaller numbers (less than 0.25) are multiplied to produce even smaller numbers. The result is that gradients diminish, and the weights are barely updated in deeper layers during the backward pass. We can avoid this by using a different activation function (e.g., ReLU) in hidden layers. We will do that in Section II.\nIssue 2 Reason: This is due to an initial weight initialization mismatch with the activation function used. By default, PyTorch uses kaiming initialization (Ref here) that works well for ReLU but not for sigmoid. It’s recommended to use Tanh or Xavior for sigmoid.\n\n\n\nCan we improve our SigmoidNet?\nLet’s train another SigmoidNet with the same configuration but with xavior initialization , and observe how it behaves now.\n\n#collapse-output\nmodel_sigmoid_v1 = SigmoidNet\nmodel_sigmoid_v1 = model_sigmoid_v1.to(device)\n\n# define a function to initialize weight with xavier uniform distribution\ndef init_weights(m):\n    if isinstance(m, nn.Linear):\n        torch.nn.init.xavier_uniform_(m.weight)\n        # m.bias.data.fill_(0.01)\n\n# reset model weights with xavier\nwith torch.no_grad():\n    model_sigmoid_v1.apply(init_weights)\n\nlr = 0.001\noptimizer_sigmoid = optim.SGD(model_sigmoid_v1.parameters(), lr=lr)\nloss_fn = nn.CrossEntropyLoss()\n\nn_epochs = 15\n\ndlp_sigmoid_v1 = DeepLearningPipeline(model_sigmoid_v1, loss_fn, optimizer_sigmoid)\ndlp_sigmoid_v1.set_loaders(train_loader, valid_loader)\ndlp_sigmoid_v1.train(n_epochs)\n\nepoch:   0, train loss: 2.49727, valid loss: 2.45576\nepoch:   1, train loss: 2.46252, valid loss: 2.42681\nepoch:   2, train loss: 2.43418, valid loss: 2.40356\nepoch:   3, train loss: 2.41090, valid loss: 2.38474\nepoch:   4, train loss: 2.39164, valid loss: 2.36943\nepoch:   5, train loss: 2.37565, valid loss: 2.35693\nepoch:   6, train loss: 2.36232, valid loss: 2.34670\nepoch:   7, train loss: 2.35119, valid loss: 2.33831\nepoch:   8, train loss: 2.34186, valid loss: 2.33143\nepoch:   9, train loss: 2.33404, valid loss: 2.32578\nepoch:  10, train loss: 2.32746, valid loss: 2.32113\nepoch:  11, train loss: 2.32192, valid loss: 2.31731\nepoch:  12, train loss: 2.31724, valid loss: 2.31417\nepoch:  13, train loss: 2.31328, valid loss: 2.31159\nepoch:  14, train loss: 2.30993, valid loss: 2.30946\n\n\n\nfig = dlp_sigmoid_v1.plot_losses()\n\n\n\n\n\n##\n#  SigmoidNet gradients with xavior initialization\nfor i in range(len(dlp_sigmoid_v1.grad)):\n    plot_gradients(dlp_sigmoid_v1.grad[i], i)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNow the plots are much smoother. Though the network is still suffering from diminishing gradients.\n\n\nRidge plot for gradients\nIn this section, we will use ridge plots for gradients. They provide a better perspective on how gradients evolve during epochs.\n\n\nCode\nimport pandas as pd\n\n# A helper function to get gradients of a weight layer for all epochs.\ndef get_layer_gradients(layer_name, layer_grads):\n    df = pd.DataFrame(columns=['x','g'])\n    for i in range(len(layer_grads)):\n        temp = {\n            'x': layer_grads[i][layer_name], # x --> gradients\n            'g': i # g --> epochs\n        }\n        epoch_df = pd.DataFrame(temp)\n        df = df.append(epoch_df, ignore_index=True)\n\n    return df\n\n\nPrint the names for the model weight layers.\n\nweight_layers_sigmoid_v1 = list(dlp_sigmoid_v1.grad[0].keys())\nweight_layers_sigmoid_v1\n\n['L1.weight', 'L2.weight', 'L3.weight', 'L4.weight']\n\n\nStore the gradients for each layer in a separate DataFrame. Each DataFrame has two columns\n\nx: for the gradient value\ng: for epoch\n\n\ndf0_sigmoid_v1 = get_layer_gradients(weight_layers_sigmoid_v1[0], dlp_sigmoid_v1.grad)\ndf1_sigmoid_v1 = get_layer_gradients(weight_layers_sigmoid_v1[1], dlp_sigmoid_v1.grad)\ndf2_sigmoid_v1 = get_layer_gradients(weight_layers_sigmoid_v1[2], dlp_sigmoid_v1.grad)\ndf3_sigmoid_v1 = get_layer_gradients(weight_layers_sigmoid_v1[3], dlp_sigmoid_v1.grad)\n\ndf3_sigmoid_v1.head()\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      x\n      g\n    \n  \n  \n    \n      0\n      -0.003590\n      0\n    \n    \n      1\n      -0.002747\n      0\n    \n    \n      2\n      -0.002291\n      0\n    \n    \n      3\n      -0.006168\n      0\n    \n    \n      4\n      -0.002711\n      0\n    \n  \n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\n\nCode\n# Another helper function to create the ridge plots\ndef plot_gradients_ridge_v1(df, layer_name):\n    sns.set_theme(style=\"white\", rc={\"axes.facecolor\": (0, 0, 0, 0)})\n\n    # Initialize the FacetGrid object\n    pal = sns.cubehelix_palette(10, rot=-.25, light=.7)\n    g = sns.FacetGrid(df, row=\"g\", hue=\"g\", aspect=15, height=.5, palette=pal)\n\n    # Draw the densities in a few steps\n    g.map(sns.kdeplot, \"x\",\n        bw_adjust=.5, clip_on=False,\n        fill=True, alpha=1, linewidth=1.5)\n    g.map(sns.kdeplot, \"x\", clip_on=False, color=\"w\", lw=2, bw_adjust=.5)\n\n    # passing color=None to refline() uses the hue mapping\n    g.refline(y=0, linewidth=2, linestyle=\"-\", color=None, clip_on=False)\n\n\n    # Define and use a simple function to label the plot in axes coordinates\n    def label(x, color, label):\n        ax = plt.gca()\n        ax.text(0, .2, label, fontweight=\"bold\", color=color,\n                ha=\"left\", va=\"center\", transform=ax.transAxes)\n\n\n    g.map(label, \"x\")\n\n    # Set the subplots to overlap\n    g.figure.subplots_adjust(hspace=-.25)\n\n    g.fig.suptitle(layer_name, ha='left', fontsize=16, fontweight=16)\n\n    # Remove axes details that don't play well with overlap\n    g.set_titles(\"\")\n    g.set(yticks=[], ylabel=\"\")\n    g.despine(bottom=True, left=True)\n\n    return g\n\n\nCreate plots for all weight layer.\n\n\nCode\n# https://stackoverflow.com/questions/35042255/how-to-plot-multiple-seaborn-jointplot-in-subplot\nimport warnings\nimport matplotlib.image as mpimg\nwarnings.filterwarnings(\"ignore\")\n\ng1 = plot_gradients_ridge_v1(df0_sigmoid_v1, weight_layers_sigmoid_v1[0])\ng2 = plot_gradients_ridge_v1(df1_sigmoid_v1, weight_layers_sigmoid_v1[1])\ng3 = plot_gradients_ridge_v1(df2_sigmoid_v1, weight_layers_sigmoid_v1[2])\ng4 = plot_gradients_ridge_v1(df3_sigmoid_v1, weight_layers_sigmoid_v1[3])\n\ng1.savefig('g1.png')\nplt.close(g1.fig)\n\ng2.savefig('g2.png')\nplt.close(g2.fig)\n\ng3.savefig('g3.png')\nplt.close(g3.fig)\n\ng4.savefig('g4.png')\nplt.close(g4.fig)\n\n############### 3. CREATE YOUR SUBPLOTS FROM TEMPORAL IMAGES\nf, axarr = plt.subplots(2, 2, figsize=(25, 16))\n\naxarr[0,0].imshow(mpimg.imread('g1.png'))\naxarr[0,1].imshow(mpimg.imread('g2.png'))\naxarr[1,0].imshow(mpimg.imread('g3.png'))\naxarr[1,1].imshow(mpimg.imread('g4.png'))\n\n\n# turn off x and y axis\n[ax.set_axis_off() for ax in axarr.ravel()]\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\nWhat do we get from these ridge plots?\n\nIt also shows that the network is suffering from diminishing gradients. gradient scale ‘x’ is decreasing exponentially between weight layers\nIt shows that gradients are very spread out at the start. They keep on saturating till around epoch 10. After that, instead of getting more saturated around zero, multiple peaks start to emerge. This could be due to our learning rate. In the later epochs, our weights start to oscillate around zero. We can avoid this by using an adaptive learning rate that decreases when weights are near the global minimum."
  },
  {
    "objectID": "posts/2022-10-23-pytorch-vanishing-gradients-deep-neural-networks.html#gradients-for-a-model-with-relu-activations",
    "href": "posts/2022-10-23-pytorch-vanishing-gradients-deep-neural-networks.html#gradients-for-a-model-with-relu-activations",
    "title": "Detect Vanishing Gradients in Deep Neural Networks by Plotting Gradient Distributions",
    "section": "Gradients for a model with ReLU activations",
    "text": "Gradients for a model with ReLU activations\nIn this section we will visualize gradients for our model with ReLU activations.\n\nfor i in range(len(dlp_relu.grad)):\n    plot_gradients(dlp_relu.grad[i], i)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhat do we get from these plots?\n\nThis time, the gradient curves are nice and smooth. They are not diminishing between weight layers.\nGradients are shrinking between epochs. This means that learning is slowing down in later epochs, and we need to increase the learning rate.\n\n\n\nRidge plot for gradients\nLet’s also do ridge plots for ReluNet.\n\n\nCode\n# a helper function relu ridge plots. \n# same as 'plot_gradients_ridge_v1' but with added limits\ndef plot_gradients_ridge_v2(df, layer_name):\n    sns.set_theme(style=\"white\", rc={\"axes.facecolor\": (0, 0, 0, 0)})\n\n    # Initialize the FacetGrid object\n    pal = sns.cubehelix_palette(10, rot=-.25, light=.7)\n    g = sns.FacetGrid(df, row=\"g\", hue=\"g\", aspect=15, height=3.5, palette=pal, sharex=False, xlim=(-0.01,0.01)) ## works best\n    # g = sns.FacetGrid(df, row=\"g\", hue=\"g\", aspect=7, height=3.5, palette=pal)#, sharex=False, xlim=(-0.01,0.01)) ## works good\n\n    # Draw the densities in a few steps\n    g.map(sns.kdeplot, \"x\", bw_adjust=.5, clip_on=True, fill=True, alpha=1, linewidth=1.5)\n    g.map(sns.kdeplot, \"x\", clip_on=True, color=\"w\", lw=2, bw_adjust=.5)\n\n    # passing color=None to refline() uses the hue mapping\n    g.refline(y=0, linewidth=2, linestyle=\"-\", color=None, clip_on=True)\n\n\n    # Define and use a simple function to label the plot in axes coordinates\n    def label(x, color, label):\n        ax = plt.gca()\n        ax.text(0, .2, label, fontsize=40, fontweight=26, color=color, ha=\"left\", va=\"center\", transform=ax.transAxes)\n\n\n    g.map(label, \"x\")\n\n    # Set the subplots to overlap\n    g.figure.subplots_adjust(hspace=-.25)\n\n    g.fig.suptitle(layer_name, ha='left', fontsize=40, fontweight=26)\n\n    # Remove axes details that don't play well with overlap\n    g.set_titles(\"\")\n    g.set(yticks=[], ylabel=\"\")\n    g.despine(bottom=True, left=True)\n    plt.xticks(fontsize=35)\n\n    return g\n\n\n\n\nCode\nweight_layers_relu = list(dlp_relu.grad[0].keys())\n\ndf0_relu = get_layer_gradients(weight_layers_relu[0], dlp_relu.grad)\ndf1_relu = get_layer_gradients(weight_layers_relu[1], dlp_relu.grad)\ndf2_relu = get_layer_gradients(weight_layers_relu[2], dlp_relu.grad)\ndf3_relu = get_layer_gradients(weight_layers_relu[3], dlp_relu.grad)\n\n\n\n\nCode\nimport matplotlib.image as mpimg\nwarnings.filterwarnings(\"ignore\")\n\ng1 = plot_gradients_ridge_v2(df0_relu, weight_layers_relu[0])\ng2 = plot_gradients_ridge_v2(df1_relu, weight_layers_relu[1])\ng3 = plot_gradients_ridge_v2(df2_relu, weight_layers_relu[2])\ng4 = plot_gradients_ridge_v2(df3_relu, weight_layers_relu[3])\n\ng1.savefig('g1.png')\nplt.close(g1.fig)\n\ng2.savefig('g2.png')\nplt.close(g2.fig)\n\ng3.savefig('g3.png')\nplt.close(g3.fig)\n\ng4.savefig('g4.png')\nplt.close(g4.fig)\n\n############### 3. CREATE YOUR SUBPLOTS FROM TEMPORAL IMAGES\nf, axarr = plt.subplots(2, 2, figsize=(25, 16))\n\naxarr[0,0].imshow(mpimg.imread('g1.png'))\naxarr[0,1].imshow(mpimg.imread('g2.png'))\naxarr[1,0].imshow(mpimg.imread('g3.png'))\naxarr[1,1].imshow(mpimg.imread('g4.png'))\n\n\n# turn off x and y axis\n[ax.set_axis_off() for ax in axarr.ravel()]\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\nWhat do we get from these ridge plots?\nThe results are consistent with earlier plots.\n\nGradients are not diminishing between weight layers.\nGradients are shrinking between epochs, and we need to increase the learning rate for later epochs."
  },
  {
    "objectID": "posts/2022-11-07-timeseries-rnn-gru-lstm-cnn-pytorch.html#credits",
    "href": "posts/2022-11-07-timeseries-rnn-gru-lstm-cnn-pytorch.html#credits",
    "title": "Build Temporal Models for Univariate Time series Data with RNN, GRU, LSTM, CNN using PyTorch",
    "section": "Credits",
    "text": "Credits\nThis notebook takes inspiration and ideas from the following sources.\n\nThe outstanding book “Deep Learning with PyTorch Step-by-Step” by “Daniel Voigt Godoy”. You can get the book from its website: pytorchstepbystep. In addition, the GitHub repository for this book has valuable notebooks: github.com/dvgodoy/PyTorchStepByStep. Parts of the code you see in this notebook are taken from chapter 3 and chapter 8 notebooks of the same book.\nVery helpful Kaggle notebook from ‘TARON ZAKARYAN’ to predict stock prices using LSTM. Link here"
  },
  {
    "objectID": "posts/2022-11-07-timeseries-rnn-gru-lstm-cnn-pytorch.html#environment",
    "href": "posts/2022-11-07-timeseries-rnn-gru-lstm-cnn-pytorch.html#environment",
    "title": "Build Temporal Models for Univariate Time series Data with RNN, GRU, LSTM, CNN using PyTorch",
    "section": "Environment",
    "text": "Environment\nThis notebook is prepared with Google Colab.\n\n\nCode\nfrom platform import python_version\nimport numpy, matplotlib, pandas, torch, seaborn\n\nprint(\"python==\" + python_version())\nprint(\"numpy==\" + numpy.__version__)\nprint(\"torch==\" + torch.__version__)\nprint(\"matplotlib==\" + matplotlib.__version__)\nprint(\"seaborn==\" + seaborn.__version__)\n\n\npython==3.7.15\nnumpy==1.21.6\ntorch==1.12.1+cu113\nmatplotlib==3.2.2\nseaborn==0.11.2"
  },
  {
    "objectID": "posts/2022-11-07-timeseries-rnn-gru-lstm-cnn-pytorch.html#introduction",
    "href": "posts/2022-11-07-timeseries-rnn-gru-lstm-cnn-pytorch.html#introduction",
    "title": "Build Temporal Models for Univariate Time series Data with RNN, GRU, LSTM, CNN using PyTorch",
    "section": "Introduction",
    "text": "Introduction\nRecurrent Neural Network (RNN) is great for exploiting data that involves one-dimensional (1D) ordered structures. We call these 1D-ordered structures sequences. Two main sequence problems are Time series and Natural Language Processing (NLP). RNN and its variants are developed to work for both types of sequence problems, but in this notebook we will only deal with time series sequences.\nI have divided this notebook into two sections. In the first section, our focus will be on understanding the structure of sequences and generating training sets and batches from them. We will develop a simple (synthetic) sequence data and then create its training set. Next, we will make batches using PyTorch DataLoaders and write a training pipeline. We will end this section by training an RNN on this data.\nIn the next section, our focus will be more on the internals of different neural architectures for sequence data problems. We will use stock price data and train multiple networks (RNN, GRU, LSTM, CNN) on it while understanding their features and behavior."
  },
  {
    "objectID": "posts/2022-11-07-timeseries-rnn-gru-lstm-cnn-pytorch.html#data-generation",
    "href": "posts/2022-11-07-timeseries-rnn-gru-lstm-cnn-pytorch.html#data-generation",
    "title": "Build Temporal Models for Univariate Time series Data with RNN, GRU, LSTM, CNN using PyTorch",
    "section": "Data generation",
    "text": "Data generation\nLet’s generate some one dimensional ordered sequence data.\n\n# from numpy.ma.core import size\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n# generate 1000 data points\nn_points = 1000\nnoise = 0.04\n\nX_synth = np.arange(1, n_points + 1, 1)\ny_synth = np.sin(X_synth * np.pi / 180) + np.random.randn(n_points) * noise\n\ndf_synth = pd.DataFrame(y_synth, index=X_synth)\n\n# plot timeseries data\ndf_synth.plot(figsize=(15, 6))\nplt.ylabel(\"value\")\nplt.xlabel(\"step\")\nplt.title(\"Synthetic time series data\")\nplt.show()\n\n\n\n\nIn the above plot, X dimension represents the time or steps. And y dimension represents the measurements. In actual data, these measurements could represent price stocks, temperature, population, etc. If we print our DataFrame, it has only one column which shows the measurements. The DataFrame index represents the time dimension.\n\ndf_synth.head()\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      0\n    \n  \n  \n    \n      1\n      0.031713\n    \n    \n      2\n      -0.000675\n    \n    \n      3\n      0.026890\n    \n    \n      4\n      0.087844\n    \n    \n      5\n      0.057978"
  },
  {
    "objectID": "posts/2022-11-07-timeseries-rnn-gru-lstm-cnn-pytorch.html#data-preparation",
    "href": "posts/2022-11-07-timeseries-rnn-gru-lstm-cnn-pytorch.html#data-preparation",
    "title": "Build Temporal Models for Univariate Time series Data with RNN, GRU, LSTM, CNN using PyTorch",
    "section": "Data preparation",
    "text": "Data preparation\nNotice that our data does not have any labels. We usually have features and labels to train our model in supervised learning problems. However, sequence data is unique as we try to predict the next value from the sequence data itself. Therefore, we don’t have to provide labels with our data separately but can generate them from the sequence itself.\nLet’s use a simple ordered sequence of 15 integers to understand how the training set is created from it.\n\n##\n# generate a simple sequential data of 15 integers\ndata_dummy = np.arange(15)\nprint(data_dummy)\n\n# create a DataFrame of this sequence\ndf_dummy = pd.DataFrame(data_dummy)\n\n[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14]\n\n\nWe can split this long sequence into multiple smaller sequences (as training and test sets). The earlier part of the sequence will contain training features, and the last element acts as a label.\nI have created a helper function that will take a sequence DataFrame, and split it into training features and labels.\n\n##\n# create a function to generate multiple sequences for training and testing\n# look_back = size of the generated sets\ndef generate_sequences(df, test_size=0.3, look_back=10):\n    data = []\n    df_raw = df.values\n\n    for index in range(len(df_raw) - look_back):\n        data.append(df_raw[index : index + look_back])\n\n    data = np.array(data)\n\n    test_set_size = int(np.round(test_size * data.shape[0]))\n    train_set_size = data.shape[0] - (test_set_size)\n\n    x_train = data[:train_set_size, :-1, :]\n    y_train = data[:train_set_size, -1, :]\n    x_test = data[train_set_size:, :-1]\n    y_test = data[train_set_size:, -1, :]\n\n    return [x_train, y_train, x_test, y_test]\n\nLet’s apply this function to our sequence and check the output.\n\n##\n# generate test and train sequences\n# x = features\n# y = labels\nx_train_dummy, y_train_dummy, x_test_dummy, y_test_dummy = generate_sequences(df_dummy)\n\n# view the training data. features and labels together\n# feature 't' = labels\ndf_train_dummy = pd.DataFrame(np.squeeze(x_train_dummy))\ndf_train_dummy[\"t\"] = np.squeeze(y_train_dummy)\ndf_train_dummy.head(10)\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      0\n      1\n      2\n      3\n      4\n      5\n      6\n      7\n      8\n      t\n    \n  \n  \n    \n      0\n      0\n      1\n      2\n      3\n      4\n      5\n      6\n      7\n      8\n      9\n    \n    \n      1\n      1\n      2\n      3\n      4\n      5\n      6\n      7\n      8\n      9\n      10\n    \n    \n      2\n      2\n      3\n      4\n      5\n      6\n      7\n      8\n      9\n      10\n      11\n    \n  \n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\nNotice that our training set has smaller sequences, with the last element acting as a label denoted by column ‘t’. This is because our generate_sequences function acts as a moving window where earlier elements become features and the last element in the window acts as a label.\nLet’s also check the generated testing set.\n\n##\n# view the testing data. features and labels together\n# feature 't' = labels\ndf_test_dummy = pd.DataFrame(np.squeeze(x_test_dummy))\ndf_test_dummy[\"t\"] = np.squeeze(y_test_dummy)\ndf_test_dummy.head(10)\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      0\n      1\n      2\n      3\n      4\n      5\n      6\n      7\n      8\n      t\n    \n  \n  \n    \n      0\n      3\n      4\n      5\n      6\n      7\n      8\n      9\n      10\n      11\n      12\n    \n    \n      1\n      4\n      5\n      6\n      7\n      8\n      9\n      10\n      11\n      12\n      13\n    \n  \n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\nFrom this sequence example, we have learned that we can generate training and test sets of different sizes using the same sequence data. The features and label aren’t provided separately but can be produced by splitting the sequence data into smaller chunks. The last element in the chunks acts as the label.\nLet’s apply this understanding to our synthetic data and generate training and test samples.\n\n##\n# generate training and test data for synthetic sequence data\nx_train_synth, y_train_synth, x_test_synth, y_test_synth = generate_sequences(df_synth)"
  },
  {
    "objectID": "posts/2022-11-07-timeseries-rnn-gru-lstm-cnn-pytorch.html#load-generated-data-into-pytorch-dataset-and-dataloader-class",
    "href": "posts/2022-11-07-timeseries-rnn-gru-lstm-cnn-pytorch.html#load-generated-data-into-pytorch-dataset-and-dataloader-class",
    "title": "Build Temporal Models for Univariate Time series Data with RNN, GRU, LSTM, CNN using PyTorch",
    "section": "Load generated data into PyTorch Dataset and DataLoader class",
    "text": "Load generated data into PyTorch Dataset and DataLoader class\nNow let’s load our data into Dataset and DataLoader classes. PyTorch Dataset is a helper class that converts data and labels into a list of tuples. DataLoader is another helper class to create batches from Dataset tuples. batch_size means the number of tuples we want in a single batch. We have used 16 here, so each fetch from DataLoader will give us a list of 16 tuples.\n\nimport torch\nfrom torch.utils.data import DataLoader, TensorDataset\n\ntrain_dataset_synth = TensorDataset(\n    torch.as_tensor(x_train_synth).float(), torch.as_tensor(y_train_synth).float()\n)\ntest_dataset_synth = TensorDataset(\n    torch.as_tensor(x_test_synth).float(), torch.as_tensor(y_test_synth).float()\n)\n\nbatch_size = 16\n\ntrain_loader_synth = DataLoader(\n    train_dataset_synth, batch_size=batch_size, shuffle=True\n)\ntest_loader_synth = DataLoader(test_dataset_synth, batch_size=batch_size)"
  },
  {
    "objectID": "posts/2022-11-07-timeseries-rnn-gru-lstm-cnn-pytorch.html#define-a-class-to-implement-training-validation-and-mini-batch-processing-pipeline",
    "href": "posts/2022-11-07-timeseries-rnn-gru-lstm-cnn-pytorch.html#define-a-class-to-implement-training-validation-and-mini-batch-processing-pipeline",
    "title": "Build Temporal Models for Univariate Time series Data with RNN, GRU, LSTM, CNN using PyTorch",
    "section": "Define a class to implement training, validation, and mini-batch processing pipeline",
    "text": "Define a class to implement training, validation, and mini-batch processing pipeline\nIn this section we will implement a class that encapsulates all the usual steps required in training a PyTorch model. This way we can focus more on the model architecture and performance, and less concerned about the boilerplate training loop. Important parts of this class are\n\n__init__: Class constructor to define the main actors in a training cycle including model, optimizer, loss function, training and validation DataLoaders\n_make_train_step_fn: Training pipeline is usually called “training step” which includes the following steps\n\nCompute our model’s predicted output - the forward pass\nCompute the loss\nCompute gradients i.e., find the direction and scale to update the weights to reduce the loss\nUpdate weight parameters using gradients and the learning rate\n\n_make_val_step_fn: Validation pipeline is usually called the “validation step” which includes the following steps\n\nCompute our model’s predicted output - the forward pass\nCompute the loss\nNote that during validation, we are only concerned about the loss, i.e., how well our model performs on the validation dataset. Therefore, we don’t use it to calculate the gradients.\n\n_mini_batch: It defines the steps to process a single minibatch in a helper function. For a mini-batch processing, we want to\n\nGet the next batch of data and labels (x, y) from the DataLoader iterator\nPerform a step on the batch. A step can be either training or validation\nCompute the average batch loss\n\ntrain: Execute training and validation steps for given number of epoch\npredict: Make a prediction from model on provided data\n\n\n\nCode\nimport numpy as np\nimport datetime\n\nclass DeepLearningPipeline(object):\n    def __init__(self, model, loss_fn, optimizer):\n        # Here we define the attributes of our class\n\n        # We start by storing the arguments as attributes\n        # to use them later\n        self.model = model\n        self.loss_fn = loss_fn\n        self.optimizer = optimizer\n        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n        # Let's send the model to the specified device right away\n        self.model.to(self.device)\n\n        # These attributes are defined here, but since they are\n        # not informed at the moment of creation, we keep them None\n        self.train_loader = None\n        self.val_loader = None\n        self.writer = None\n\n        # These attributes are going to be computed internally\n        self.losses = []\n        self.val_losses = []\n        self.total_epochs = 0\n\n        # Creates the train_step function for our model,\n        # loss function and optimizer\n        # Note: there are NO ARGS there! It makes use of the class\n        # attributes directly\n        self.train_step_fn = self._make_train_step_fn()\n        # Creates the val_step function for our model and loss\n        self.val_step_fn = self._make_val_step_fn()\n\n    def set_loaders(self, train_loader, val_loader=None):\n        # This method allows the user to define which train_loader (and val_loader, optionally) to use\n        # Both loaders are then assigned to attributes of the class\n        # So they can be referred to later\n        self.train_loader = train_loader\n        self.val_loader = val_loader\n\n    def _make_train_step_fn(self):\n        # This method does not need ARGS... it can refer to\n        # the attributes: self.model, self.loss_fn and self.optimizer\n\n        # Builds function that performs a step in the train loop\n        def perform_train_step_fn(x, y):\n            # Sets model to TRAIN mode\n            self.model.train()\n\n            # Step 1 - Computes our model's predicted output - forward pass\n            yhat = self.model(x)\n            # Step 2 - Computes the loss\n            loss = self.loss_fn(yhat, y)\n            # Step 3 - Computes gradients for both \"a\" and \"b\" parameters\n            loss.backward()\n            # Step 4 - Updates parameters using gradients and the learning rate\n            self.optimizer.step()\n            self.optimizer.zero_grad()\n\n            # Returns the loss\n            return loss.item()\n\n        # Returns the function that will be called inside the train loop\n        return perform_train_step_fn\n\n    def _make_val_step_fn(self):\n        # Builds function that performs a step in the validation loop\n        def perform_val_step_fn(x, y):\n            # Sets model to EVAL mode\n            self.model.eval()\n\n            # Step 1 - Computes our model's predicted output - forward pass\n            yhat = self.model(x)\n            # Step 2 - Computes the loss\n            loss = self.loss_fn(yhat, y)\n            # There is no need to compute Steps 3 and 4,\n            # since we don't update parameters during evaluation\n            return loss.item()\n\n        return perform_val_step_fn\n\n    def _mini_batch(self, validation=False):\n        # The mini-batch can be used with both loaders\n        # The argument `validation`defines which loader and\n        # corresponding step function is going to be used\n        if validation:\n            data_loader = self.val_loader\n            step_fn = self.val_step_fn\n        else:\n            data_loader = self.train_loader\n            step_fn = self.train_step_fn\n\n        if data_loader is None:\n            return None\n\n        # Once the data loader and step function, this is the\n        # same mini-batch loop we had before\n        mini_batch_losses = []\n        for x_batch, y_batch in data_loader:\n            x_batch = x_batch.to(self.device)\n            y_batch = y_batch.to(self.device)\n\n            mini_batch_loss = step_fn(x_batch, y_batch)\n            mini_batch_losses.append(mini_batch_loss)\n\n        loss = np.mean(mini_batch_losses)\n        return loss\n\n    def set_seed(self, seed=42):\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\n        torch.manual_seed(seed)\n        np.random.seed(seed)\n\n    def train(self, n_epochs, seed=42, print_loss=False):\n        # To ensure reproducibility of the training process\n        self.set_seed(seed)\n\n        for epoch in range(n_epochs):\n            # Keeps track of the numbers of epochs\n            # by updating the corresponding attribute\n            self.total_epochs += 1\n\n            # inner loop\n            # Performs training using mini-batches\n            loss = self._mini_batch(validation=False)\n            self.losses.append(loss)\n\n            if print_loss:\n                if epoch % 10 == 0 and epoch != 0:\n                    print(\"Epoch \", epoch, \"MSE: \", loss)\n\n            # VALIDATION\n            # no gradients in validation!\n            with torch.no_grad():\n                # Performs evaluation using mini-batches\n                val_loss = self._mini_batch(validation=True)\n                self.val_losses.append(val_loss)\n\n            # If a SummaryWriter has been set...\n            if self.writer:\n                scalars = {\"training\": loss}\n                if val_loss is not None:\n                    scalars.update({\"validation\": val_loss})\n                # Records both losses for each epoch under the main tag \"loss\"\n                self.writer.add_scalars(\n                    main_tag=\"loss\", tag_scalar_dict=scalars, global_step=epoch\n                )\n\n        if self.writer:\n            # Closes the writer\n            self.writer.close()\n\n    def predict(self, x):\n        # Set is to evaluation mode for predictions\n        self.model.eval()\n        # Takes aNumpy input and make it a float tensor\n        x_tensor = torch.as_tensor(x).float()\n        # Send input to device and uses model for prediction\n        y_hat_tensor = self.model(x_tensor.to(self.device))\n        # Set it back to train mode\n        self.model.train()\n        # Detaches it, brings it to CPU and back to Numpy\n        return y_hat_tensor.detach().cpu().numpy()\n\n    def plot_losses(self):\n        fig = plt.figure(figsize=(10, 4))\n        plt.plot(self.losses, label=\"Training Loss\", c=\"b\")\n        plt.plot(self.val_losses, label=\"Validation Loss\", c=\"r\")\n        plt.yscale(\"log\")\n        plt.xlabel(\"Epochs\")\n        plt.ylabel(\"Loss\")\n        plt.legend()\n        plt.tight_layout()\n        return fig"
  },
  {
    "objectID": "posts/2022-11-07-timeseries-rnn-gru-lstm-cnn-pytorch.html#model-configuration-and-training",
    "href": "posts/2022-11-07-timeseries-rnn-gru-lstm-cnn-pytorch.html#model-configuration-and-training",
    "title": "Build Temporal Models for Univariate Time series Data with RNN, GRU, LSTM, CNN using PyTorch",
    "section": "Model configuration and training",
    "text": "Model configuration and training\nWe have all the pieces ready to train a neural network on the ordered sequence data. So here, I will train an RNN model on the generated data. At this point, I will not go into the details of the structure and working of RNN. But in the next section, we will discuss it in much more detail.\n\n\nCode\n# configure an RNN model\nimport torch.nn as nn\nimport torch.optim as optim\n\nclass RnnModel(nn.Module):\n    def __init__(self, n_features, hidden_dim, n_outputs, n_layers):\n        super(RnnModel, self).__init__()\n        self.hidden_dim = hidden_dim\n        self.n_features = n_features\n        self.n_outputs = n_outputs\n        self.n_layers = n_layers\n        self.hidden = None\n\n        # Simple RNN\n        self.basic_rnn = nn.RNN(\n            self.n_features, self.hidden_dim, self.n_layers, batch_first=True\n        )\n        # Classifier to produce as many logits as outputs\n        self.classifier = nn.Linear(self.hidden_dim, self.n_outputs)\n\n    def forward(self, X):\n        # X is batch first (N, L, F)\n        # output is (N, L, H)\n        # final hidden state is (1, N, H)\n        # print(X.shape)\n        batch_first_output, self.hidden = self.basic_rnn(X)\n\n        # print(\"check1\")\n        # only last item in sequence (N, 1, H)\n        last_output = batch_first_output[:, -1]\n        # classifier will output (N, 1, n_outputs)\n        out = self.classifier(last_output)\n\n        # final output is (N, n_outputs)\n        return out.view(-1, self.n_outputs)\n\n\nConfigure model loss and optimizer.\n\ntorch.manual_seed(21)\nrnn_model = RnnModel(n_features=1, hidden_dim=10, n_outputs=1, n_layers=1)\nrnn_loss = nn.MSELoss()\nrnn_optimizer = optim.Adam(rnn_model.parameters(), lr=0.01)\n\nRun the training pipeline.\n\ndlp_rnn = DeepLearningPipeline(rnn_model, rnn_loss, rnn_optimizer)\ndlp_rnn.set_loaders(train_loader_synth, test_loader_synth)\ndlp_rnn.train(100, print_loss=True)\n\nEpoch  10 MSE:  0.0043875276698434554\nEpoch  20 MSE:  0.003170915104088966\nEpoch  30 MSE:  0.0032213201127226716\nEpoch  40 MSE:  0.003209590242477134\nEpoch  50 MSE:  0.0030302550162146376\nEpoch  60 MSE:  0.0031480757964097643\nEpoch  70 MSE:  0.002840602589210241\nEpoch  80 MSE:  0.0030571757948068394\nEpoch  90 MSE:  0.0031562208208594134\n\n\nPlot the model loss.\n\nfig = dlp_rnn.plot_losses()\n\n\n\n\nGet predictions on the test data.\n\ny_test_pred_synth = dlp_rnn.predict(x_test_synth)\ny_train_pred_synth = dlp_rnn.predict(x_train_synth)\n\nCalculate mean squared error.\n\nimport math\nfrom sklearn.metrics import mean_squared_error\nfrom math import sqrt\n\n# calculate root mean squared error\ntrainScore = math.sqrt(\n    mean_squared_error(y_train_synth[:, 0], y_train_pred_synth[:, 0])\n)\nprint(\"Train Score: %.2f RMSE\" % (trainScore))\ntestScore = math.sqrt(mean_squared_error(y_test_synth[:, 0], y_test_pred_synth[:, 0]))\nprint(\"Test Score: %.2f RMSE\" % (testScore))\n\nTrain Score: 0.05 RMSE\nTest Score: 0.05 RMSE\n\n\nPlot the predicted values along with true values on the test data.\n\ndef plot_predictions(y, y_pred, model_name=\"\"):\n    plt.figure(figsize=(15, 7))\n\n    x = np.arange(len(y))\n    plt.plot(x, y, color=\"red\", label=\"True values\")\n    plt.plot(x, y_pred, color=\"blue\", label=\"Predicted values\")\n\n    title = \"Comparison of true and predicted values\"\n    if len(model_name):\n        title = model_name + \": \" + title\n\n    plt.title(title)\n    plt.xlabel(\"Steps\")\n    plt.ylabel(\"Values\")\n    plt.legend()\n    plt.show()\n\n\nplot_predictions(y_test_synth, y_test_pred_synth)\n\n\n\n\nThat is the end of Section I. We have successfully trained a recurrent neural network on ordered sequence data, and our predicted values are very close to the actual values. We have also learned to use ordered sequences to generate training and test data sets with features and labels."
  },
  {
    "objectID": "posts/2022-11-07-timeseries-rnn-gru-lstm-cnn-pytorch.html#data-preparation-1",
    "href": "posts/2022-11-07-timeseries-rnn-gru-lstm-cnn-pytorch.html#data-preparation-1",
    "title": "Build Temporal Models for Univariate Time series Data with RNN, GRU, LSTM, CNN using PyTorch",
    "section": "Data preparation",
    "text": "Data preparation\nLet’s load this data and view the stock prices as a plot.\n\nfolder = \"./datasets/2022-11-07-timeseries-rnn-gru-lstm-cnn-pytorch/\"\nfile_name = \"MSFT_2006-01-01_to_2018-01-01.csv\"\ndf_msft = pd.read_csv(folder + file_name, parse_dates=True, index_col=0)\ndf_msft[[\"Close\"]].plot(figsize=(15, 6))\nplt.ylabel(\"stock_price\")\nplt.title(\"MSFT Stock\")\nplt.show()\n\n\n\n\n\n##\n# Range of the stock price\nprint(\"Minimum stock price: \", min(df_msft['Close'].values))\nprint(\"Maximum stock price: \", max(df_msft['Close'].values)) \n\nMinimum stock price:  15.15\nMaximum stock price:  86.85\n\n\nFrom the above plot, we can see that the price value continuously increases over time, and the range of prices is roughly between 15 to 87 USD. This scale is not good news for neural networks as they work best when they get data on a scale closer to zero. Preferably -1 to 1. So in the next cell, we will convert our data to a much smaller scale.\n\nfrom sklearn.preprocessing import MinMaxScaler\n\nscaler = MinMaxScaler(feature_range=(-1, 1))\n\ndf_msft = df_msft[[\"Close\"]]\n# fill any missing values as a precaution\ndf_msft = df_msft.fillna(method=\"ffill\")\n\n# create a copy for scaling and keep original data\ndf_msft_scaled = df_msft.copy(deep=True)\ndf_msft_scaled[\"Close\"] = scaler.fit_transform(df_msft[\"Close\"].values.reshape(-1, 1))\n\nprint(\"*** Before scaling ***\\n\", df_msft.tail())\nprint(\"\\n*** After scaling ***\\n\", df_msft_scaled.tail())\n\n*** Before scaling ***\n             Close\nDate             \n2017-12-22  85.51\n2017-12-26  85.40\n2017-12-27  85.71\n2017-12-28  85.72\n2017-12-29  85.54\n\n*** After scaling ***\n                Close\nDate                \n2017-12-22  0.962622\n2017-12-26  0.959554\n2017-12-27  0.968201\n2017-12-28  0.968480\n2017-12-29  0.963459\n\n\nIn the next step we will generate training and test sets for our data.\n\n# look_back = size of a sequence in training and test set\nlook_back = 30\nx_train_scaled, y_train_scaled, x_test_scaled, y_test_scaled = generate_sequences(\n    df_msft_scaled, look_back=look_back\n)\n\nNow let’s load this data into PyTorch Dataset and DataLoader class.\n\nimport torch\nfrom torch.utils.data import DataLoader, Dataset, random_split, TensorDataset\n\ntrain_data = TensorDataset(\n    torch.as_tensor(x_train_scaled).float(), torch.as_tensor(y_train_scaled).float()\n)\ntest_data = TensorDataset(\n    torch.as_tensor(x_test_scaled).float(), torch.as_tensor(y_test_scaled).float()\n)\n\nbatch_size = 32\ntrain_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\ntest_loader = DataLoader(test_data, batch_size=batch_size)"
  },
  {
    "objectID": "posts/2022-11-07-timeseries-rnn-gru-lstm-cnn-pytorch.html#recurrent-neural-network-rnn",
    "href": "posts/2022-11-07-timeseries-rnn-gru-lstm-cnn-pytorch.html#recurrent-neural-network-rnn",
    "title": "Build Temporal Models for Univariate Time series Data with RNN, GRU, LSTM, CNN using PyTorch",
    "section": "Recurrent Neural Network (RNN)",
    "text": "Recurrent Neural Network (RNN)\nLet’s train the same RNN we built in section 1 on stock prices data, and check it’s performance.\n\n##\n# configure model, its loss and optimizer\ntorch.manual_seed(21)\nmodel = RnnModel(n_features=1, hidden_dim=32, n_outputs=1, n_layers=1)\nloss = nn.MSELoss()\noptimizer = optim.Adam(model.parameters(), lr=0.01)\n\n\n##\n# check the dimension for one batch\ntemp = next(iter(train_loader))\nlen(temp[0]), len(temp[0][0]), len(temp[0][0][0]) # N (batch_size), L (seq len), F (n_features)\n\n(32, 29, 1)\n\n\n\n##\n# start training pipeline\ndlp_rnn = DeepLearningPipeline(model, loss, optimizer)\ndlp_rnn.set_loaders(train_loader, test_loader)\ndlp_rnn.train(100, print_loss=True)\n\nEpoch  10 MSE:  0.00026397304452874585\nEpoch  20 MSE:  0.00022604088944993265\nEpoch  30 MSE:  0.00023406338930891997\nEpoch  40 MSE:  0.00029436370514855355\nEpoch  50 MSE:  0.000270840552151309\nEpoch  60 MSE:  0.000291384112201878\nEpoch  70 MSE:  0.00026263904168665636\nEpoch  80 MSE:  0.00022778069747304968\nEpoch  90 MSE:  0.00023518060378123528\n\n\nPlot training and validation loss.\n\nfig = dlp_rnn.plot_losses()\n\n\n\n\nIn the next cell, we will make predictions on the test data. After that, we will invert (or rescale) predicted and actual values to their original scale. Once that is done, we will use predicted and actual values to calculate RMSE (Root Mean Squared Error).\n\n##\n# make predictions on the test data\ny_test_pred_scaled = dlp_rnn.predict(x_test_scaled)\ny_train_pred_scaled = dlp_rnn.predict(x_train_scaled)\n\n# invert predictions and true values\ny_train_pred = scaler.inverse_transform(y_train_pred_scaled)\ny_train = scaler.inverse_transform(y_train_scaled)\ny_test_pred = scaler.inverse_transform(y_test_pred_scaled)\ny_test = scaler.inverse_transform(y_test_scaled)\n\n# calculate root mean squared error\ntrainScore = math.sqrt(mean_squared_error(y_train[:, 0], y_train_pred[:, 0]))\ntestScore = math.sqrt(mean_squared_error(y_test[:, 0], y_test_pred[:, 0]))\n\nprint(\"Train Score: %.2f RMSE\" % (trainScore))\nprint(\"Test Score: %.2f RMSE\" % (testScore))\n\nTrain Score: 0.52 RMSE\nTest Score: 2.37 RMSE\n\n\nPlot the True and Predicted values.\n\nplot_predictions(y_test, y_test_pred)\n\n\n\n\nThe above plot shows that the RNN model can correctly predict values till about 500 steps, but after that predictions start to diverge, and the gap keeps increasing as time passes.\n\nRNN cell in detail\nIf you revisit section 1 topic ‘Model configuration and training’, we have built an RNN model using PyTorch nn.RNN class.\nself.basic_rnn = nn.RNN(self.n_features, self.hidden_dim, self.n_layers, batch_first=True)\nFrom PyTorch documentation, we don’t get much information on the internal working of this class as it only gives a short description. (Link here)\n\nApplies a multi-layer Elman RNN with tanh or ReLU non-linearity to an input sequence.\n\nPyTorch Step By Step Chapter 8 does a great job of explaining the internal working of an RNN Cell. I have taken the following image from the book’s official GitHub repo.\n Image Source: dvgodoy/PyTorchStepByStep/blob/master/Chapter08\nFrom this image we can reason the following\n\nThere are two types of weight layers\n\nFirst weight layer (Wi) which processes the input. Let’s call it as ‘input linear layer’ or in short linear_input\nSecond weight layer (Wh) which processes the hidden state. Let’s call it as hidden linear layer or in short linear_hidden\n\nRNN cell processes two types of input at the same time\n\nSequence data input or X\nand the Hidden state. Hidden state is also the output of the RNN cell at each step, and it is returned as an input for the next step\n\n\nThe processing of an RNN cell can be described in the following steps\n\nPass input (X=[x0, x1]) to input linear layer (linear_input) and get the output (tx=[t0, t1])\nPass the last “hidden state” to the hidden linear layer (linear_hidden), and get the output (th=[h0, h1]). Since at the start we don’t have a hidden state from the last step, we can manually assign hidden state as zeros and pass it to hidden linear layer.\nAdd both outputs tx and th. Let’s call it adding\npass the ‘adding’ to activation function tanh. The output is the new “hidden state” and will be used in the next step.\n\nNow that we have learned how an RNN cell works let’s build it ourselves without relying on PyTorch nn.RNN class. To ensure that our custom RNN cell produces the same output as nn.RNN, we will do the following test\n\nCreate two linear layers that will represent input and hidden layers (linear_input and linear_hidden respectively)\nCreate an nn.RNN cell\nCopy and assign the weights from nn.RNN cell to input and hidden linear layers\nTrain both linear layers and nn.RNN cell on an input data point\nCompare the weight states of both. If they match, then we have successfully replicated the internal functionality of an.RNN cell.\n\n\n##\n# create input and hidden linear layers\ntorch.manual_seed(19)\nn_features = 2\nhidden_dim = 2\n\nlinear_input = nn.Linear(n_features, hidden_dim)\nlinear_hidden = nn.Linear(hidden_dim, hidden_dim)\n\n\n##\n# create nn.RNN cell from PyTorch class\nrnn_cell = nn.RNNCell(input_size=n_features, hidden_size=hidden_dim)\nrnn_state = rnn_cell.state_dict()\nrnn_state\n\nOrderedDict([('weight_ih', tensor([[-0.6701, -0.5811],\n                      [-0.0170, -0.5856]])),\n             ('weight_hh', tensor([[ 0.1159, -0.6978],\n                      [ 0.3241, -0.0983]])),\n             ('bias_ih', tensor([-0.3163, -0.2153])),\n             ('bias_hh', tensor([ 0.0722, -0.3242]))])\n\n\nIn the last two cells, we have created two linear layers for our custom RNN cell and an instance of PyTorch nn.RNN class. In the next step, we will assign a copy of weights from nn.RNN to linear layers. This way both will have the same initial weights.\n\n##\n# assgin weight from nn.RNN to linear layers\nwith torch.no_grad():\n    linear_input.weight = nn.Parameter(rnn_state[\"weight_ih\"])\n    linear_input.bias = nn.Parameter(rnn_state[\"bias_ih\"])\n    linear_hidden.weight = nn.Parameter(rnn_state[\"weight_hh\"])\n    linear_hidden.bias = nn.Parameter(rnn_state[\"bias_hh\"])\n\nLet’s create an input data point X with two dimensions x0 and x1 * X = [x0, x1] * x0 = 1.0349 * x1 = 0.9661\n\nX = torch.as_tensor(np.array([1.0349, 0.9661])).float()\nX\n\ntensor([1.0349, 0.9661])\n\n\nNow let’s follow the steps we have defined for working of an RNN cell.\n\n##\n# 1. Pass input (`X=[x0, x1]`) to input linear layer (linear_input) and get the output (`tx=[t0, t1`])\ntx = linear_input(X)\ntx\n\ntensor([-1.5712, -0.7985], grad_fn=<AddBackward0>)\n\n\n\n##\n# 2. Pass the last \"hidden state\" to the hidden linear layer (linear_hidden), and get the output (`th=[h0, h1`]). \n# Since this is the first step, and we don't have a hidden state from the last step, \n# we can manually assign hidden state as zeros and pass it to hidden linear layer.\ninitial_hidden = torch.zeros(1, hidden_dim)\n\nth = linear_hidden(initial_hidden)\nth\n\ntensor([[ 0.0722, -0.3242]], grad_fn=<AddmmBackward0>)\n\n\n\n##\n# 3. Add both outputs `tx` and `th`. Let's call it `adding`\nt_hx = th + tx\nt_hx\n\ntensor([[-1.4991, -1.1227]], grad_fn=<AddBackward0>)\n\n\n\n##\n# 4. pass the 'adding' to activation function `tanh`. The output is the new \"hidden state\" and will be used for upcoming inputs\nnew_hidden_state = torch.tanh(t_hx)\nnew_hidden_state\n\ntensor([[-0.9050, -0.8085]], grad_fn=<TanhBackward0>)\n\n\nWe have an output from our custom RNN cell. This is the new hidden state that will be passed to the linear_hidden layer in the next step.\nNow time to compare this output with that of nn.RNN to see if they match or not.\n\nrnn_cell(X)\n\ntensor([-0.9050, -0.8085], grad_fn=<SqueezeBackward1>)\n\n\nNotice that the output from both the custom RNN cell and nn.RNN match. This means that we are successful in replicating the internal working of nn.RNN class."
  },
  {
    "objectID": "posts/2022-11-07-timeseries-rnn-gru-lstm-cnn-pytorch.html#gated-recurrent-units-gru",
    "href": "posts/2022-11-07-timeseries-rnn-gru-lstm-cnn-pytorch.html#gated-recurrent-units-gru",
    "title": "Build Temporal Models for Univariate Time series Data with RNN, GRU, LSTM, CNN using PyTorch",
    "section": "Gated Recurrent Units (GRU)",
    "text": "Gated Recurrent Units (GRU)\nIn this section we will apply gated recurrent units on the stock price data, and compare its performance with simple RNNs.\n\n##\n# GRU model configuration\nclass GruModel(nn.Module):\n    def __init__(self, n_features, hidden_dim, n_outputs, n_layers):\n        super(GruModel, self).__init__()\n        self.n_features = n_features\n        self.hidden_dim = hidden_dim\n        self.n_outputs = n_outputs\n        self.n_layers = n_layers\n        self.hidden = None\n        # Simple GRU\n        self.basic_rnn = nn.GRU(\n            self.n_features, self.hidden_dim, self.n_layers, batch_first=True\n        )\n        # Classifier to produce as many logits as outputs\n        self.classifier = nn.Linear(self.hidden_dim, self.n_outputs)\n\n    def forward(self, X):\n        # X is batch first (N, L, F)\n        # output is (N, L, H)\n        # final hidden state is (1, N, H)\n        batch_first_output, self.hidden = self.basic_rnn(X)\n\n        # only last item in sequence (N, 1, H)\n        last_output = batch_first_output[:, -1]\n        # classifier will output (N, 1, n_outputs)\n        out = self.classifier(last_output)\n\n        # final output is (N, n_outputs)\n        return out.view(-1, self.n_outputs)\n\nConfigure model loss and optimizer.\n\ntorch.manual_seed(21)\ngru_model = GruModel(n_features=1, hidden_dim=32, n_outputs=1, n_layers=1)\ngru_loss = nn.MSELoss()\ngru_optimizer = optim.Adam(gru_model.parameters(), lr=0.01)\n\nRun the training pipeline for 100 epochs.\n\ndlp_gru = DeepLearningPipeline(gru_model, gru_loss, gru_optimizer)\ndlp_gru.set_loaders(train_loader, test_loader)\ndlp_gru.train(100, print_loss=True)\n\nEpoch  10 MSE:  0.00022010822761909697\nEpoch  20 MSE:  0.00020518084370864514\nEpoch  30 MSE:  0.00020595710090922446\nEpoch  40 MSE:  0.00020482327377204925\nEpoch  50 MSE:  0.00022252999384143163\nEpoch  60 MSE:  0.0002140117964396874\nEpoch  70 MSE:  0.00023651681564815314\nEpoch  80 MSE:  0.00020522110384208094\nEpoch  90 MSE:  0.0002454350946980853\n\n\nPlot the training and validation loss.\n\nfig = dlp_gru.plot_losses()\n\n\n\n\nMake prediction on test data and calculate the loss.\n\n# make predictions\ny_test_pred_scaled = dlp_gru.predict(x_test_scaled)\ny_train_pred_scaled = dlp_gru.predict(x_train_scaled)\n\n# invert predictions\ny_train_pred = scaler.inverse_transform(y_train_pred_scaled)\ny_train = scaler.inverse_transform(y_train_scaled)\ny_test_pred = scaler.inverse_transform(y_test_pred_scaled)\ny_test = scaler.inverse_transform(y_test_scaled)\n\n# calculate root mean squared error\ntrainScore = math.sqrt(mean_squared_error(y_train[:, 0], y_train_pred[:, 0]))\ntestScore = math.sqrt(mean_squared_error(y_test[:, 0], y_test_pred[:, 0]))\n\nprint(\"Train Score: %.2f RMSE\" % (trainScore))\nprint(\"Test Score: %.2f RMSE\" % (testScore))\n\nTrain Score: 0.48 RMSE\nTest Score: 2.68 RMSE\n\n\nPlot predictions along with actual data.\n\nplot_predictions(y_test, y_test_pred)\n\n\n\n\nGRU seems to be on par with RNN. It has a slightly better training score, but at the same time, it performed somewhat poorly on the validation data.\n\nGRU cell in detail\n Image Source: dvgodoy/PyTorchStepByStep/blob/master/Chapter08\nFrom the above image we can see that GRU cell is more advanced than a simple RNN cell. It has two more weight layers commonly referred as gates\n\nReset gate: This weight layer is used to control how much of the past is needed to neglect or forget. This gate has a control r which is learned during training.\n\nIf we decrease r to 0, then the current state of cell n is less and less influenced by the old hidden state\nIf we increase r all the way to 1, then the current state will have maximum affect of the last hidden state.\n\nUpdate gate: This weight layer is used to control how much of the past information is needed to be passed on to the next step. This gate has a control z which is also learned during training.\n\nIf we decrease z all the way to 0, then the new hidden state h` is closer and closer to current state of the cell. In the figure the current state is n\nIf we increase z all the way to 1, then new hidden state h` is simply a copy of last hidden state h\n\nIf we decrease both r and z to 0, then GRU is simply a linear layer followed by an activation layer.\n\nHow does having two extra learnable weight layers and their controls make GRU better than RNN?\n\nIt is like giving more control to a neural network to decide which information it wants to retain and which to forget as time passes.\nIt may seem like both layers are trying to achieve the same thing: What information to keep or forget? But there is more to it. Suppose that we have a single weight layer as in RNN. For RNN, if the neural network has decided to forget something, then that information is gone. If it is needed in future steps, then the network will have to relearn it. In the case of GRU, the network has the luxury that the information it wants to forget can be parked in a separate layer (forget layer). If, in the future, that information is needed again, then it can simply change the gate control and make that information available.\n\nWhy learning and forgetting are important for recurrent neural networks?\nPatterns change over time for ordered sequence data like stock prices, and we want our networks to be sensitive to such changes. A repeating hidden state helps the network connect the dots between new information it has received and the past it has learned. If the network finds the new information it received has changed from past learning, it will try to unlearn some of the past experiences and learn the new pattern. Different variants of RNN are designed to give more and more such controls to the network and make it efficient in deciding which information to learn or forget."
  },
  {
    "objectID": "posts/2022-11-07-timeseries-rnn-gru-lstm-cnn-pytorch.html#long-short-term-memory-lstm",
    "href": "posts/2022-11-07-timeseries-rnn-gru-lstm-cnn-pytorch.html#long-short-term-memory-lstm",
    "title": "Build Temporal Models for Univariate Time series Data with RNN, GRU, LSTM, CNN using PyTorch",
    "section": "Long Short Term Memory (LSTM)",
    "text": "Long Short Term Memory (LSTM)\nIn this section we will apply lstm on the stock price data, and compare its performance with RNN and GRU.\n\n##\n# LSTM model configuration\nclass LstmModel(nn.Module):\n    def __init__(self, n_features, hidden_dim, n_outputs, n_layers):\n        super(LstmModel, self).__init__()\n        self.hidden_dim = hidden_dim\n        self.n_features = n_features\n        self.n_outputs = n_outputs\n        self.n_layers = n_layers\n\n        self.hidden = None\n        self.cell = None\n        # Simple LSTM\n        self.basic_rnn = nn.LSTM(\n            self.n_features, self.hidden_dim, self.n_layers, batch_first=True\n        )\n        # Classifier to produce as many logits as outputs\n        self.classifier = nn.Linear(self.hidden_dim, self.n_outputs)\n\n    def forward(self, X):\n        # X is batch first (N, L, F)\n        # output is (N, L, H)\n        # final hidden state is (1, N, H)\n        # final cell state is (1, N, H)\n        batch_first_output, (self.hidden, self.cell) = self.basic_rnn(X)\n\n        # only last item in sequence (N, 1, H)\n        last_output = batch_first_output[:, -1]\n        # classifier will output (N, 1, n_outputs)\n        out = self.classifier(last_output)\n\n        # final output is (N, n_outputs)\n        return out.view(-1, self.n_outputs)\n\nDefine model loss and optimizer.\n\ntorch.manual_seed(21)\nlstm_model = LstmModel(n_features=1, hidden_dim=32, n_outputs=1, n_layers=1)\nlstm_loss = nn.MSELoss()\nlstm_optimizer = optim.Adam(lstm_model.parameters(), lr=0.01)\n\nRun the training pipeline.\n\ndlp_lstm = DeepLearningPipeline(lstm_model, lstm_loss, lstm_optimizer)\ndlp_lstm.set_loaders(train_loader, test_loader)\ndlp_lstm.train(100, print_loss=True)\n\nEpoch  10 MSE:  0.0003263879698351957\nEpoch  20 MSE:  0.000262940919569563\nEpoch  30 MSE:  0.0002264043755668822\nEpoch  40 MSE:  0.000254558740076997\nEpoch  50 MSE:  0.0002543745165784265\nEpoch  60 MSE:  0.00028126772259852396\nEpoch  70 MSE:  0.00025442599127762315\nEpoch  80 MSE:  0.00020528354511814982\nEpoch  90 MSE:  0.00022827486629301512\n\n\nPrint the training and validation loss.\n\nfig = dlp_lstm.plot_losses()\n\n\n\n\nMake predictions on the test data and calculate the error.\n\n# make predictions\ny_test_pred_scaled = dlp_lstm.predict(x_test_scaled)\ny_train_pred_scaled = dlp_lstm.predict(x_train_scaled)\n\n# invert predictions\ny_train_pred = scaler.inverse_transform(y_train_pred_scaled)\ny_train = scaler.inverse_transform(y_train_scaled)\ny_test_pred = scaler.inverse_transform(y_test_pred_scaled)\ny_test = scaler.inverse_transform(y_test_scaled)\n\n# calculate root mean squared error\ntrainScore = math.sqrt(mean_squared_error(y_train[:, 0], y_train_pred[:, 0]))\ntestScore = math.sqrt(mean_squared_error(y_test[:, 0], y_test_pred[:, 0]))\n\nprint(\"Train Score: %.2f RMSE\" % (trainScore))\nprint(\"Test Score: %.2f RMSE\" % (testScore))\n\nTrain Score: 0.53 RMSE\nTest Score: 0.93 RMSE\n\n\nPlot predicted values along with true values.\n\nplot_predictions(y_test, y_test_pred)\n\n\n\n\nLSTM has performed much better on the test data compared to both RNN and GRU. But it has also taken more time to get trained.\n\nLSTM cell in detail\n Image Source: dvgodoy/PyTorchStepByStep/blob/master/Chapter08\nThe above image shows that LSTM network has more learnable parameters and controls compared to RNN and GRU. There is\n\nForget gate: Similar to the reset gate in GRU, it controls which information needs attention and which can be ignored.\nInput gate and Cell state: LSTM is unique in that besides the hidden state, it also maintains a separate state called cell state. Cell state acts as a long-term memory, while the hidden state acts like a working or short-term memory. Input gate controls how to update the cell state based on past hidden state, past cell state, and new input.\nUpdate gate: Update gate controls how to update the hidden state to generate a new hidden state value. It gets influenced by past hidden states and new input. Cell state does not affect this gate."
  },
  {
    "objectID": "posts/2022-11-07-timeseries-rnn-gru-lstm-cnn-pytorch.html#one-dimensional-convolutional-neural-network-1d-convnet",
    "href": "posts/2022-11-07-timeseries-rnn-gru-lstm-cnn-pytorch.html#one-dimensional-convolutional-neural-network-1d-convnet",
    "title": "Build Temporal Models for Univariate Time series Data with RNN, GRU, LSTM, CNN using PyTorch",
    "section": "One Dimensional Convolutional Neural Network (1D ConvNet)",
    "text": "One Dimensional Convolutional Neural Network (1D ConvNet)\nIn this section we will take an alternate approach and apply a type of CNN on stock price data.\n\n##\n# 1D CNN model configuration\nclass CNNmodel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.c1 = nn.Conv1d(1, 32, 2)\n        self.p1 = nn.AvgPool1d(2)\n        self.c2 = nn.Conv1d(32, 64, 1)\n        self.p2 = nn.AvgPool1d(2)\n        self.tanh = nn.Tanh()\n        self.fc1 = nn.Linear(448, 64)\n        self.fc2 = nn.Linear(64, 1)\n\n    def forward(self, x):\n        \"\"\"\n        x1:  torch.Size([32, 32, 28])\n        x2:  torch.Size([32, 32, 14])\n        x3:  torch.Size([32, 64, 14])\n        x4:  torch.Size([32, 64, 7])\n        x5:  torch.Size([32, 448])\n        x6:  torch.Size([32, 64])\n        x7:  torch.Size([32, 1])\n        \"\"\"\n\n        x1 = self.c1(x)\n        x2 = self.p1(x1)\n        x3 = self.c2(x2)\n        x4 = self.p2(x3)\n\n        x4 = self.tanh(x4)\n        x5 = x4.reshape(x4.shape[0], -1)\n\n        x6 = self.fc1(x5)\n        x7 = self.fc2(x6)\n\n        return x7\n\n\n##\n# configure model loss and optimizer\nmodel = CNNmodel()\nloss = nn.MSELoss()\noptimizer = optim.Adam(model.parameters(), lr=0.01)\n\n\n##\n# change the dimension of dataset\n# for CNN use: N (batch_size), F (n_features), L (seq len)\ntrain_data_1d = TensorDataset(\n    torch.as_tensor(x_train_scaled).float().permute(0, 2, 1),\n    torch.as_tensor(y_train_scaled).float(),\n)\ntest_data_1d = TensorDataset(\n    torch.as_tensor(x_test_scaled).float().permute(0, 2, 1),\n    torch.as_tensor(y_test_scaled).float(),\n)\n\ntrain_loader = DataLoader(train_data_1d, batch_size=32, shuffle=True)\ntest_loader = DataLoader(test_data_1d, batch_size=32)\n\n\n##\n# check the dimensions of one batch\ntemp = next(iter(train_loader))\nlen(temp[0]), len(temp[0][0]), len(temp[0][0][0]) # N (batch_size), F (n_features), L (seq len)\n\n(32, 1, 29)\n\n\n\n##\n# run the training pipeline\ndlp_conv1d = DeepLearningPipeline(model, loss, optimizer)\ndlp_conv1d.set_loaders(train_loader, test_loader)\ndlp_conv1d.train(100, print_loss=True)\n\nEpoch  10 MSE:  0.000601009127973212\nEpoch  20 MSE:  0.0005247063511915533\nEpoch  30 MSE:  0.0004160549495171643\nEpoch  40 MSE:  0.00038349507733735004\nEpoch  50 MSE:  0.0005176076665520668\nEpoch  60 MSE:  0.00043436023538974536\nEpoch  70 MSE:  0.0004034905033415381\nEpoch  80 MSE:  0.00036779195050423203\nEpoch  90 MSE:  0.00027141175329840433\n\n\n\n##\n# plot the training and validation loss\nfig = dlp_conv1d.plot_losses()\n\n\n\n\n\n##\n# make predictions\ny_test_pred_scaled = dlp_conv1d.predict(\n    torch.as_tensor(x_test_scaled).float().permute(0, 2, 1)\n)\ny_train_pred_scaled = dlp_conv1d.predict(\n    torch.as_tensor(x_train_scaled).float().permute(0, 2, 1)\n)\n\n# invert predictions\ny_train_pred = scaler.inverse_transform(y_train_pred_scaled)\ny_train = scaler.inverse_transform(y_train_scaled)\ny_test_pred = scaler.inverse_transform(y_test_pred_scaled)\ny_test = scaler.inverse_transform(y_test_scaled)\n\n# calculate root mean squared error\ntrainScore = math.sqrt(mean_squared_error(y_train[:, 0], y_train_pred[:, 0]))\ntestScore = math.sqrt(mean_squared_error(y_test[:, 0], y_test_pred[:, 0]))\n\nprint(\"Train Score: %.2f RMSE\" % (trainScore))\nprint(\"Test Score: %.2f RMSE\" % (testScore))\n\nTrain Score: 0.61 RMSE\nTest Score: 2.07 RMSE\n\n\n\n##\n# plot true and predicted values\nplot_predictions(y_test, y_test_pred)\n\n\n\n\n1D ConvNet results stand between GRU and LSTM. It is performing better than RNN and GRU but less than LSTM.\nHow does 1D ConvNet compare to RNN?\n1D ConvNets have 1D convolutions, i.e. they move the filter in one dimension from left to right like a moving window. These kernels (or filters) behave similarly to the hidden state in RNN\n\nIn RNN, we process one data point at a step and move forward, incrementing the steps till we reach the end of the sequence\nIn 1D-ConvNet, we process one sequence at a time and move the filter along the entire length of the sequence"
  },
  {
    "objectID": "posts/22022-11-19-pytorch-lstm-text-generation.html#credits",
    "href": "posts/22022-11-19-pytorch-lstm-text-generation.html#credits",
    "title": "Generating Text with Recurrent Neural Networks in PyTorch",
    "section": "Credits",
    "text": "Credits\nThis notebook takes inspiration and ideas from the following sources.\n\n“Machine learning with PyTorch and Scikit-Learn” by “Sebastian Raschka, Yuxi (Hayden) Liu, and Vahid Mirjalili”. You can get the book from its website: Machine learning with PyTorch and Scikit-Learn. In addition, the GitHub repository for this book has valuable notebooks: github.com/rasbt/machine-learning-book. Parts of the code you see in this notebook are taken from chapter 15 notebook of the same book.\n“Intro to Deep Learning and Generative Models Course” lecture series from “Sebastian Raschka”. Course website: stat453-ss2021. YouTube Link: Intro to Deep Learning and Generative Models Course. Lectures that are related to this post are L15.5 Long Short-Term Memory and L15.7 An RNN Sentiment Classifier in PyTorch\n“Andrej Karpathy” blog post The Unreasonable Effectiveness of Recurrent Neural Networks."
  },
  {
    "objectID": "posts/22022-11-19-pytorch-lstm-text-generation.html#environment",
    "href": "posts/22022-11-19-pytorch-lstm-text-generation.html#environment",
    "title": "Generating Text with Recurrent Neural Networks in PyTorch",
    "section": "Environment",
    "text": "Environment\nThis notebook GitHub link here is prepared with Google Colab.\n\n\nCode\nfrom platform import python_version\nimport numpy, matplotlib, pandas, torch\n\nprint(\"python==\" + python_version())\nprint(\"numpy==\" + numpy.__version__)\nprint(\"torch==\" + torch.__version__)\nprint(\"matplotlib==\" + matplotlib.__version__)\n\n\npython==3.7.15\nnumpy==1.21.6\ntorch==1.12.1+cu113\nmatplotlib==3.2.2"
  },
  {
    "objectID": "posts/22022-11-19-pytorch-lstm-text-generation.html#introduction",
    "href": "posts/22022-11-19-pytorch-lstm-text-generation.html#introduction",
    "title": "Generating Text with Recurrent Neural Networks in PyTorch",
    "section": "Introduction",
    "text": "Introduction\nRecurrent Neural Network (RNN) works well for sequence problems, i.e., predicting the next sequence item. Stock prices, for example, are a type of sequence data more commonly known as time-series data. A similar notion can be applied to the NLP domain to build a character-level language model. Here language textual data becomes the sequence data, and from our model, we try to predict the next character in the input text. For training, the input text is broken into a sequence of characters and fed to the model one character at a time. The network will process the new character in relation to previously seen characters and use this information to predict the next alphabet."
  },
  {
    "objectID": "posts/22022-11-19-pytorch-lstm-text-generation.html#data-preparation",
    "href": "posts/22022-11-19-pytorch-lstm-text-generation.html#data-preparation",
    "title": "Generating Text with Recurrent Neural Networks in PyTorch",
    "section": "Data Preparation",
    "text": "Data Preparation\n\nDownload data\nFor input text, we will use a famous English folk story (though any other text will work equally well) with the name Cinderella. To download the story text, you may use Project Gutenberg site or Archive.org.\n\ndownload_link = \"https://ia600204.us.archive.org/30/items/cinderella10830gut/10830.txt\"\n\n## alternate download link\n# download_link = \"https://www.gutenberg.org/cache/epub/10830/pg10830.txt\"\n\nfile_name = 'input.txt'\n\n\n##\n# download the story text and save it as {file_name}\n! curl {download_link} -o {file_name}\n\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n100 45278  100 45278    0     0  38865      0  0:00:01  0:00:01 --:--:-- 38831\n\n\nThe download is complete. We can now open the file and read its contents.\n\n##\n# Reading and processing text\nwith open(file_name, \"r\", encoding=\"utf8\") as fp:\n    text = fp.read()\n\n\n\nPreprocess data\nThe downloaded text has been published as a volunteer effort under Project Gutenberg. They have added some project and license information after the original story text as part of the project requirements. We are not interested in that text (boilerplate text), so let’s omit that and limit our input text to the folk story.\n\n##\n# truncate text till story start and end\nstart_indx = text.find(\n    \"There once lived a gentleman and his wife, who were the parents of a\\nlovely little daughter.\"\n)\nend_indx = text.find(\"*       *       *       *       *\")\n\ntext = text[start_indx:end_indx]\n\n# total length of the text\nprint(\"Total Length (character count):\", len(text))\n\nTotal Length (character count): 21831\n\n\n\n\nHow does the data look?\nLet’s view the first 500 characters from the story text.\n\n# view the text start\ntext[:500]\n\n'There once lived a gentleman and his wife, who were the parents of a\\nlovely little daughter.\\n\\nWhen this child was only nine years of age, her mother fell sick.\\nFinding her death coming on, she called her child to her and said to\\nher, \"My child, always be good; bear every thing that happens to you\\nwith patience, and whatever evil and troubles you may suffer, you will\\nbe happy in the end if you are so.\" Then the poor lady died, and her\\ndaughter was full of great grief at the loss of a mother so go'\n\n\nAnd the last 500 characters.\n\n# view the text end\ntext[-500:]\n\n'their affection.\\nShe was then taken to the palace of the young prince, in whose eyes she\\nappeared yet more lovely than before, and who married her shortly after.\\n\\nCinderella, who was as good as she was beautiful, allowed her sisters to\\nlodge in the palace, and gave them in marriage, that same day, to two\\nlords belonging to the court.\\n\\n[Illustration: MARRIAGE OF THE PRINCE AND CINDERELLA.]\\n\\nThe amiable qualities of Cinderella were as conspicuous after as they\\nhad been before marriage.\\n\\n\\n\\n\\n       '\n\n\n\n\nPreparing data dictionary\nOur data is a string and can’t be used to train a model. So instead, we have to convert it into integers. For this encoding, we will use a simple methodology where each unique character in the text is assigned an integer and then replaced with all occurrences of that character in the text with that integer value.\nFor this, let’s first create a set of all the unique characters in the text.\n\nimport numpy as np\n\n# find unique chars from text\nchar_set = set(text)\nprint(\"Unique Characters:\", len(char_set))\n\n# sort char set\nchars_sorted = sorted(char_set)\nprint(chars_sorted)\n\nUnique Characters: 65\n['\\n', ' ', '!', '\"', \"'\", ',', '-', '.', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'Y', 'Z', '[', ']', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n\n\nWe now know all the unique characters in our input text. Accordingly, we can create a dictionary and assign each character in char_set a unique integer.\n\n# encode chars\nchar2int = {ch: i for i, ch in enumerate(chars_sorted)}\n\n# `char2int` dictionary for char -> int\nprint(char2int)\n\n{'\\n': 0, ' ': 1, '!': 2, '\"': 3, \"'\": 4, ',': 5, '-': 6, '.': 7, ':': 8, ';': 9, '?': 10, 'A': 11, 'B': 12, 'C': 13, 'D': 14, 'E': 15, 'F': 16, 'G': 17, 'H': 18, 'I': 19, 'J': 20, 'K': 21, 'L': 22, 'M': 23, 'N': 24, 'O': 25, 'P': 26, 'Q': 27, 'R': 28, 'S': 29, 'T': 30, 'U': 31, 'V': 32, 'W': 33, 'Y': 34, 'Z': 35, '[': 36, ']': 37, '_': 38, 'a': 39, 'b': 40, 'c': 41, 'd': 42, 'e': 43, 'f': 44, 'g': 45, 'h': 46, 'i': 47, 'j': 48, 'k': 49, 'l': 50, 'm': 51, 'n': 52, 'o': 53, 'p': 54, 'q': 55, 'r': 56, 's': 57, 't': 58, 'u': 59, 'v': 60, 'w': 61, 'x': 62, 'y': 63, 'z': 64}\n\n\nBut more than just the encoding, we also need a way to convert the encoded characters back to the original form. For this, we will use a separate array that will hold the index of each char in the dictionary. Together with char2int and int2char we can move back and forth between encoded and decoded characters.\n\nint2char = np.array(chars_sorted)\n\n# `int2char` for int -> char\nprint(int2char)\n\n['\\n' ' ' '!' '\"' \"'\" ',' '-' '.' ':' ';' '?' 'A' 'B' 'C' 'D' 'E' 'F' 'G'\n 'H' 'I' 'J' 'K' 'L' 'M' 'N' 'O' 'P' 'Q' 'R' 'S' 'T' 'U' 'V' 'W' 'Y' 'Z'\n '[' ']' '_' 'a' 'b' 'c' 'd' 'e' 'f' 'g' 'h' 'i' 'j' 'k' 'l' 'm' 'n' 'o'\n 'p' 'q' 'r' 's' 't' 'u' 'v' 'w' 'x' 'y' 'z']\n\n\n\n\nEncode input text\nIn this step, we will use the char2int dictionary to encode our story text. The encoded version of text is called text_encoded.\n\n##\n# encode original text\ntext_encoded = np.array([char2int[ch] for ch in text], dtype=np.int32)\n\nprint(\"Text encoded shape: \", text_encoded.shape)\n\nText encoded shape:  (21831,)\n\n\nLet’s use int2char to decode and return the original text.\n\n##\n# decoding original text\nfor ex in text_encoded[:5]:\n    print(\"{} -> {}\".format(ex, int2char[ex]))\n\n30 -> T\n46 -> h\n43 -> e\n56 -> r\n43 -> e\n\n\nAnother example of encoding and decoding. This time I used multiple words together.\n\nprint(text[:18], \"     == Encoding ==> \", text_encoded[:18])\nprint(text_encoded[19:41], \" == Reverse  ==> \", \"\".join(int2char[text_encoded[19:41]]))\n\nThere once lived a      == Encoding ==>  [30 46 43 56 43  1 53 52 41 43  1 50 47 60 43 42  1 39]\n[45 43 52 58 50 43 51 39 52  1 39 52 42  1 46 47 57  1 61 47 44 43]  == Reverse  ==>  gentleman and his wife\n\n\n\n\nPrepare data sequences\nWe have our encoded data ready. Next, we will convert it into sequences of fixed length. The last sequence element will act as a target, and the remaining elements will be the input. For sequencing, we will use length 41.\n\nThe first 40 characters in sequence form the input\nThe last character in sequence (41) represents the output\n\n\n##\n# make sequences of encoded text as `text_chunks`\nseq_length = 40\nchunk_size = seq_length + 1\n\ntext_chunks = [\n    text_encoded[i : i + chunk_size] for i in range(len(text_encoded) - chunk_size + 1)\n]\n\n\n##\n# inspect the first chuck\nfor seq in text_chunks[:1]:\n    input_seq = seq[:-1]\n    target = seq[-1]\n\n    print(input_seq, \" -> \", target)\n    print(repr(\"\".join(int2char[input_seq])), \" -> \", repr(\"\".join(int2char[target])))\n\n[30 46 43 56 43  1 53 52 41 43  1 50 47 60 43 42  1 39  1 45 43 52 58 50\n 43 51 39 52  1 39 52 42  1 46 47 57  1 61 47 44]  ->  43\n'There once lived a gentleman and his wif'  ->  'e'\n\n\n\n##\n# inspect the second chuck\nfor seq in text_chunks[1:2]:\n    input_seq = seq[:-1]\n    target = seq[-1]\n\n    print(input_seq, \" -> \", target)\n    print(repr(\"\".join(int2char[input_seq])), \" -> \", repr(\"\".join(int2char[target])))\n\n[46 43 56 43  1 53 52 41 43  1 50 47 60 43 42  1 39  1 45 43 52 58 50 43\n 51 39 52  1 39 52 42  1 46 47 57  1 61 47 44 43]  ->  5\n'here once lived a gentleman and his wife'  ->  ','"
  },
  {
    "objectID": "posts/22022-11-19-pytorch-lstm-text-generation.html#load-data-into-dataset-and-dataloader-class",
    "href": "posts/22022-11-19-pytorch-lstm-text-generation.html#load-data-into-dataset-and-dataloader-class",
    "title": "Generating Text with Recurrent Neural Networks in PyTorch",
    "section": "Load Data into Dataset and DataLoader class",
    "text": "Load Data into Dataset and DataLoader class\nIn this section, we will load our encoded data sequences into Dataset and DataLoader class to prepare batches for model training.\n\nLoad data into Dataset class\nclass TextDataset is derived from PyTorch Dataset. When we get a sequence using this class, it will return the sequence as a tuple of input and target.\n\n\nCode\nimport torch\nfrom torch.utils.data import Dataset\n\n\nclass TextDataset(Dataset):\n    def __init__(self, text_chunks):\n        self.text_chunks = text_chunks\n\n    def __len__(self):\n        return len(self.text_chunks)\n\n    def __getitem__(self, idx):\n        text_chunk = self.text_chunks[idx]\n        return text_chunk[:-1].long(), text_chunk[1:].long()  # return input, target\n\n\nseq_dataset = TextDataset(torch.tensor(text_chunks))\n\n\n/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:15: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:201.)\n  from ipykernel import kernelapp as app\n\n\nEach element from the seq_dataset consists of\n\ninput data that we will feed to the model for training\ntarget data that we will use to compare the model output\n\nRemember that both input and target sequences are derived from the same encoded text. We train our model to predict the next character from the given input. One character is given as an input to the model, and one character output comes out of the model. In an ideal case, the model output character should represent the next character in a sequence. And our target sequence is just that: one next character from the input sequence.\n\nfor i, (seq, target) in enumerate(seq_dataset):\n    print(\" Input (x):\", repr(\"\".join(int2char[seq])))\n    print(\"Target (y):\", repr(\"\".join(int2char[target])))\n    print()\n    if i == 1:\n        break\n\n Input (x): 'There once lived a gentleman and his wif'\nTarget (y): 'here once lived a gentleman and his wife'\n\n Input (x): 'here once lived a gentleman and his wife'\nTarget (y): 'ere once lived a gentleman and his wife,'\n\n\n\n\n\nLoad data into DataLoader class to prepare batches\nIn this step, we have prepared training batches using the PyTorch DataLoader class.\n\nfrom torch.utils.data import DataLoader\n\nbatch_size = 64\n\ntorch.manual_seed(1)\nseq_dl = DataLoader(seq_dataset, batch_size=batch_size, shuffle=True, drop_last=True)"
  },
  {
    "objectID": "posts/22022-11-19-pytorch-lstm-text-generation.html#model-configuration-and-training",
    "href": "posts/22022-11-19-pytorch-lstm-text-generation.html#model-configuration-and-training",
    "title": "Generating Text with Recurrent Neural Networks in PyTorch",
    "section": "Model Configuration and Training",
    "text": "Model Configuration and Training\nIn this section, we will configure a model for character-level language modeling. This model will have an Embedding layer at the start. Next, output from the embedding layer will be passed to the LSTM layer. Finally, at the output, we have a fully connected linear layer.\nFor an in-depth analysis of the working of an Embedding layer, I recommend this article Embeddings in Machine Learning: Everything You Need to Know\n\n\nCode\nimport torch.nn as nn\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\nclass RNN(nn.Module):\n    def __init__(self, vocab_size, embed_dim, rnn_hidden_size):\n        super().__init__()\n        self.embedding = nn.Embedding(vocab_size, embed_dim)\n        self.rnn_hidden_size = rnn_hidden_size\n        self.rnn = nn.LSTM(embed_dim, rnn_hidden_size, batch_first=True)\n        self.fc = nn.Linear(rnn_hidden_size, vocab_size)\n\n    def forward(self, x, hidden, cell):\n        out = self.embedding(x).unsqueeze(1)\n        out, (hidden, cell) = self.rnn(out, (hidden, cell))\n        out = self.fc(out).reshape(out.size(0), -1)\n        return out, hidden, cell\n\n    def init_hidden(self, batch_size):\n        hidden = torch.zeros(1, batch_size, self.rnn_hidden_size)\n        cell = torch.zeros(1, batch_size, self.rnn_hidden_size)\n        return hidden.to(device), cell.to(device)\n\n\n\ntorch.manual_seed(1)\n\n# define model dimensions\nvocab_size = len(int2char)\nembed_dim = 256\nrnn_hidden_size = 512\n\n# initialize model\nmodel = RNN(vocab_size, embed_dim, rnn_hidden_size)\nmodel = model.to(device)\nmodel\n\nRNN(\n  (embedding): Embedding(65, 256)\n  (rnn): LSTM(256, 512, batch_first=True)\n  (fc): Linear(in_features=512, out_features=65, bias=True)\n)\n\n\n\nConfigure loss function and optimizer\n\nFor the loss function, we will use CrossEntropyLoss. This is because we are dealing with a classification problem, and our model has to predict the next character from vocab_size of 65 classes.\nFor optimization, we will use torch.optim.Adam\n\n\nloss_fn = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.005)\n\n\n\nModel training\nAll parts are ready so let’s start the training. Google Colab “CPU” runtime can take significantly longer to train. I would suggest using “GPU” runtime instead.\n\n\nCode\n# for execution time measurement\nfrom timeit import default_timer as timer\n\nnum_epochs = 10000\nmodel.train()\n\nstart = timer()  # timer start\nfor epoch in range(num_epochs):\n    hidden, cell = model.init_hidden(batch_size)\n\n    seq_batch, target_batch = next(iter(seq_dl))\n    seq_batch = seq_batch.to(device)\n    target_batch = target_batch.to(device)\n\n    optimizer.zero_grad()\n    loss = 0\n\n    for c in range(seq_length):\n        pred, hidden, cell = model(seq_batch[:, c], hidden, cell)\n        loss += loss_fn(pred, target_batch[:, c])\n\n    loss.backward()\n    optimizer.step()\n\n    loss = loss.item() / seq_length\n    if epoch % 500 == 0:\n        print(f\"Epoch {epoch} loss: {loss:.4f}\")\n\nend = timer()  # timer end\nprint(\"Total execution time in seconds: \", \"%.2f\" % (end - start))\nprint(\"Device type: \", device)\n\n\nEpoch 0 loss: 2.6252\nEpoch 500 loss: 0.3377\nEpoch 1000 loss: 0.2502\nEpoch 1500 loss: 0.2403\nEpoch 2000 loss: 0.2501\nEpoch 2500 loss: 0.2374\nEpoch 3000 loss: 0.2368\nEpoch 3500 loss: 0.2499\nEpoch 4000 loss: 0.2643\nEpoch 4500 loss: 0.2555\nEpoch 5000 loss: 0.3854\nEpoch 5500 loss: 0.2326\nEpoch 6000 loss: 0.2390\nEpoch 6500 loss: 0.2270\nEpoch 7000 loss: 0.2663\nEpoch 7500 loss: 0.3403\nEpoch 8000 loss: 0.2475\nEpoch 8500 loss: 0.2370\nEpoch 9000 loss: 0.2126\nEpoch 9500 loss: 0.2308\nTotal execution time in seconds:  378.14\nDevice type:  cuda"
  },
  {
    "objectID": "posts/22022-11-19-pytorch-lstm-text-generation.html#process-output-from-the-model",
    "href": "posts/22022-11-19-pytorch-lstm-text-generation.html#process-output-from-the-model",
    "title": "Generating Text with Recurrent Neural Networks in PyTorch",
    "section": "Process output from the model",
    "text": "Process output from the model\nGetting a prediction (text generation) from the model takes some extra work. Since the model is trained on encoded text, the output generated from the model is also encoded. Further, any input used for prediction itself needs to be encoded using the same encoding dictionary model it is trained with. For this, we have defined a helper function.\n\nThis function will take the input text and encode it before passing it to the model\nIt will take the output from the model and decode it before returning\nNote that LSTM model output has logits, hidden state, and cell state . Logits give us the next predicted character. Hidden state and cell state are for keeping the context (or memory) of characters processed so far and are supplied to the model for the next prediction.\nFor the output logits, we can predict the next character using the index of the highest logit value. This will make our model predict the exact text on the same input each time. To introduce some randomness, we take help from PyTorch class torch.distributions.categorical.Categorical. This is how it works\n\nWe obtain output probabilities by applying softmax to logits and pass them to a Categorical object to create a distribution.\nGenerate a sample from a Categorical object. Samples generated from the same distribution may be different. This way, we get different outputs with the same input text.\nThis way, we can also control the predictability of the model output by controlling the probability distribution (calculated from logits) passed to the Categorical object. If we can make probabilities a lot more similar (through scaling), the sample generated by Categorical will also be mostly the same. On the other hand, if we can make the probabilities further apart, then we can also increase the randomness of the output from the Categorical class.\n\n\n\n\nCode\nfrom torch.distributions.categorical import Categorical\n\ndef sample(model, starting_str, len_generated_text=500, scale_factor=1.0):\n\n    encoded_input = torch.tensor([char2int[s] for s in starting_str])\n    encoded_input = torch.reshape(encoded_input, (1, -1))\n\n    generated_str = starting_str\n\n    model.eval()\n    hidden, cell = model.init_hidden(1)\n    hidden = hidden.to(\"cpu\")\n    cell = cell.to(\"cpu\")\n    for c in range(len(starting_str) - 1):\n        _, hidden, cell = model(encoded_input[:, c].view(1), hidden, cell)\n\n    last_char = encoded_input[:, -1]\n    for i in range(len_generated_text):\n        logits, hidden, cell = model(last_char.view(1), hidden, cell)\n        logits = torch.squeeze(logits, 0)\n        scaled_logits = logits * scale_factor\n        m = Categorical(logits=scaled_logits)\n        last_char = m.sample()\n        generated_str += str(int2char[last_char])\n\n    return generated_str"
  },
  {
    "objectID": "posts/22022-11-19-pytorch-lstm-text-generation.html#generating-new-text-passages",
    "href": "posts/22022-11-19-pytorch-lstm-text-generation.html#generating-new-text-passages",
    "title": "Generating Text with Recurrent Neural Networks in PyTorch",
    "section": "Generating new text passages",
    "text": "Generating new text passages\nWe are processing text and model output on the ‘CPU’ device in the ‘sample’ function. So let’s also move the model to the same device.\n\n##\n# move model to cpu\nmodel.to('cpu')\n\nRNN(\n  (embedding): Embedding(65, 256)\n  (rnn): LSTM(256, 512, batch_first=True)\n  (fc): Linear(in_features=512, out_features=65, bias=True)\n)\n\n\nBefore generating some lengthy text, let’s experiment with simple words and see if our model can complete them.\nAt first, I used the string “fat” and asked the model to generate the following three characters to complete this word. But at the same time, I have passed a tiny scaling factor meaning I have decreased the model’s predictability.\n\nprint(sample(model, starting_str=\"fat\", len_generated_text=3, scale_factor=0.1))\n\nfat, i\n\n\nNext, I asked the model to use the same input and predict the following three characters, but I increased the model’s predictability ten times. So let’s see the output this time.\n\nprint(sample(model, starting_str='fat', len_generated_text=3, scale_factor=1.0))\n\nfather\n\n\nThe second time model generated the correct word “father” it had seen before in the training text. So let’s now generate some lengthy texts.\n\n##\n# text generation example 1\nprint(sample(model, starting_str=\"The father\"))\n\nThe father too was she was one of those good faeries who protect children. Her\nspirits revived, and she wiped away her tears.\n\nThe faery took Cinderella by the hand, and old woman, assuming her character of Queen of the\nFaeries, that only jumped up behind the\ncarriage as nimbly as if they had been footmen and laced so tight, touched Cinderella's clothes with her wand, and said, \"Now, my dear good child,\" said the faery, \"here you have a coach and\nhorses, much handsomer than your sisters', to say the least\n\n\n\n##\n# text generation example 2\nprint(sample(model, starting_str=\"The mother\"))\n\nThe mother so good crust. But\nif you like to give the household. It was she who washed the dishes, and\nscrubbed down the step-sisters were very cruel to Cinderella,\nthat he did not eat one morsel of the supper.\n\nCinderella drew the fellow slipper\nout of her godmother\nwould do with it. Her godmother took the pumpkin, and scooped out the\ninside of it, leaving nothing but rind; she then struck it with her\ngodmother then said, \"My dear Cinderella,\nthat he did not eat one morsel of the supper.\n\nCinderella drew\n\n\n\n##\n# text generation example 3\nprint(sample(model, starting_str=\"The three sisters\"))\n\nThe three sisters were very cruel to Cinderella,\nthat he delicacies which she had\nreceived from the prince:  but they did not eat one morsel for a\ncouple of days. They spent their whole time before a looking-glass, and\nthey would be laced so tight, tossing her head disdainfully, \"that I\nshould lend my clothes to a dirty Cinderella like you!\"\n\nCinderella quite amazed; but their\nastonishment at her dancing was still greater.\n\nGracefulness seemed to play in the attempt.\n\nThe long-wished-for evening came at last, an\n\n\n\n##\n# text generation example 4\nprint(sample(model, starting_str=\"The lovely prince\"))\n\nThe lovely prince\nimmediately jumped up behind the\ncarriage as nimbly as conspicuous after as they\nhad been before mocking me,\" replied the poor girl to do all the\ndrudgery of the household. It was she who washed the dishes, and\nscrubbed down the stairs, who tried with all their might to force their unwould stration: CINDERELLA IS PRESENTED BY THE PRINCE TO THE KING AND\nQUEEN, WHO WELCOME HER WITH THE HONORS DUE TO A GREAT PRINCESS, AND IS\nTHEN LED INTO THE ROYAL BY THE HER WITH THE HONORS DUE TO A GREAT PRINCES"
  },
  {
    "objectID": "posts/2022-11-09-pytorch-lstm-imdb-sentiment-prediction.html#credits",
    "href": "posts/2022-11-09-pytorch-lstm-imdb-sentiment-prediction.html#credits",
    "title": "Predicting the Sentiment of IMDB Movie Reviews using LSTM in PyTorch",
    "section": "Credits",
    "text": "Credits\nThis notebook takes inspiration and ideas from the following sources.\n\n“Machine learning with PyTorch and Scikit-Learn” by “Sebastian Raschka, Yuxi (Hayden) Liu, and Vahid Mirjalili”. You can get the book from its website: Machine learning with PyTorch and Scikit-Learn. In addition, the GitHub repository for this book has valuable notebooks: github.com/rasbt/machine-learning-book. Parts of the code you see in this notebook are taken from chapter 15 notebook of the same book.\n“Intro to Deep Learning and Generative Models Course” lecture series from “Sebastian Raschka”. Course website: stat453-ss2021. YouTube Link: Intro to Deep Learning and Generative Models Course. Lectures that are related to this post are L15.5 Long Short-Term Memory and L15.7 An RNN Sentiment Classifier in PyTorch"
  },
  {
    "objectID": "posts/2022-11-09-pytorch-lstm-imdb-sentiment-prediction.html#environment",
    "href": "posts/2022-11-09-pytorch-lstm-imdb-sentiment-prediction.html#environment",
    "title": "Predicting the Sentiment of IMDB Movie Reviews using LSTM in PyTorch",
    "section": "Environment",
    "text": "Environment\nThis notebook is prepared with Google Colab. For “runtime type” choose hardware accelerator as “GPU”. It will take a long time to complete the training without any GPU.\nThis notebook also depends on the PyTorch library TorchText. We will use this library to fetch IMDB review data. While using the torchtext latest version, I found more dependencies on other libraries like torchdata. Even after resolving them, it threw strange encoding errors while fetching IMDB data. So I have downgraded this library till the version I found working without external dependencies. Consequently, torch is also downgraded to a compatible version, but I did not find any issue while working with a lower version of PyTorch for this notebook. It is preferred to restart the runtime after the library installation is complete.\n\n#collapse-output\n! pip install torchtext==0.11.0\n\nLooking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\nRequirement already satisfied: torchtext==0.11.0 in /usr/local/lib/python3.7/dist-packages (0.11.0)\nRequirement already satisfied: torch==1.10.0 in /usr/local/lib/python3.7/dist-packages (from torchtext==0.11.0) (1.10.0)\nRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchtext==0.11.0) (1.21.6)\nRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchtext==0.11.0) (2.23.0)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torchtext==0.11.0) (4.64.1)\nRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.10.0->torchtext==0.11.0) (4.1.1)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.11.0) (2022.9.24)\nRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.11.0) (1.24.3)\nRequirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.11.0) (2.10)\nRequirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.11.0) (3.0.4)\n\n\n\n\nCode\nfrom platform import python_version\nimport numpy, matplotlib, pandas, torch, torchtext\n\nprint(\"python==\" + python_version())\nprint(\"numpy==\" + numpy.__version__)\nprint(\"torch==\" + torch.__version__)\nprint(\"torchtext==\" + torchtext.__version__)\nprint(\"matplotlib==\" + matplotlib.__version__)\n\n\npython==3.7.15\nnumpy==1.21.6\ntorch==1.10.0+cu102\ntorchtext==0.11.0\nmatplotlib==3.2.2"
  },
  {
    "objectID": "posts/2022-11-09-pytorch-lstm-imdb-sentiment-prediction.html#data-preparation",
    "href": "posts/2022-11-09-pytorch-lstm-imdb-sentiment-prediction.html#data-preparation",
    "title": "Predicting the Sentiment of IMDB Movie Reviews using LSTM in PyTorch",
    "section": "Data Preparation",
    "text": "Data Preparation\n\nDownload data\nLet’s download our movie review dataset. This dataset is also known as Large Movie Review Dataset, and can also be obtained in a compressed zip file from this link. Using the torchtext library makes downloading, extracting, and reading files a lot easier. ‘torchtext.datasets’ comes with many more NLP related datasets, and a full list can be found here.\n\nfrom torchtext.datasets import IMDB\nfrom torch.utils.data.dataset import random_split\n\ntorch.manual_seed(1)\n\ntrain_dataset_raw = IMDB(split=\"train\")\ntest_dataset_raw = IMDB(split=\"test\")\n\nCheck the size of the downloaded data.\n\nprint(\"Train dataset size: \", len(train_dataset_raw))\nprint(\"Test dataset size: \", len(test_dataset_raw))\n\nTrain dataset size:  25000\nTest dataset size:  25000\n\n\n\n\nSplit train data further into train and validation set\nBoth train and test datasets have 25000 reviews. Therefore, we can split the training set further into the train and validation sets.\n\ntrain_set_size = 20000\nvalid_set_size = 5000\n\ntrain_dataset, valid_dataset = random_split(list(train_dataset_raw), [20000, 5000])\n\n\n\nHow does this data look?\nThe data we have is in the form of tuples. The first index has the sentiment label, and the second contains the review text. Let’s check the first element in our training dataset.\n\ntrain_dataset[0]\n\n('pos',\n 'An extra is called upon to play a general in a movie about the Russian Revolution. However, he is not any ordinary extra. He is Serguis Alexander, former commanding general of the Russia armies who is now being forced to relive the same scene, which he suffered professional and personal tragedy in, to satisfy the director who was once a revolutionist in Russia and was humiliated by Alexander. It can now be the time for this broken man to finally \"win\" his penultimate battle. This is one powerful movie with meticulous direction by Von Sternberg, providing the greatest irony in Alexander\\'s character in every way he can. Jannings deserved his Oscar for the role with a very moving performance playing the general at his peak and at his deepest valley. Powell lends a sinister support as the revenge minded director and Brent is perfect in her role with her face and movements showing so much expression as Jannings\\' love. All around brilliance. Rating, 10.')\n\n\nCheck the first index of the validation set.\n\nvalid_dataset[0]\n\n('neg',\n 'The Dereks did seem to struggle to find rolls for Bo after \"10\".<br /><br />I used to work for a marine park in the Florida Keys. One day, the script for \"Ghosts Can\\'t Do It\" was circulating among the trainers in the \"fish house\" where food was prepared for the dolphins. There was one scene where a -dolphin- supposedly propositions Bo (or Bo the dolphin), asking to \"go make eggs.\" Reading the script, we -lauuughed-...<br /><br />We did not end up doing any portion of this movie at our facility, although our dolphins -were- in \"The Big Blue!\"<br /><br />This must have been very close to the end of Anthony Quinn\\'s life. I hope he had fun in this film, as it certainly didn\\'t do anything for his legacy.')\n\n\n\n\nData preprocessing steps\nFrom these two reviews, we can deduce that\n\nWe have two labels. ‘pos’ for a positive and ‘neg’ for a negative review\nFrom the second review (from valid_dataset), we also get that text may contain HTML tags, special characters, and emoticons besides normal English words. It will require some preprocessing to remove them for proper word tokenization.\nReviews can have varying text lengths. It will require some padding to make all review texts the same size.\n\nLet’s take a simple text example and apply these steps to understand why these steps are essential in preprocessing. In the last step, we will create tokens from the preprocessed text.\n\nexample_text = '''This is awesome movie <br /><br />. I loved it so much :-) I\\'m goona watch it again :)'''\nexample_text\n\n\"This is awesome movie <br /><br />. I loved it so much :-) I'm goona watch it again :)\"\n\n\n\n##\n# step 1. remove HTML tags. they are not helpful in understanding the sentiments of a review\nimport re\n\ntext = re.sub('<[^>]*>', '', example_text)\ntext\n\n\"This is awesome movie . I loved it so much :-) I'm goona watch it again :)\"\n\n\n\n##\n# step 2: use lowercase for all text to keep symmetry\ntext = text.lower()\ntext\n\n\"this is awesome movie . i loved it so much :-) i'm goona watch it again :)\"\n\n\n\n##\n# step 3: extract emoticons. keep them as they are important sentiment signals\nemoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text)\nemoticons\n\n[':-)', ':)']\n\n\n\n##\n# step 4: remove punctuation marks\ntext = re.sub('[\\W]+', ' ', text)\ntext\n\n'this is awesome movie i loved it so much i m goona watch it again '\n\n\n\n##\n# step 5: put back emoticons\ntext = text + ' '.join(emoticons).replace('-', '')\ntext\n\n'this is awesome movie i loved it so much i m goona watch it again :) :)'\n\n\n\n##\n# step 6: generate word tokens\ntext = text.split()\ntext\n\n['this',\n 'is',\n 'awesome',\n 'movie',\n 'i',\n 'loved',\n 'it',\n 'so',\n 'much',\n 'i',\n 'm',\n 'goona',\n 'watch',\n 'it',\n 'again',\n ':)',\n ':)']\n\n\nLet’s put all the preprocessing steps in a nice function and give it a name.\n\ndef tokenizer(text):\n    # step 1. remove HTML tags. they are not helpful in understanding the sentiments of a review\n    # step 2: use lowercase for all text to keep symmetry\n    # step 3: extract emoticons. keep them as they are important sentiment signals\n    # step 4: remove punctuation marks\n    # step 5: put back emoticons\n    # step 6: generate word tokens\n    text = re.sub(\"<[^>]*>\", \"\", text)\n    text = text.lower()\n    emoticons = re.findall(\"(?::|;|=)(?:-)?(?:\\)|\\(|D|P)\", text)\n    text = re.sub(\"[\\W]+\", \" \", text)\n    text = text + \" \".join(emoticons).replace(\"-\", \"\")\n    tokenized = text.split()\n    return tokenized\n\nApply tokenizer on the example_text to verify the output.\n\nexample_tokens = tokenizer(example_text)\nexample_tokens\n\n['this',\n 'is',\n 'awesome',\n 'movie',\n 'i',\n 'loved',\n 'it',\n 'so',\n 'much',\n 'i',\n 'm',\n 'goona',\n 'watch',\n 'it',\n 'again',\n ':)',\n ':)']\n\n\n\n\nPreparing data dictionary\nWe are successful in creating word tokens from our example_text. But there is one more problem. Some of the tokens are repeating. If we can convert these tokens into a dictionary along with their frequency count, we can significantly reduce the generated token size from these reviews. Let’s do that.\n\nfrom collections import Counter\n\ntoken_counts = Counter()\ntoken_counts.update(example_tokens)\ntoken_counts\n\nCounter({'this': 1,\n         'is': 1,\n         'awesome': 1,\n         'movie': 1,\n         'i': 2,\n         'loved': 1,\n         'it': 2,\n         'so': 1,\n         'much': 1,\n         'm': 1,\n         'goona': 1,\n         'watch': 1,\n         'again': 1,\n         ':)': 2})\n\n\nLet’s sort the output to have the most common words at the top.\n\nsorted_by_freq_tuples = sorted(token_counts.items(), key=lambda x: x[1], reverse=True)\nsorted_by_freq_tuples\n\n[('i', 2),\n ('it', 2),\n (':)', 2),\n ('this', 1),\n ('is', 1),\n ('awesome', 1),\n ('movie', 1),\n ('loved', 1),\n ('so', 1),\n ('much', 1),\n ('m', 1),\n ('goona', 1),\n ('watch', 1),\n ('again', 1)]\n\n\nIt shows that in our example text, the top place is taken by pronouns (i and it) followed by the emoticon. Though our data is now correctly processed, it needs to be prepared to be fed to a model. Because [machine] models love math and work with numbers exclusively. To convert our dictionary of word tokens into integers, we can take help from torchtext.vocab. Its purpose in the official documentation is defined as link here\n\nFactory method for creating a vocab object which maps tokens to indices.\n\n\nNote that the ordering in which key value pairs were inserted in the ordered_dict will be respected when building the vocab. Therefore if sorting by token frequency is important to the user, the ordered_dict should be created in a way to reflect this.\n\nIt highlights three points:\n\nIt maps tokens to indices\nIt requires an ordered dictionary (OrderedDict) to work\nTokens in vocab at the starting indices reflect higher frequency\n\n\n##\n# step 1: convert our sorted list of tokens to OrderedDict\nfrom collections import OrderedDict\n\nordered_dict = OrderedDict(sorted_by_freq_tuples)\nordered_dict\n\nOrderedDict([('i', 2),\n             ('it', 2),\n             (':)', 2),\n             ('this', 1),\n             ('is', 1),\n             ('awesome', 1),\n             ('movie', 1),\n             ('loved', 1),\n             ('so', 1),\n             ('much', 1),\n             ('m', 1),\n             ('goona', 1),\n             ('watch', 1),\n             ('again', 1)])\n\n\n\n##\n# Check the length of our dictionary\nlen(ordered_dict)\n\n14\n\n\n\n##\n# step 2: convert the ordered dict to torchtext.vocab\nfrom torchtext.vocab import vocab\n\nvb = vocab(ordered_dict)\nvb.get_stoi()\n\n{'goona': 11,\n 'much': 9,\n 'm': 10,\n 'loved': 7,\n 'watch': 12,\n 'so': 8,\n 'movie': 6,\n 'it': 1,\n 'again': 13,\n 'this': 3,\n 'i': 0,\n 'awesome': 5,\n ':)': 2,\n 'is': 4}\n\n\nThis generated vocabulary shows that tokens with higher frequency (i, it) have been assigned lower indices (or integers). This vocabulary will act as a lookup table for us, and during training for each word token, we will find a corresponding index from this vocab and pass it to our model.\nWe have done many steps while processing our example_text. Let’s summarize them here before moving further\n\nSummary of data dictionary preparation steps\n\nGenerate tokens from text using the function tokenizer\nFind the frequency of tokens using Python collections.Counter\nSort the tokens based on their frequency in descending order\nPut the sorted tokens in Python collections.OrderedDict\nConvert the tokens into integers using torchtext.vocab\n\nLet’s apply all these steps on our IMDB reviews training dataset.\n\n##\n# step 1: convert reviews into tokens\n# step 2: find frequency of tokens\n\ntoken_counts = Counter()\n\nfor label, line in train_dataset:\n    tokens = tokenizer(line)\n    token_counts.update(tokens)\n \nprint('IMDB vocab size:', len(token_counts))\n\nIMDB vocab size: 69023\n\n\nAfter tokenizing IMDB reviews, we find that there 69023 unique tokens.\n\n##\n# step 3: sort the token based on their frequency\n# step 4: put the sorted tokens in OrderedDict\n# step 5: convert token to integers using vocab object\n\nsorted_by_freq_tuples = sorted(token_counts.items(), key=lambda x: x[1], reverse=True)\nordered_dict = OrderedDict(sorted_by_freq_tuples)\n\nvb = vocab(ordered_dict)\n\nvb.insert_token(\"<pad>\", 0)  # special token for padding\nvb.insert_token(\"<unk>\", 1)  # special token for unknown words\nvb.set_default_index(1)\n\n# print some token indexes from vocab\nfor token in [\"this\", \"is\", \"an\", \"example\"]:\n    print(token, \" --> \", vb[token])\n\nthis  -->  11\nis  -->  7\nan  -->  35\nexample  -->  457\n\n\nWe have added two extra tokens to our vocabulary.\n\n“pad” for padding. This token will come in handy when we pad our reviews to make them of the same length\n“unk” for unknown. This token will come in handy if we find any token in the validation or test set that was not part of the train set\n\nLet’s also print the tokens present at the first ten indices of our vocab object.\n\nvb.get_itos()[:10]\n\n['<pad>', '<unk>', 'the', 'and', 'a', 'of', 'to', 'is', 'it', 'in']\n\n\nIt shows that articles, prepositions, and pronouns are the most common words in the training dataset. So let’s also check the least common words.\n\nvb.get_itos()[-10:]\n\n['hairband',\n 'ratt',\n 'bettiefile',\n 'queueing',\n 'johansen',\n 'hemmed',\n 'jardine',\n 'morland',\n 'seriousuly',\n 'fictive']\n\n\nThe least common words seem to be people or place names or misspelled words like ‘queueing’ and ‘seriousuly’."
  },
  {
    "objectID": "posts/2022-11-09-pytorch-lstm-imdb-sentiment-prediction.html#define-data-processing-pipelines",
    "href": "posts/2022-11-09-pytorch-lstm-imdb-sentiment-prediction.html#define-data-processing-pipelines",
    "title": "Predicting the Sentiment of IMDB Movie Reviews using LSTM in PyTorch",
    "section": "Define data processing pipelines",
    "text": "Define data processing pipelines\nAt this point, we have our tokenizer function and vocabulary lookup ready. For each review item from the dataset, we are supposed to perform the following preprocessing steps:\nFor review text\n\nCreate tokens from the review text\nAssign a unique integer to each token from the vocab lookup\n\nFor review label\n\nAssign 1 for pos and 0 for neg label\n\nLet’s create two simple functions (inline lambda) for review text and label processing.\n\n##\n# inline lambda functions for text and label precessing\ntext_pipeline = lambda x: [vb[token] for token in tokenizer(x)]\nlabel_pipeline = lambda x: 1.0 if x == \"pos\" else 0.0\n\n\n##\n# apply text_pipeline to example_text\ntext_pipeline(example_text)\n\n[11, 7, 1166, 18, 10, 450, 8, 37, 74, 10, 142, 1, 104, 8, 174, 2287, 2287]\n\n\nInstead of processing a single review at a time, we always prefer to work with a batch of them during model training. For each review item in the batch, we will be doing the same preprocessing steps i.e. review text processing and label processing. For handling preprocessing steps at a batch level, we can create another higher-level function that applies preprocessing steps at a batch level.\n\n##\n# setting device on GPU if available, else CPU\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint('Using device:', device)\n\nUsing device: cuda\n\n\n\n##\n# a function to apply pre-processing steps at a batch level\nimport torch.nn as nn\n\ndef collate_batch(batch):\n    label_list, text_list, lengths = [], [], []\n\n    # iterate over all reviews in a batch\n    for _label, _text in batch:\n        # label preprocessing\n        label_list.append(label_pipeline(_label))\n        # text preprocessing\n        processed_text = torch.tensor(text_pipeline(_text), dtype=torch.int64)\n\n        # store the processed text in a list\n        text_list.append(processed_text)\n        \n        # store the length of processed text\n        # this will come handy in future when we want to know the original size of a text (without padding)\n        lengths.append(processed_text.size(0))\n    \n    label_list = torch.tensor(label_list)\n    lengths = torch.tensor(lengths)\n    \n    # pad the processed reviews to make their lengths consistant\n    padded_text_list = nn.utils.rnn.pad_sequence(\n        text_list, batch_first=True)\n    \n    # return\n    # 1. a list of processed and padded review texts\n    # 2. a list of processed labels\n    # 3. a list of review text original lengths (before padding)\n    return padded_text_list.to(device), label_list.to(device), lengths.to(device)\n\n\nSequence padding\nIn the above collate_batch function, I added one extra padding step.\nadded_text_list = nn.utils.rnn.pad_sequence(text_list, batch_first=True)\nWe intend to make all review texts in a batch of the same length. For this, we take the maximum length of a text in a batch, all pad all the smaller text with extra dummy tokens (‘pad’) to make their sizes equal. Finally, with all the data in a batch of the same dimension, we convert it into a tensor matrix for faster processing.\nTo understand how PyTorch utility nn.utils.rnn.pad_sequence works, we can take a simple example of three tensors (a, b, c) of varying sizes (1, 3, 5).\n\n##\n# initialize three tensors of varying sizes\na = torch.tensor([1])\nb = torch.tensor([2, 3, 4])\nc = torch.tensor([5, 6, 7, 8, 9])\na, b, c\n\n(tensor([1]), tensor([2, 3, 4]), tensor([5, 6, 7, 8, 9]))\n\n\nNow let’s pad them to make sizes consistant.\n\n##\n# apply padding on tensors\npad_seq = nn.utils.rnn.pad_sequence([a, b, c])\npad_seq\n\ntensor([[1, 2, 5],\n        [0, 3, 6],\n        [0, 4, 7],\n        [0, 0, 8],\n        [0, 0, 9]])\n\n\n\n\nSequence packing\nFrom the above output, we can see that after padding tensors of varying sizes, we can convert them into a single matrix for faster processing. But the drawback of this approach is that we can have many, many padded tokens in our matrix. They are not helping us in any way, instead of occupying a lot of machine memory. To avoid this, we can also squish these matrixes into a much condensed form called packed padded sequences using PyTorch utility nn.utils.rnn.pack_padded_sequence.\n\npack_pad_seq = nn.utils.rnn.pack_padded_sequence(\n    pad_seq, [1, 3, 5], enforce_sorted=False, batch_first=False\n)\npack_pad_seq.data\n\ntensor([5, 2, 1, 6, 3, 7, 4, 8, 9])\n\n\nHere the tensor still holds all the original tensor values (1 to 9) but is very condensed and has no extra padded token. So how does this tensor know which tokens belong to which token? For this, it stores some additional information.\n\nbatch sizes (or original tensor length)\ntensor indices\n\nWe can move back and forth between the padded pack and unpacked sequences using this information.\n\npack_pad_seq\n\nPackedSequence(data=tensor([5, 2, 1, 6, 3, 7, 4, 8, 9]), batch_sizes=tensor([3, 2, 2, 1, 1]), sorted_indices=tensor([2, 1, 0]), unsorted_indices=tensor([2, 1, 0]))\n\n\n\n\nRun data preprocessing pipelines on an example batch\nLet’s load our data in the PyTorch DataLoader class and create a small batch of 4 reviews. Preprocess the entire set with collate_batch function.\n\nfrom torch.utils.data import DataLoader\n\ndataloader = DataLoader(\n    train_dataset, batch_size=4, shuffle=False, collate_fn=collate_batch\n)\ntext_batch, label_batch, length_batch = next(iter(dataloader))\n\nprint(\"text_batch.shape: \", text_batch.shape)\nprint(\"label_batch: \", label_batch)\nprint(\"length_batch: \", length_batch)\n\ntext_batch.shape:  torch.Size([4, 218])\nlabel_batch:  tensor([1., 1., 1., 0.], device='cuda:0')\nlength_batch:  tensor([165,  86, 218, 145], device='cuda:0')\n\n\n\ntext_batch.shape: torch.Size([4, 218]) tells us that in this batch, there are four reviews (or their tokens) and all have the same length of 218\nlabel_batch:  tensor([1., 1., 1., 0.]) tells us that the first three reviews are positive and the last is negative\nlength_batch:  tensor([165,  86, 218, 145]) tells us that before padding the original length of review tokens\n\nLet’s check what the first review in this batch looks like after preprocessing and padding.\n\nprint(text_batch[0])\n\ntensor([   35,  1739,     7,   449,   721,     6,   301,     4,   787,     9,\n            4,    18,    44,     2,  1705,  2460,   186,    25,     7,    24,\n          100,  1874,  1739,    25,     7, 34415,  3568,  1103,  7517,   787,\n            5,     2,  4991, 12401,    36,     7,   148,   111,   939,     6,\n        11598,     2,   172,   135,    62,    25,  3199,  1602,     3,   928,\n         1500,     9,     6,  4601,     2,   155,    36,    14,   274,     4,\n        42945,     9,  4991,     3,    14, 10296,    34,  3568,     8,    51,\n          148,    30,     2,    58,    16,    11,  1893,   125,     6,   420,\n         1214,    27, 14542,   940,    11,     7,    29,   951,    18,    17,\n        15994,   459,    34,  2480, 15211,  3713,     2,   840,  3200,     9,\n         3568,    13,   107,     9,   175,    94,    25,    51, 10297,  1796,\n           27,   712,    16,     2,   220,    17,     4,    54,   722,   238,\n          395,     2,   787,    32,    27,  5236,     3,    32,    27,  7252,\n         5118,  2461,  6390,     4,  2873,  1495,    15,     2,  1054,  2874,\n          155,     3,  7015,     7,   409,     9,    41,   220,    17,    41,\n          390,     3,  3925,   807,    37,    74,  2858,    15, 10297,   115,\n           31,   189,  3506,   667,   163,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0],\n       device='cuda:0')\n\n\nTo complete the picture, I have re-printed the original text of the first review and manually processed a part of it. You can verify that the tokens match.\n\n##\n# first review\ntrain_dataset[0]\n\n('pos',\n 'An extra is called upon to play a general in a movie about the Russian Revolution. However, he is not any ordinary extra. He is Serguis Alexander, former commanding general of the Russia armies who is now being forced to relive the same scene, which he suffered professional and personal tragedy in, to satisfy the director who was once a revolutionist in Russia and was humiliated by Alexander. It can now be the time for this broken man to finally \"win\" his penultimate battle. This is one powerful movie with meticulous direction by Von Sternberg, providing the greatest irony in Alexander\\'s character in every way he can. Jannings deserved his Oscar for the role with a very moving performance playing the general at his peak and at his deepest valley. Powell lends a sinister support as the revenge minded director and Brent is perfect in her role with her face and movements showing so much expression as Jannings\\' love. All around brilliance. Rating, 10.')\n\n\n\n##\n# manually preprocessing a part of review text\n# notice that the generated tokens match\ntext = 'An extra is called upon to play a general in a movie about the Russian Revolution'\n[vb[token] for token in tokenizer(text)]\n\n[35, 1739, 7, 449, 721, 6, 301, 4, 787, 9, 4, 18, 44, 2, 1705, 2460]"
  },
  {
    "objectID": "posts/2022-11-09-pytorch-lstm-imdb-sentiment-prediction.html#batching-the-training-validation-and-test-dataset",
    "href": "posts/2022-11-09-pytorch-lstm-imdb-sentiment-prediction.html#batching-the-training-validation-and-test-dataset",
    "title": "Predicting the Sentiment of IMDB Movie Reviews using LSTM in PyTorch",
    "section": "Batching the training, validation, and test dataset",
    "text": "Batching the training, validation, and test dataset\nLet’s proceed on creating DataLoaders for train, valid, and test data with batch_size = 32\n\nbatch_size = 32\n\ntrain_dl = DataLoader(\n    train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_batch\n)\nvalid_dl = DataLoader(\n    valid_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_batch\n)\ntest_dl = DataLoader(\n    test_dataset_raw, batch_size=batch_size, shuffle=False, collate_fn=collate_batch\n)"
  },
  {
    "objectID": "posts/2022-11-09-pytorch-lstm-imdb-sentiment-prediction.html#define-model-training-and-evaluation-pipelines",
    "href": "posts/2022-11-09-pytorch-lstm-imdb-sentiment-prediction.html#define-model-training-and-evaluation-pipelines",
    "title": "Predicting the Sentiment of IMDB Movie Reviews using LSTM in PyTorch",
    "section": "Define model training and evaluation pipelines",
    "text": "Define model training and evaluation pipelines\nI have defined two simple functions to train and evaluate the model in this section.\n\n##\n# model training pipeline\n# https://github.com/rasbt/machine-learning-book/blob/main/ch15/ch15_part2.ipynb\ndef train(dataloader):\n    model.train()\n    total_acc, total_loss = 0, 0\n    for text_batch, label_batch, lengths in dataloader:\n        optimizer.zero_grad()\n        pred = model(text_batch, lengths)[:, 0]\n        loss = loss_fn(pred, label_batch)\n        loss.backward()\n        optimizer.step()\n        total_acc += ((pred >= 0.5).float() == label_batch).float().sum().item()\n        total_loss += loss.item() * label_batch.size(0)\n    return total_acc / len(dataloader.dataset), total_loss / len(dataloader.dataset)\n\n\n# model evaluation pipeline\ndef evaluate(dataloader):\n    model.eval()\n    total_acc, total_loss = 0, 0\n    with torch.no_grad():\n        for text_batch, label_batch, lengths in dataloader:\n            pred = model(text_batch, lengths)[:, 0]\n            loss = loss_fn(pred, label_batch)\n            total_acc += ((pred >= 0.5).float() == label_batch).float().sum().item()\n            total_loss += loss.item() * label_batch.size(0)\n    return total_acc / len(dataloader.dataset), total_loss / len(dataloader.dataset)"
  },
  {
    "objectID": "posts/2022-11-09-pytorch-lstm-imdb-sentiment-prediction.html#rnn-model-configuration-loss-function-and-optimizer",
    "href": "posts/2022-11-09-pytorch-lstm-imdb-sentiment-prediction.html#rnn-model-configuration-loss-function-and-optimizer",
    "title": "Predicting the Sentiment of IMDB Movie Reviews using LSTM in PyTorch",
    "section": "RNN model configuration, loss function, and optimizer",
    "text": "RNN model configuration, loss function, and optimizer\nWe have seen the review text, which can be long sequences. We will use the LSTM layer for capturing the long-term dependencies. Our sentiment analysis model is composed of the following layers\n\nStart with an Embedding layer. Placing the embedding layer is similar to one-hot-encoding, where each word token is converted to a separate feature (or vector or column). But this can lead to too many features (curse of dimensionality or dimensional explosion). To avoid this, we try to map tokens to fixed-size vectors (or columns). In such a feature matrix, different elements denote different tokens. Tokens that are closed are also placed together. Further, during training, we also learn and update the positioning of tokens. Similar tokens are placed into closer and closer locations. Such a matrix layer is termed an embedding layer.\nAfter the embedding layer, there is the RNN layer (LSTM to be specific).\nThen we have a fully connected layer followed by activation and another fully connected layer.\nFinally, we have a logistic sigmoid layer for prediction\n\n\n##\n# https://github.com/rasbt/machine-learning-book/blob/main/ch15/ch15_part2.ipynb\nclass RNN(nn.Module):\n    def __init__(self, vocab_size, embed_dim, rnn_hidden_size, fc_hidden_size):\n        super().__init__()\n        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n        self.rnn = nn.LSTM(embed_dim, rnn_hidden_size, batch_first=True)\n        self.fc1 = nn.Linear(rnn_hidden_size, fc_hidden_size)\n        self.relu = nn.ReLU()\n        self.fc2 = nn.Linear(fc_hidden_size, 1)\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, text, lengths):\n        out = self.embedding(text)\n        out = nn.utils.rnn.pack_padded_sequence(\n            out, lengths.cpu().numpy(), enforce_sorted=False, batch_first=True\n        )\n        out, (hidden, cell) = self.rnn(out)\n        out = hidden[-1, :, :]\n        out = self.fc1(out)\n        out = self.relu(out)\n        out = self.fc2(out)\n        out = self.sigmoid(out)\n        return out\n\n\nvocab_size = len(vb)\nembed_dim = 20\nrnn_hidden_size = 64\nfc_hidden_size = 64\n\ntorch.manual_seed(1)\nmodel = RNN(vocab_size, embed_dim, rnn_hidden_size, fc_hidden_size)\nmodel = model.to(device)\n\n\nDefine model loss function and optimizer\nFor loss function (or criterion), I have used Binary Cross Entropy, and for loss optimization, I have used Adam algorithm\n\ntorch.manual_seed(1)\n\nloss_fn = nn.BCELoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
  },
  {
    "objectID": "posts/2022-11-09-pytorch-lstm-imdb-sentiment-prediction.html#model-training-and-evaluation",
    "href": "posts/2022-11-09-pytorch-lstm-imdb-sentiment-prediction.html#model-training-and-evaluation",
    "title": "Predicting the Sentiment of IMDB Movie Reviews using LSTM in PyTorch",
    "section": "Model training and evaluation",
    "text": "Model training and evaluation\nLet’s run the pipeline for ten epochs and compare the training and validation accuracy.\n\nnum_epochs = 10\nfor epoch in range(num_epochs):\n    acc_train, loss_train = train(train_dl)\n    acc_valid, loss_valid = evaluate(valid_dl)\n    print(\n        f\"Epoch {epoch} train accuracy: {acc_train:.4f}; val accuracy: {acc_valid:.4f}\"\n    )\n\nEpoch 0 train accuracy: 0.6085; val accuracy: 0.6502\nEpoch 1 train accuracy: 0.7206; val accuracy: 0.7462\nEpoch 2 train accuracy: 0.7613; val accuracy: 0.6250\nEpoch 3 train accuracy: 0.8235; val accuracy: 0.8232\nEpoch 4 train accuracy: 0.8819; val accuracy: 0.8482\nEpoch 5 train accuracy: 0.9132; val accuracy: 0.8526\nEpoch 6 train accuracy: 0.9321; val accuracy: 0.8374\nEpoch 7 train accuracy: 0.9504; val accuracy: 0.8502\nEpoch 8 train accuracy: 0.9643; val accuracy: 0.8608\nEpoch 9 train accuracy: 0.9747; val accuracy: 0.8636\n\n\n\nEvaluate sentiments on random texts\nLet’s create another helper method to evaluate sentiments on random texts.\n\ndef classify_review(text):\n    text_list, lengths = [], []\n\n    # process review text with text_pipeline\n    # note: \"text_pipeline\" has dependency on data vocabulary\n    processed_text = torch.tensor(text_pipeline(text), dtype=torch.int64)\n    text_list.append(processed_text)\n\n    # get processed review tokens length\n    lengths.append(processed_text.size(0))\n    lengths = torch.tensor(lengths)\n        \n    # change the dimensions from (torch.Size([8]), torch.Size([1, 8]))\n    # nn.utils.rnn.pad_sequence(text_list, batch_first=True) does this too\n    padded_text_list = torch.unsqueeze(processed_text, 0)\n\n    # move tensors to correct device\n    padded_text_list = padded_text_list.to(device)\n    lengths = lengths.to(device)\n\n    # get prediction\n    model.eval()\n    pred = model(padded_text_list, lengths)\n    print(\"model pred: \", pred)\n\n    # positive or negative review\n    review_class = 'negative' # else case\n    if (pred>=0.5) == 1:\n        review_class = \"positive\"\n\n    print(\"review type: \", review_class)\n\n\n##\n# create two random texts with strong positive and negative sentiments\npos_review = 'i love this movie. it was so good.'\nneg_review = 'slow and boring. waste of time.'\n\n\nclassify_review(pos_review)\n\nmodel pred:  tensor([[0.9388]], device='cuda:0', grad_fn=<SigmoidBackward0>)\nreview type:  positive\n\n\n\nclassify_review(neg_review)\n\nmodel pred:  tensor([[0.0057]], device='cuda:0', grad_fn=<SigmoidBackward0>)\nreview type:  negative"
  },
  {
    "objectID": "posts/2022-12-02-pytorch-word2vec-embedding.html#credits",
    "href": "posts/2022-12-02-pytorch-word2vec-embedding.html#credits",
    "title": "Implementing Word2Vec with PyTorch",
    "section": "Credits",
    "text": "Credits\nThis notebook takes inspiration and ideas from the following sources.\n\nAn excellent word2vec introduction from “Jay Alammar”: illustrated-word2vec.\nBlog post by “Musashi (sometimes Jacobs-) Harukawa” with the same title. You can find the original post here: word2vec-from-scratch. Parts of the code you see in this notebook are taken from this post.\nAnother very detailed and well-explained blog post by “Olga Chernytska”. You can find the original post here: word2vec-with-pytorch-implementing-original-paper. Parts of the code you see in this notebook are taken from this post."
  },
  {
    "objectID": "posts/2022-12-02-pytorch-word2vec-embedding.html#environment",
    "href": "posts/2022-12-02-pytorch-word2vec-embedding.html#environment",
    "title": "Implementing Word2Vec with PyTorch",
    "section": "Environment",
    "text": "Environment\nThis notebook GitHub link here is prepared with Google Colab. Hugging Face Datasets library is required for this post.\n\n# install hugging face datasets\n! pip install datasets\n\n\n\nCode\nfrom platform import python_version\nimport numpy, matplotlib, pandas, torch, datasets\n\nprint(\"python==\" + python_version())\nprint(\"numpy==\" + numpy.__version__)\nprint(\"torch==\" + torch.__version__)\nprint(\"matplotlib==\" + matplotlib.__version__)\nprint(\"datasets==\" + datasets.__version__)\n\n\npython==3.8.15\nnumpy==1.21.6\ntorch==1.12.1+cu113\nmatplotlib==3.2.2\ndatasets==2.7.1"
  },
  {
    "objectID": "posts/2022-12-02-pytorch-word2vec-embedding.html#introduction",
    "href": "posts/2022-12-02-pytorch-word2vec-embedding.html#introduction",
    "title": "Implementing Word2Vec with PyTorch",
    "section": "Introduction",
    "text": "Introduction\n\nFollowing are the word2vec and word embedding explanations provided by OpenAI ChatGPT. If unsatisfied, I suggest using the links provided in the “Credits” section (illustrated-word2vec from Jay Alammar).\n\nprompt > word2vec blog introduction\nWord2vec\nWord2vec is a widely-used natural language processing (NLP) algorithm that uses deep learning to learn the relationships between words in a corpus (a large collection of text data). A team of researchers developed it at Google, led by Tomas Mikolov in 2013.\nWord2vec uses a neural network to learn the relationships between words by training on a large corpus of text data. The algorithm uses a shallow neural network with a single hidden layer to predict the surrounding words of a given word based on its context. The network’s hidden layer is trained to capture the relationships between words, and the resulting vectors (or “word embeddings”) can be used to represent the words in the corpus in a lower-dimensional space.\nThe benefits of using word2vec include the ability to capture the relationships between words in a corpus and the ability to handle large amounts of data efficiently. This makes it a useful tool for many NLP tasks, such as text classification, language translation and named entity recognition.\nOverall, word2vec is a powerful and effective algorithm for learning the relationships between words in a corpus and has become a cornerstone of modern NLP techniques.\nprompt > word embedding short description\nWord Embedding\nA word embedding represents a word in a low-dimensional, continuous vector space. This representation captures the semantic relationships between words in a corpus, allowing for more accurate and efficient natural language processing. Word embeddings are typically learned using neural network models, such as the word2vec algorithm. These learned embeddings can then be used in various NLP tasks, such as text classification and language translation.\nSummary of the steps followed in this notebook\n\nDownload hugging face tweets_hate_speech_detection dataset\nPreprocess the data to remove special characters and normalize the text\nCreate tokens from the text and their vocabulary, and finally encode the tokens\nCreate training dataset, model configuration, loss, and optimizer\nTrain the model and extract the embedding layer weights\nAnalyse the word embeddings learned from the tweets"
  },
  {
    "objectID": "posts/2022-12-02-pytorch-word2vec-embedding.html#data-preparation",
    "href": "posts/2022-12-02-pytorch-word2vec-embedding.html#data-preparation",
    "title": "Implementing Word2Vec with PyTorch",
    "section": "Data Preparation",
    "text": "Data Preparation\n\nLoad Hugging Face Dataset\nFor this notebook, I will use Hugging Face Twitter Hate Speech Dataset. It contains training data with approximately 32K tweets divided into two groups: hate speech and not a hate speech. This dataset is originally for classification tasks, but we may use it to learn word embeddings (or word contexts). This approach can also be used to identify inherent biases present in the data.\n\nimport datasets\n\ndataset = datasets.load_dataset(\"tweets_hate_speech_detection\")\n\nWARNING:datasets.builder:Found cached dataset tweets_hate_speech_detection (/root/.cache/huggingface/datasets/tweets_hate_speech_detection/default/0.0.0/c6b6f41e91ac9113e1c032c5ecf7a49b4e1e9dc8699ded3c2d8425c9217568b2)\n\n\n\n\n\n\n# Let's check the downloaded object\ndataset\n\nDatasetDict({\n    train: Dataset({\n        features: ['label', 'tweet'],\n        num_rows: 31962\n    })\n})\n\n\nIt shows that we have only a training set, and each element from the set has a label and tweet text.\n\ntrain_ds = dataset[\"train\"]\ntrain_ds.features\n\n{'label': ClassLabel(names=['no-hate-speech', 'hate-speech'], id=None),\n 'tweet': Value(dtype='string', id=None)}\n\n\nLet’s check what the raw data looks like.\n\n# print a few labels\nprint(\"** labels **\\n\", train_ds[\"label\"][:5])\n\n# print a few tweet texts\nprint(\"\\n** tweets **\")\nfor t in train_ds[\"tweet\"][:5]:\n    print(t)\n\n** labels **\n [0, 0, 0, 0, 0]\n\n** tweets **\n@user when a father is dysfunctional and is so selfish he drags his kids into his dysfunction.   #run\n@user @user thanks for #lyft credit i can't use cause they don't offer wheelchair vans in pdx.    #disapointed #getthanked\nbihday your majesty\n#model   i love u take with u all the time in urð±!!! ððððð¦ð¦ð¦  \nfactsguide: society now    #motivation\n\n\nThis raw data shows that the labels are encoded as 0 and 1 for no-hate and hate speech, respectively. Therefore, we can improve our view by putting labels and tweets in Pandas’s DataFrame and analyzing them side by side.\n\nimport pandas as pd\n\npd.set_option(\"display.max_colwidth\", None)\n\ntrain_ds.set_format(type=\"pandas\")\ndf = train_ds[:]\n\n# a function to convert label codes to string value\ndef label_int2str(row):\n    return train_ds.features[\"label\"].int2str(row)\n\n\ndf[\"label_name\"] = df[\"label\"].apply(label_int2str)\ndf.head(10)\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      label\n      tweet\n      label_name\n    \n  \n  \n    \n      0\n      0\n      @user when a father is dysfunctional and is so selfish he drags his kids into his dysfunction.   #run\n      no-hate-speech\n    \n    \n      1\n      0\n      @user @user thanks for #lyft credit i can't use cause they don't offer wheelchair vans in pdx.    #disapointed #getthanked\n      no-hate-speech\n    \n    \n      2\n      0\n      bihday your majesty\n      no-hate-speech\n    \n    \n      3\n      0\n      #model   i love u take with u all the time in urð±!!! ððððð¦ð¦ð¦\n      no-hate-speech\n    \n    \n      4\n      0\n      factsguide: society now    #motivation\n      no-hate-speech\n    \n    \n      5\n      0\n      [2/2] huge fan fare and big talking before they leave. chaos and pay disputes when they get there. #allshowandnogo\n      no-hate-speech\n    \n    \n      6\n      0\n      @user camping tomorrow @user @user @user @user @user @user @user dannyâ¦\n      no-hate-speech\n    \n    \n      7\n      0\n      the next school year is the year for exams.ð¯ can't think about that ð­ #school #exams   #hate #imagine #actorslife #revolutionschool #girl\n      no-hate-speech\n    \n    \n      8\n      0\n      we won!!! love the land!!! #allin #cavs #champions #cleveland #clevelandcavaliers  â¦\n      no-hate-speech\n    \n    \n      9\n      0\n      @user @user welcome here !  i'm   it's so #gr8 !\n      no-hate-speech\n    \n  \n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\nAll these tweets are labeled as no-hate-speech. Let’s view some of the tweets from the other class.\n\ndf[df[\"label\"]==1].head(10)\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      label\n      tweet\n      label_name\n    \n  \n  \n    \n      13\n      1\n      @user #cnn calls #michigan middle school 'build the wall' chant '' #tcot\n      hate-speech\n    \n    \n      14\n      1\n      no comment!  in #australia   #opkillingbay #seashepherd #helpcovedolphins #thecove  #helpcovedolphins\n      hate-speech\n    \n    \n      17\n      1\n      retweet if you agree!\n      hate-speech\n    \n    \n      23\n      1\n      @user @user lumpy says i am a . prove it lumpy.\n      hate-speech\n    \n    \n      34\n      1\n      it's unbelievable that in the 21st century we'd need something like this. again. #neverump  #xenophobia\n      hate-speech\n    \n    \n      56\n      1\n      @user lets fight against  #love #peace\n      hate-speech\n    \n    \n      68\n      1\n      ð©the white establishment can't have blk folx running around loving themselves and promoting our greatness\n      hate-speech\n    \n    \n      77\n      1\n      @user hey, white people: you can call people 'white' by @user  #race  #identity #medâ¦\n      hate-speech\n    \n    \n      82\n      1\n      how the #altright uses  &amp; insecurity to lure men into #whitesupremacy\n      hate-speech\n    \n    \n      111\n      1\n      @user i'm not interested in a #linguistics that doesn't address #race &amp; . racism is about #power. #raciolinguistics bringsâ¦\n      hate-speech\n    \n  \n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\n# reset dataset to its original format\ntrain_ds.reset_format()\n\n\n\nPreprocess data\nWe can not use raw text directly to train a model because machines understand numbers and not alphabets. But to properly encode our text (convert it into numbers), we need to do some extra steps.\n\ntweets also contain special characters from emoticons or emojis. However, they do not help learn word embeddings. So we need to strip them from the text.\nsplit the tweet text into proper words. Even though we can use all the words from the tweet text, experience has shown that not all are useful in learning embeddings. Words that are commonly omitted are either uncommon or rare words, or stopwords (commonly used words)\nCreate a dictionary or vocabulary to filter words and encode them. This vocabulary helps move between encoded (integer) and character (or string) representations of words.\n\nFor some of these preprocessing tasks, we will use Natural Language Toolkit (NLTK library).\n\nNLTK is a leading platform for building Python programs to work with human language data. It provides easy-to-use interfaces to over 50 corpora and lexical resources such as WordNet, along with a suite of text processing libraries for classification, tokenization, stemming, tagging, parsing, and semantic reasoning, wrappers for industrial-strength NLP libraries.\n\n\nimport nltk\n\nnltk.download(\"stopwords\")  # for filtering common words\nnltk.download(\"wordnet\")  # for lemmatization of words\nnltk.download(\"omw-1.4\")  # required for 'wordnet'\n\n[nltk_data] Downloading package stopwords to /root/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package wordnet to /root/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n[nltk_data]   Package omw-1.4 is already up-to-date!\n\n\nTrue\n\n\nI have created a helper function in the next cell to preprocess and split the tweet text into tokens.\n\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\n\nimport re\n\nsw = stopwords.words(\"english\")\nwl = WordNetLemmatizer()\n\n\ndef split_tokens(row):\n    # step 1: lower the text\n    t = row[\"tweet\"].lower()\n    # step 2: remove all other characters except alphabets, numbers, and a space\n    t = re.sub(r\"[^a-z 0-9]\", \"\", t)\n    # step 3: split the text into words or tokens. split is made at each \"space\" character\n    t = re.split(r\" +\", t)\n    # step 4: remove stop words\n    t = [i for i in t if (i not in sw) and (i not in [\"user\"]) and len(i)]\n    # step 5: lemmatize words\n    t = [wl.lemmatize(i) for i in t]\n\n    row[\"all_tokens\"] = t\n    return row\n\nLet’s use a sample tweet text to uncover the working of this function.\n\nsample_tweet = train_ds[0][\"tweet\"]\nsample_tweet\n\n'@user when a father is dysfunctional and is so selfish he drags his kids into his dysfunction.   #run'\n\n\nStep 1: lower the text\n\nt = sample_tweet.lower()\nt\n\n'@user when a father is dysfunctional and is so selfish he drags his kids into his dysfunction.   #run'\n\n\nStep 2: remove all other characters except alphabets, numbers, and spaces\n\nt = re.sub(r\"[^a-z 0-9]\", \"\",t)\nt\n\n'user when a father is dysfunctional and is so selfish he drags his kids into his dysfunction   run'\n\n\nStep 3: split the text into words or tokens. split is made at each “space” character\n\nt = re.split(r\" +\", t)\nt\n\n['user',\n 'when',\n 'a',\n 'father',\n 'is',\n 'dysfunctional',\n 'and',\n 'is',\n 'so',\n 'selfish',\n 'he',\n 'drags',\n 'his',\n 'kids',\n 'into',\n 'his',\n 'dysfunction',\n 'run']\n\n\nStep 4: remove stop words Besides stop words, I have also filtered “user” from the text. This is because in the original tweet text, any reference to a Twitter user (e.g., @hassaanbinaslam) is replaced with @user to hide identity.\n\nt = [i for i in t if (i not in sw) and (i not in [\"user\"]) and len(i)]\nt\n\n['father', 'dysfunctional', 'selfish', 'drags', 'kids', 'dysfunction', 'run']\n\n\nEnglish language stop words taken from the NLTK library include the following list.\n\n# englist language stopwords from NLTK\nprint(sw)\n\n['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n\n\nStep 5: lemmatize words What is lemmatization anyway? Lemmatization is applied to normalize the text. It considers the context and converts the word to its meaningful base form, which is called Lemma. to read more about it, refer to what-is-the-difference-between-lemmatization-vs-stemming\n\nt = [wl.lemmatize(i) for i in t]\nt\n\n['father', 'dysfunctional', 'selfish', 'drag', 'kid', 'dysfunction', 'run']\n\n\nLet’s use the helper function split_tokens and compare our output.\n\nsplit_tokens(train_ds[0])\n\n{'label': 0,\n 'tweet': '@user when a father is dysfunctional and is so selfish he drags his kids into his dysfunction.   #run',\n 'all_tokens': ['father',\n  'dysfunctional',\n  'selfish',\n  'drag',\n  'kid',\n  'dysfunction',\n  'run']}\n\n\nOur preprocessing steps and tokenize function is ready. So let’s apply it to our entire tweet dataset.\n\n# tokenize tweet dataset\ntrain_ds_all_tokens = train_ds.map(split_tokens)\ntrain_ds_all_tokens\n\nWARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/tweets_hate_speech_detection/default/0.0.0/c6b6f41e91ac9113e1c032c5ecf7a49b4e1e9dc8699ded3c2d8425c9217568b2/cache-4f6808d01b1a8972.arrow\n\n\nDataset({\n    features: ['label', 'tweet', 'all_tokens'],\n    num_rows: 31962\n})\n\n\n\n\nCreate vocabulary\nA vocabulary is a dictionary where each key represents the token (word ) string form. The value obtained from the dictionary defines the token’s unique integer or encoded form. To create our vocabulary, we will follow the following approach.\n\nIterate through all the tweets (entire dataset) and count the frequency of all the tokens.\nRemove the rare or uncommon tokens. This helps in reducing the size of the vocabulary. However, these uncommon tokens also include words created with typos e.g., while tweeting someone incorrectly typed fahtr instead of father. In our case, we will remove all tokens with a frequency of less than 10.\nAfter that, we will put our filtered tokens in an ordered dictionary\nPass the ordered dictionary to Pytorch Vocab class. A vocab object automatically maps tokens to indices.\n\n\n# create a frequency map for all toekns\nfrom collections import Counter\n\ntoken_count = Counter()\nfor row_tokens in train_ds_all_tokens[\"all_tokens\"]:\n    token_count.update(row_tokens)\n\nprint(\"Number of tokens found: \", len(token_count))\n\nNumber of tokens found:  38988\n\n\n\n# remove uncommon tokens\nmin_token_freq = 10\ntoken_count_filtered = {k: v for k, v in token_count.items() if v > min_token_freq}\n\nprint(\"Number of tokens after filtering: \", len(token_count_filtered))\n\nNumber of tokens after filtering:  3007\n\n\nNotice that by removing uncommon tokens, we have significantly reduced the size of our vocabulary (almost 13x less). In the next step, we will sort them, convert them to OrdreredDict, and pass them to torchtext.vocab.\n\nfrom torchtext.vocab import vocab\nfrom collections import OrderedDict\n\n# sort the tokens based on their frequency\nsorted_by_freq_tuples = sorted(\n    token_count_filtered.items(), key=lambda x: x[1], reverse=True\n)\n# create a dictionary of tokens\nordered_dict = OrderedDict(sorted_by_freq_tuples)\n# convert the dictionary into a vocabulary\nvb = vocab(ordered_dict)\n\nUsing the following methods, we can use the vocabulary to move between the token’s integer and string form.\n\nvb['love'] to get the id ‘0’ for token ‘love’\nvb.get_stoi()['love'] to get id ‘0’ for token ‘love’\nvb.lookup_token('love') to get id ‘0’ for token ‘love’\nvb.lookup_tokens(['love', 'happy']) to get id ‘0’ and ‘2’ for token ‘love’ and ‘happy’\nvb.get_itos()[0] to get token ‘love’ from id ‘0’\nvb.lookup_indices([0, 2]) to get token ‘love’ and ‘happy’ from id ‘0’ and ‘2’\n\n\nvb[\"love\"], vb.get_stoi()[\"love\"], vb.get_itos()[0]\n\n(0, 0, 'love')\n\n\n\n# let's check a few more tokens\nfor i in range(5):\n    token = vb.get_itos()[i]\n    print(f\"{i} ---> {token}\")\n    print(f\"{token} ---> {vb.get_stoi()[token]}\")\n    print()\n\n0 ---> love\nlove ---> 0\n\n1 ---> day\nday ---> 1\n\n2 ---> happy\nhappy ---> 2\n\n3 ---> u\nu ---> 3\n\n4 ---> amp\namp ---> 4\n\n\n\nAlright, we have our vocabulary ready. Remember that we have filtered our vocabulary to remove uncommon tokens. Let’s use this vocabulary to do the same for our tweet text tokens.\n\n# use vocabulary to filter uncommon tokens from tweets\ndef remove_rare_tokens(row):\n    row[\"tokens\"] = [t for t in row[\"all_tokens\"] if t in vb]\n    return row\n\n\ntrain_ds_tokens = train_ds_all_tokens.map(remove_rare_tokens)\n\nWARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/tweets_hate_speech_detection/default/0.0.0/c6b6f41e91ac9113e1c032c5ecf7a49b4e1e9dc8699ded3c2d8425c9217568b2/cache-84d95f24a0e6d681.arrow\n\n\nIn the following cell output, note that only three tokens are left for a sample tweet after removing uncommon tokens.\n\n# verify the dataset after filtering\ntrain_ds_tokens[0]\n\n{'label': 0,\n 'tweet': '@user when a father is dysfunctional and is so selfish he drags his kids into his dysfunction.   #run',\n 'all_tokens': ['father',\n  'dysfunctional',\n  'selfish',\n  'drag',\n  'kid',\n  'dysfunction',\n  'run'],\n 'tokens': ['father', 'kid', 'run']}\n\n\n\n\nCreate training samples using skip-gram\nThe next step is to create training samples using a ” skip-gram ” technique. To understand it, let’s take an example of a sample tweet text: i get to see my daddy today!. After applying preprocessing steps, the word tokens produced from the tweet are: ['get', 'see', 'daddy', 'today']. The vocabulary indices (integer value) for these tokens are provided below.\n\n\n\nTokens\nIndices\n\n\n\n\nget\n10\n\n\nsee\n22\n\n\ndaddy\n404\n\n\ntoday\n9\n\n\n\nFor creating training samples using skip-gram, we take each token as an input and a surrounding token as the label. But we also need to decide the window size, meaning how many surrounding tokens to create training samples. Suppose we take window_size=2; then the training sample for this tweet will be.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntext\n\n\n\n\ninput\noutput\n\ninput\noutput\n\n\n\n\nget\nsee\ndaddy\ntoday\n—>\nget\nsee\n—>\n10\n22\n\n\n\n\n\n\n\nget\ndaddy\n\n10\n404\n\n\nget\nsee\ndaddy\ntoday\n—>\nsee\nget\n—>\n22\n10\n\n\n\n\n\n\n\nsee\ndaddy\n\n22\n404\n\n\n\n\n\n\n\nsee\ntoday\n\n22\n9\n\n\nget\nsee\ndaddy\ntoday\n—>\ndaddy\nget\n—>\n404\n10\n\n\n\n\n\n\n\ndaddy\nsee\n\n404\n22\n\n\n\n\n\n\n\ndaddy\ntoday\n\n404\n9\n\n\nget\nsee\ndaddy\ntoday\n—>\ntoday\nsee\n—>\n9\n22\n\n\n\n\n\n\n\ntoday\ndaddy\n\n9\n404\n\n\n\nIn the above table, we have generated ten training samples from 4 tokens with a window size of 2. In each sample, we have two tokens\n\ninput or the training data\noutput or the label value\n\nGiven the tokens and window size, let’s create a function to create training samples for us.\n\n# code source: https://muhark.github.io/python/ml/nlp/2021/10/21/word2vec-from-scratch.html\ndef windowizer(row, wsize=3):\n    # default 'window size' (wsize) is three\n    doc = row[\"tokens\"]\n    wsize = wsize\n    out = []\n    for i, wd in enumerate(doc):\n        target = vb[wd]\n        window = [\n            i + j\n            for j in range(-wsize, wsize + 1, 1)\n            if (i + j >= 0) & (i + j < len(doc)) & (j != 0)\n        ]\n\n        out += [(target, vb[doc[w]]) for w in window]\n    row[\"moving_window\"] = out\n    return row\n\n\ntrain_ds_tokens = train_ds_tokens.map(windowizer)\n\nWARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/tweets_hate_speech_detection/default/0.0.0/c6b6f41e91ac9113e1c032c5ecf7a49b4e1e9dc8699ded3c2d8425c9217568b2/cache-e1806b88a0961df3.arrow\n\n\nLet’s check a sample tweet with generated training samples.\n\n# note that we have used skip-gram window size=3\ntrain_ds_tokens[12]\n\n{'label': 0,\n 'tweet': 'i get to see my daddy today!!   #80days #gettingfed',\n 'all_tokens': ['get', 'see', 'daddy', 'today', '80days', 'gettingfed'],\n 'tokens': ['get', 'see', 'daddy', 'today'],\n 'moving_window': [[10, 22],\n  [10, 404],\n  [10, 9],\n  [22, 10],\n  [22, 404],\n  [22, 9],\n  [404, 10],\n  [404, 22],\n  [404, 9],\n  [9, 10],\n  [9, 22],\n  [9, 404]]}\n\n\n\n\nCreate dataset and dataloader\nThe preprocessing part of the data is complete. Now we only need to load this data into the Pytorch Dataset class and create batches for model training using DataLoader class. Both are utilities or helper classes provided by PyTorch to make our dataset (data preparation) code decoupled from our model training code for better readability and modularity.\n\ntorch.utils.data.Dataset stores the samples and their corresponding labels\ntorch.utils.data.DataLoader wraps an iterable around the Dataset to enable easy access to the samples.\n\n\n# source: https://muhark.github.io/python/ml/nlp/2021/10/21/word2vec-from-scratch.html\n# Helper class to make our data compatible with PyTorch Dataset\nfrom torch.utils.data import Dataset\n\nclass Word2VecDataset(Dataset):\n    def __init__(self, dataset, vocab_size):\n        self.dataset = dataset\n        self.vocab_size = vocab_size\n        self.data = [i for s in dataset[\"moving_window\"] for i in s]\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        return self.data[idx]\n\n\nvb_size = len(vb)\n\nword2vec_ds = Word2VecDataset(train_ds_tokens, vocab_size=vb_size)\n\nprint(\n    f\"a word2vec_ds entry: {word2vec_ds[0]}\",\n    f\"\\nnumber of word2vec_ds entries: {len(word2vec_ds)}\",\n)\n\na word2vec_ds entry: [13, 123] \nnumber of word2vec_ds entries: 761044\n\n\nword2vec_ds is our training dataset, and a single entry from it is of shape (input, label). Notice that there are many entries (or training samples) in word2vec_ds, and these samples represent all of the tweets tokens. For training, we need to create batches from them for faster processing. So let’s do that next.\n\nfrom torch.utils.data import DataLoader\n\nBATCH_SIZE = 2 ** 14\nN_LOADER_PROCS = 10\n\ndataloader = DataLoader(\n    word2vec_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=N_LOADER_PROCS\n)\n\nprint(f\"number of training batches: {len(dataloader)}\")\n\nnumber of training batches: 47\n\n\n/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:563: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n\n\n\ntmp = next(iter(dataloader))\ntmp\n\n[tensor([  46, 1743,    0,  ...,   28,    8,  122]),\n tensor([  15, 2197,  197,  ...,  179,  846,    0])]\n\n\nIn the last cell, I fetched a single batch from the dataloader. Its output is two long tensors.\n\nThe first tensor has all the entries for the input data\nThe second tensor has all the entries for the labels\n\nThe size of both these tensors is the same."
  },
  {
    "objectID": "posts/2022-12-02-pytorch-word2vec-embedding.html#model-configuration",
    "href": "posts/2022-12-02-pytorch-word2vec-embedding.html#model-configuration",
    "title": "Implementing Word2Vec with PyTorch",
    "section": "Model Configuration",
    "text": "Model Configuration\nNow we will configure the model to be used for training.\nWhat are we trying to solve with our model?\nWe are trying to train a model that can predict the surrounding words of a given input. And this is how we have designed our training data too. Each training sample has (input, label). Where input is the given token, and label is some nearby token. It is like forcing the model to predict the context of an input word.\nHow can such a model be helpful to us?\nSuch a model that can predict the context of a word is not helpful to us in any actual scenario. So we are not going to use it. Instead, we are only interested in the learned weights of such a model, and we call them word embedding. It is like faking a problem (creating a pseudo-problem), training a model, and then using the learned weights for other tasks.\n\n# source: https://muhark.github.io/python/ml/nlp/2021/10/21/word2vec-from-scratch.html\nimport torch.nn as nn\n\nclass Word2Vec(nn.Module):\n    def __init__(self, vocab_size, embedding_size):\n        super().__init__()\n        self.embed = nn.Embedding(vocab_size, embedding_size)\n        self.expand = nn.Linear(embedding_size, vocab_size, bias=False)\n\n    def forward(self, input):\n        # Encode input to lower-dimensional representation\n        hidden = self.embed(input)\n        # Expand hidden layer to predictions\n        logits = self.expand(hidden)\n        return logits\n\nNotice that in the model configuration, the size of the output layer (nn.Linear) is equal to our vocabulary size. So, our model is configured for a multi-classification problem.\n\nEMBED_SIZE = 100\nmodel = Word2Vec(vb_size, EMBED_SIZE)\n\n# Relevant if you have a GPU\ndevice = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\nprint(device)\n\nmodel.to(device)\n\ncuda\n\n\nWord2Vec(\n  (embed): Embedding(3007, 100)\n  (expand): Linear(in_features=100, out_features=3007, bias=False)\n)\n\n\nNow configure the loss function and a training optimizer.\n\nimport torch\n\nLR = 3e-4  # learning rate\nloss_fn = nn.CrossEntropyLoss()\noptimizer = torch.optim.AdamW(model.parameters(), lr=LR)"
  },
  {
    "objectID": "posts/2022-12-02-pytorch-word2vec-embedding.html#model-training",
    "href": "posts/2022-12-02-pytorch-word2vec-embedding.html#model-training",
    "title": "Implementing Word2Vec with PyTorch",
    "section": "Model Training",
    "text": "Model Training\nIn this section we are going to train our model for 100 epochs.\n\n# source: https://muhark.github.io/python/ml/nlp/2021/10/21/word2vec-from-scratch.html\nfrom tqdm import tqdm  # For progress bar. https://github.com/tqdm/tqdm\n\nEPOCHS = 100\nprogress_bar = tqdm(range(EPOCHS * len(dataloader)))\nrunning_loss = []\n\nfor epoch in range(EPOCHS):\n    epoch_loss = 0\n    for center, context in dataloader:\n        center, context = center.to(device), context.to(device)\n\n        optimizer.zero_grad()\n        logits = model(input=context)\n        loss = loss_fn(logits, center)\n        epoch_loss += loss.item()\n        loss.backward()\n        optimizer.step()\n\n        progress_bar.update(1)\n    epoch_loss /= len(dataloader)\n    running_loss.append(epoch_loss)\n\n100%|█████████▉| 4699/4700 [05:59<00:00, 40.21it/s]\n\n\nLet’s plot the training loss.\n\nimport matplotlib.pyplot as plt\nplt.plot(running_loss)"
  },
  {
    "objectID": "posts/2022-12-02-pytorch-word2vec-embedding.html#analyze-word-embeddings-word-vectors",
    "href": "posts/2022-12-02-pytorch-word2vec-embedding.html#analyze-word-embeddings-word-vectors",
    "title": "Implementing Word2Vec with PyTorch",
    "section": "Analyze Word Embeddings (Word Vectors)",
    "text": "Analyze Word Embeddings (Word Vectors)\nTraining is complete, and our model has learned something. In this section, we will analyze the learned weights and their quality.\nSo, let’s extract the weights.\nWe have weights in two layers: embedding and linear. Both have similar dimensions, so which should we use as embeddings? I have experimented with weights from both these layers, and here are some suggestions.\n\nEmbeddings from the layer closer to the output layer give better results. In our case, it is the Linear layer. Our linear layer has the same dimensions as the embedding layer, and we will use weights from this layer.\nIf the dimensions of the layers closer to the output layer are different than the Embedding layer, then we should stick to the embedding layer. Sometimes, people prefer to concatenate weights from multiple layers to create the final output.\n\n\n# embedding = model.embed.weight.cpu().detach().numpy()\nembedding = model.expand.weight.cpu().detach().numpy()\nembedding.shape\n\n(3007, 100)\n\n\nNext, I have created some functions to calculate the distance between these word vectors. We are doing this to find a word similarity score with other words (cosine similarity). A smaller distance between two vectors means that these words are often used in similar contexts.\n\n# source: https://muhark.github.io/python/ml/nlp/2021/10/21/word2vec-from-scratch.html\nfrom scipy.spatial import distance\nimport numpy as np\n\ndef get_distance_matrix(embedding, metric):\n    dist_matrix = distance.squareform(distance.pdist(embedding, metric))\n    return dist_matrix\n\n\ndef get_k_similar_words(word, dist_matrix, k=10):\n    idx = vb[word]\n    dists = dist_matrix[idx]\n    ind = np.argpartition(dists, k)[: k + 1]\n    ind = ind[np.argsort(dists[ind])][1:]\n    out = [(i, vb.lookup_token(i), dists[i]) for i in ind]\n    return out\n\n\n# calculate 2d distance matrix\ndmat = get_distance_matrix(embedding, \"cosine\")\ndmat.shape\n\n(3007, 3007)\n\n\nAnother helper function is to print similar words identified based on their distance.\n\ndef similar_words(tokens):\n    for word in tokens:\n        print(word, [t[1] for t in get_k_similar_words(word, dmat)], \"\\n\")\n\nNext, I selected some tokens and used embeddings to find words most similar to them.\n\ntokens = [\"father\", \"mother\", \"boy\", \"girl\"]  # tokens for relations\ntokens += [\"job\", \"sad\", \"happy\", \"hate\"]  # for emotions\ntokens += [\"america\", \"england\", \"india\"]  # for regions\ntokens += [\"football\", \"swimming\", \"cycling\"]  # for sports\ntokens += [\"exercise\", \"health\", \"fitness\"]  # for health\n\nsimilar_words(tokens)\n\nfather ['dad', 'fathersday', 'day', 'happy', 'love', 'one', 'bihday', 'kid', 'u', 'today'] \n\nmother ['care', 'child', 'son', 'died', 'father', 'kill', 'men', 'ten', 'born', 'wife'] \n\nboy ['girl', 'guy', 'man', 'smile', 'little', 'family', 'love', 'selfie', 'day', 'face'] \n\ngirl ['smile', 'summer', 'love', 'guy', 'happy', 'fun', 'friend', 'boy', 'beautiful', 'today'] \n\njob ['one', 'amp', 'im', 'getting', 'got', 'going', 'first', 'even', 'get', 'cant'] \n\nsad ['make', 'people', 'life', 'know', 'dont', 'help', 'world', 'amp', 'like', 'say'] \n\nhappy ['day', 'today', 'love', 'great', 'life', 'make', 'amp', 'weekend', 'smile', 'one'] \n\nhate ['people', 'america', 'say', 'dont', 'even', 'many', 'still', 'world', 'much', 'amp'] \n\namerica ['hate', 'orlando', 'many', 'trump', 'people', 'say', 'even', 'american', 'still', 'shooting'] \n\nengland ['eng', 'football', 'euro2016', 'v', 'wale', 'soccer', 'match', 'russia', 'player', 'clinton'] \n\nindia ['received', 'test', 'sign', 'local', 'em', 'rude', 'amount', 'forget', 'called', 'shocking'] \n\nfootball ['england', 'euro2016', 'france', 'fan', 'v', 'making', 'review', 'match', 'game', 'award'] \n\nswimming ['swim', 'bbq', 'sunglass', 'ink', 'lovemylife', 'ceremony', 'placement', 'follow4follow', 'brunette', 'pool'] \n\ncycling ['pub', 'nutrition', 'musictherapy', 'exploring', 'letsgo', 'niece', 'cook', 'taste', 'pougal', 'mount'] \n\nexercise ['wellness', 'weightloss', 'lifeisgood', 'fitfam', 'inspire', 'madrid', 'namaste', 'runner', 'yoga', 'workout'] \n\nhealth ['healthy', 'food', 'happiness', 'amazing', 'fitness', 'beauty', 'gym', 'positive', 'run', 'lifestyle'] \n\nfitness ['workout', 'gym', 'fit', 'food', 'running', 'run', 'yoga', 'health', 'selfie', 'monday'] \n\n\n\n\nVisualize embeddings\nWe can also analyze the embeddings by visualizing them on a plot. Similar word vectors should appear closer in the plot. But each word vector has 300 dimensions, so how can we plot them on a 2D surface? For this, we can take the help of the dimension reduction technique t-SNE to reduce the word vectors dimensions from 300 to 2. PCA can also be applied to achieve similar results.\nWe need first to normalize the weights to get better results.\n\n# source: https://github.com/OlgaChernytska/word2vec-pytorch/blob/main/notebooks/Inference.ipynb\n# normalization of weights\nnorms = (embedding ** 2).sum(axis=1) ** (1 / 2)\nnorms = np.reshape(norms, (len(norms), 1))\nembedding_norm = embedding / norms\n\nembedding_norm.shape\n\n(3007, 100)\n\n\nNow apply t-distributed Stochastic Neighbor Embedding (t-SNE) to reduce the dimensions of our embedding.\n\n# source: https://github.com/OlgaChernytska/word2vec-pytorch/blob/main/notebooks/Inference.ipynb\nfrom sklearn.manifold import TSNE\n\n# embeddings DataFrame\nembedding_df = pd.DataFrame(embedding)\n\n# t-SNE transform\ntsne = TSNE(n_components=2)\nembedding_df_trans = tsne.fit_transform(embedding_df)\nembedding_df_trans = pd.DataFrame(embedding_df_trans)\n\n/usr/local/lib/python3.8/dist-packages/sklearn/manifold/_t_sne.py:780: FutureWarning: The default initialization in TSNE will change from 'random' to 'pca' in 1.2.\n  warnings.warn(\n/usr/local/lib/python3.8/dist-packages/sklearn/manifold/_t_sne.py:790: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n  warnings.warn(\n100%|██████████| 4700/4700 [06:10<00:00, 40.21it/s]\n\n\nWe have some tokens selected in the last section. Let’s highlight them in the plot to locate them easily. For this, I have created a color coding for their indices.\n\n# get token order\nembedding_df_trans.index = vb.get_itos()\n\n# create color codes for selected tokens\ncolor_codes = []\nfor s in embedding_df_trans.index:\n    if s in tokens:\n        color_codes.append(True)\n    else:\n        color_codes.append(False)\n\nNow create a scatter plot of these embeddings using Plotly.\n\n# source: https://github.com/OlgaChernytska/word2vec-pytorch/blob/main/notebooks/Inference.ipynb\nimport plotly.graph_objects as go\n\ncolor = np.where(color_codes, \"red\", \"grey\")\nfig = go.Figure()\n\nfig.add_trace(\n    go.Scatter(\n        x=embedding_df_trans[0],\n        y=embedding_df_trans[1],\n        mode=\"text\",\n        text=embedding_df_trans.index,\n        textposition=\"middle center\",\n        textfont=dict(color=color),\n    )\n)\n\n\nI am zooming in on some of the highlighted tokens at the top right corner.\n\nFurther zooming."
  },
  {
    "objectID": "posts/2022-12-14-pytorch-autoencoder.html#credits",
    "href": "posts/2022-12-14-pytorch-autoencoder.html#credits",
    "title": "Implementing AutoEncoder with PyTorch",
    "section": "Credits",
    "text": "Credits\nThis notebook takes inspiration and ideas from the following sources.\n\nIntroduction to Deep Learning course by Sebastian Raschka. Sebastian is one of my favorite instructors. You can find all his lectures free on his YouTube channel, with tons of practice material he regularly shares on his blog. Lectures I find helpful on this topic include L16 AutoEncoder. Parts of the code in this notebook are taken from his course material.\n\nL16.0 Introduction to Autoencoders – Lecture Overview\nL16.1 Dimensionality Reduction\nL16.2 A Fully-Connected Autoencoder\nL16.3 Convolutional Autoencoders & Transposed Convolutions\nL16.4 A Convolutional Autoencoder in PyTorch\nL16.5 Other Types of Autoencoders\nNotebook: 1_VAE_mnist_sigmoid_mse.ipynb\n\nModern Computer Vision with PyTorch book published by Packt has tons of useful material on its GitHub repository. Chapter 11 from this book is related to AutoEncoder. Parts of the code you see in this notebook are taken from the following notebooks.\n\nNotebook: simple_auto_encoder_with_different_latent_size.ipynb\nNotebook: conv_auto_encoder.ipynb"
  },
  {
    "objectID": "posts/2022-12-14-pytorch-autoencoder.html#environment",
    "href": "posts/2022-12-14-pytorch-autoencoder.html#environment",
    "title": "Implementing AutoEncoder with PyTorch",
    "section": "Environment",
    "text": "Environment\nThis notebook is prepared with Google Colab.\n\nGitHub: 2022-12-14-pytorch-autoencoder.ipynb\n\n\n\n\nCode\nfrom platform import python_version\nimport numpy, matplotlib, pandas, torch\n\nprint(\"python==\" + python_version())\nprint(\"numpy==\" + numpy.__version__)\nprint(\"torch==\" + torch.__version__)\nprint(\"matplotlib==\" + matplotlib.__version__)\n\n\npython==3.8.16\nnumpy==1.21.6\ntorch==1.13.0+cu116\nmatplotlib==3.2.2\n\n\nLet’s also set the device and seed for results reproducibility.\n\n\nCode\nimport torch, os, random\nimport numpy as np\n\n# https://wandb.ai/sauravmaheshkar/RSNA-MICCAI/reports/How-to-Set-Random-Seeds-in-PyTorch-and-Tensorflow--VmlldzoxMDA2MDQy\ndef set_seed(seed: int = 42) -> None:\n    np.random.seed(seed)\n    random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    # When running on the CuDNN backend, two further options must be set\n    if torch.cuda.is_available():\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\n    # Set a fixed value for the hash seed\n\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    print(f\"Random seed set as {seed}\")\n\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Device:\", device)\nset_seed()\n\n\nDevice: cuda\nRandom seed set as 42"
  },
  {
    "objectID": "posts/2022-12-14-pytorch-autoencoder.html#introduction",
    "href": "posts/2022-12-14-pytorch-autoencoder.html#introduction",
    "title": "Implementing AutoEncoder with PyTorch",
    "section": "Introduction",
    "text": "Introduction\n\nFollowing is the AutoEncoder explanation provided by OpenAI ChatGPT. If unsatisfied, I suggest using the links provided in the “Credits” section (L16.0 Introduction to Autoencoders by Sebastian Raschka)\n\nprompt > introduction to autoencoder\nAutoEncoder\nAn autoencoder is a type of artificial neural network that is used to learn efficient data codings in an unsupervised manner. Autoencoders are composed of two parts: an encoder and a decoder. The encoder takes in an input and converts it into a compressed representation, typically in the form of a lower-dimensional code. The decoder then takes this code and converts it back into a reconstruction of the original input.\nThe goal of an autoencoder is to learn a representation of the input data that is more efficient than the original input. This is achieved by training the network to reconstruct the input from the lower-dimensional code, which forces the encoder to learn a compact representation of the data. Autoencoders can be used for dimensionality reduction, feature learning, and generating new data samples.\n\n\n\nFigure 1: A basic fully connected AutoEncoder\n\n\nFrom these descriptions, we can deduce the following points about an autoencoder working.\n\nAn autoencoder can learn a compact representation of the data. This representation is stored in the center hidden layer of the network. We have multiple names for this center hidden layer, including bottleneck, latent space, embedded space, and hidden units.\nThe latent space has a dimension less than the input data dimension.\nAutoencoder can be used for dimensionality reduction. In fact, if we don’t use any non-linearity (e.g., ReLU) then autoencoder will function similarly to PCA since PCA is a linear dimensionality reduction method.\nEncoder part of the autoencoder model compresses the data to a latent space. The decoder can use the latent space to reconstruct (or decode) the original image.\n\nSummary of the steps followed in this notebook\n\nDownload MNIST handwritten digit dataset\nTrain an autoencoder with 2-dimensional (2 pixels) latent space. Use 2 pixels to deconstruct the whole MNIST digit image. Visualize the latent space learned by the model. Use random latent space points to decode the images\nCreate multiple autoencoders with varying latent space [2, 5, 10, 20, 50] and use them to decode the images. Compare the results to analyze the effect of latent dimension in storing information and decoded image quality.\nVisualize the latent space of the model with a latent space of 50 dimensions. Use random points from the latent space to decode (or construct new) images.\nFinally, discuss the limitations of autoencoders."
  },
  {
    "objectID": "posts/2022-12-14-pytorch-autoencoder.html#data-preparation",
    "href": "posts/2022-12-14-pytorch-autoencoder.html#data-preparation",
    "title": "Implementing AutoEncoder with PyTorch",
    "section": "Data Preparation",
    "text": "Data Preparation\n\nLoad MNIST Dataset\nIn the next cell, I downloaded the MNIST dataset and created a DataLoader with a batch size of 256.\n\nfrom torchvision import datasets\nfrom torch.utils.data import DataLoader\nfrom torchvision import transforms\n\nbatch_size = 256\n\ntrain_dataset = datasets.MNIST(\n    root=\"data\", train=True, transform=transforms.ToTensor(), download=True\n)\n\ntest_dataset = datasets.MNIST(root=\"data\", train=False, transform=transforms.ToTensor())\n\ntrain_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=False)\n\ntest_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n\nDownloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\nDownloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to data/MNIST/raw/train-images-idx3-ubyte.gz\n\n\n\n\n\nExtracting data/MNIST/raw/train-images-idx3-ubyte.gz to data/MNIST/raw\n\nDownloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\nDownloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to data/MNIST/raw/train-labels-idx1-ubyte.gz\n\n\n\n\n\nExtracting data/MNIST/raw/train-labels-idx1-ubyte.gz to data/MNIST/raw\n\nDownloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\nDownloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to data/MNIST/raw/t10k-images-idx3-ubyte.gz\n\n\n\n\n\nExtracting data/MNIST/raw/t10k-images-idx3-ubyte.gz to data/MNIST/raw\n\nDownloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\nDownloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n\n\n\n\n\nExtracting data/MNIST/raw/t10k-labels-idx1-ubyte.gz to data/MNIST/raw\n\n\n\nNotice that while creating Datasets, I used a transformer transform=transforms.ToTensor(). torchvision.transforms.ToTensor.html\n\n[transforms.ToTensor] Convert a PIL Image or numpy.ndarray to tensor … Converts a PIL Image or numpy.ndarray (H x W x C) in the range [0, 255] to a torch.FloatTensor of shape (C x H x W) in the range [0.0, 1.0].\n\nSo it manipulates the MNSIT images in two ways\n\nConverts image dimensions from (H x W x C) to (C x H x W)\nScales image pixel values from the range [0, 255] to range [0.0, 1.0]\n\nLet’s check the dimension of our dataset.\n\nfor images, labels in train_loader:\n    print(\n        \"Image batch dimensions:\", images.shape\n    )  # [batch size, channels, img height, img width]\n    print(\"Image label dimensions:\", labels.shape)  # [batch size]\n    break\n\nImage batch dimensions: torch.Size([256, 1, 28, 28])\nImage label dimensions: torch.Size([256])"
  },
  {
    "objectID": "posts/2022-12-14-pytorch-autoencoder.html#model-configuration",
    "href": "posts/2022-12-14-pytorch-autoencoder.html#model-configuration",
    "title": "Implementing AutoEncoder with PyTorch",
    "section": "Model configuration",
    "text": "Model configuration\nLet’s configure a simple AutoEncoder model. It is made of two fully connected multilayer perceptrons. The first perceptron will gradually decrease the dimensions of the input data till it reaches the latent dimension size. The second perceptron will gradually increase the dimensions of data obtained from latent space till it reaches the input size. Notice that\n\nLatent space dimension (latent_dim) is kept configurable so we can use the same model class to configure autoencoder having different latent spaces\nI have used torch.sigmoid() in the forward pass. This is to squash the data into the range [0, 1]. This is done because the input image received by the model is assumed to be in this range (remember transforms.ToTensor() function while creating datasets). So we want the image created (or returned) by the model to be in the same distribution range.\n\n\nimport torch.nn as nn\n\nclass AutoEncoder(nn.Module):\n    def __init__(self, num_features, latent_dim):\n        super().__init__()\n        self.latent_dim = latent_dim\n\n        self.encoder = nn.Sequential(\n            nn.Linear(num_features, 128),\n            nn.ReLU(True),\n            nn.Linear(128, 64),\n            nn.ReLU(True),\n            nn.Linear(64, latent_dim),\n        )\n\n        self.decoder = nn.Sequential(\n            nn.Linear(latent_dim, 64),\n            nn.ReLU(True),\n            nn.Linear(64, 128),\n            nn.ReLU(True),\n            nn.Linear(128, num_features),\n        )\n\n    def forward(self, x):\n        x = self.encoder(x)\n        x = self.decoder(x)\n        x = torch.sigmoid(x)\n        return x"
  },
  {
    "objectID": "posts/2022-12-14-pytorch-autoencoder.html#autoencoder-with-latent-space-dimension2",
    "href": "posts/2022-12-14-pytorch-autoencoder.html#autoencoder-with-latent-space-dimension2",
    "title": "Implementing AutoEncoder with PyTorch",
    "section": "AutoEncoder with latent space dimension=2",
    "text": "AutoEncoder with latent space dimension=2\nLet’s create our first autoencoder with a latent space of 2 features. This means we will compress our input image of 28*28=784 features to only two in the latent space. Then we will use the information stored in these two features to reconstruct (or decode) the full image having 784 features. Why only two features for the latent space?\n\nWe want to try an extreme case, and you will be surprised to see that even with two features in latent space, we can construct the whole image with acceptable quality.\nWe want to visualize the 2D latent space. Visualizing (plotting) latent space with higher dimensions will not be possible.\n\n\nlearning_rate = 0.005\nnum_features = 28 * 28  # image height X image width = 784\nlatent_dim = 2\n\nmodel = AutoEncoder(num_features, latent_dim)\nmodel = model.to(device)\noptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n\nWe have configured our model and optimizer, and we can start training our model.\n\nimport time\nimport torch.nn.functional as F\n\nnum_epochs = 10\nstart_time = time.time()\n\nfor epoch in range(num_epochs):\n    for batch_idx, (features, targets) in enumerate(train_loader):\n        # don't need labels, only the images (features)\n        features = features.view(-1, num_features).to(device)\n\n        ### FORWARD AND BACK PROP\n        decoded = model(features)\n        loss = F.mse_loss(decoded, features)\n        optimizer.zero_grad()\n\n        loss.backward()\n\n        ### UPDATE MODEL PARAMETERS\n        optimizer.step()\n\n        ### LOGGING\n        if not batch_idx % 50:\n            print(\n                \"Epoch: %03d/%03d | Batch %03d/%03d | Training Loss: %.4f\"\n                % (epoch + 1, num_epochs, batch_idx, len(train_loader), loss)\n            )\n\nprint(\"Total Training Time: %.2f min\" % ((time.time() - start_time) / 60))\n\nEpoch: 001/010 | Batch 000/235 | Training Loss: 0.2315\nEpoch: 001/010 | Batch 050/235 | Training Loss: 0.0655\nEpoch: 001/010 | Batch 100/235 | Training Loss: 0.0523\nEpoch: 001/010 | Batch 150/235 | Training Loss: 0.0517\nEpoch: 001/010 | Batch 200/235 | Training Loss: 0.0511\nEpoch: 002/010 | Batch 000/235 | Training Loss: 0.0479\nEpoch: 002/010 | Batch 050/235 | Training Loss: 0.0486\nEpoch: 002/010 | Batch 100/235 | Training Loss: 0.0441\nEpoch: 002/010 | Batch 150/235 | Training Loss: 0.0456\nEpoch: 002/010 | Batch 200/235 | Training Loss: 0.0461\nEpoch: 003/010 | Batch 000/235 | Training Loss: 0.0438\nEpoch: 003/010 | Batch 050/235 | Training Loss: 0.0460\nEpoch: 003/010 | Batch 100/235 | Training Loss: 0.0423\nEpoch: 003/010 | Batch 150/235 | Training Loss: 0.0442\nEpoch: 003/010 | Batch 200/235 | Training Loss: 0.0439\nEpoch: 004/010 | Batch 000/235 | Training Loss: 0.0415\nEpoch: 004/010 | Batch 050/235 | Training Loss: 0.0447\nEpoch: 004/010 | Batch 100/235 | Training Loss: 0.0411\nEpoch: 004/010 | Batch 150/235 | Training Loss: 0.0423\nEpoch: 004/010 | Batch 200/235 | Training Loss: 0.0425\nEpoch: 005/010 | Batch 000/235 | Training Loss: 0.0406\nEpoch: 005/010 | Batch 050/235 | Training Loss: 0.0434\nEpoch: 005/010 | Batch 100/235 | Training Loss: 0.0404\nEpoch: 005/010 | Batch 150/235 | Training Loss: 0.0419\nEpoch: 005/010 | Batch 200/235 | Training Loss: 0.0415\nEpoch: 006/010 | Batch 000/235 | Training Loss: 0.0397\nEpoch: 006/010 | Batch 050/235 | Training Loss: 0.0435\nEpoch: 006/010 | Batch 100/235 | Training Loss: 0.0399\nEpoch: 006/010 | Batch 150/235 | Training Loss: 0.0412\nEpoch: 006/010 | Batch 200/235 | Training Loss: 0.0405\nEpoch: 007/010 | Batch 000/235 | Training Loss: 0.0391\nEpoch: 007/010 | Batch 050/235 | Training Loss: 0.0426\nEpoch: 007/010 | Batch 100/235 | Training Loss: 0.0394\nEpoch: 007/010 | Batch 150/235 | Training Loss: 0.0409\nEpoch: 007/010 | Batch 200/235 | Training Loss: 0.0398\nEpoch: 008/010 | Batch 000/235 | Training Loss: 0.0389\nEpoch: 008/010 | Batch 050/235 | Training Loss: 0.0423\nEpoch: 008/010 | Batch 100/235 | Training Loss: 0.0392\nEpoch: 008/010 | Batch 150/235 | Training Loss: 0.0405\nEpoch: 008/010 | Batch 200/235 | Training Loss: 0.0398\nEpoch: 009/010 | Batch 000/235 | Training Loss: 0.0377\nEpoch: 009/010 | Batch 050/235 | Training Loss: 0.0429\nEpoch: 009/010 | Batch 100/235 | Training Loss: 0.0386\nEpoch: 009/010 | Batch 150/235 | Training Loss: 0.0403\nEpoch: 009/010 | Batch 200/235 | Training Loss: 0.0392\nEpoch: 010/010 | Batch 000/235 | Training Loss: 0.0377\nEpoch: 010/010 | Batch 050/235 | Training Loss: 0.0413\nEpoch: 010/010 | Batch 100/235 | Training Loss: 0.0384\nEpoch: 010/010 | Batch 150/235 | Training Loss: 0.0404\nEpoch: 010/010 | Batch 200/235 | Training Loss: 0.0388\nTotal Training Time: 0.96 min\n\n\nNote that in the above training loop.\n\nfeatures = input images\ndecoded = decoded or reconstructed images from latent space. This is because it runs the forward pass when we predict from our model (decoded = model(features)). And we know that during the forward pass, we will first encode the input image, then use the output (latent representation) to reconstruct the image using the decoder.\n\nLet’s plot these original images and decoded ones to see how much we are successful in doing that.\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nn_images = 15\nimage_width = 28\n\nfig, axes = plt.subplots(nrows=2, ncols=n_images, sharex=True, sharey=True, figsize=(20, 2.5))\n\norig_images = features[:n_images]\ndecoded_images = decoded[:n_images]\nlabel_images = targets[:n_images]\n\nfor i in range(n_images):\n    for ax, img in zip(axes, [orig_images, decoded_images]):\n        curr_img = img[i].detach().to(torch.device('cpu'))\n        ax[i].imshow(curr_img.view((image_width, image_width)), cmap='binary')\n\n\n\n\nThese results are remarkable, considering we only use information from two pixels to generate a complete image of 784 pixels. So what information is stored in those two pixels (embedding)? Let’s also print embeddings for the above generated images.\n\nfor feature, target in zip(orig_images, label_images):\n    feature = feature.view(-1, num_features).to(device)\n    with torch.no_grad():\n        embedding = model.encoder(feature)\n\n    print(f\"Label: {target}, Embedding: {embedding.cpu().numpy()}\")\n\nLabel: 3, Embedding: [[-1.6004068  -0.96022624]]\nLabel: 4, Embedding: [[3.5773783 1.7578685]]\nLabel: 5, Embedding: [[ 9.145074 -2.872149]]\nLabel: 6, Embedding: [[ 1.7183754 -2.08906  ]]\nLabel: 7, Embedding: [[10.19044   9.111174]]\nLabel: 8, Embedding: [[-2.115856   1.4071143]]\nLabel: 9, Embedding: [[6.411824  5.6826334]]\nLabel: 0, Embedding: [[  8.0015   -13.730832]]\nLabel: 1, Embedding: [[-7.0476646  6.571097 ]]\nLabel: 2, Embedding: [[-5.4590693  2.0496612]]\nLabel: 3, Embedding: [[-1.0255219 -0.9620053]]\nLabel: 4, Embedding: [[14.75902   7.228238]]\nLabel: 8, Embedding: [[14.938157   -0.26024085]]\nLabel: 9, Embedding: [[8.782047 6.697904]]\nLabel: 0, Embedding: [[ 13.757708 -11.047596]]\n\n\nBy analyzing these values, we can say that our autoencoder has learned to use slightly different ranges for embeddings each of the digits. Next, let’s try to visualize the distribution of latent space for the complete training set. For this, I have used a helper function. This function creates scatter plots of the latent space for all the digits.\n\nimport numpy as np\nimport matplotlib.colors as mcolors\n\n# https://github.com/rasbt/stat453-deep-learning-ss21/blob/main/L17/helper_plotting.py\ndef plot_latent_space_with_labels(num_classes, data_loader, encoding_fn, device):\n    d = {i: [] for i in range(num_classes)}\n\n    with torch.no_grad():\n        for i, (features, targets) in enumerate(data_loader):\n\n            features = features.view(-1, num_features).to(device)\n            targets = targets.to(device)\n\n            embedding = encoding_fn(features)\n\n            for i in range(num_classes):\n                if i in targets:\n                    mask = targets == i\n                    d[i].append(embedding[mask].to(\"cpu\").numpy())\n\n    colors = list(mcolors.TABLEAU_COLORS.items())\n    plt.figure(figsize=(8, 8))\n    for i in range(num_classes):\n        d[i] = np.concatenate(d[i])\n\n        plt.scatter(d[i][:, 0], d[i][:, 1], color=colors[i][1], label=f\"{i}\", alpha=0.5)\n\n    plt.legend()\n\nLet’s create the plot.\n\nplot_latent_space_with_labels(\n    num_classes=len(train_dataset.classes),  # 10\n    data_loader=train_loader,\n    encoding_fn=model.encoder,\n    device=device,\n)\n\nplt.legend()\nplt.show()\n\n\n\n\nThis plot tells us different digits are occupying different areas in the latent space. If we want to draw a sample for a number (e.g., 0), we must take the point from its range. It is a 2D plot, so If I take a sample from it, I will get two points (x, y), and then I can use that as an embedding. I can then reconstruct an image using the decoder of my model from this embedding.\nIn the next cell, I have created a helper function that can take the embeddings list and decode an image from it.\n\ndef decode_images(model, embedding_list):\n    fig, axs = plt.subplots(1, len(embedding_list), figsize=(20, 2.5))\n\n    for i, (label, embedding) in enumerate(embedding_list):\n        with torch.no_grad():\n            new_image = model.decoder(torch.tensor(embedding).to(device))\n\n        new_image = torch.sigmoid(new_image)\n        new_image = new_image.view(28, 28)\n        new_image = new_image.detach().to(torch.device(\"cpu\"))\n\n        axs[i].set_title(label)\n        axs[i].imshow(new_image, cmap=\"binary\")\n\n    plt.show()\n\nNext, I created some embeddings using the above plot.\n\nembedding_list = [\n    (\"0\", [5.0, -10.0]),\n    (\"1\", [-4.0, 13.0]),\n    (\"2\", [-5.4, 2.0]),\n    (\"2 overlap with 3\", [-4.0, 0.0]),\n    (\"3 ouside sample space\", [-5.0, -5.0]),\n    (\"7\", [6.0, 11.0]),\n    (\"9 overlap with 8\", [0.0, 0.0]),\n    (\"5\", [9.0, -3.0]),\n]\n\nLet’s use these embeddings to generate some images. Note a few points.\n\nThere are no clear boundaries between the digits’ latent space\nIf you select a point that is on the edge or at the boundary of the space, then you may get a more distorted image\n\nDigit 2 image that has some overlap with digit 3\nDigit 9 image that has some overlap with 8\n\nThere is no definite range of the latent space\n\nDigit 3 image generated from the embedding range not visible in the plot\n\n\n\ndecode_images(model, embedding_list)"
  },
  {
    "objectID": "posts/2022-12-14-pytorch-autoencoder.html#autoencoder-with-latent-space-dimension5-10-20-50",
    "href": "posts/2022-12-14-pytorch-autoencoder.html#autoencoder-with-latent-space-dimension5-10-20-50",
    "title": "Implementing AutoEncoder with PyTorch",
    "section": "AutoEncoder with latent space dimension=[5, 10, 20, 50]",
    "text": "AutoEncoder with latent space dimension=[5, 10, 20, 50]\nWe have seen the quality of decoded images from 2d latent space. Our understanding is that if we increase the dimension, it will retain more information and improve the image quality.\nNext, I have created a function that can take a model to train it. We will iteratively create models with increasing dimensions and use this function to train them.\n\ndef training_loop(train_loader, model, optimizer, device, num_epochs=10):\n    start_time = time.time()\n    for epoch in range(num_epochs):\n        for batch_idx, (features, targets) in enumerate(train_loader):\n            # don't need labels, only the images (features)\n            features = features.view(-1, num_features).to(device)\n\n            ### FORWARD AND BACK PROP\n            decoded = model(features)\n            loss = F.mse_loss(decoded, features)\n            optimizer.zero_grad()\n\n            loss.backward()\n\n            ### UPDATE MODEL PARAMETERS\n            optimizer.step()\n\n        ### LOGGING\n        print(\"Epoch: %03d | Training Loss: %.4f\" % (epoch + 1, loss))\n\n    print(\"Total Training Time: %.2f min\" % ((time.time() - start_time) / 60))\n\n\nmodels_list = [model]  # model with latent_dim=2\nlatent_dimensions = [5, 10, 20, 50]\n\nfor latent_dim in latent_dimensions:\n    model_n = AutoEncoder(num_features, latent_dim)\n    model_n = model_n.to(device)\n    optimizer_n = torch.optim.Adam(model_n.parameters(), lr=learning_rate)\n    models_list.append(model_n)\n\n    print(f\"\\n*** Training AutoEncoder with latent_dim={latent_dim} ***\\n\")\n    training_loop(train_loader, model_n, optimizer_n, device)\n\n\n*** Training AutoEncoder with latent_dim=5 ***\n\nEpoch: 001 | Training Loss: 0.0396\nEpoch: 002 | Training Loss: 0.0315\nEpoch: 003 | Training Loss: 0.0292\nEpoch: 004 | Training Loss: 0.0278\nEpoch: 005 | Training Loss: 0.0270\nEpoch: 006 | Training Loss: 0.0262\nEpoch: 007 | Training Loss: 0.0256\nEpoch: 008 | Training Loss: 0.0251\nEpoch: 009 | Training Loss: 0.0248\nEpoch: 010 | Training Loss: 0.0247\nTotal Training Time: 0.92 min\n\n*** Training AutoEncoder with latent_dim=10 ***\n\nEpoch: 001 | Training Loss: 0.0372\nEpoch: 002 | Training Loss: 0.0279\nEpoch: 003 | Training Loss: 0.0235\nEpoch: 004 | Training Loss: 0.0209\nEpoch: 005 | Training Loss: 0.0194\nEpoch: 006 | Training Loss: 0.0186\nEpoch: 007 | Training Loss: 0.0180\nEpoch: 008 | Training Loss: 0.0176\nEpoch: 009 | Training Loss: 0.0175\nEpoch: 010 | Training Loss: 0.0170\nTotal Training Time: 0.90 min\n\n*** Training AutoEncoder with latent_dim=20 ***\n\nEpoch: 001 | Training Loss: 0.0392\nEpoch: 002 | Training Loss: 0.0290\nEpoch: 003 | Training Loss: 0.0244\nEpoch: 004 | Training Loss: 0.0212\nEpoch: 005 | Training Loss: 0.0193\nEpoch: 006 | Training Loss: 0.0181\nEpoch: 007 | Training Loss: 0.0168\nEpoch: 008 | Training Loss: 0.0158\nEpoch: 009 | Training Loss: 0.0150\nEpoch: 010 | Training Loss: 0.0144\nTotal Training Time: 0.92 min\n\n*** Training AutoEncoder with latent_dim=50 ***\n\nEpoch: 001 | Training Loss: 0.0383\nEpoch: 002 | Training Loss: 0.0273\nEpoch: 003 | Training Loss: 0.0225\nEpoch: 004 | Training Loss: 0.0197\nEpoch: 005 | Training Loss: 0.0172\nEpoch: 006 | Training Loss: 0.0159\nEpoch: 007 | Training Loss: 0.0145\nEpoch: 008 | Training Loss: 0.0137\nEpoch: 009 | Training Loss: 0.0130\nEpoch: 010 | Training Loss: 0.0120\nTotal Training Time: 0.91 min\n\n\nVerify the latent space dimension of our models.\n\nfor model_n in models_list:\n    print(model_n.latent_dim)\n\n2\n5\n10\n20\n50\n\n\nNext, we will compare the results from these models. This time I have used the test dataset that our model has not seen. From the results, you may note that the results from the models with latent_dim 2 and 5 are blurry and make mistakes while decoding the input image. In comparison, the results from the rest are more accurate and of better quality.\nIt also tells us that we can significantly compress the images without losing much information. For example, our model_50 (model with latent_dim=50) can generate an original image with 16x less information. But this performance also depends on the type of images used. For example, we have black-and-white digit images with large white spaces, so we were able to compress them significantly. On the other hand, we may get different performances with images having more patterns.\n\nimg_sample_count = 25\n\nfor idx in range(img_sample_count):\n    # get an image from test_dataset\n    input_img, _ = test_dataset[idx]\n    # flatten the image\n    input_img = input_img.view(-1, num_features).to(device)\n\n    fig, ax = plt.subplots(1, len(models_list) + 1, figsize=(10, 4))\n\n    # iterate over all the models list\n    for i, model_n in enumerate(models_list):\n        # put the model in eval mode to stop accumulating gradients\n        model_n.eval()\n        # make a prediction\n        decoded_img = model_n(input_img)\n        # squach the prediction between [0,1]\n        decoded_img = torch.sigmoid(decoded_img)\n        # detach the prediction and convert back to 28,28 shape\n        decoded_img = decoded_img.detach().to(torch.device(\"cpu\"))\n        decoded_img = decoded_img.view(28, 28)\n\n        ax[i + 1].set_title(f\"prediction\\nlatent-dim:{model_n.latent_dim}\")\n        ax[i + 1].imshow(decoded_img, cmap=\"binary\")\n\n    # plot the input image\n    ax[0].set_title(f\"input: {idx}\")\n    input_img = input_img.detach().to(torch.device(\"cpu\"))\n    input_img = input_img.view(28, 28)\n    ax[0].imshow(input_img, cmap=\"binary\")\n\n    plt.tight_layout()\n    plt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFrom model_50, we are getting considerably good results compared to model_2 and model_5. The last time we visualized the embeddings for model_2, we also found that the latent space of digits was overlapping. Since the output quality of model_50 has improved, the overlap in the latent space should be less. Let’s confirm this assumption.\nFor this, we will first gather the embeddings of the test images. Remember that the output from the model.encoder is the embedding of an image (or compressed representation).\n\nlatent_vectors = []\nclasses = []\nmodel_50 = models_list[-1]\n\nprint(\"model_50 latent_dim: \", model_50.latent_dim)\n\nmodel_50 latent_dim:  50\n\n\nIn the next cell, I am collecting the embeddings. Note that each fetch from the DataLoader iterator returns a batch of images (batch_size=256).\n\nfor images_batch,labels_batch in test_loader:\n    # images_batch.shape = torch.Size([256, 1, 28, 28]\n    # labels_batch.shape = torch.Size([256])\n    # images_batch.view(-1, num_features).shape = torch.Size([256, 784])\n    # encoded_batch.shape =torch.Size([256, 50])\n\n    images_batch = images_batch.view(-1, num_features).to(device)\n    encoded_batch = model_50.encoder(images_batch)\n\n    latent_vectors.append(encoded_batch)\n    classes.extend(labels_batch)    \n\nLet’s confirm the dimensions.\n\nlen(latent_vectors) = 40. It means we have 40 latent vectors. Each vector holds the embeddings for a batch of images\nlatent_vectors[0].shape = torch.Size([256, 50]. It means that each latent vector holds embeddings of size 50 for a batch of 256 images\ntorch.cat(latent_vectors).shape = torch.Size([10000, 50]). It means that if we concatenate the embeddings of all the images, we will have a tensor of 10,000 rows with 50 dimensions (or features/columns)\n\n\nlen(latent_vectors), latent_vectors[0].shape, torch.cat(latent_vectors).shape\n\n(40, torch.Size([256, 50]), torch.Size([10000, 50]))\n\n\n\n# concatenate all test images embeddings\nlatent_vectors_cat = torch.cat(latent_vectors).cpu().detach().numpy()\nlatent_vectors_cat.shape\n\n(10000, 50)\n\n\nNow that we have all the embeddings, we can visualize them too. But how can we visualize embeddings with 50 features? We can use sklearn.manifold.TSNE.html.\nchatGPT prompt > explain T-distributed Stochastic Neighbor Embedding\nT-distributed Stochastic Neighbor Embedding (t-SNE) is a machine learning algorithm that is used to reduce the dimensionality of high-dimensional data and visualize it in a lower-dimensional space. It works by measuring the similarity between the data points and representing them as points in a lower-dimensional space, such that similar data points are grouped together, and dissimilar ones are separated. This allows for better visualization and understanding of the structure of the data.\n\nfrom sklearn.manifold import TSNE\n\ntsne = TSNE(2)\nclustered = tsne.fit_transform(latent_vectors_cat)\n\nfig = plt.figure(figsize=(12, 10))\ncmap = plt.get_cmap(\"Spectral\", 10)\nplt.scatter(*zip(*clustered), c=classes, cmap=cmap)\nplt.colorbar(drawedges=True)\n\n/usr/local/lib/python3.8/dist-packages/sklearn/manifold/_t_sne.py:780: FutureWarning: The default initialization in TSNE will change from 'random' to 'pca' in 1.2.\n  warnings.warn(\n/usr/local/lib/python3.8/dist-packages/sklearn/manifold/_t_sne.py:790: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n  warnings.warn(\n\n\n<matplotlib.colorbar.Colorbar at 0x7f686a71de50>\n\n\n\n\n\nWhat does this plot tell us?\n\nIt shows that using a latent space of higher dimension, there is less overlap in digits’ latent space\nIt also tells us that these digits’ distribution of latent space is discreet and has no boundary. Meaning that the range of values of these embeddings is not continuous\n\nembedding values for 0 share a range between 0 to 20\nembedding values for 1 share a range between -40 to -60\nthere can be areas in between that choosing a sample from it may result in a distorted image that is neither 1 or 0\n\n\nWe can prove this by adding some noise to these embeddings and trying to regenerate images from them.\n\n# step 1: transpose to get 50 embeddings with 10000 features\n# len(latent_vectors_transpose) = 50\n\nlatent_vectors_transpose = latent_vectors_cat.transpose(1, 0)\nlatent_vectors_cat.shape, latent_vectors_transpose.shape\n\n((10000, 50), (50, 10000))\n\n\n\n# step 2: add some noise.\n# take each row, calucate mean and std\n# use mean and std to generate 100 new features. each time add some noise to std\n\nrand_vectors = []  # randomized latent vectors\nfor col in latent_vectors_transpose:\n    mu, sigma = col.mean(), col.std()\n    rand_vectors.append(sigma * torch.randn(1, 100) + mu)\n\n\n# step 3: verify dimensions\n\nlen(rand_vectors), rand_vectors[0].shape, torch.cat(rand_vectors).shape\n\n(50, torch.Size([1, 100]), torch.Size([50, 100]))\n\n\n\n# step 4: concat 100 features\n\nrand_vectors_cat = torch.cat(rand_vectors)\nrand_vectors_cat.shape\n\ntorch.Size([50, 100])\n\n\n\n# step 5: transpose back to have 100 embeddings of dimension 50\n\nrand_vectors_transpose = rand_vectors_cat.transpose(1, 0)\nrand_vectors_cat.shape, rand_vectors_transpose.shape\n\n(torch.Size([50, 100]), torch.Size([100, 50]))\n\n\nWe have generated 100 new (random) embeddings by adding some noise to the original embeddings. We have done it in a way so that these embeddings do not represent any digit. They are like a mix of embeddings of all the numbers. So when we try to use them, they will generate distorted images. Similar to decoded images generated by taking a sample from an overlapping latent space.\n\nrand_vectors_transpose = rand_vectors_transpose.to(device)\n\nfig, ax = plt.subplots(10, 10, figsize=(20, 20))\nax = iter(ax.flat)\nfor idx, rand_latent_vector in enumerate(rand_vectors_transpose):\n    decoded_img = model_50.decoder(rand_latent_vector)\n    decoded_img = torch.sigmoid(decoded_img)\n    decoded_img = decoded_img.view(28, 28)\n    decoded_img = decoded_img.detach().to(torch.device(\"cpu\"))\n    ax[idx].imshow(decoded_img, cmap=\"binary\")\n\n\n\n\nWhat does this plot tell us? It shows that the latent space of the (vanilla) autoencoder is restricted to specific ranges and has discrete values. Therefore, we cannot use any value from latent space to generate an image. It can be a problem since it is difficult to know an autoencoder’s range of latent space beforehand, making them less useful for image generation.\nAn improvement on this is variational autoencoders (VAE), where the latent space is continuous and follows normal distribution making them more useful for image generation.\nchatGPT prompt > compare the latent space of VAE with traditional autoencoder\nThe latent space of a VAE is continuous, whereas the latent space of a traditional autoencoder is typically discrete. This means that in a VAE, the latent representation of the data can take on any value in a continuous range, whereas in a traditional autoencoder, the latent representation is restricted to a set of discrete values.\nThis has several implications:\n\nA continuous latent space allows a VAE to capture more fine-grained variations in the data, which can be useful for tasks such as image generation.\nIt allows the VAE to produce more diverse outputs, which can be beneficial for tasks such as anomaly detection.\nIt makes the VAE more flexible and easier to train, since it can capture complex distributions in the data.\n\nOverall, the continuous latent space of a VAE is one of its key advantages over traditional autoencoders."
  }
]