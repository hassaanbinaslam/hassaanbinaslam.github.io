[
  {
    "objectID": "posts/2025-08-30-xception-reading-notes.html#introduction",
    "href": "posts/2025-08-30-xception-reading-notes.html#introduction",
    "title": "Xception Explained: A Deep Dive into Extreme Inception and Smarter Convolutions",
    "section": "Introduction",
    "text": "Introduction\nIn the world of deep learning, certain papers stand out not just for their results, but for the elegance and power of their core ideas. They shift our perspective on how to design neural networks. François Chollet’s 2017 paper, “Xception: Deep Learning with Depthwise Separable Convolutions,” is one of those papers. While many know Chollet as the creator of the popular Keras library, his work on the Xception architecture provides a masterclass in reasoning about the fundamental building blocks of computer vision models.\n\n\n\nPaper Title\n\n\nThese architectures, with names like ResNet, Inception, and VGG, can often feel like complex, intimidating black boxes. How did their creators come up with these designs? What intuition were they following? The Xception paper is a perfect case study because it lays out its chain of reasoning with remarkable clarity. It starts with a well-known architecture (Inception), re-examines its core hypothesis, and pushes that hypothesis to its logical conclusion.\nIn this post, we’ll take a guided tour through the Xception paper, breaking it down step by step to reveal the simple yet powerful ideas behind it. By the end, you’ll understand what makes Xception work and why its core — the depthwise separable convolution — has become a key element of modern network design.\nLet’s start with the abstract."
  },
  {
    "objectID": "posts/2025-08-30-xception-reading-notes.html#the-abstract-xception-in-a-nutshell",
    "href": "posts/2025-08-30-xception-reading-notes.html#the-abstract-xception-in-a-nutshell",
    "title": "Xception Explained: A Deep Dive into Extreme Inception and Smarter Convolutions",
    "section": "The Abstract: Xception in a Nutshell",
    "text": "The Abstract: Xception in a Nutshell\nLet’s start at the very beginning, on page 1, with the Abstract. A paper’s abstract is its elevator pitch; it contains the core idea, the proposed solution, the main results, and the key takeaway all in one tight package. Xception’s abstract is a perfect example of this, laying out the entire story in just five sentences.\n\n\n\nPaper Abstract\n\n\n\nThe Core Insight: Placing Inception on a Spectrum\nThe abstract begins with the paper’s central theoretical claim:\n\n(Page 1, Abstract): “We present an interpretation of Inception modules in convolutional neural networks as being an intermediate step in-between regular convolution and the depthwise separable convolution operation (a depthwise convolution followed by a pointwise convolution).”\n\nThis single sentence is the foundation for everything that follows. Let’s break it down. The author, François Chollet, isn’t just proposing a new component; he’s suggesting a new way to think about existing ones. He outlines a spectrum of operations:\n\nAt one end: The regular convolution, the workhorse of classic networks like VGG. A single filter in a regular convolution is tasked with doing two jobs at once: identifying patterns across spatial locations (like the shape of an edge) and identifying patterns across its input channels (like which features tend to appear together).\nAt the other end: The depthwise separable convolution. This operation explicitly decouples the two jobs. It first performs a “depthwise” spatial convolution (finding spatial patterns in each channel independently) and then a “pointwise” convolution (a 1x1 convolution that combines information across channels).\nIn the middle: The Inception module. Chollet’s key insight is that Inception is a compromise between these two extremes. It partially separates the two jobs but doesn’t go all the way.\n\n\n\nThe “Extreme” Hypothesis\nThe next sentence makes this “spectrum” idea even more concrete:\n\n(Page 1, Abstract): “In this light, a depthwise separable convolution can be understood as an Inception module with a maximally large number of towers.”\n\nAn Inception module is famously built from parallel paths or “towers” (e.g., a 1x1 convolution tower, a 3x3 tower, etc.). Chollet asks us to imagine taking this design to its logical limit. What if, instead of 3 or 4 towers, you created a separate, tiny tower for every single input channel? If you did that, he argues, you would have effectively reinvented the depthwise separable convolution. It is the most extreme, most factored version of an Inception module possible.\n\n\nThe Proposal and the Results\nThis insight directly leads to a practical experiment: if a depthwise separable convolution is an “Extreme Inception” module, what happens if we build a whole network out of them?\n\n(Page 1, Abstract): “This observation leads us to propose a novel deep convolutional neural network architecture inspired by Inception, where Inception modules have been replaced with depthwise separable convolutions. We show that this architecture, dubbed Xception, slightly outperforms Inception V3 on the ImageNet dataset (which Inception V3 was designed for), and significantly outperforms Inception V3 on a larger image classification dataset…”\n\nHere we get the name, Xception, and the top-line results:\n\nOn the standard ImageNet benchmark, it’s slightly better than its predecessor, Inception V3.\nOn a massive internal Google dataset (JFT) with 350 million images, it’s significantly better. This is a crucial finding, suggesting that the Xception design principle scales more effectively when given enormous amounts of data.\n\n\n\nThe Reason for Success: Efficiency\nThe final sentence addresses the most important question: why is it better? Is it just a bigger model?\n\n(Page 1, Abstract): “Since the Xception architecture has the same number of parameters as Inception V3, the performance gains are not due to increased capacity but rather to a more efficient use of model parameters.”\n\nThis is the punchline. The improvement doesn’t come from brute force. Xception has roughly the same number of learnable parameters as Inception V3. This implies that the architecture itself—the principle of completely decoupling spatial and cross-channel correlations—is a fundamentally more efficient way for a model to learn. It gets more mileage out of the same capacity.\nIn just one paragraph, we have our entire story. Now, let’s dive into the Introduction to see how Chollet builds his case in greater detail."
  },
  {
    "objectID": "posts/2025-08-30-xception-reading-notes.html#the-introduction-setting-the-stage-for-a-new-architecture",
    "href": "posts/2025-08-30-xception-reading-notes.html#the-introduction-setting-the-stage-for-a-new-architecture",
    "title": "Xception Explained: A Deep Dive into Extreme Inception and Smarter Convolutions",
    "section": "The Introduction: Setting the Stage for a New Architecture",
    "text": "The Introduction: Setting the Stage for a New Architecture\nThe introduction of a paper serves to provide context, frame the problem, and lay out the argument that the rest of the paper will support. On page 1, Chollet walks us through the history of convolutional neural network (CNN) design to show us exactly where his contribution fits in.\n\n\n\n\n\n\nIntroduction\n\n\n\n\n\n\n\n\n\nA Brief History of CNN Design (Pre-Inception)\nThe first paragraph is a whirlwind tour of the evolution of CNNs, highlighting a clear trend.\n\n(Page 1, Introduction, Para 1): “The history of convolutional neural network design started with LeNet-style models…, which were simple stacks of convolutions for feature extraction and max-pooling… In 2012, these ideas were refined into the AlexNet architecture…, where convolution operations were being repeated multiple times in-between max-pooling operations… What followed was a trend to make this style of network increasingly deeper… first with Zeiler and Fergus in 2013… and then with the VGG architecture in 2014.”\n\nThis narrative establishes a dominant design philosophy that we can call the “linear stacking” era:\n\nLeNet: The original recipe was simple: a convolution layer to find features, immediately followed by a pooling layer to reduce the image size. Think Conv -&gt; Pool -&gt; Conv -&gt; Pool.\nAlexNet: The 2012 breakthrough that kickstarted the modern deep learning revolution. The key innovation was to stack multiple convolution layers back-to-back before pooling (e.g., Conv -&gt; Conv -&gt; Pool). This allowed the network to learn more complex and hierarchical features at a given spatial resolution.\nZFNet & VGG: The main trend following AlexNet’s success was simply to go deeper. Architects kept adding more layers, driven by the belief that depth was the primary driver of performance. The VGG network is the epitome of this philosophy—an extremely deep but structurally simple stack of 3x3 convolutions and pooling layers.\n\nThis history lesson successfully paints a picture of a research field pursuing one primary direction: making networks deeper and deeper. This sets the stage for a paradigm shift.\n\n\nThe Paradigm Shift: Enter Inception\nThe next few paragraphs introduce the architecture that broke this linear trend.\n\n(Page 1, Introduction, Para 2): “At this point a new style of network emerged, the Inception architecture… Since its first introduction, Inception has been one of the best performing family of models on the ImageNet dataset…”\n(Page 1, Introduction, Para 3): “The fundamental building block of Inception-style models is the Inception module… This is a departure from earlier VGG-style networks which were stacks of simple convolution layers.”\n\nInstead of just stacking simple convolution layers, the Inception family (starting with GoogLeNet) introduced a new fundamental building block: the Inception module. An entire Inception network is a stack of these more complex modules. This was a radical departure from the simple, VGG-style design. The author establishes that this new style wasn’t just a novelty; it was (and is) a family of top-performing models, justifying why it’s a worthy subject of study.\n\n\nThe Central Question: Why Does Inception Work?\nHaving established Inception’s success, the author poses the core questions that motivate the rest of the paper.\n\n(Page 1, Introduction, Para 4): “While Inception modules are conceptually similar to convolutions…, they empirically appear to be capable of learning richer representations with less parameters. How do they work, and how do they differ from regular convolutions? What design strategies come after Inception?”\n\nThis is the most important part of the introduction. It highlights the key advantage of the Inception style: parameter efficiency. Inception modules achieve better results than their predecessors while using fewer parameters, making them computationally cheaper and more powerful. This naturally leads to the three questions the Xception paper sets out to answer:\n\nWhat is the underlying principle that makes Inception modules so efficient?\nHow is this principle different from what a regular convolution does?\nCan we take this principle even further to design the next generation of architectures?\n\nWith these questions hanging in the air, we are perfectly primed for the paper’s core thesis."
  },
  {
    "objectID": "posts/2025-08-30-xception-reading-notes.html#section-1.1-the-inception-hypothesis-separating-concerns",
    "href": "posts/2025-08-30-xception-reading-notes.html#section-1.1-the-inception-hypothesis-separating-concerns",
    "title": "Xception Explained: A Deep Dive into Extreme Inception and Smarter Convolutions",
    "section": "Section 1.1: The Inception Hypothesis (Separating Concerns)",
    "text": "Section 1.1: The Inception Hypothesis (Separating Concerns)\nNow we arrive at the heart of the argument. In this section on page 1, Chollet articulates the fundamental assumption—the “Inception hypothesis”—that he believes explains the efficiency of the Inception module.\n\n\n\nInception Hypothesis\n\n\n\nThe Dual Role of a Standard Convolution\nFirst, he defines what a standard convolution is trying to accomplish, and why that might be an inefficient way to learn.\n\n(Page 1, Section 1.1, Para 1): “A convolution layer attempts to learn filters in a 3D space, with 2 spatial dimensions (width and height) and a channel dimension; thus a single convolution kernel is tasked with simultaneously mapping cross-channel correlations and spatial correlations.”\n\nThis is a crucial observation. A standard convolution filter (e.g., a single 3x3 kernel) is asked to do two very different jobs at the same time:\n\nMap Spatial Correlations: It looks for patterns in a small neighborhood of pixels. For example, it learns what arrangement of pixels constitutes a horizontal edge, a corner, or a patch of a certain texture. This is about relationships in the height and width dimensions.\nMap Cross-Channel Correlations: Deeper in a network, the channels don’t represent colors (RGB) anymore; they represent abstract features (e.g., “is there a vertical line here?”, “is there fur texture here?”, “is there a shiny reflection here?”). Cross-channel correlations are the relationships between these features at the same location. For instance, the network might learn that the co-occurrence of a “fur texture” feature and an “ear-shape” feature is a strong indicator of a “cat.” This is about relationships in the channel dimension.\n\nThe key word here is “simultaneously.” A standard convolution tries to learn both types of patterns with a single set of weights, which may not be the most effective way to model these two potentially independent sets of correlations.\n\nA Concrete Example: The Standard Convolution at Work\nTo make this idea of “simultaneously mapping correlations” clearer, let’s walk through a quick example.\nImagine we are in the middle of a deep network. The input to our next layer is a feature map with the following dimensions:\n\nHeight: 14 pixels\nWidth: 14 pixels\nChannels (Depth): 256\n\nOur goal is to apply a standard 3x3 convolutional layer that will produce a new feature map with 512 channels.\nThe Anatomy of a Filter\nTo achieve this, our convolutional layer needs 512 separate filters (sometimes called kernels). Here’s the crucial part: each one of those 512 filters must have a depth that matches the input depth.\nSo, a single filter in our layer will have the dimensions:\n\nFilter Height: 3 pixels\nFilter Width: 3 pixels\nFilter Channels (Depth): 256\n\nEach filter is a 3x3x256 cube of learnable weights.\nThe Operation\nWhen we apply just one of these filters to the input feature map, the following happens:\n\nThe 3x3x256 filter is placed over a 3x3 patch of the input.\nA dot product is computed. This involves multiplying every one of the 3 * 3 * 256 = 2,304 weights in the filter with the corresponding input value and summing them all up.\nThe result is a single number, which becomes one pixel in the output feature map.\n\nNotice what happened in that dot product. The filter’s weights had to learn:\n\nThe correct spatial pattern (which of the 9 spatial locations are important).\nThe correct cross-channel pattern (which of the 256 input features are important and how they should be combined).\n\nIt learned both of these things simultaneously within that single cube of 2,304 weights. The convolutional layer then slides this filter across the entire input and repeats the process with the other 511 filters to produce the final 14x14x512 output.\nThis is the standard, powerful, but potentially inefficient process that the Inception Hypothesis seeks to improve upon.\n\n\n\nConvolutional Layer Illustration\n\n\nImage taken from Medium post Understanding the Convolutional Filter Operation in CNN’s by Frederik vom Lehn\n\n\n\nThe Inception Solution: Factorization\nThe Inception module, he argues, is based on the hypothesis that it’s better to explicitly separate, or “factorize,” these two jobs.\n\n(Page 1, Section 1.1, Para 2): “This idea behind the Inception module is to make this process easier and more efficient by explicitly factoring it into a series of operations that would independently look at cross-channel correlations and at spatial correlations… In effect, the fundamental hypothesis behind Inception is that cross-channel correlations and spatial correlations are sufficiently decoupled that it is preferable not to map them jointly.”\n\nThis is the punchline. The Inception module operates as a two-step process:\n\nFirst, map cross-channel correlations: It uses a set of 1x1 convolutions. A 1x1 convolution is a brilliant tool for this job. Because it only looks at one pixel location at a time, it can’t see any spatial patterns. Its only job is to look at the vector of all channel values at that single pixel and learn smart combinations of them—it maps the cross-channel correlations.\nThen, map spatial correlations: After the 1x1 convolutions have created new, rich feature combinations, the module then applies standard 3x3 or 5x5 convolutions within these new, smaller feature spaces to find spatial patterns.\n\nThis leads to the formal Inception Hypothesis: The model assumes that spatial patterns and channel patterns are independent enough that learning them separately is more efficient and effective than learning them together. By decoupling these tasks, the network can learn each type of correlation more easily.\n\n\n\nInception Module\n\n\nTo close out this section, Chollet includes a footnote that adds another layer to this idea of factorization, noting that even the spatial convolutions themselves can be factored (e.g., a 7x7 convolution can be replaced by a 7x1 followed by a 1x7). This further strengthens the case that factorization is a powerful and general principle in designing efficient neural networks.\n\n\nFrom Inception to “Extreme Inception”\nHaving established the Inception hypothesis — that separating channel and spatial correlations is a good idea—Chollet now scrutinizes how Inception achieves this separation. This leads to a brilliant reframing that paves the way for Xception.\n\n\n\nPaper Page 2, Para 1\n\n\n\nA New Way to Look at Inception\nThe author first simplifies the Inception module for clarity and then presents an entirely new, but equivalent, way of looking at it. This progression is brilliantly illustrated in Figures 2, 3, and 4.\n\n(Page 2, Introduction, Para 1): “Consider a simplified version of an Inception module… (figure 2). This Inception module can be reformulated as a large 1x1 convolution followed by spatial convolutions that would operate on non-overlapping segments of the output channels (figure 3).”\n\nLet’s trace this visual argument:\n\nFigure 2 (Simplified Inception): This is an Inception module with three parallel towers, each containing a 1x1 convolution followed by a 3x3 convolution. The key is that these towers are independent before being concatenated.\nFigure 3 (Equivalent Reformulation): This is the “aha!” moment. Chollet shows that Figure 2 is mathematically identical to a different structure:\n\n\nFirst, perform one large 1x1 convolution.\nThen, take the output channels from that convolution and split them into 3 separate, non-overlapping groups or “segments.”\nFinally, apply a 3x3 convolution to each group independently.\n\n\n\n\n\n\n\nFigure 2\n\n\n\n\n\n\n\nFigure 3\n\n\n\n\n\nThis reformulation is a conceptual breakthrough. It reveals that the essence of an Inception module is channel partitioning. It’s an operation that splits its channels into a small number of groups and performs spatial convolutions within each group.\n\n\nPushing the Hypothesis to Its Limit\nThis new perspective immediately begs a question, which Chollet poses directly.\n\n(Page 2, Introduction, Para 1): “This observation naturally raises the question: what is the effect of the number of segments in the partition…? Would it be reasonable to make a much stronger hypothesis than the Inception hypothesis, and assume that cross-channel correlations and spatial correlations can be mapped completely separately?”\n\nIf an Inception module uses 3 or 4 segments, what would happen if we used 10? Or 100? What is the most extreme version of this idea?\nThe logical extreme is to make the number of segments equal to the number of channels, meaning each segment is only one channel deep.\nThis leads to the “Extreme Inception” or Xception Hypothesis: Let’s assume that channel and spatial correlations are completely decoupled. We can first use a 1x1 convolution to handle all the cross-channel learning, and then a spatial convolution that operates on every single channel independently, with zero cross-talk between them. This idea is visualized in Figure 4.\n\n\n\nFigure 4"
  },
  {
    "objectID": "posts/2025-08-30-xception-reading-notes.html#section-1.2-connecting-to-depthwise-separable-convolutions",
    "href": "posts/2025-08-30-xception-reading-notes.html#section-1.2-connecting-to-depthwise-separable-convolutions",
    "title": "Xception Explained: A Deep Dive into Extreme Inception and Smarter Convolutions",
    "section": "Section 1.2: Connecting to Depthwise Separable Convolutions",
    "text": "Section 1.2: Connecting to Depthwise Separable Convolutions\nIt turns out this “extreme” idea is not entirely new. It has a name.\n\n\n\n\n\n\nSection 1.2 Para 1\n\n\n\n\n\n\n\nSection 1.2 Para 2\n\n\n\n\n\nThis is the central connection of the paper. The logical conclusion of the Inception design philosophy is an existing (but perhaps underutilized at the time) operation called the depthwise separable convolution. This operation consists of two parts that perfectly match the “Extreme Inception” idea:\n\nA Depthwise Convolution: A spatial convolution (e.g., 3x3) that is applied to every single input channel independently. This handles the spatial correlations.\nA Pointwise Convolution: A 1x1 convolution that is used to combine the outputs of the depthwise step. This handles the cross-channel correlations.\n\nChollet notes two minor differences between his “Extreme Inception” formulation (pointwise first) and the standard library implementation of a depthwise separable convolution (depthwise first), but he later argues these are not significant in a deep, stacked network.\nBy making this connection, the paper grounds its theoretical exploration in a concrete, practical, and efficient operation. The proposal is no longer just “let’s build a crazy Inception module with hundreds of towers,” but rather, “let’s build a network out of depthwise separable convolutions.”\n\n\n\nExample of the convolution process using the depthwise separable convolution.\n\n\nImage taken from paper A lightweight double-channel depthwise separable convolutional neural network for multimodal fusion gait recognition\n\nFinalizing the Argument and Stating the Goal\nHaving established the powerful idea that an Inception module is just one point on a spectrum of convolutions, Chollet uses the top of page 3 to tie up loose ends and clearly state his research plan.\n\n\n\nPaper page 3, para 1, 2, 3\n\n\n\nThe Spectrum of Convolutions\nFirst, the paper explicitly lays out this “spectrum” idea, which is the core theoretical takeaway.\n\n(Page 3, Introduction, Para 2): “…in effect, there is a discrete spectrum between regular convolutions and depthwise separable convolutions, parametrized by the number of independent channel-space segments used for performing spatial convolutions.”\n\nThis elegant summary clarifies the entire conceptual framework:\n\nOne Extreme (1 segment): A standard convolution block. A 1x1 convolution is followed by a regular 3x3 convolution that sees all the channels at once.\nThe Middle (3 or 4 segments): An Inception module. The channels are divided into a few large groups, and a 3x3 convolution is applied within each group.\nThe Other Extreme (N segments, for N channels): A depthwise separable convolution. The channels are divided into the maximum possible number of segments (one per channel), and a 3x3 spatial convolution is applied to each one completely independently.\n\nChollet also wisely points out that the properties of other “intermediate modules” on this spectrum (e.g., what if you used 8 or 16 segments?) are a potential area for future research.\n\n\nThe Grand Proposal\nWith this framework in place, the paper makes its final, clear proposal.\n\n(Page 3, Introduction, Para 3): “Having made these observations, we suggest that it may be possible to improve upon the Inception family of architectures by replacing Inception modules with depthwise separable convolutions, i.e. by building models that would be stacks of depthwise separable convolutions.”\n\nThis is it. The plan is simple and elegant: take a proven, high-performing architecture (Inception) and replace its core building block with the “extreme” version of itself. Instead of a stack of Inception modules, the new network—Xception—will be a stack of depthwise separable convolution layers.\nThe author notes that this is only made practical by the availability of an efficient implementation of this operation in modern deep learning frameworks like TensorFlow. With the theory, the hypothesis, and the practical plan now in place, the introduction is complete. The stage is set for a detailed look at the proposed architecture and its performance."
  },
  {
    "objectID": "posts/2025-08-30-xception-reading-notes.html#section-2-standing-on-the-shoulders-of-giants-prior-work",
    "href": "posts/2025-08-30-xception-reading-notes.html#section-2-standing-on-the-shoulders-of-giants-prior-work",
    "title": "Xception Explained: A Deep Dive into Extreme Inception and Smarter Convolutions",
    "section": "Section 2: Standing on the Shoulders of Giants (Prior Work)",
    "text": "Section 2: Standing on the Shoulders of Giants (Prior Work)\nOn page 3, before diving into the specifics of the new architecture, the paper takes a moment to acknowledge the key ideas and previous works it builds upon. As the saying goes, “if I have seen further, it is by standing on the shoulders of giants.” The Prior Work section shows us exactly which giants’ shoulders Xception is standing on.\n\n\n\n\n\n\nPrior Work 1\n\n\n\n\n\n\n\nPrior Work 2\n\n\n\n\n\nChollet identifies three main pillars of research that made his work possible.\n\nPillar 1: The VGG-16 Macro-Architecture\n\n(Page 3, Section 2, Bullet 1): “Convolutional neural networks…, in particular the VGG-16 architecture…, which is schematically similar to our proposed architecture in a few respects.”\n\nThe first pillar is the general history of CNNs, with a special mention of VGG-16. This might seem surprising at first—VGG is known for its simple, brute-force depth, which seems philosophically opposite to the intricate efficiency of Inception. However, the author is borrowing VGG’s macro-architecture. VGG’s design is a clean, linear stack of repeating, near-identical blocks. This simple and scalable design principle—a deep stack of modules—is something Xception will adopt. In essence, Xception takes the simple, scalable body of VGG but fills it with a much smarter brain.\n\n\nPillar 2: The Inception Philosophy\n\n(Page 3, Section 2, Bullet 2): “The Inception architecture family…, which first demonstrated the advantages of factoring convolutions into multiple branches operating successively on channels and then on space.”\n\nThe second and most important pillar is, of course, the Inception family. This is where the core philosophy of Xception comes from. The Inception papers were the first to demonstrate that “factoring” convolutions—splitting the work of mapping cross-channel and spatial correlations—was a powerful design principle that led to more efficient and accurate models. Xception is the direct intellectual descendant of this idea, seeking to take this very same principle to its logical conclusion.\n\n\nPillar 3: The History and Rise of Depthwise Separable Convolutions\nThe third pillar is the specific mathematical tool that makes the “Extreme Inception” idea a reality. The author provides a detailed history of this operation.\n\n(Page 3, Section 2, Bullet 3): “Laurent Sifre developed depthwise separable convolutions during an internship at Google Brain in 2013, and used them in AlexNet to obtain small gains in accuracy and large gains in convergence speed, as well as a significant reduction in model size… Later, a depthwise separable convolution was used as the first layer of Inception V1 and Inception V2… Within Google, Andrew Howard [6] has introduced efficient mobile models called MobileNets using depthwise separable convolutions…”\n\nThis paragraph is rich with context. It tells us that the depthwise separable convolution wasn’t an obscure, brand-new idea.\n\nIt was developed by Laurent Sifre as early as 2013 and showed promising results in improving AlexNet.\nIt was even used in a limited capacity in the very first layers of early Inception models, hinting that the Google Brain team was aware of its potential.\nIt became the core building block for another famous family of models, MobileNets, which are designed for extreme efficiency on mobile devices.\nFinally, the author reiterates that his work is only practical because of the efficient implementation available in TensorFlow. This grounds the research in real-world engineering.\n\n\n\nPillar 4: The Power of Residual Connections\nThe final ingredient is arguably one of the most important deep learning innovations of the 2010s.\n\n(Page 3, Section 2, Bullet 4): “Residual connections, introduced by He et al. in [4], which our proposed architecture uses extensively.”\n\nThis is a crucial addition. Residual connections (or “skip connections”), made famous by the ResNet paper, are shortcuts that allow the gradient to flow more easily through very deep networks. They work by adding the input of a block to its output, making it easier for the network to learn identity functions and preventing performance from degrading as more layers are added. By stating that Xception “uses [them] extensively,” Chollet is signaling that his architecture is not just a pure “Inception-style” model, but a hybrid that marries the “Extreme Inception” idea with the deep-training stability of ResNet.\nIn summary, Xception is a brilliant synthesis: it combines the clean, scalable structure of VGG with the factorization philosophy of Inception, made stable by the residual connections of ResNet, and all implemented using the efficient tool of the depthwise separable convolution."
  },
  {
    "objectID": "posts/2025-08-30-xception-reading-notes.html#section-3-the-blueprint-of-the-xception-architecture",
    "href": "posts/2025-08-30-xception-reading-notes.html#section-3-the-blueprint-of-the-xception-architecture",
    "title": "Xception Explained: A Deep Dive into Extreme Inception and Smarter Convolutions",
    "section": "Section 3: The Blueprint of the Xception Architecture",
    "text": "Section 3: The Blueprint of the Xception Architecture\nAfter setting the historical and theoretical stage, the paper finally unveils its proposed architecture on page 3. This section explains the high-level design principles and structure of Xception.\n\n\n\n\n\n\nXception Architecture 1\n\n\n\n\n\n\n\nXception Architecture 2\n\n\n\n\n\n\nThe Core Principle and the Name\nThe section begins by restating the core idea and formally giving the architecture its memorable name.\n\n(Page 3, Section 3, Para 1): “We propose a convolutional neural network architecture based entirely on depthwise separable convolution layers… Because this hypothesis is a stronger version of the hypothesis underlying the Inception architecture, we name our proposed architecture Xception, which stands for ‘Extreme Inception’.”\n\nThis is the mission statement. The network is a pure expression of a single idea: what happens if you build a deep CNN entirely out of depthwise separable convolutions? The name: “Extreme Inception,” perfectly captures this philosophy of taking the Inception idea to its logical limit. The central hypothesis is that spatial and cross-channel correlations can be “entirely decoupled,” a stronger and cleaner assumption than the partial decoupling seen in standard Inception modules.\n\n\nThe High-Level Structure\nNext, the paper outlines the macro-structure of the network, giving us a bird’s-eye view before we dive into the details shown in Figure 5.\n\n(Page 3, Section 3, Para 2): “The Xception architecture has 36 convolutional layers forming the feature extraction base… structured into 14 modules, all of which have linear residual connections around them, except for the first and last modules.”\n\nThis gives us several key architectural specifications:\n\nDepth: The network is deep, consisting of 36 convolutional layers that form its main body or “feature extraction base.”\nModularity: Echoing the design of VGG and ResNet, these 36 layers are not just a monolithic stack. They are organized into 14 repeating modules. This makes the architecture clean, scalable, and easy to reason about.\nResidual Connections: This is a crucial design choice borrowed from ResNet. Almost all of the 14 modules are wrapped in a residual connection. This means the input to a module is added to the module’s output, creating a “shortcut” that is famously effective at enabling the training of very deep networks by improving gradient flow. This makes Xception a hybrid architecture, combining the ideas of Inception and ResNet.\nThe Classifier: For the experiments, this feature extraction base is followed by a simple classifier, typically a global average pooling layer and a final logistic regression (or softmax) layer. The paper also notes that it will test a version with optional fully-connected layers, a common practice in older architectures.\n\nIn essence, the blueprint for Xception is elegantly simple: a linear stack of residual modules, where each module is built from depthwise separable convolutions.\nWith this high-level overview in mind, we are now ready to examine the detailed diagram of the network flow.\n\n\nA Visual Walkthrough of the Xception Architecture (Figure 5)\nOn page 5, the paper presents Figure 5, a detailed diagram of the full Xception architecture. While it might look complex at first, it follows a clean and logical flow. The architecture is divided into three distinct stages: an Entry flow, a Middle flow, and an Exit flow. Let’s trace the journey of an image as it passes through the network.\n\n\n\nXception Architecture\n\n\n\nThe Entry Flow\nThe Entry flow’s job is to take the raw input image (299x299x3) and progressively transform it into a rich set of feature maps at a smaller spatial resolution.\n\nInitial Convolutions: The flow begins not with a separable convolution, but with two standard 3x3 convolution layers. This is a common and effective practice in many modern networks. These first layers quickly and aggressively process the raw pixel data, expanding the channel dimension from 3 (RGB) to 64.\nModular Blocks: The rest of the Entry flow is a sequence of modules built from SeparableConv layers. Each module has a similar structure: one or more separable convolutions followed by a max-pooling layer to reduce the spatial dimensions (e.g., from 149x149 down to 75x75, and so on).\nResidual Connections: Crucially, each of these modules is wrapped in a residual connection (indicated by the + circle). The input to the module is passed through a simple 1x1 convolution (to match the channel dimensions of the output) and is then added to the output of the main branch. This pattern of SeparableConv blocks with residual connections is the core of the Xception design.\nOutcome: By the end of the Entry flow, the initial 299x299x3 image has been transformed into a 19x19x728 feature map, ready for the main processing stage.\n\n\n\nThe Middle Flow\nThe Middle flow is the workhorse of the network. Its design is strikingly simple and elegant.\n\nA Single Repeating Block: The entire Middle flow consists of one block structure that is repeated eight times. This block contains three SeparableConv layers in sequence.\nNo Downsampling: Unlike the Entry flow, this stage does not change the shape of the data. The input is 19x19x728, and the output is 19x19x728. The sole purpose of this deep stack of repeated blocks is to learn increasingly complex and refined feature representations at this specific spatial scale.\nResidual Connections: Again, a residual connection is key. A shortcut skips over all three SeparableConv layers in the block, allowing gradients to flow easily through these eight repeated modules.\n\n\n\nThe Exit Flow\nThe Exit flow’s job is to perform the final feature extraction and prepare the data for classification.\n\nFinal Feature Extraction: It starts with a module similar to those in the Entry flow, which further transforms the features and increases the channel depth to 1024. This is followed by a sequence of SeparableConv layers that dramatically expand the feature representation, culminating in a deep 2048-channel feature map.\nGlobal Average Pooling: Instead of flattening the final feature map into a massive vector (which would require huge, parameter-heavy fully-connected layers), Xception uses Global Average Pooling. This simple operation takes the final feature map (e.g., 10x10x2048) and calculates the average value for each of the 2048 channels, producing a compact 2048-dimensional vector. This is a highly efficient and modern technique for connecting a convolutional base to a classifier.\nClassification: This final 2048 dimensional vector is then fed to a Logistic regression layer (a simple dense layer with softmax activation) to produce the final class predictions.\n\nThe diagram’s caption also provides two critical implementation details: every convolution layer is followed by Batch Normalization (essential for stable training), and the separable convolutions use a depth multiplier of 1 (meaning the number of channels is only ever changed by the 1x1 pointwise convolutions).\nThis end-to-end structure—a clear entry, a deep middle, and a decisive exit—built from repeating blocks of residual separable convolutions, is the concrete embodiment of the Xception philosophy.\n\n\nTwo Hidden Details That Make It All Work\nThe diagram in Figure 5 gives us the blueprint, but the caption adds two implementation details that are absolutely critical for making a deep network like Xception train successfully:\n\nEvery Convolution is Followed by Batch Normalization: Although not shown in the diagram to keep it clean, after every single Conv and SeparableConv layer, there is a Batch Normalization layer. Think of Batch Normalization as a regulator for your network’s data flow. As data passes through many layers, the distribution of values can shift wildly, making it hard for the network to learn effectively (a problem called “internal covariate shift”). Batch Normalization constantly recalibrates the data at each step, ensuring the signal remains stable. For deep networks, this isn’t just a nice-to-have; it’s an essential ingredient that dramatically speeds up and stabilizes training.\n“Depth Multiplier of 1” Simplifies the Design: The caption notes that all separable convolutions use a depth multiplier of 1. This sounds technical, but it reveals a simple and elegant design choice. A depthwise separable convolution has two parts: the spatial “depthwise” step and the channel-mixing “pointwise” step. A depth multiplier greater than 1 would allow the first (depthwise) step to expand the number of channels. By setting the multiplier to 1, the author ensures that the depthwise step never changes the number of channels. Its only job is to find spatial patterns. This means that in the entire Xception architecture, the number of channels is only ever changed in one place: the 1x1 pointwise convolutions. This enforces a clean separation of concerns and makes the architecture’s flow much easier to reason about."
  },
  {
    "objectID": "posts/2025-08-30-xception-reading-notes.html#section-4-putting-xception-to-the-test-experimental-evaluation",
    "href": "posts/2025-08-30-xception-reading-notes.html#section-4-putting-xception-to-the-test-experimental-evaluation",
    "title": "Xception Explained: A Deep Dive into Extreme Inception and Smarter Convolutions",
    "section": "Section 4: Putting Xception to the Test (Experimental Evaluation)",
    "text": "Section 4: Putting Xception to the Test (Experimental Evaluation)\nNow we get to the proof. A beautiful theory and an elegant architecture are one thing, but do they actually work? The rest of the paper is dedicated to a rigorous experimental evaluation. On page 4, the author lays out the framework for this evaluation, ensuring the comparisons are fair and the results are meaningful.\n\nThe Main Event: Xception vs. Inception V3\nThe first paragraph of Section 4 establishes the primary comparison.\n\n\n\nExperimental Evaluation\n\n\nThis is the cornerstone of a good scientific experiment. To test if a new idea is better, you must control for other variables. By choosing to compare Xception against a model of roughly the same size and parameter count (Inception V3), the author ensures that any performance difference is due to the architectural design itself, not simply due to one model being bigger than the other.\nThe author then states that this comparison will take place on two very different battlegrounds to test the robustness and scalability of the architecture: the well-known ImageNet dataset and a massive internal Google dataset called JFT.\n\n\nSection 4.1: The Proving Ground—The JFT Dataset\nWhile ImageNet is the standard academic benchmark, the paper also evaluates the models on a much larger and more challenging internal Google dataset called JFT.\n\n\n\nJFT Dataset\n\n\n\n(Page 4, Section 4.1): “JFT is an internal Google dataset for large-scale image classification dataset… which comprises over 350 million high-resolution images annotated with labels from a set of 17,000 classes.”\n\nLet’s put those numbers in perspective. The standard ImageNet dataset has about 1.2 million training images and 1,000 classes. JFT is orders of magnitude larger:\n\n~300x more images (350 million vs. 1.2 million)\n17x more classes (17,000 vs. 1,000)\n\nFurthermore, unlike ImageNet where each image has a single label, the images in JFT are multi-label, meaning a single image can be associated with several classes simultaneously (e.g., a picture could be labeled “cat,” “pet,” “sofa,” and “living room”).\nTraining on a dataset of this scale is a true test of an architecture’s ability to learn from a massive amount of data and its ability to generalize. It moves beyond the curated world of academic benchmarks into a domain that more closely resembles real-world, web-scale data. As the author later suggests, an architecture’s performance on JFT can reveal how well its core principles scale when data is virtually unlimited.\nTo evaluate performance, the paper uses a separate validation set called FastEval14k and a metric called Mean Average Precision (MAP@100), which is better suited for multi-label tasks than simple accuracy.\n\n\nSection 4.2: The Rules of the Race (Optimization Configuration)\nTraining a deep neural network is a complex process with many “hyperparameters”—knobs you can tune to affect how the model learns. In Section 4.2 on page 4, the paper details the exact settings used to train both Xception and Inception V3.\n\n\n\n\n\n\nOptimization and configuration 1\n\n\n\n\n\n\n\nOptimization and configuration 2\n\n\n\n\n\nThe author first notes that the two datasets require different training strategies:\n\nFor ImageNet, a standard and well-established recipe is used: the SGD (Stochastic Gradient Descent) optimizer with momentum, a relatively high initial learning rate, and a schedule that decreases the learning rate every two epochs.\nFor JFT, which is a much larger and noisier dataset, the RMSprop optimizer is used with a very small initial learning rate.\n\nThis is standard practice, as different data distributions often benefit from different optimization techniques. However, the most important detail comes next.\n\n(Page 4, Section 4.2, Para 2): “For both datasets, the same exact same optimization configuration was used for both Xception and Inception V3. Note that this configuration was tuned for best performance with Inception V3; we did not attempt to tune optimization hyperparameters for Xception.”\n\nThis is a crucial and refreshingly honest statement. The author is essentially giving Inception V3 a “home-field advantage.” The training settings—the learning rate, the optimizer, etc.—were all carefully selected and optimized over time at Google to squeeze the best possible performance out of the Inception V3 architecture. Xception was then trained using these same settings, without any special tuning in its favor.\nThis makes the subsequent results even more compelling. If Xception manages to outperform Inception V3, it will have done so under conditions that were explicitly optimized for its competitor. This strengthens the claim that Xception’s advantage comes from its superior architectural design, not from lucky hyperparameter tuning.\nFinally, the author mentions using Polyak averaging at inference time. This is a simple technique where, instead of using the very last set of model weights from training, you use an average of the weights from the last several training steps. This often results in a final model that is slightly more stable and generalizes better.\n\nA Quick Detour: What is Polyak Averaging?\nBefore moving on, the paper mentions a small but interesting technique used during evaluation:\n\n(Page 4, Section 4.2, Para 3): “Additionally, all models were evaluated using Polyak averaging [13] at inference time.”\n\nSo, what is Polyak averaging (also known as Polyak-Ruppert averaging)? In simple terms, it’s a method to get a more stable and often better-performing final model by averaging the model’s weights over time.\nHere’s the intuition. During training with an optimizer like SGD, the model’s weights tend to bounce around the area of a good solution in the loss landscape. If you stop training at a random step, you might catch the weights at a lucky peak or an unlucky trough.\nInstead of just taking the weights from the very last training step, Polyak averaging maintains a running average of the weights over the last several hundred or thousand steps. The final model used for prediction is this “averaged” model. This process tends to smooth out the noise from the training process and find a point closer to the center of the optimal region, often leading to better generalization on unseen data. It’s a simple, low-cost trick that can provide a small but consistent boost in performance.\n\n\n\nSection 4.3: Preventing Overfitting (Regularization Configuration)\nOverfitting is a constant concern in machine learning. It happens when a model becomes too specialized in memorizing the training data, losing its ability to generalize to new, unseen examples. Regularization techniques are designed to combat this. In Section 4.3 on page 4, the paper outlines the regularization strategies used.\n\n\n\nRegularization\n\n\n\nWeight Decay (L2 Regularization): This technique discourages the model from learning overly complex patterns by adding a small penalty for large weight values. The paper notes that the weight decay rate that was optimal for Inception V3 (4e-5) was “quite suboptimal” for Xception. They had to perform a small search and settled on a different value (1e-5). This is one of the few instances where a hyperparameter was tuned specifically for Xception, likely because the default value was actively harming its performance.\nDropout: Dropout is a technique where, during training, a random fraction of neurons are temporarily “dropped” or ignored. This forces the network to learn more robust and redundant representations. For the ImageNet experiments, a standard dropout layer with a 50% drop rate was added just before the final classification layer for both models. However, for the massive JFT dataset, no dropout was used. The dataset is so enormous that the risk of the model overfitting was considered negligible within a reasonable training timeframe.\nAuxiliary Loss Tower: The Inception V3 architecture includes an optional “auxiliary classifier”—a small side branch deep in the network that also tries to predict the final class. This provides an additional gradient signal during training and acts as a strong regularizer. For the sake of a cleaner and simpler comparison, the author chose not to include this auxiliary tower in the Inception V3 models he benchmarked. This ensures that the performance comparison is focused squarely on the design of the core modules themselves.\n\nBy being transparent about these choices, the paper gives us a clear picture of the experimental conditions and strengthens the fairness of the final comparison.\n\n\nSection 4.4: The Engine Room (Training Infrastructure)\nIn the final part of the setup on page 4, the paper details the immense computational resources required to run these experiments. This context is vital for appreciating the effort involved and the challenges of working with web-scale datasets.\n\n\n\nTraining Infrastructure\n\n\n\n(Page 4, Section 4.4): “All networks were implemented using the TensorFlow framework and trained on 60 NVIDIA K80 GPUs each.”\n\nThis is a massive amount of hardware. The experiments were run on a distributed system of 60 high-end (at the time) GPUs working in parallel. This highlights that deep learning research at this scale is a significant engineering endeavor.\nThe paper also notes two different strategies for parallelizing the training:\n\nFor ImageNet, they used synchronous gradient descent. In this mode, all 60 GPUs calculate their updates, and then they all wait to synchronize and average those updates before proceeding. This is generally slower but often leads to slightly better final accuracy and more stable training.\nFor the enormous JFT dataset, they switched to asynchronous gradient descent. Here, each GPU works more independently, applying its updates without waiting for all the others. This is much faster and more scalable, which is essential when training would otherwise take an impractically long time.\n\nFinally, the paper gives us the training times, which are staggering:\n\n(Page 4, Section 4.4): “The ImageNet experiments took approximately 3 days each, while the JFT experiments took over one month each. The JFT models were not trained to full convergence, which would have taken over three month per experiment.”\n\nThe ImageNet training run took about three days on this 60-GPU cluster. The JFT experiment, even with the faster asynchronous training, took over a month and still wasn’t fully finished. A complete run to “convergence” (the point where the model stops improving) would have taken more than three months. This really underscores the difference in scale between standard academic benchmarks and true, web-scale industrial datasets, and it shows the immense investment required to validate new architectures at that level.\nWith the entire experimental setup now laid out, we are finally ready to see the results.\n\n\nSection 4.5: The Results Are In—A Head-to-Head Comparison\nAfter meticulously detailing the experimental setup, the paper presents the core results of its comparison between Xception and Inception V3. The findings are presented in tables and graphs, starting on page 5.\n\n\n\nResults Overview\n\n\n\nSection 4.5.1: Performance on the ImageNet Benchmark\nThe first test is on the classic ImageNet dataset, the standard proving ground for computer vision models. The main results are summarized in Table 1.\n\n(Page 5, Table 1): Classification performance comparison on ImageNet (single crop, single model)\n\n\n\n\nModel\nTop-1 accuracy\nTop-5 accuracy\n\n\n\n\n…\n…\n…\n\n\nInception V3\n0.782\n0.941\n\n\nXception\n0.790\n0.945\n\n\n\nThe results are clear. On Top-1 accuracy (the most common metric, which checks if the model’s single best guess is correct), Xception achieves 79.0% accuracy, surpassing Inception V3’s 78.2%.\nAs the author notes in the text:\n\n(Page 5, Section 4.5.1, Para 2): “On ImageNet, Xception shows marginally better results than Inception V3.”\n\nWhile not a massive leap, this is a decisive win. It’s especially impressive given that the training hyperparameters were optimized for Inception V3. The table also shows that Xception’s performance places it firmly in the top tier of models at the time, outperforming even a much deeper ResNet-152.\n\n\n\nFigure 6: Training profile on ImageNet\n\n\nThe training progress is visualized in Figure 6 on page 6. The graph shows the validation accuracy over time, and it’s clear that Xception’s curve (in blue) is consistently above Inception V3’s (in red), confirming its superior performance throughout the training process.\n\n\nAn Insightful Interpretation\nThe author doesn’t just present the numbers; he offers a compelling interpretation for why the performance gap on ImageNet is relatively small compared to the gap on the JFT dataset.\n\n(Page 5, Section 4.5.1, Para 3): “We believe this may be due to the fact that Inception V3 was developed with a focus on ImageNet and may thus be by design over-fit to this specific task.”\n\nThis is a fascinating hypothesis. The Inception architecture had been refined over several years, with each new version (V1, V2, V3) carefully tuned to squeeze out every last bit of performance on the ImageNet benchmark. It’s possible that some of its specific design choices, while excellent for ImageNet, were not as generalizable. Xception, on the other hand, is based on a cleaner, more fundamental principle. The author suggests that this “purer” design might have an advantage when applied to new and different datasets.\nThis sets the stage perfectly for the next set of results: the performance on the massive JFT dataset, where the true power of Xception’s design may be revealed.\n\n\n\nPerformance on the JFT Benchmark: Where Xception Shines\nIf the results on ImageNet were a modest win for Xception, the results on the massive JFT dataset are a decisive victory. This comparison highlights how the two architectures scale when presented with an enormous amount of data. The results are presented in Table 2 on page 6.\n\n(Page 6, Table 2): Classification performance comparison on JFT (single crop, single model)\n\n\n\n\nModel\nFastEval14k MAP@100\n\n\n\n\nInception V3 - no FC layers\n6.36\n\n\nXception - no FC layers\n6.70\n\n\nInception V3 with FC layers\n6.50\n\n\nXception with FC layers\n6.78\n\n\n\nThe table compares two versions of each model: one that uses Global Average Pooling directly (no FC layers) and another that adds two fully-connected (FC) layers before the final classifier.\nIn both scenarios, the result is the same: Xception is significantly better.\n\nWithout FC layers, Xception achieves a MAP score of 6.70, a substantial improvement over Inception V3’s 6.36. As the author notes on the previous page, this represents a 4.3% relative improvement, a very meaningful gain on a large-scale benchmark.\nWith FC layers, Xception maintains its wide lead, scoring 6.78 to Inception V3’s 6.50.\n\nThis result strongly supports the author’s hypothesis from the previous section. The Inception V3 architecture, highly tuned for ImageNet, does not seem to generalize as well to this much larger and more complex dataset. Xception’s cleaner, more fundamental design appears to be more scalable, allowing it to take better advantage of the 350 million images in the JFT dataset.\n\n\n\n\n\n\nFigure 7 (JFT, no FC layers)\n\n\n\n\n\n\n\nFigure 8 (JFT, with FC layers)\n\n\n\n\n\nThis performance gap is vividly illustrated in the training profile graphs.\n\nFigure 7 (JFT, no FC layers): Shows the training progress for the models without fully-connected layers. The performance gap between Xception (blue) and Inception V3 (red) is wide and consistent throughout the entire month-long training run.\nFigure 8 (JFT, with FC layers): Tells the same story. Xception establishes an early lead and never relinquishes it.\n\nThese results are the strongest piece of evidence in the paper, demonstrating that the “Extreme Inception” hypothesis leads to a more robust and scalable architecture.\n\n\nSection 4.5.2: The Source of the Gains—Size and Speed\nHaving established that Xception performs better, the paper now addresses the crucial question of why. Is it simply a bigger, more cumbersome model? Section 4.5.2 and Table 3 on page 6 provide a clear answer: No. The gains come from efficiency, not size.\n\n\n\nSize and speed comparison\n\n\n\n(Page 6, Table 3): Size and training speed comparison\n\n\n\n\nModel\nParameter count\nSteps/second\n\n\n\n\nInception V3\n23,626,728\n31\n\n\nXception\n22,855,952\n28\n\n\n\nThis small table contains one of the most important findings of the paper.\n\nParameter Count (Size): Xception is actually slightly smaller than Inception V3, with about 22.9 million parameters compared to 23.6 million. For all practical purposes, their capacities are identical. This is a critical piece of evidence. It proves that the superior performance of Xception is not due to it having more parameters or a larger capacity.\nTraining Speed: Xception is marginally slower, processing 28 batches (or “steps”) per second compared to Inception V3’s 31 on the 60-GPU hardware setup. The author suggests this is likely not due to a fundamental flaw, but rather to the fact that the low-level library implementations of depthwise convolutions were less mature and optimized at the time than the heavily-used standard convolutions.\n\nThis leads to the paper’s central conclusion about its performance:\n\n(Page 6, Text below Table 3): “The fact that both architectures have almost the same number of parameters indicates that the improvement seen on ImageNet and JFT does not come from added capacity but rather from a more efficient use of the model parameters.”\n\nThis is the punchline of the entire experimental section. Xception’s design—the complete decoupling of spatial and cross-channel correlations—is a fundamentally more effective way for a neural network to use its limited number of parameters to learn about the visual world. It’s a smarter, not just bigger, architecture."
  },
  {
    "objectID": "posts/2025-08-30-xception-reading-notes.html#architectural-dissection-part-1-the-critical-role-of-residual-connections",
    "href": "posts/2025-08-30-xception-reading-notes.html#architectural-dissection-part-1-the-critical-role-of-residual-connections",
    "title": "Xception Explained: A Deep Dive into Extreme Inception and Smarter Convolutions",
    "section": "Architectural Dissection Part 1: The Critical Role of Residual Connections",
    "text": "Architectural Dissection Part 1: The Critical Role of Residual Connections\nA key part of good research is not just showing that a new design works, but understanding why it works. To do this, researchers perform “ablation studies”—experiments where they systematically remove or alter parts of their model to see how performance is affected.\nIn Section 4.6 on page 6, the paper investigates a crucial component of the Xception design that was borrowed not from Inception, but from ResNet: the residual connections.\n\nThe Question: Are the Skip Connections Necessary?\nAs we saw in the architecture diagram (Figure 5), almost every module in Xception is wrapped in a “skip” or “residual” connection, where the input to the module is added to its output. To quantify how important these connections are, the author conducted a simple but powerful experiment: he created a modified version of Xception with all the residual connections removed and trained it on ImageNet.\nThe results are shown in Figure 9.\n\n\n\nFigure 9: Training profile with and without residual connections\n\n\n\nThe red curve, representing the standard Xception with residual connections, shows a smooth and rapid learning curve, quickly climbing to its final high accuracy of ~79%.\nThe blue curve, representing the “Xception - Non-residual” version, tells a very different story. The learning is dramatically slower, and the model’s performance quickly stagnates at a much lower accuracy of less than 65%.\n\n\nThe Conclusion: Residual Connections are “Essential”\nThe visual evidence from the graph leads to a clear conclusion:\n\n\n(Page 7, Text at the top): “Residual connections are clearly essential in helping with convergence, both in terms of speed and final classification performance.”\n\nFor this specific architecture, the residual connections are not just a minor optimization; they are a critical component that enables the network to be trained effectively. Without them, the gradients would struggle to propagate through the 36-layer-deep network, and the model would fail to learn properly.\nThe author does add two important notes of caution. First, he acknowledges that the non-residual model might have performed better if its training hyperparameters were tuned differently. Second, he clarifies that while residual connections are essential for this specific deep architecture, they are not a universal requirement for using separable convolutions. He mentions that he was also able to achieve excellent results with a simpler, VGG-style (non-residual) stack of separable convolutions.\nThis study beautifully demonstrates that the success of Xception is not just due to the “Extreme Inception” idea alone, but to its powerful combination with the deep-training stability provided by residual connections."
  },
  {
    "objectID": "posts/2025-08-30-xception-reading-notes.html#architectural-dissection-part-2-the-surprising-effect-of-intermediate-activations",
    "href": "posts/2025-08-30-xception-reading-notes.html#architectural-dissection-part-2-the-surprising-effect-of-intermediate-activations",
    "title": "Xception Explained: A Deep Dive into Extreme Inception and Smarter Convolutions",
    "section": "Architectural Dissection Part 2: The Surprising Effect of Intermediate Activations",
    "text": "Architectural Dissection Part 2: The Surprising Effect of Intermediate Activations\nThe final experiment in the paper, detailed in Section 4.7 on page 7, revisits a subtle question raised early on. A standard Inception module typically places a ReLU activation function after every convolution. A depthwise separable convolution, as implemented in libraries, usually does not have an activation function between its depthwise (spatial) and pointwise (channel) steps.\nWhich way is better? Does adding a non-linearity in the middle of a separable convolution block help or hurt?\n\nThe Experiment: To ReLU or Not to ReLU?\nTo answer this, the author tested three versions of the Xception architecture:\n\nNo intermediate activation: The standard Xception model used for all the main results.\nIntermediate ReLU: A version with a ReLU activation placed between the depthwise and pointwise operations.\nIntermediate ELU: A version using a different but popular activation function, ELU.\n\nThe results of this experiment on ImageNet are shown in Figure 10.\n\n\n\nFigure 10. Training profile with different activations between the depthwise and pointwise operations of the separable convolution layers.\n\n\n\nThe red curve (No intermediate activation) is the clear winner. It converges the fastest and achieves the best final performance.\nThe green curve (Intermediate ELU) is next.\nThe blue curve (Intermediate ReLU) performs the worst of the three.\n\n\n\nThe Conclusion: Linearity is Better (Here)\nThis leads to a “remarkable observation.”\n\n\n(Page 7, Section 4.7, Para 2): “…the absence of any non-linearity leads to both faster convergence and better final performance. This is a remarkable observation, since Szegedy et al. report the opposite result in [21] for Inception modules.”\n\nThis result is surprising because it directly contradicts the findings from the original Inception papers, where adding the intermediate ReLU was beneficial. The author doesn’t just present this contradiction; he offers a compelling and insightful hypothesis to explain it: it all depends on the depth of the feature space.\n\nIn a standard Inception module, the spatial convolutions (like 3x3s) operate on “deep” feature maps with many channels (e.g., 64, 96, or 128). In such a rich, high-dimensional space, an activation function like ReLU can help the network learn more complex, non-linear features.\nIn a depthwise separable convolution, the spatial convolution operates on each channel independently. This means it’s working on an extremely “shallow” feature space—just a single channel at a time. The author speculates that applying a harsh non-linearity like ReLU to a single channel’s feature map might be destructive. It could be erasing valuable information by clamping all negative values to zero before that information has a chance to be recombined with other channels in the subsequent pointwise step.\n\nThis is a brilliant piece of scientific reasoning that provides a plausible explanation for a counter-intuitive result and gives us a deeper intuition for how these different architectural components work."
  },
  {
    "objectID": "posts/2025-08-30-xception-reading-notes.html#future-directions-and-final-conclusions",
    "href": "posts/2025-08-30-xception-reading-notes.html#future-directions-and-final-conclusions",
    "title": "Xception Explained: A Deep Dive into Extreme Inception and Smarter Convolutions",
    "section": "Future Directions and Final Conclusions",
    "text": "Future Directions and Final Conclusions\nAfter a thorough series of experiments, the paper concludes on page 7 by summarizing its findings and looking toward the future.\n\nSection 5: What’s Next? (Future Directions)\nA hallmark of a great research paper is that it often raises as many questions as it answers. The author revisits the “discrete spectrum” of convolutions he introduced earlier.\n\n\n(Page 7, Section 5): “We showed in our empirical evaluation that the extreme formulation of an Inception module, the depthwise separable convolution, may have advantages over regular a regular Inception module. However, there is no reason to believe that depthwise separable convolutions are optimal.”\n\nThis is a humble and insightful point. The paper has shown that one extreme of the spectrum (Xception, with 1 segment per channel) is better than a point in the middle (Inception, with 3-4 segments). But is it the absolute best point? The author suggests that it may not be. Perhaps an intermediate point—for example, using “grouped convolutions” with 8 or 16 channel segments—could offer an even better trade-off between computational cost and model accuracy. He leaves this as an open question for future investigation.\n\n\nSection 6: The Final Word (Conclusions)\nThe final section of the paper is a concise and elegant summary of its entire narrative, bringing the story full circle.\n\n\nThe Core Idea: The paper showed that standard convolutions and depthwise separable convolutions can be viewed as two endpoints of a spectrum, with the famous Inception module living somewhere in between.\nThe Proposal: This insight led to the creation of Xception, a novel architecture that takes the Inception philosophy to its logical extreme by replacing Inception modules entirely with depthwise separable convolutions.\nThe Results: When compared against Inception V3, a model with a similar number of parameters, Xception demonstrated small performance gains on the highly-tuned ImageNet dataset and, more importantly, large gains on the massive JFT dataset.\nThe Impact: The paper concludes with a prediction: depthwise separable convolutions are poised to become a “cornerstone of convolutional neural network architecture design.” They offer the same powerful properties of factorization as Inception modules, but they are conceptually simpler, more general, and just as easy to implement as regular convolution layers.\n\nLooking back from today, it’s clear that this prediction was remarkably accurate. Depthwise separable convolutions are now a fundamental tool used in countless state-of-the-art architectures, especially those designed for mobile and efficient computing (like the MobileNet family).\nThe Xception paper remains a landmark not just for the architecture it proposed, but for the clear and principled way it deconstructed an existing idea and pushed it to a new, more powerful conclusion. It serves as a masterclass in how to reason about the very building blocks of deep learning."
  },
  {
    "objectID": "posts/2025-08-21-squeezenet-reading-notes.html#introduction",
    "href": "posts/2025-08-21-squeezenet-reading-notes.html#introduction",
    "title": "SqueezeNet Paper Explained: A Deep Dive into 50x Smaller, AlexNet-Level AI",
    "section": "Introduction",
    "text": "Introduction\nIf you were involved in the world of AI and deep learning around 2015, you’d know one mantra ruled them all: bigger is better. The race to conquer computer vision benchmarks like ImageNet was a horsepower competition. Researchers were building deeper, wider, and more complex Convolutional Neural Networks (CNNs), like the famous VGGNet (VGG16), which had a staggering 138 million parameters. These massive models were breaking accuracy records, but they came with a hefty price tag. They were slow to train, expensive to store, and nearly impossible to run on anything but a powerful server with high-end GPUs.\nThen, in 2016, a paper from researchers at Berkeley and Stanford turned this idea on its head. Titled “SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and &lt;0.5MB model size”, it wasn’t just another incremental improvement. It was a radical rethinking of what a neural network could be.\n\n\n\nPaper title and abstract\n\n\nThe authors posed a simple but revolutionary question: Instead of just pushing for higher accuracy, what if we could achieve the same industry-standard accuracy with a model that was drastically more efficient?\nThis is the story of SqueezeNet. It’s the story of how a small, intelligently designed architecture not only matched the performance of the legendary AlexNet (the model that kickstarted the deep learning revolution) but did so with 50 times fewer parameters. And if that wasn’t enough, they showed that with modern compression techniques, their model could be shrunk to be 510 times smaller than AlexNet—weighing in at less than 0.5 megabytes. That’s small enough to fit on even the most constrained microcontrollers.\nIn this post, we’re going to take a deep dive into the SqueezeNet paper, page by page. We’ll unpack the core ideas, understand the brilliant design choices, and see the results that made this paper a landmark in the field of efficient AI. Whether you’re a student, a practitioner, or just curious about how AI models work under the hood, this breakdown will give you a clear, step-by-step understanding of this tiny giant.\nHere’s what we’ll cover:\n\nFirst, we’ll explore the motivation behind the paper: why small models are a really big deal for real-world applications.\nNext, we’ll break down the three secret ingredients in SqueezeNet’s design recipe that make it so incredibly parameter-efficient.\nWe’ll then meet the “Fire module,” the clever building block that powers the entire network.\nFinally, we’ll look at the jaw-dropping results and the scientific experiments the authors ran to prove their ideas and provide timeless lessons for anyone building AI models today.\n\nLet’s get started."
  },
  {
    "objectID": "posts/2025-08-21-squeezenet-reading-notes.html#why-small-models-are-a-big-deal",
    "href": "posts/2025-08-21-squeezenet-reading-notes.html#why-small-models-are-a-big-deal",
    "title": "SqueezeNet Paper Explained: A Deep Dive into 50x Smaller, AlexNet-Level AI",
    "section": "Why Small Models are a Big Deal",
    "text": "Why Small Models are a Big Deal\nBefore we dive into the technical brilliance of how SqueezeNet was built, let’s start with the fundamental question the authors address on the very first page: Why should we even care about making models smaller? If we can get high accuracy with a big model, isn’t that good enough?\n\n\n\nIntroduction and motivation\n\n\nAs it turns out, model size isn’t just an academic curiosity; it’s one of the biggest roadblocks to deploying AI in the real world. The SqueezeNet paper lays out three compelling, practical reasons why smaller is better.\n\n1. More Efficient Distributed Training\nTraining a state-of-the-art deep learning model on a massive dataset like ImageNet can take days or even weeks on a single machine. To speed this up, companies use “distributed training,” where the workload is split across multiple servers. In the most common setup (called data-parallel training), each server gets a copy of the model and a different chunk of the data. After processing its data, each server needs to communicate its learnings (the updated model parameters) to all the other servers so they can sync up and stay consistent.\nThis communication step is often the bottleneck that limits how fast you can train. The amount of data that needs to be sent is directly proportional to the number of parameters in the model.\n\nA big model like AlexNet: has ~60 million parameters. At 4 bytes per parameter (a 32-bit float), that’s 240MB of data that needs to be communicated across the network at every single training step.\nA small model like SqueezeNet: has only ~1.2 million parameters, which is just 4.8MB of data.\n\nBy requiring less communication, smaller models train significantly faster in a distributed environment, saving both time and money.\n\n\n2. Less Overhead for “Over-the-Air” Updates\nImagine you’re an engineer at a company like Tesla. You’ve just improved the self-driving algorithm, and you need to push this new model out to every car on the road. This is called an “over-the-air” (OTA) update.\nIf your model is the 240MB AlexNet, you have to send a quarter-gigabyte file over a cellular connection to hundreds of thousands of vehicles. This is slow, expensive (in terms of data costs), and can be unreliable. It limits how often you can send out safety improvements.\nNow, imagine your model is the &lt;1MB compressed SqueezeNet. The update is tiny. You can push it out quickly, cheaply, and frequently, ensuring your entire fleet is always running the latest and safest software. This isn’t just a convenience; as the paper notes, the safety of Tesla’s Autopilot has been shown to improve with these incremental updates.\n\n\n3. Feasible Deployment on Edge Devices (The Embedded World)\nThis is perhaps the most critical advantage. The future of AI isn’t just in the cloud; it’s on the “edge”—the billions of small devices that surround us, like our smartphones, smart cameras, drones, and industrial sensors. These devices have a very strict power and memory budget.\nThe paper gives the example of an FPGA (Field-Programmable Gate Array), a type of customizable chip often used in embedded systems. These chips are great for high-speed processing but often have very little on-chip memory (e.g., less than 10MB) and no way to access slower, off-chip memory.\n\nA 240MB model is a non-starter. It simply will not fit.\nA &lt;1MB SqueezeNet model, however, can be stored entirely within the FPGA’s fast on-chip memory. This eliminates the single biggest bottleneck in embedded AI: the slow, power-draining process of fetching model weights from external memory. This allows the device to process data (like a video stream) in real-time with very low latency and power consumption.\n\nBy making on-device AI feasible, small models unlock a world of applications that are simply impossible with their larger counterparts. And with these powerful motivations in mind, we’re now ready to see exactly how the SqueezeNet authors pulled it off."
  },
  {
    "objectID": "posts/2025-08-21-squeezenet-reading-notes.html#the-building-blocks---what-inspired-squeezenet",
    "href": "posts/2025-08-21-squeezenet-reading-notes.html#the-building-blocks---what-inspired-squeezenet",
    "title": "SqueezeNet Paper Explained: A Deep Dive into 50x Smaller, AlexNet-Level AI",
    "section": "The Building Blocks - What Inspired SqueezeNet?",
    "text": "The Building Blocks - What Inspired SqueezeNet?\nBefore the SqueezeNet authors could build their revolutionary architecture, they stood on the shoulders of giants. On page 2 of the paper, they review the “Related Work” — the key ideas and trends in the field that set the stage for their breakthrough. Understanding this context helps us appreciate why SqueezeNet’s design choices were so clever and timely.\n\n\n\nRelated work\n\n\n\nApproach #1: Compressing a Big Model\nAs we discussed, the dominant approach to getting a smaller model was to take a pre-trained behemoth like AlexNet and shrink it. The paper highlights a few popular techniques:\n\nPruning: Imagine that many of the connections in a trained network are redundant or useless (their weight value is close to zero). Network Pruning, pioneered by SqueezeNet author Song Han, simply removes these weak connections, creating a “sparse” model that has fewer parameters to store.\nQuantization: Instead of storing each number (or “weight”) in the model as a high-precision 32-bit floating-point number, quantization reduces their precision to 8-bit integers or even less. This dramatically reduces the file size.\nDeep Compression: This was the state-of-the-art technique at the time, also from Song Han’s group. It’s a powerful three-step pipeline: Prune the network, Quantize the remaining weights, and then use standard Huffman encoding (like a .zip file) for even more compression.\n\nThe key takeaway here is that SqueezeNet was entering a world that was already very good at model compression. This sets a high bar and makes SqueezeNet’s “small architecture” approach even more impressive.\n\n\nApproach #2: Designing a Better Architecture\nThe authors also drew inspiration from new ideas in how to design the networks themselves. They break this down into two key concepts:\n1. CNN Microarchitecture (The Design of a Single LEGO Brick)\n\n\n\nCNN Microarchitecture\n\n\nThis refers to the design of a single layer or a small, repeating block of layers. The paper traces a fascinating historical trend in the size of convolution filters:\n\nEarly Days (LeNet): Used 5x5 filters.\nThe Deep Era (VGG): Standardized on 3x3 filters, showing that stacking smaller filters was more efficient.\nThe Efficient Era (GoogLeNet, Network-in-Network): Introduced the powerful 1x1 filter. While it can’t see spatial patterns, the 1x1 filter is incredibly efficient at processing information across the channels of a feature map.\n\nThis trend toward smaller, more efficient filters, especially the 1x1 filter, is a massive clue to SqueezeNet’s design. They also note that modern networks like GoogLeNet were moving away from simple layers and toward complex “modules” (like the Inception module) as the main building blocks. SqueezeNet would follow this path by designing its own unique module.\n2. CNN Macroarchitecture (How You Stack the LEGOs)\n\n\n\nCNN Macroarchitecture\n\n\nThis refers to the high-level, end-to-end structure of the entire network. The big topics of discussion here were:\n\nNetwork Depth: How many layers should you have? Researchers were showing that deeper networks (like VGG and later ResNet) could achieve higher accuracy.\nConnections: Do layers always have to follow a simple sequence? The groundbreaking ResNet paper introduced “bypass” or “skip” connections, which allowed information to skip over several layers. This helped very deep networks train more effectively and achieve even higher accuracy.\n\nSqueezeNet’s design would incorporate lessons from both of these areas, and as we’ll see later in the experiments, the idea of bypass connections would prove to be especially powerful.\nWith this background in mind—a world focused on compressing big models and experimenting with new micro- and macro-architectures—we’re now ready to see how the SqueezeNet authors combined these ideas into their three secret strategies.\n\n\nA Note on “Design Space Exploration”\n\n\n\nDesign Space Exploration\n\n\nBefore they unveil their own design strategies, the authors make one final and important point about how they approached the problem of designing a new network.\nThe universe of all possible neural network architectures is unimaginably vast. You have countless choices for the number of layers, the size of filters, the connections between them, and dozens of other hyperparameters. This vast universe of possibilities is what researchers call the “design space.”\nThe authors point out that a lot of research at the time focused on automated approaches to search this space. These techniques use algorithms to try and automatically “discover” a network that delivers the highest possible accuracy. Some popular methods included:\n\nRandomized Search: Literally trying random combinations of hyperparameters.\nBayesian Optimization: A smarter search method that builds a model of the design space to guide its search toward more promising areas.\nGenetic Algorithms: An approach inspired by evolution, where architectures are “bred” and “mutated” over generations to produce better-performing offspring.\n\nWhile these automated methods are powerful for finding a single, high-accuracy model, the SqueezeNet authors note a key drawback: they don’t provide much intuition. You get a final answer, but you don’t necessarily understand why that architecture works well or what general principles you can learn from it.\nSqueezeNet’s Approach: Principled and Systematic Exploration\nThe authors deliberately choose a different path. Instead of using an automated black-box search, they propose a more disciplined, scientific approach. Their goal isn’t just to find a good model, but to understand the principles that make a model good.\nThey state that in the later sections of the paper, they will:\n\n“…refactor CNNs in such a way that we can do principled A/B comparisons to investigate how CNN architectural decisions influence model size and accuracy.”\n\nThis is a key philosophical point. They are setting themselves up not just as engineers building a product (SqueezeNet), but as scientists conducting experiments to understand the fundamental trade-offs in network design. This is what will allow them to derive general lessons, like the optimal Squeeze Ratio or the best mix of 1x1 and 3x3 filters, which are valuable insights for any network architect.\nThis commitment to understanding the why is what makes the SqueezeNet paper so influential and educational. And now, with the stage fully set, we’re ready to learn their three core strategies for doing just that."
  },
  {
    "objectID": "posts/2025-08-21-squeezenet-reading-notes.html#the-secret-sauce---squeezenets-3-core-design-strategies",
    "href": "posts/2025-08-21-squeezenet-reading-notes.html#the-secret-sauce---squeezenets-3-core-design-strategies",
    "title": "SqueezeNet Paper Explained: A Deep Dive into 50x Smaller, AlexNet-Level AI",
    "section": "The Secret Sauce - SqueezeNet’s 3 Core Design Strategies",
    "text": "The Secret Sauce - SqueezeNet’s 3 Core Design Strategies\nNow we get to the heart of the matter. On page 3 of the paper, the authors lay out the blueprint for how they achieved their incredible results. They didn’t rely on a secret algorithm or a complex new type of layer. Instead, they used three elegant and intuitive design strategies to attack the parameter count at its source.\n\n\n\nSqueezeNet’s 3 Core Design Strategies\n\n\nTheir overarching objective is stated plainly: “to identify CNN architectures that have few parameters while maintaining competitive accuracy.”\nHere’s the three-part recipe they developed to achieve this goal.\n\nStrategy 1: Replace 3x3 filters with 1x1 filters.\nThis is the most direct way to cut down on parameters. As we learned from the related work, the 1x1 convolution was a powerful new tool, and the SqueezeNet authors decided to use it aggressively.\nThe logic is simple arithmetic. The number of parameters in a single filter is its height × width.\n\nA 3x3 filter has 3 * 3 = 9 parameters.\nA 1x1 filter has 1 * 1 = 1 parameter.\n\nThis means a 1x1 filter is 9 times more parameter-efficient than a 3x3 filter. While you can’t replace all the 3x3 filters (you still need them to see spatial patterns), the strategy is to make the vast majority of filters in the network 1x1, saving a huge number of parameters.\n\n\nStrategy 2: Decrease the number of input channels to 3x3 filters.\nThis strategy is the most clever and is the key to the “Squeeze” in SqueezeNet. The authors realized that the cost of a convolution layer doesn’t just depend on the filter size; it’s also directly multiplied by the number of channels coming into the layer.\nThe full formula for parameters in a layer is: (number of input channels) × (number of filters) × (filter size)\nSo, even if you are using “expensive” 3x3 filters, you can make them much cheaper if you can first reduce the number of input channels they have to process.\nThis is exactly what SqueezeNet does. It introduces a “squeeze layer” made of cheap 1x1 filters whose only job is to act as a channel-wise bottleneck. It takes a “thick” input with many channels and “squeezes” it down into a “thin” output with very few channels. Only then is this thin output fed to the 3x3 filters.\nBy protecting the expensive 3x3 filters with a cheap squeeze layer, they drastically reduce the total parameter count of the entire module.\n\n\n\nSqueezeNet’s 3 Core Design Strategies\n\n\nBelow is an illustration of how a 1x1 Pointwise convolution layer can be used to reduce the number of input channels.\n\n\n\n1x1 Pointwise Convolution\n\n\n\n\nStrategy 3: Downsample late in the network.\nThe first two strategies are about reducing model size while preserving accuracy. This third strategy is about maximizing accuracy on a tight parameter budget.\nIn a CNN, “downsampling” (usually done with pooling layers) reduces the height and width of the feature maps.\n\nEarly Downsampling: If you downsample early, the feature maps become small quickly. This is computationally cheap, but it means most of your layers are working with low-resolution information, which can make it hard to detect small objects and fine details, ultimately hurting accuracy.\nLate Downsampling: If you delay the downsampling and place the pooling layers toward the end of the network, most of your convolution layers get to operate on large, high-resolution feature maps.\n\nThe authors’ intuition, backed by other research, is that large activation maps lead to higher classification accuracy. The challenge, of course, is that operating on large maps is computationally expensive.\nAnd now we can see how these three strategies brilliantly lock together:\n\nStrategy 3 says “keep the feature maps large” to boost accuracy.\nThis choice makes the network very expensive, which forces them to be hyper-efficient with their parameters.\nStrategies 1 and 2 provide the tools (heavy use of 1x1 filters and squeeze layers) to manage this cost effectively.\n\nThese three principles form a complete and coherent design philosophy. Next, we’ll see how they are embodied in SqueezeNet’s custom building block: the Fire module."
  },
  {
    "objectID": "posts/2025-08-21-squeezenet-reading-notes.html#meet-the-fire-module---the-engine-of-squeezenet",
    "href": "posts/2025-08-21-squeezenet-reading-notes.html#meet-the-fire-module---the-engine-of-squeezenet",
    "title": "SqueezeNet Paper Explained: A Deep Dive into 50x Smaller, AlexNet-Level AI",
    "section": "Meet the “Fire Module” - The Engine of SqueezeNet",
    "text": "Meet the “Fire Module” - The Engine of SqueezeNet\nTheory and design principles are great, but how do you actually put them into practice? On the second half of page 3 and the top of page 4, the authors introduce the elegant building block that brings their strategies to life: the Fire module.\nThink of the Fire module as SqueezeNet’s signature LEGO brick. The entire network is built by stacking these clever little modules on top of each other. It’s a beautiful piece of microarchitecture designed specifically to be lean and efficient.\nLet’s look at its two-part structure, which directly implements Strategies 1 and 2.\n\n\n\nFire module\n\n\n\n1. The squeeze Layer\nThe first part of the Fire module is a squeeze convolution layer.\n\nWhat it is: A simple convolution layer that contains only 1x1 filters.\nIts Purpose: To be a bottleneck. It takes the input, which might have a large number of channels, and “squeezes” it down to an intermediate feature map with a much smaller number of channels.\nThis is Strategy #2 in action: Decrease the number of input channels to 3x3 filters. By putting this layer first, the Fire module ensures that the expensive 3x3 filters that come next will have far fewer input channels to process, saving a massive number of parameters.\n\n\n\n2. The expand Layer\nThe second part of the Fire module is the expand layer. It takes the “thin” output from the squeeze layer and feeds it into two parallel sets of filters.\n\nWhat it is: A combination of both 1x1 filters and 3x3 filters. The outputs of these two sets of filters are then concatenated (stacked together) in the channel dimension to form the final output of the Fire module.\nIts Purpose: This is where the actual feature learning happens. The 3x3 filters can learn spatial patterns (like edges and textures), while the 1x1 filters learn to combine information across channels.\nThis is Strategy #1 in action: The liberal use of 1x1 filters in the expand layer, working alongside the more traditional 3x3 filters, helps keep the parameter count low.\n\n\n\nThe Golden Rule of the Fire Module\n\n\n\nFire module\n\n\nTo ensure the squeeze layer always acts as a bottleneck, the authors define a crucial rule. If we let s_1x1 be the number of filters in the squeeze layer, and e_1x1 and e_3x3 be the number of 1x1 and 3x3 filters in the expand layer, then they always set:\ns_1x1 &lt; (e_1x1 + e_3x3)\nThis simple inequality guarantees that the number of channels is always reduced before being expanded again.\nIn the example shown in Figure 1, s_1x1 = 3, while e_1x1 = 4 and e_3x3 = 4. The total expand filters are 4 + 4 = 8. Since 3 &lt; 8, the rule holds, and a bottleneck is successfully created.\nThis elegant two-part module is the workhorse of SqueezeNet. By combining a channel-reducing squeeze layer with a mixed-filter expand layer, it perfectly embodies the principles of a small-but-powerful architecture. Now, let’s see how these modules are stacked together to build the full network."
  },
  {
    "objectID": "posts/2025-08-21-squeezenet-reading-notes.html#assembling-the-full-squeezenet---from-modules-to-a-complete-network",
    "href": "posts/2025-08-21-squeezenet-reading-notes.html#assembling-the-full-squeezenet---from-modules-to-a-complete-network",
    "title": "SqueezeNet Paper Explained: A Deep Dive into 50x Smaller, AlexNet-Level AI",
    "section": "Assembling the Full SqueezeNet - From Modules to a Complete Network",
    "text": "Assembling the Full SqueezeNet - From Modules to a Complete Network\nNow that we have our custom LEGO brick—the Fire module—it’s time to build the spaceship. On pages 4 and 5, the authors describe the macroarchitecture of SqueezeNet: the high-level, end-to-end organization of the entire network.\n\n\n\nThe complete SqueezeNet macroarchitecture (left), as shown in Figure 2 of the paper. Middle and Right show experimental variations we’ll discuss later.\n\n\nThe diagram on the left of Figure 2 shows the complete, vanilla SqueezeNet. Let’s walk through its structure from input to output.\n\nThe High-Level Structure\nThe overall design is a clean, sequential stack of layers:\n\nconv1: The network starts with a single, standard convolution layer. As we discussed, this is a common best practice. It takes the raw 224x224x3 image and performs an initial round of feature extraction and downsampling, preparing the data for the main body of the network.\nfire2 to fire9: The core of the network consists of 8 Fire modules, stacked one after another. This is where the bulk of the computation and feature learning happens.\nconv10: The network ends with a final 1x1 convolution layer that acts as the classifier. It has 1000 filters, one for each class in the ImageNet dataset.\nglobal avgpool and softmax: Instead of a bulky, parameter-heavy fully-connected layer, SqueezeNet uses a modern Global Average Pooling layer. This layer takes the output of conv10 (which you can think of as 1000 class “heatmaps”) and averages each map down to a single number, producing the final 1000-dimensional vector for the softmax classifier.\n\n\n\nKey Macro-Level Design Choices\nLooking closer, we can see how the authors implemented their third design strategy at the macro-level:\n\nGradually Increasing Channels: Notice the numbers under each Fire module (128, 256, 384, 512). The authors gradually increase the number of filters (and thus the channel depth) as the network gets deeper. This is a standard design pattern that allows the network to learn progressively more complex features.\nLate Downsampling (Strategy #3 in Action): The maxpool/2 layers, which cut the height and width of the feature maps in half, are placed sparingly. There’s one after conv1, one after fire4, and one after fire8. By spacing them out, they ensure that long chains of Fire modules (fire2-4 and fire5-8) operate on feature maps of a constant, large spatial size. This is the direct implementation of “downsample late in the network” to maximize accuracy.\n\n\n\nOther Important Details\n\n\n\nOther Important Details\n\n\nThe authors also list several other crucial, nitty-gritty details required to build and train the model successfully:\n\nPadding: They add a 1-pixel border of zero-padding to the 3x3 filters to ensure their output has the same dimensions as the 1x1 filters, allowing them to be concatenated.\nActivation Function: They use the standard ReLU activation after every squeeze and expand layer.\nDropout: To prevent overfitting, they apply dropout with a 50% ratio after the fire9 module.\nNo Fully-Connected Layers: This is a major design choice that saves millions of parameters compared to older models like AlexNet. This idea was inspired by the “Network in Network” paper.\nTraining: They provide details on their learning rate schedule and, importantly, link to their GitHub repository with the exact Caffe configuration files, promoting reproducible research.\n\nWith the complete recipe for the SqueezeNet architecture now laid out, from the micro-level Fire module to the macro-level network stack, we are finally ready to see the results of their work."
  },
  {
    "objectID": "posts/2025-08-21-squeezenet-reading-notes.html#digging-deeper-answering-your-but-why-questions",
    "href": "posts/2025-08-21-squeezenet-reading-notes.html#digging-deeper-answering-your-but-why-questions",
    "title": "SqueezeNet Paper Explained: A Deep Dive into 50x Smaller, AlexNet-Level AI",
    "section": "Digging Deeper: Answering Your “But Why?” Questions",
    "text": "Digging Deeper: Answering Your “But Why?” Questions\nAs we walk through the design of SqueezeNet, a few key concepts of modern CNNs come up that are worth exploring in more detail. If you’ve ever wondered how channels “grow” inside a network or why certain layers are placed where they are, this section is for you.\n\nDeep Dive A: How Do Channels Grow Beyond the Initial 3 (RGB)?\nThis is a fantastic and fundamental question. The input image has just 3 channels (Red, Green, and Blue), so how do we end up with layers that have 512 channels?\nThe magic happens in the convolution layers themselves. The number of filters in a convolution layer determines the number of channels (the “depth”) of its output.\nLet’s trace it:\n\nInput to conv1: A 224x224x**3** image.\nInside conv1: We apply a set of, say, 96 different filters. Each filter is designed to look for a specific low-level feature (like a vertical edge, a green-to-blue gradient, etc.). Each filter slides over the 3-channel input and produces a single 2D output map showing where it found its feature.\nOutput of conv1: When we stack the 96 output maps from our 96 filters, we get a new data volume of size 111x111x**96**.\n\nWe now have 96 channels! These are no longer “color” channels; they are “feature” channels. The input to the next layer (fire2) will have 96 channels, and its filters will learn to combine these basic edge and color features into more complex patterns. As we go deeper, the spatial dimensions (height/width) shrink, but the channel depth grows, allowing the network to build a richer, more abstract understanding of the image content. This is how SqueezeNet’s Strategy 2 becomes so critical in the deeper layers.\n\n\nDeep Dive B: The Trade-off: Spatial Size vs. Channel Depth\nYou’re right to notice that as a CNN processes an image, two things are happening: the channel depth is increasing, while the spatial resolution (height x width) is decreasing. Shouldn’t these two effects cancel each other out?\nNot quite. The total volume of activations (H x W x C) often increases dramatically in the early layers. This is because pooling (which reduces spatial size) doesn’t happen at every layer, and the increase in channels is often more aggressive than the decrease in space.\nThis tension is exactly what Strategy 3 (“Downsample Late”) addresses. The authors make a deliberate choice to prioritize keeping the spatial dimensions (H x W) as large as possible for as long as possible. Why? Because high-resolution feature maps retain more detailed information, which is crucial for achieving high accuracy.\nThe consequence of this choice is that the activation volumes are massive, which would be computationally unaffordable. This is why Strategies 1 and 2 (the hyper-efficient Fire modules) are not just nice-to-have; they are absolutely essential to make Strategy 3 viable.\n\n\nDeep Dive C: The Modern Classifier - conv10 Followed by Pooling\nShouldn’t the pooling layer come before the classifier? In older architectures like AlexNet, yes. But SqueezeNet uses a more modern and efficient technique called Global Average Pooling (GAP).\nHere’s the flow:\n\nThe conv10 layer is a 1x1 convolution with 1000 filters (one for each ImageNet class). Its output is a 13x13x1000 volume. Think of this as 1000 “heatmaps,” where each map shows where the network “sees” features corresponding to that class.\nThe Global Average Pooling layer then takes each of these 13x13 heatmaps and calculates its average value, squashing it down to a single number.\nThe result is a final 1x1x1000 vector, ready for the softmax function.\n\nThis conv -&gt; GAP structure is vastly superior to the old pool -&gt; flatten -&gt; fully-connected structure because:\n\nIt saves millions of parameters by completely eliminating the massive fully-connected layers. This is a core reason SqueezeNet is so small.\nIt’s more interpretable. You can actually look at the heatmaps before the GAP layer to see what parts of the image the network is paying attention to for its classification.\n\n\n\nDeep Dive D: Why Use a “Big” 7x7 Filter in a “Small” Network?\nIt seems counterintuitive: if the whole point of SqueezeNet is to use small filters (Strategy 1), why does it start with a comparatively massive 7x7 filter in its very first layer (conv1)?\nThis is a deliberate and wise exception to the rule. The first convolution layer has a unique and challenging job that makes a larger filter the better tool.\n\nIt Sees Raw Pixels: conv1 is the only layer that processes the raw image. A larger filter has a wider receptive field, meaning it can see a larger patch of pixels at once (a 7x7 patch vs. a 3x3 patch). This allows it to capture more meaningful, slightly larger initial patterns like gradients and textures directly from the noisy pixel space.\nIt’s an Efficient Downsampler: The input image is spatially huge (224x224). The network needs to shrink this down quickly to be computationally manageable. The conv1 layer uses a stride of 2, which means it slides the filter by 2 pixels at a time. Combining a large 7x7 filter with a stride of 2 is a very efficient way to perform both feature extraction and downsampling in a single operation.\nThe “Cost” is Deceptively Low: Here’s the most important part. The cost of a filter depends heavily on the number of input channels. The conv1 layer’s 7x7 filter operates on only 3 input channels (RGB). The total parameter cost is just (7 * 7 * 3) * 96 filters = 14,112 parameters. This is a drop in the bucket, accounting for only ~1% of SqueezeNet’s total 1.25 million parameters.\n\nIn contrast, a “small” 3x3 filter deep in the network might operate on 512 input channels, costing (3 * 3 * 512) * 512 filters = 2,359,296 parameters!\nThe authors make a calculated trade-off: they “spend” a tiny fraction of their parameter budget on conv1 to get a more effective and efficient start to the network, and then apply their extreme parameter-saving rules in the deeper layers where it truly matters."
  },
  {
    "objectID": "posts/2025-08-21-squeezenet-reading-notes.html#the-results---proof-in-the-numbers",
    "href": "posts/2025-08-21-squeezenet-reading-notes.html#the-results---proof-in-the-numbers",
    "title": "SqueezeNet Paper Explained: A Deep Dive into 50x Smaller, AlexNet-Level AI",
    "section": "The Results - Proof in the Numbers",
    "text": "The Results - Proof in the Numbers\nAfter meticulously detailing their design philosophy and architecture, the authors dedicate Section 4 to the crucial evaluation. This is where the rubber meets the road. They compare their creation against the reigning champion, AlexNet, and the state-of-the-art in model compression.\n\n\n\nResults Overview\n\n\nThe setup is simple: train the models on the massive ImageNet (ILSVRC 2012) dataset and compare their size and accuracy.\n\nTable 1: The Anatomy of SqueezeNet\nBefore we get to the final comparison, Table 1 provides a detailed layer-by-layer breakdown of the SqueezeNet architecture. It’s a goldmine of information, but the most important number is at the very bottom right:\n\nTotal parameters (before pruning): 1,248,424\n\nLet’s put that in context. AlexNet, the model they are comparing against, has roughly 60 million parameters. Right out of the gate, with no compression tricks at all, the SqueezeNet architecture is ~50 times smaller than AlexNet.\n\n\nThe Main Event: SqueezeNet vs. The World\n\n\n\nComparison Overview\n\n\nThis is the money shot. Table 2 summarizes the entire story of the paper in a few powerful lines. To make it clear, let’s compare the two fundamental approaches to getting a small, accurate model.\nApproach A: Compress a Big Model (The Old Way)\nThis was the state-of-the-art before SqueezeNet. You start with the huge 240MB AlexNet model and use sophisticated tools to shrink it. The best result at the time was from the Deep Compression technique:\n\nModel: AlexNet + Deep Compression\nModel Size: 6.9 MB (a 35x reduction from the original 240MB)\nTop-5 Accuracy: 80.3%\n\nThis was an impressive achievement. A 35x reduction with no loss in accuracy is fantastic. But could we do better?\nApproach B: Design a Small Model from Scratch (The SqueezeNet Way)\nThis is the core hypothesis of the SqueezeNet paper. What if, instead of compressing a bloated model, we just designed a lean, efficient one from the beginning?\n\nModel: SqueezeNet (uncompressed, using standard 32-bit floats)\nModel Size: 4.8 MB (a 50x reduction)\nTop-5 Accuracy: 80.3%\n\nThis result is stunning. The SqueezeNet architecture, by itself, is smaller and just as accurate as the best-compressed version of AlexNet. This proves that intelligent architecture design is a more powerful tool for efficiency than post-hoc compression.\n\n\nThe Knockout Punch: The Best of Both Worlds\nThe authors then asked a brilliant follow-up question: what happens if we apply the best compression techniques to our already-small model? Are there still more savings to be had?\n\nModel: SqueezeNet + Deep Compression (with 6-bit quantization)\nModel Size: 0.47 MB (a staggering 510x reduction from AlexNet)\nTop-5 Accuracy: 80.3%\n\nThis is the ultimate result. By combining a small, efficient architecture with state-of-the-art compression, the authors achieved a model that is over 500 times smaller than the original industry standard, without sacrificing a single drop of accuracy. They proved that these two approaches are not competing, but complementary.\nThis section validates every design choice we’ve discussed. The three strategies, the Fire module, and the macro-architecture all come together to produce a model that fundamentally changed how researchers think about the trade-off between size and performance."
  },
  {
    "objectID": "posts/2025-08-21-squeezenet-reading-notes.html#going-deeper---scientific-experiments-on-squeezenets-design",
    "href": "posts/2025-08-21-squeezenet-reading-notes.html#going-deeper---scientific-experiments-on-squeezenets-design",
    "title": "SqueezeNet Paper Explained: A Deep Dive into 50x Smaller, AlexNet-Level AI",
    "section": "Going Deeper - Scientific Experiments on SqueezeNet’s Design",
    "text": "Going Deeper - Scientific Experiments on SqueezeNet’s Design\nThe SqueezeNet authors didn’t just want to present a new model; they wanted to understand the principles that made it work. In Sections 5 and 6, they go back and systematically test their own design choices. This “Design Space Exploration” is one of the most valuable parts of the paper, offering timeless lessons for anyone building a CNN.\n\n\n\nCNN Design Space Exploration\n\n\nFirst, they tackle the microarchitecture—the internal guts of the Fire module. They ask: What happens if we tweak the knobs of the Fire module? Are our original design choices actually optimal?\nTo make this exploration manageable, they first introduce a brilliant simplification. Instead of tweaking the 24 individual hyperparameters of the 8 Fire modules, they define a few high-level “metaparameters” that control the whole network’s structure.\n\n\n\nCNN Microarchitecture Metaparameters\n\n\nThese master controls include: * The Squeeze Ratio (SR): Controls the tightness of the bottleneck. * The pct_3x3: Controls the percentage of 3x3 filters in the expand layer. * Other knobs to control the growth of filters through the network (base_e, incr_e, etc.).\nWith this simpler set of controls, they can now run principled A/B tests.\n\nExperiment 1: How Much Should We “Squeeze”?\n\n\n\nSqueeze Ratio Experiment\n\n\nThe first experiment investigates Strategy #2 by testing the Squeeze Ratio (SR). This ratio determines the size of the bottleneck in the Fire modules. A small SR (like SqueezeNet’s 0.125) means a very tight bottleneck, while a large SR (like 1.0) means no bottleneck at all.\nThe Findings (as seen in Figure 3a): The results revealed a clear and important trade-off:\n\nMore Parameters = More Accuracy (to a point): As they increased the Squeeze Ratio from 0.125, the model size grew, but so did the accuracy. The original 4.8MB SqueezeNet achieved 80.3% accuracy, but a larger 19MB version with a looser bottleneck (SR=0.75) reached a much higher accuracy of 86.0%.\nA Point of Diminishing Returns: After SR=0.75, the accuracy completely plateaued. Making the model even bigger by removing the bottleneck entirely (SR=1.0) offered no extra performance boost.\n\nThe Lesson: The Squeeze Ratio is a powerful tuning knob for balancing size and accuracy. If your primary goal is the absolute smallest model that hits a certain target (like AlexNet), a tight bottleneck is best. But if you have a bit more memory to spare, you can significantly boost accuracy by loosening that bottleneck.\n\n\nExperiment 2: What’s the Right Mix of 1x1 and 3x3 Filters?\n\n\n\n1x1 vs 3x3 Filters Experiment\n\n\nThe second experiment investigates Strategy #1 by testing the ideal percentage of 3x3 filters (pct_3x3) in the expand layer. Remember, 1x1 filters are cheap, while 3x3 filters are expensive but are needed to see spatial patterns.\nThe Findings (as seen in Figure 3b): The results were once again clear and insightful:\n\nMore 3x3s = Bigger Model: Unsurprisingly, as the percentage of expensive 3x3 filters increased, the overall model size grew.\nAccuracy Peaks at a 50/50 Split: The model’s accuracy peaked at 85.6% when the expand layer’s filters were split 50% 3x3 and 50% 1x1.\nDiminishing Returns (Again): Increasing the percentage of 3x3 filters beyond 50% made the model much larger but gave zero improvement in accuracy.\n\nThe Lesson: You don’t need to overload your network with expensive, spatially-aware 3x3 filters. A balanced 50/50 diet of cheap 1x1 filters and expensive 3x3 filters provides the best bang for your buck, delivering peak accuracy without wasting parameters.\nThese microarchitecture experiments are a masterclass in principled design. They not only validate the choices made for the original SqueezeNet but also provide invaluable, general-purpose rules of thumb for anyone designing an efficient CNN."
  },
  {
    "objectID": "posts/2025-08-21-squeezenet-reading-notes.html#macro-level-tweaks---can-we-make-squeezenet-even-better-with-resnets-superpower",
    "href": "posts/2025-08-21-squeezenet-reading-notes.html#macro-level-tweaks---can-we-make-squeezenet-even-better-with-resnets-superpower",
    "title": "SqueezeNet Paper Explained: A Deep Dive into 50x Smaller, AlexNet-Level AI",
    "section": "Macro-Level Tweaks - Can We Make SqueezeNet Even Better with ResNet’s Superpower?",
    "text": "Macro-Level Tweaks - Can We Make SqueezeNet Even Better with ResNet’s Superpower?\nHaving fine-tuned the internals of the Fire module, the authors turned their attention to the macroarchitecture—the high-level wiring of the entire network.\n\n\n\nMacroarchitecture Experiments\n\n\nAt the time, the hottest new idea in CNN design was the “bypass” or “skip” connection from the ResNet paper. These connections allow information to skip over layers, which was shown to help train deeper networks and improve accuracy. The SqueezeNet team wisely decided to see if this powerful idea could benefit their architecture.\nThey set up a fascinating A/B/C test, comparing three different versions of their network:\n\nVanilla SqueezeNet: The original, purely sequential model.\nSqueezeNet with Simple Bypass: A version with parameter-free skip connections added wherever possible.\nSqueezeNet with Complex Bypass: A version where the skip connections themselves have a 1x1 convolution, allowing them to be used more broadly.\n\n\n\n\nThe three macroarchitectures tested, from Figure 2 of the paper. Note the curved ‘bypass’ connections in the middle and right diagrams.\n\n\n\nWhy Bypass Connections Are a Great Idea for SqueezeNet\nThe authors had two strong reasons to believe this would work:\n\nThe ResNet Reason: Just like in ResNet, skip connections could help with gradient flow and enable the layers to learn “residual” functions, potentially boosting overall accuracy.\nThe SqueezeNet-Specific Reason: This is the most brilliant insight. The original SqueezeNet has a very aggressive squeeze layer that acts as an information bottleneck (reducing channels by 8x). A bypass connection creates an information superhighway that allows data to flow around this bottleneck. This could preserve important features that might otherwise be lost in the squeeze, leading to a smarter, more accurate model.\n\n\n\nThe Surprising Results\n\n\n\nMacroarchitecture Experiment Results\n\n\nThe results from this experiment, summarized in Table 3, were both exciting and surprising:\n\nVanilla SqueezeNet (Baseline):\n\nTop-1 Accuracy: 57.5%\nModel Size: 4.8MB\n\nSqueezeNet + Complex Bypass:\n\nTop-1 Accuracy: 58.8% (A nice improvement!)\nModel Size: 7.7MB (But it got bigger.) The extra 1x1 convolutions in the bypass paths added nearly 3MB to the model.\n\nSqueezeNet + Simple Bypass:\n\nTop-1 Accuracy: 60.4% (An even BIGGER improvement!)\nModel Size: 4.8MB (And it’s FREE!)\n\n\nThis is a fantastic result. The simpler, parameter-free bypass connections gave a larger accuracy boost than the more complex ones, and they did it without adding a single byte to the model’s size. They provided a nearly 3 percentage point jump in Top-1 accuracy for free.\nThe Lesson: This experiment provides a clear and powerful recipe for an improved “SqueezeNet v2.” The core architecture is great, but it becomes even better when you augment it with simple bypass connections. This discovery showcases the power of combining architectural ideas (SqueezeNet’s efficiency + ResNet’s connectivity) to achieve results that are better than the sum of their parts.\nWith these experiments complete, the authors have not only given us a great model but have also armed us with the knowledge and intuition to adapt and improve upon it."
  },
  {
    "objectID": "posts/2025-08-21-squeezenet-reading-notes.html#the-legacy-of-squeezenet---small-is-the-new-big",
    "href": "posts/2025-08-21-squeezenet-reading-notes.html#the-legacy-of-squeezenet---small-is-the-new-big",
    "title": "SqueezeNet Paper Explained: A Deep Dive into 50x Smaller, AlexNet-Level AI",
    "section": "The Legacy of SqueezeNet - Small is the New Big",
    "text": "The Legacy of SqueezeNet - Small is the New Big\nWe’ve journeyed through the entire SqueezeNet paper, from its core motivations to its clever design, stunning results, and insightful experiments. Now, let’s tie it all together and reflect on the paper’s lasting impact.\n\n\n\nThe Legacy of SqueezeNet\n\n\n\nA Summary of Achievements\nThe SqueezeNet paper is a masterclass in efficient deep learning. Its contributions can be summed up in a few key points:\n\nA New Philosophy: It championed a shift from the “bigger is better” mindset to a focus on architectural efficiency. It proved that intelligent design could be more effective than brute-force compression.\nA Powerful Architecture: It delivered a concrete model, SqueezeNet, that achieved AlexNet-level accuracy with 50x fewer parameters.\nThe Best of Both Worlds: It showed that architecture and compression are complementary, combining them to create a final model that was 510x smaller than its predecessor, at under 0.5MB.\nActionable Design Principles: Through its systematic “Design Space Exploration,” it gave us invaluable rules of thumb, like the effectiveness of a 50/50 mix of 1x1 and 3x3 filters and the “free” accuracy boost from simple bypass connections.\n\n\n\nReal-World Impact and Follow-Up Work\nThe conclusion of the paper also highlights how SqueezeNet’s ideas immediately began to bear fruit in the real world:\n\nOn-Device Deployment: The authors’ dream of running models on resource-constrained hardware was realized almost immediately. They note that another researcher successfully implemented SqueezeNet on an FPGA, fitting the entire model into the chip’s fast on-board memory—a huge win for embedded AI.\nA Foundation for Future Research: The paper also points to exciting follow-up work, like Dense-Sparse-Dense (DSD) training, which used SqueezeNet as a foundation and managed to make it both smaller AND more accurate. This shows that SqueezeNet wasn’t just a one-off trick, but a robust platform for future innovation.\n\n\n\nThe Final Takeaway\nSqueezeNet is more than just a single, clever architecture. It is a compelling demonstration of a more disciplined, scientific approach to neural network design. The authors didn’t just give us a fish; they taught us how to fish by providing the principles and the methodology to explore the vast universe of possible network designs.\nThe final paragraph of the paper says it best:\n\n“We hope that SqueezeNet will inspire the reader to consider and explore the broad range of possibilities in the design space of CNN architectures and to perform that exploration in a more systematic manner.”\n\nAnd it did. The ideas pioneered in SqueezeNet—the aggressive use of 1x1 convolutions, the squeeze-and-expand bottleneck design, and the focus on parameter efficiency—are now standard practice. They live on in the DNA of modern, highly efficient architectures like MobileNet and ShuffleNet, which power countless AI applications on the phones in our pockets and the smart devices in our homes.\nSqueezeNet taught the world a valuable lesson: with thoughtful design, small can be incredibly powerful."
  },
  {
    "objectID": "posts/2025-08-21-squeezenet-reading-notes.html#final-thoughts",
    "href": "posts/2025-08-21-squeezenet-reading-notes.html#final-thoughts",
    "title": "SqueezeNet Paper Explained: A Deep Dive into 50x Smaller, AlexNet-Level AI",
    "section": "Final Thoughts",
    "text": "Final Thoughts\nThis concludes our deep dive into the SqueezeNet paper. I hope this step-by-step breakdown has given you a clear understanding of its core concepts and a deeper appreciation for its impact on the field of artificial intelligence. Thanks for studying along."
  },
  {
    "objectID": "posts/2025-07-30-the-horse-in-motion-ffmpeg-gotchas-part-2.html#introduction",
    "href": "posts/2025-07-30-the-horse-in-motion-ffmpeg-gotchas-part-2.html#introduction",
    "title": "The Horse in Motion and FFmpeg Gotchas - Part 2 Serverless on AWS Lambda",
    "section": "Introduction",
    "text": "Introduction\nIn Part 1 of this series, we successfully used FFmpeg to create the classic “Horse in Motion” video from a sequence of still images. We did it all within the comfortable and feature-rich environment of a Google Colab notebook. But personal notebooks are for development and experimentation. To build a real-world application, we need to run our code in a scalable, automated environment.\nWelcome to Part 2, where we take our show on the road—to the cloud! Our goal is to run the exact same process inside an AWS Lambda function.\nThis move introduces a whole new set of “gotchas.” We can’t just apt-get install ffmpeg. Lambda functions run in a tightly controlled environment. We need to package our own FFmpeg binary, deal with file system limitations, and ensure our chosen build has the features we need.\nIn this post, we’ll walk through:\n\nSetting up a basic serverless application with the AWS SAM CLI.\nThe challenge of finding the right static FFmpeg build.\nCreating an AWS Lambda Layer to make FFmpeg available to our function.\nModifying our code to execute FFmpeg, download images, and save the final video to an S3 bucket.\n\nLet’s dive in and see what it takes to get our horse running in the serverless world."
  },
  {
    "objectID": "posts/2025-07-30-the-horse-in-motion-ffmpeg-gotchas-part-2.html#the-scaffolding-a-basic-sam-application",
    "href": "posts/2025-07-30-the-horse-in-motion-ffmpeg-gotchas-part-2.html#the-scaffolding-a-basic-sam-application",
    "title": "The Horse in Motion and FFmpeg Gotchas - Part 2 Serverless on AWS Lambda",
    "section": "The Scaffolding: A Basic SAM Application",
    "text": "The Scaffolding: A Basic SAM Application\nBefore we even think about FFmpeg, let’s build the basic structure of our serverless application. We’ll use the AWS Serverless Application Model (SAM), a framework that makes defining and deploying serverless applications much easier. Think of it as a blueprint for our Lambda function and its related resources, like S3 buckets and permissions.\nOur first step is to create a simple Lambda function that can write a file to an S3 bucket. This proves that our basic plumbing (permissions, environment variables, S3 access) is working correctly.\nHere’s our initial template.yaml file:\n\n\nShow the code\nAWSTemplateFormatVersion: \"2010-09-09\"\nTransform: AWS::Serverless-2016-10-31\nDescription: &gt;\n  ffmpeg-lambda-demo\n  A simple Lambda function to process video with FFmpeg.\n\nGlobals:\n  Function:\n    Timeout: 60 # Set a longer timeout for video processing\n    MemorySize: 512 # Provide enough memory\n\nResources:\n  # The S3 bucket where our output videos will be stored\n  OutputBucket:\n    Type: AWS::S3::Bucket\n\n  # The Lambda function that will run our code\n  FFmpegFunction:\n    Type: AWS::Serverless::Function\n    Properties:\n      CodeUri: src/\n      Handler: app.lambda_handler\n      Runtime: python3.13\n      Architectures:\n        - x86_64\n\n      # Pass the bucket name to the function as an environment variable\n      Environment:\n        Variables:\n          OUTPUT_BUCKET: !Ref OutputBucket\n\n      # Give the function permission to write to the S3 bucket\n      Policies:\n        - S3WritePolicy:\n            BucketName: !Ref OutputBucket\n\nOutputs:\n  OutputBucketName:\n    Description: \"Name of the S3 bucket for output videos\"\n    Value: !Ref OutputBucket\n\n\nAnd here is the corresponding Python code in src/app.py. For now, it just creates a simple text file and uploads it to S3.\n\n\nShow the code\n# src/app.py\nimport os\nimport boto3\n\n# Get the S3 bucket name from the environment variables\nOUTPUT_BUCKET = os.environ.get(\"OUTPUT_BUCKET\")\ns3_client = boto3.client(\"s3\")\n\n\ndef lambda_handler(event, context):\n    \"\"\"\n    A simple handler to test writing a file to S3.\n    \"\"\"\n    try:\n        file_content = \"Hello from Lambda! The connection to S3 is working.\"\n        file_path = \"/tmp/test.txt\"\n\n        # Lambda functions can only write to the /tmp directory\n        with open(file_path, \"w\") as f:\n            f.write(file_content)\n\n        # Upload the file to our S3 bucket\n        s3_client.upload_file(file_path, OUTPUT_BUCKET, \"test-output.txt\")\n\n        return {\n            \"statusCode\": 200,\n            \"body\": \"Successfully created and uploaded test.txt to S3.\",\n        }\n\n    except Exception as e:\n        print(e)\n        raise e\n\n\nFinally, our src/requirements.txt only needs boto3, which is the AWS SDK for Python.\nboto3\nTo deploy this, you can run the standard SAM commands from your terminal:\n# Build the application\nsam build\n\n# Deploy it to your AWS account with a guided process\nsam deploy --guided\n\n\n\nSAM deploy\n\n\nAfter deployment, you can test the function from the AWS Console. If it runs successfully, you’ll find a test-output.txt file in the newly created S3 bucket.\n\n\n\n\n\n\nTest Lambda function\n\n\n\n\n\n\n\nS3 bucket output file\n\n\n\n\n\nNow that our basic infrastructure is in place, it’s time to tackle the main challenge: getting FFmpeg to run.\n\n\n\n\n\n\nCode checkpoint\n\n\n\nAll the code for this post till this point is available in the Github repo horse-in-motion-ffmpeg-gotchas-part-2 (1c09f)"
  },
  {
    "objectID": "posts/2025-07-30-the-horse-in-motion-ffmpeg-gotchas-part-2.html#the-first-hurdle-finding-the-right-ffmpeg-build",
    "href": "posts/2025-07-30-the-horse-in-motion-ffmpeg-gotchas-part-2.html#the-first-hurdle-finding-the-right-ffmpeg-build",
    "title": "The Horse in Motion and FFmpeg Gotchas - Part 2 Serverless on AWS Lambda",
    "section": "The First Hurdle: Finding the Right FFmpeg Build",
    "text": "The First Hurdle: Finding the Right FFmpeg Build\nIn a standard Linux environment, installing FFmpeg is as simple as sudo apt-get install ffmpeg. In AWS Lambda, we don’t have that luxury. We need a static binary — a single, self-contained executable file that we can package with our code. This binary needs to have all its dependencies compiled into it, so it can run anywhere without needing external libraries.\n\nAttempt #1: The John Van Sickle Build\nA very popular and reliable source for static FFmpeg builds is John Van Sickle’s website. These builds are fantastic and widely used. Let’s download one and see if it fits our needs.\nWe’ll grab the amd64 build, as our Lambda function is configured for the x86_64 architecture.\n\n# Download and extract the build\n!wget https://johnvansickle.com/ffmpeg/releases/ffmpeg-release-amd64-static.tar.xz\n\n--2025-07-30 12:18:11--  https://johnvansickle.com/ffmpeg/releases/ffmpeg-release-amd64-static.tar.xz\nResolving johnvansickle.com (johnvansickle.com)... 107.180.57.212\nConnecting to johnvansickle.com (johnvansickle.com)|107.180.57.212|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 41888096 (40M) [application/x-xz]\nSaving to: ‘ffmpeg-release-amd64-static.tar.xz’\n\nffmpeg-release-amd6 100%[===================&gt;]  39.95M  11.9MB/s    in 3.4s    \n\n2025-07-30 12:18:15 (11.9 MB/s) - ‘ffmpeg-release-amd64-static.tar.xz’ saved [41888096/41888096]\n\n\n\n\n!tar -xf ffmpeg-release-amd64-static.tar.xz\n\nNow, let’s inspect this build, just like we did in Part 1. We need two things:\n\nAn H.264 encoder (like libx264).\nThe drawtext filter for adding our title.\n\nFirst, let’s check for libx264 by listing the available codecs.\n\n!/content/ffmpeg-7.0.2-amd64-static/ffmpeg -codecs | grep libx264\n\nffmpeg version 7.0.2-static https://johnvansickle.com/ffmpeg/  Copyright (c) 2000-2024 the FFmpeg developers\n  built with gcc 8 (Debian 8.3.0-6)\n  configuration: --enable-gpl --enable-version3 --enable-static --disable-debug --disable-ffplay --disable-indev=sndio --disable-outdev=sndio --cc=gcc --enable-fontconfig --enable-frei0r --enable-gnutls --enable-gmp --enable-libgme --enable-gray --enable-libaom --enable-libfribidi --enable-libass --enable-libvmaf --enable-libfreetype --enable-libmp3lame --enable-libopencore-amrnb --enable-libopencore-amrwb --enable-libopenjpeg --enable-librubberband --enable-libsoxr --enable-libspeex --enable-libsrt --enable-libvorbis --enable-libopus --enable-libtheora --enable-libvidstab --enable-libvo-amrwbenc --enable-libvpx --enable-libwebp --enable-libx264 --enable-libx265 --enable-libxml2 --enable-libdav1d --enable-libxvid --enable-libzvbi --enable-libzimg\n  libavutil      59.  8.100 / 59.  8.100\n  libavcodec     61.  3.100 / 61.  3.100\n  libavformat    61.  1.100 / 61.  1.100\n  libavdevice    61.  1.100 / 61.  1.100\n  libavfilter    10.  1.100 / 10.  1.100\n  libswscale      8.  1.100 /  8.  1.100\n  libswresample   5.  1.100 /  5.  1.100\n  libpostproc    58.  1.100 / 58.  1.100\n DEV.LS h264                 H.264 / AVC / MPEG-4 AVC / MPEG-4 part 10 (decoders: h264 h264_v4l2m2m) (encoders: libx264 libx264rgb h264_v4l2m2m)\n\n\nGreat news! We see (encoders: libx264 ...), so our preferred encoder is available.\nNow for the critical test: does this build support the drawtext filter? As we learned, this filter depends on the libfreetype library being enabled during compilation (check the ffmpeg drawtext docs). Let’s check the build configuration.\n\n!/content/ffmpeg-7.0.2-amd64-static/ffmpeg -buildconf\n\nffmpeg version 7.0.2-static https://johnvansickle.com/ffmpeg/  Copyright (c) 2000-2024 the FFmpeg developers\n  built with gcc 8 (Debian 8.3.0-6)\n  configuration: --enable-gpl --enable-version3 --enable-static --disable-debug --disable-ffplay --disable-indev=sndio --disable-outdev=sndio --cc=gcc --enable-fontconfig --enable-frei0r --enable-gnutls --enable-gmp --enable-libgme --enable-gray --enable-libaom --enable-libfribidi --enable-libass --enable-libvmaf --enable-libfreetype --enable-libmp3lame --enable-libopencore-amrnb --enable-libopencore-amrwb --enable-libopenjpeg --enable-librubberband --enable-libsoxr --enable-libspeex --enable-libsrt --enable-libvorbis --enable-libopus --enable-libtheora --enable-libvidstab --enable-libvo-amrwbenc --enable-libvpx --enable-libwebp --enable-libx264 --enable-libx265 --enable-libxml2 --enable-libdav1d --enable-libxvid --enable-libzvbi --enable-libzimg\n  libavutil      59.  8.100 / 59.  8.100\n  libavcodec     61.  3.100 / 61.  3.100\n  libavformat    61.  1.100 / 61.  1.100\n  libavdevice    61.  1.100 / 61.  1.100\n  libavfilter    10.  1.100 / 10.  1.100\n  libswscale      8.  1.100 /  8.  1.100\n  libswresample   5.  1.100 /  5.  1.100\n  libpostproc    58.  1.100 / 58.  1.100\n\n  configuration:\n    --enable-gpl\n    --enable-version3\n    --enable-static\n    --disable-debug\n    --disable-ffplay\n    --disable-indev=sndio\n    --disable-outdev=sndio\n    --cc=gcc\n    --enable-fontconfig\n    --enable-frei0r\n    --enable-gnutls\n    --enable-gmp\n    --enable-libgme\n    --enable-gray\n    --enable-libaom\n    --enable-libfribidi\n    --enable-libass\n    --enable-libvmaf\n    --enable-libfreetype\n    --enable-libmp3lame\n    --enable-libopencore-amrnb\n    --enable-libopencore-amrwb\n    --enable-libopenjpeg\n    --enable-librubberband\n    --enable-libsoxr\n    --enable-libspeex\n    --enable-libsrt\n    --enable-libvorbis\n    --enable-libopus\n    --enable-libtheora\n    --enable-libvidstab\n    --enable-libvo-amrwbenc\n    --enable-libvpx\n    --enable-libwebp\n    --enable-libx264\n    --enable-libx265\n    --enable-libxml2\n    --enable-libdav1d\n    --enable-libxvid\n    --enable-libzvbi\n    --enable-libzimg\n\n\nIf you scan through the long list of flags in the output, you’ll see something encouraging:\n--enable-libfribidi\n--enable-libfreetype\nIt looks like we’re in luck! --enable-libfreetype is present, which is the primary dependency for the drawtext filter. So, our filter should be available, right?\nLet’s confirm by explicitly searching for drawtext in the list of available filters.\n\n!/content/ffmpeg-7.0.2-amd64-static/ffmpeg -filters | grep drawtext\n\nffmpeg version 7.0.2-static https://johnvansickle.com/ffmpeg/  Copyright (c) 2000-2024 the FFmpeg developers\n  built with gcc 8 (Debian 8.3.0-6)\n  configuration: --enable-gpl --enable-version3 --enable-static --disable-debug --disable-ffplay --disable-indev=sndio --disable-outdev=sndio --cc=gcc --enable-fontconfig --enable-frei0r --enable-gnutls --enable-gmp --enable-libgme --enable-gray --enable-libaom --enable-libfribidi --enable-libass --enable-libvmaf --enable-libfreetype --enable-libmp3lame --enable-libopencore-amrnb --enable-libopencore-amrwb --enable-libopenjpeg --enable-librubberband --enable-libsoxr --enable-libspeex --enable-libsrt --enable-libvorbis --enable-libopus --enable-libtheora --enable-libvidstab --enable-libvo-amrwbenc --enable-libvpx --enable-libwebp --enable-libx264 --enable-libx265 --enable-libxml2 --enable-libdav1d --enable-libxvid --enable-libzvbi --enable-libzimg\n  libavutil      59.  8.100 / 59.  8.100\n  libavcodec     61.  3.100 / 61.  3.100\n  libavformat    61.  1.100 / 61.  1.100\n  libavdevice    61.  1.100 / 61.  1.100\n  libavfilter    10.  1.100 / 10.  1.100\n  libswscale      8.  1.100 /  8.  1.100\n  libswresample   5.  1.100 /  5.  1.100\n  libpostproc    58.  1.100 / 58.  1.100\n\n\nThis is puzzling. The output is empty. Even though the libfreetype dependency is enabled, the drawtext filter itself is not included in this build. This is a perfect example of a deep FFmpeg gotcha: a build can have the libraries needed for a feature, but the feature itself might still be disabled through other configuration flags during the compile process (like --disable-filters=drawtext). The reasons can vary, from reducing binary size to avoiding potential licensing conflicts.\nSo, despite getting our hopes up, this build won’t work for us. The key takeaway remains the same: you must verify that the specific codec and filter you need are present in your chosen build.\nIt’s time to continue our search for a different static build that has both an H.264 encoder and the drawtext filter enabled.\n\n\nAttempt #2: A Build That Strikes the Right Balance\nOur search for the perfect FFmpeg binary leads us to another highly-regarded source: the automated builds from GitHub user BtbN, which are linked from the official FFmpeg download page. These builds come in various flavors, so we have a better chance of finding one that fits our specific needs.\nWe’ll try the ffmpeg-n7.1-latest-linux64-lgpl-7.1.tar.xz version.\n\n!wget https://github.com/BtbN/FFmpeg-Builds/releases/download/latest/ffmpeg-n7.1-latest-linux64-lgpl-7.1.tar.xz\n\n--2025-07-30 12:47:03--  https://github.com/BtbN/FFmpeg-Builds/releases/download/latest/ffmpeg-n7.1-latest-linux64-lgpl-7.1.tar.xz\nResolving github.com (github.com)... 20.27.177.113\nConnecting to github.com (github.com)|20.27.177.113|:443... connected.\nHTTP request sent, awaiting response... 302 Found\nLocation: https://release-assets.githubusercontent.com/github-production-release-asset/292087234/75ce9a3f-bd4b-4ae9-bb05-385c7b8da63b?sp=r&sv=2018-11-09&sr=b&spr=https&se=2025-07-30T13%3A47%3A07Z&rscd=attachment%3B+filename%3Dffmpeg-n7.1-latest-linux64-lgpl-7.1.tar.xz&rsct=application%2Foctet-stream&skoid=96c2d410-5711-43a1-aedd-ab1947aa7ab0&sktid=398a6654-997b-47e9-b12b-9515b896b4de&skt=2025-07-30T12%3A47%3A03Z&ske=2025-07-30T13%3A47%3A07Z&sks=b&skv=2018-11-09&sig=bGeoLsQzeAvxofmrtVK39Na%2BVyj%2BO5mxz2M0FccEBOI%3D&jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmVsZWFzZS1hc3NldHMuZ2l0aHVidXNlcmNvbnRlbnQuY29tIiwia2V5Ijoia2V5MSIsImV4cCI6MTc1Mzg3OTkyNCwibmJmIjoxNzUzODc5NjI0LCJwYXRoIjoicmVsZWFzZWFzc2V0cHJvZHVjdGlvbi5ibG9iLmNvcmUud2luZG93cy5uZXQifQ.pS2FQvAc99cW6JgwYOMMf-u25upRiKdaFW3lSaJ4Wew&response-content-disposition=attachment%3B%20filename%3Dffmpeg-n7.1-latest-linux64-lgpl-7.1.tar.xz&response-content-type=application%2Foctet-stream [following]\n--2025-07-30 12:47:04--  https://release-assets.githubusercontent.com/github-production-release-asset/292087234/75ce9a3f-bd4b-4ae9-bb05-385c7b8da63b?sp=r&sv=2018-11-09&sr=b&spr=https&se=2025-07-30T13%3A47%3A07Z&rscd=attachment%3B+filename%3Dffmpeg-n7.1-latest-linux64-lgpl-7.1.tar.xz&rsct=application%2Foctet-stream&skoid=96c2d410-5711-43a1-aedd-ab1947aa7ab0&sktid=398a6654-997b-47e9-b12b-9515b896b4de&skt=2025-07-30T12%3A47%3A03Z&ske=2025-07-30T13%3A47%3A07Z&sks=b&skv=2018-11-09&sig=bGeoLsQzeAvxofmrtVK39Na%2BVyj%2BO5mxz2M0FccEBOI%3D&jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmVsZWFzZS1hc3NldHMuZ2l0aHVidXNlcmNvbnRlbnQuY29tIiwia2V5Ijoia2V5MSIsImV4cCI6MTc1Mzg3OTkyNCwibmJmIjoxNzUzODc5NjI0LCJwYXRoIjoicmVsZWFzZWFzc2V0cHJvZHVjdGlvbi5ibG9iLmNvcmUud2luZG93cy5uZXQifQ.pS2FQvAc99cW6JgwYOMMf-u25upRiKdaFW3lSaJ4Wew&response-content-disposition=attachment%3B%20filename%3Dffmpeg-n7.1-latest-linux64-lgpl-7.1.tar.xz&response-content-type=application%2Foctet-stream\nResolving release-assets.githubusercontent.com (release-assets.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.111.133, ...\nConnecting to release-assets.githubusercontent.com (release-assets.githubusercontent.com)|185.199.108.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 105102440 (100M) [application/octet-stream]\nSaving to: ‘ffmpeg-n7.1-latest-linux64-lgpl-7.1.tar.xz’\n\nffmpeg-n7.1-latest- 100%[===================&gt;] 100.23M  32.5MB/s    in 3.1s    \n\n2025-07-30 12:47:07 (32.5 MB/s) - ‘ffmpeg-n7.1-latest-linux64-lgpl-7.1.tar.xz’ saved [105102440/105102440]\n\n\n\n\n!tar -xf ffmpeg-n7.1-latest-linux64-lgpl-7.1.tar.xz\n\nLet’s repeat our inspection process. First, the most important question: does it have our drawtext filter?\n\n# The ffmpeg binary is in the 'bin' subdirectory\n!/content/ffmpeg-n7.1-latest-linux64-lgpl-7.1/bin/ffmpeg -filters | grep drawtext\n\nffmpeg version n7.1.1-56-gc2184b65d2-20250729 Copyright (c) 2000-2025 the FFmpeg developers\n  built with gcc 15.1.0 (crosstool-NG 1.27.0.42_35c1e72)\n  configuration: --prefix=/ffbuild/prefix --pkg-config-flags=--static --pkg-config=pkg-config --cross-prefix=x86_64-ffbuild-linux-gnu- --arch=x86_64 --target-os=linux --enable-version3 --disable-debug --enable-iconv --enable-zlib --enable-libfribidi --enable-gmp --enable-libxml2 --enable-openssl --enable-lzma --enable-fontconfig --enable-libharfbuzz --enable-libfreetype --enable-libvorbis --enable-opencl --enable-libpulse --enable-libvmaf --enable-libxcb --enable-xlib --enable-amf --enable-libaom --enable-libaribb24 --disable-avisynth --enable-chromaprint --enable-libdav1d --disable-libdavs2 --disable-libdvdread --disable-libdvdnav --disable-libfdk-aac --enable-ffnvcodec --enable-cuda-llvm --disable-frei0r --enable-libgme --enable-libkvazaar --enable-libaribcaption --enable-libass --enable-libbluray --enable-libjxl --enable-libmp3lame --enable-libopus --enable-librist --enable-libssh --enable-libtheora --enable-libvpx --enable-libwebp --enable-libzmq --enable-lv2 --enable-libvpl --enable-openal --enable-libopencore-amrnb --enable-libopencore-amrwb --enable-libopenh264 --enable-libopenjpeg --enable-libopenmpt --enable-librav1e --disable-librubberband --disable-schannel --enable-sdl2 --enable-libsnappy --enable-libsoxr --enable-libsrt --enable-libsvtav1 --enable-libtwolame --enable-libuavs3d --enable-libdrm --enable-vaapi --disable-libvidstab --enable-vulkan --enable-libshaderc --enable-libplacebo --enable-libvvenc --disable-libx264 --disable-libx265 --disable-libxavs2 --disable-libxvid --enable-libzimg --enable-libzvbi --extra-cflags=-DLIBTWOLAME_STATIC --extra-cxxflags= --extra-libs='-ldl -lgomp' --extra-ldflags=-pthread --extra-ldexeflags=-pie --cc=x86_64-ffbuild-linux-gnu-gcc --cxx=x86_64-ffbuild-linux-gnu-g++ --ar=x86_64-ffbuild-linux-gnu-gcc-ar --ranlib=x86_64-ffbuild-linux-gnu-gcc-ranlib --nm=x86_64-ffbuild-linux-gnu-gcc-nm --extra-version=20250729\n  libavutil      59. 39.100 / 59. 39.100\n  libavcodec     61. 19.101 / 61. 19.101\n  libavformat    61.  7.100 / 61.  7.100\n  libavdevice    61.  3.100 / 61.  3.100\n  libavfilter    10.  4.100 / 10.  4.100\n  libswscale      8.  3.100 /  8.  3.100\n  libswresample   5.  3.100 /  5.  3.100\n T.C drawtext          V-&gt;V       Draw text on top of video frames using libfreetype library.\n\n\nSuccess! The drawtext filter is present and accounted for. This is a huge step forward.\nNow, let’s check for our preferred H.264 video encoder, libx264.\n\n!/content/ffmpeg-n7.1-latest-linux64-lgpl-7.1/bin/ffmpeg -codecs | grep libx264\n\nffmpeg version n7.1.1-56-gc2184b65d2-20250729 Copyright (c) 2000-2025 the FFmpeg developers\n  built with gcc 15.1.0 (crosstool-NG 1.27.0.42_35c1e72)\n  configuration: --prefix=/ffbuild/prefix --pkg-config-flags=--static --pkg-config=pkg-config --cross-prefix=x86_64-ffbuild-linux-gnu- --arch=x86_64 --target-os=linux --enable-version3 --disable-debug --enable-iconv --enable-zlib --enable-libfribidi --enable-gmp --enable-libxml2 --enable-openssl --enable-lzma --enable-fontconfig --enable-libharfbuzz --enable-libfreetype --enable-libvorbis --enable-opencl --enable-libpulse --enable-libvmaf --enable-libxcb --enable-xlib --enable-amf --enable-libaom --enable-libaribb24 --disable-avisynth --enable-chromaprint --enable-libdav1d --disable-libdavs2 --disable-libdvdread --disable-libdvdnav --disable-libfdk-aac --enable-ffnvcodec --enable-cuda-llvm --disable-frei0r --enable-libgme --enable-libkvazaar --enable-libaribcaption --enable-libass --enable-libbluray --enable-libjxl --enable-libmp3lame --enable-libopus --enable-librist --enable-libssh --enable-libtheora --enable-libvpx --enable-libwebp --enable-libzmq --enable-lv2 --enable-libvpl --enable-openal --enable-libopencore-amrnb --enable-libopencore-amrwb --enable-libopenh264 --enable-libopenjpeg --enable-libopenmpt --enable-librav1e --disable-librubberband --disable-schannel --enable-sdl2 --enable-libsnappy --enable-libsoxr --enable-libsrt --enable-libsvtav1 --enable-libtwolame --enable-libuavs3d --enable-libdrm --enable-vaapi --disable-libvidstab --enable-vulkan --enable-libshaderc --enable-libplacebo --enable-libvvenc --disable-libx264 --disable-libx265 --disable-libxavs2 --disable-libxvid --enable-libzimg --enable-libzvbi --extra-cflags=-DLIBTWOLAME_STATIC --extra-cxxflags= --extra-libs='-ldl -lgomp' --extra-ldflags=-pthread --extra-ldexeflags=-pie --cc=x86_64-ffbuild-linux-gnu-gcc --cxx=x86_64-ffbuild-linux-gnu-g++ --ar=x86_64-ffbuild-linux-gnu-gcc-ar --ranlib=x86_64-ffbuild-linux-gnu-gcc-ranlib --nm=x86_64-ffbuild-linux-gnu-gcc-nm --extra-version=20250729\n  libavutil      59. 39.100 / 59. 39.100\n  libavcodec     61. 19.101 / 61. 19.101\n  libavformat    61.  7.100 / 61.  7.100\n  libavdevice    61.  3.100 / 61.  3.100\n  libavfilter    10.  4.100 / 10.  4.100\n  libswscale      8.  3.100 /  8.  3.100\n  libswresample   5.  3.100 /  5.  3.100\n\n\nAnd just when we thought we were in the clear, another gotcha appears. This build has drawtext, but it does not include libx264. This is often due to licensing. libx264 is licensed under GPL, and distributing a build with it can have certain legal implications.\nSo, are we stuck? Not at all. This is where knowing about alternatives pays off. If we search for H.264 encoders in this build, we find another option:\n\n!/content/ffmpeg-n7.1-latest-linux64-lgpl-7.1/bin/ffmpeg -codecs | grep h264\n\nffmpeg version n7.1.1-56-gc2184b65d2-20250729 Copyright (c) 2000-2025 the FFmpeg developers\n  built with gcc 15.1.0 (crosstool-NG 1.27.0.42_35c1e72)\n  configuration: --prefix=/ffbuild/prefix --pkg-config-flags=--static --pkg-config=pkg-config --cross-prefix=x86_64-ffbuild-linux-gnu- --arch=x86_64 --target-os=linux --enable-version3 --disable-debug --enable-iconv --enable-zlib --enable-libfribidi --enable-gmp --enable-libxml2 --enable-openssl --enable-lzma --enable-fontconfig --enable-libharfbuzz --enable-libfreetype --enable-libvorbis --enable-opencl --enable-libpulse --enable-libvmaf --enable-libxcb --enable-xlib --enable-amf --enable-libaom --enable-libaribb24 --disable-avisynth --enable-chromaprint --enable-libdav1d --disable-libdavs2 --disable-libdvdread --disable-libdvdnav --disable-libfdk-aac --enable-ffnvcodec --enable-cuda-llvm --disable-frei0r --enable-libgme --enable-libkvazaar --enable-libaribcaption --enable-libass --enable-libbluray --enable-libjxl --enable-libmp3lame --enable-libopus --enable-librist --enable-libssh --enable-libtheora --enable-libvpx --enable-libwebp --enable-libzmq --enable-lv2 --enable-libvpl --enable-openal --enable-libopencore-amrnb --enable-libopencore-amrwb --enable-libopenh264 --enable-libopenjpeg --enable-libopenmpt --enable-librav1e --disable-librubberband --disable-schannel --enable-sdl2 --enable-libsnappy --enable-libsoxr --enable-libsrt --enable-libsvtav1 --enable-libtwolame --enable-libuavs3d --enable-libdrm --enable-vaapi --disable-libvidstab --enable-vulkan --enable-libshaderc --enable-libplacebo --enable-libvvenc --disable-libx264 --disable-libx265 --disable-libxavs2 --disable-libxvid --enable-libzimg --enable-libzvbi --extra-cflags=-DLIBTWOLAME_STATIC --extra-cxxflags= --extra-libs='-ldl -lgomp' --extra-ldflags=-pthread --extra-ldexeflags=-pie --cc=x86_64-ffbuild-linux-gnu-gcc --cxx=x86_64-ffbuild-linux-gnu-g++ --ar=x86_64-ffbuild-linux-gnu-gcc-ar --ranlib=x86_64-ffbuild-linux-gnu-gcc-ranlib --nm=x86_64-ffbuild-linux-gnu-gcc-nm --extra-version=20250729\n  libavutil      59. 39.100 / 59. 39.100\n  libavcodec     61. 19.101 / 61. 19.101\n  libavformat    61.  7.100 / 61.  7.100\n  libavdevice    61.  3.100 / 61.  3.100\n  libavfilter    10.  4.100 / 10.  4.100\n  libswscale      8.  3.100 /  8.  3.100\n  libswresample   5.  3.100 /  5.  3.100\n DEV.LS h264                 H.264 / AVC / MPEG-4 AVC / MPEG-4 part 10 (decoders: h264 h264_v4l2m2m h264_qsv libopenh264 h264_cuvid) (encoders: libopenh264 h264_amf h264_nvenc h264_qsv h264_v4l2m2m h264_vaapi h264_vulkan)\n\n\nThis build includes libopenh264, an open-source H.264 encoder provided by Cisco. While libx264 is often considered the highest-quality software encoder, libopenh264 is more than capable for most use cases, including ours. Its more permissive license (BSD) makes it a popular choice for distributable builds.\nWe have a winner! This build strikes the perfect balance for our needs:\n\nIt’s a static binary, perfect for AWS Lambda.\nIt includes the essential drawtext filter.\nIt provides a solid H.264 encoder, libopenh264.\n\nNow that we’ve found our champion binary, it’s time to package it up as a Lambda Layer and put it to work."
  },
  {
    "objectID": "posts/2025-07-30-the-horse-in-motion-ffmpeg-gotchas-part-2.html#putting-it-all-together-building-the-layer-and-deploying-the-function",
    "href": "posts/2025-07-30-the-horse-in-motion-ffmpeg-gotchas-part-2.html#putting-it-all-together-building-the-layer-and-deploying-the-function",
    "title": "The Horse in Motion and FFmpeg Gotchas - Part 2 Serverless on AWS Lambda",
    "section": "Putting It All Together: Building the Layer and Deploying the Function",
    "text": "Putting It All Together: Building the Layer and Deploying the Function\nWe’ve found our FFmpeg binary. Now it’s time to integrate it into our serverless application. This involves two main steps: packaging the binary into a Lambda Layer and updating our Python code to use it.\n\nStep 1: Create the FFmpeg Lambda Layer\nA Lambda Layer is a .zip file archive that can contain additional code or data. By packaging FFmpeg as a layer, we can keep it separate from our function code. This makes our deployment package smaller and our project more organized.\nLambda has a specific directory structure it expects for layers. For executables, they need to be placed in a bin directory inside the zip file.\nLet’s create this structure and package our chosen FFmpeg binary:\n\n# Create the directory structure for the layer\n!mkdir -p ffmpeg-layer/bin\n\n\n# Copy the FFmpeg binary we downloaded into the correct location\n!cp /content/ffmpeg-n7.1-latest-linux64-lgpl-7.1/bin/ffmpeg ffmpeg-layer/bin/\n\n\n# Now, create the zip archive for the layer\n# Navigate into the layer directory to get the zip structure right\n%cd ffmpeg-layer\n\n/content/ffmpeg-layer\n\n\n\n# verify that we are in the correct folder (ffmpeg-layer)\n!pwd\n\n/content/ffmpeg-layer\n\n\n\n# Now, zip the CONTENTS of the current directory (which is 'ffmpeg-layer')\n# The '*' ensures you select all files/folders directly within ffmpeg-layer\n# The -D flag helps to not store directory entries (sometimes helps with cleaner zips)\n!zip -r -D ../ffmpeg-layer.zip *\n\n  adding: bin/ffmpeg (deflated 58%)\n\n\nYou should now have a ffmpeg-layer.zip file in your project’s root directory. This is our Lambda Layer, ready to be deployed.\n\n\nStep 2: Update the SAM Template\nNext, we need to tell our template.yaml file about this new layer and attach it to our function. We also need to give our function a bit more memory and a longer timeout, as video processing is resource-intensive.\nHere are the key additions to template.yaml:\n\n\nShow the code\nAWSTemplateFormatVersion: \"2010-09-09\"\nTransform: AWS::Serverless-2016-10-31\nDescription: &gt;\n  ffmpeg-lambda-demo\n  A simple Lambda function to process video with FFmpeg.\n\nGlobals:\n  Function:\n    Timeout: 90 # Increased timeout for video processing\n    MemorySize: 1024 # Increased memory for FFmpeg\n\nResources:\n  # The S3 bucket where our output videos will be stored\n  OutputBucket:\n    Type: AWS::S3::Bucket\n\n    # Define the Lambda Layer\n  FFmpegLayer:\n    Type: AWS::Serverless::LayerVersion\n    Properties:\n      LayerName: ffmpeg-layer\n      Description: FFmpeg static build for video processing\n      ContentUri: layers/ffmpeg-layer.zip # Points to our local zip file\n      CompatibleRuntimes:\n        - python3.12\n        - python3.13\n\n  # The Lambda function that will run our code\n  FFmpegFunction:\n    Type: AWS::Serverless::Function\n    Properties:\n      CodeUri: src/\n      Handler: app.lambda_handler\n      Runtime: python3.13\n      Architectures:\n        - x86_64\n\n      # Pass the bucket name to the function as an environment variable\n      Environment:\n        Variables:\n          OUTPUT_BUCKET: !Ref OutputBucket\n\n      # Give the function permission to write to the S3 bucket\n      Policies:\n        - S3WritePolicy:\n            BucketName: !Ref OutputBucket\n\n      # Attach the layer to the function\n      Layers:\n        - !Ref FFmpegLayer\n\nOutputs:\n  OutputBucketName:\n    Description: \"Name of the S3 bucket for output videos\"\n    Value: !Ref OutputBucket\n\n\n\n\nStep 3: Update the Lambda Function Code\nThis is the final piece of the puzzle. We need to update our src/app.py to perform the full workflow:\n\nDownload the horse frames into the /tmp directory (the only writable location in a Lambda function).\nUse Python’s subprocess module to call the FFmpeg binary.\nUpload the resulting video from /tmp to our S3 bucket.\n\n\n\n\n\n\n\nImportant Note\n\n\n\nTo use the drawtext filter, FFmpeg needs a font file. Lambda environment does not have any so you need to provide it. Download a font like Liberation Sans, and place LiberationSans-Regular.ttf inside a new src/fonts/ directory. We will package this font with our function code.\n\n\nHere is the final src/app.py:\n\n\nShow the code\n# src/app.py\nimport os\nimport boto3\nimport requests\nimport subprocess\n\nOUTPUT_BUCKET = os.environ.get(\"OUTPUT_BUCKET\")\ns3_client = boto3.client(\"s3\")\nTMP_DIR = \"/tmp\"\n\n\ndef download_frames():\n    \"\"\"Downloads the 15 horse frames into /tmp/video_frames\"\"\"\n    frames_dir = os.path.join(TMP_DIR, \"video_frames\")\n    os.makedirs(frames_dir, exist_ok=True)\n\n    base_url = \"https://raw.githubusercontent.com/hassaanbinaslam/myblog/5c15e72dde03112c5c8dea177bfed7c835aca399/posts/images/2025-07-28-the-horse-in-motion-ffmpeg-gotchas-part-1/video_frames\"\n\n    for i in range(1, 16):\n        frame_number = str(i).zfill(2)\n        image_url = f\"{base_url}/frame{frame_number}.png\"\n        response = requests.get(image_url)\n        if response.status_code == 200:\n            with open(os.path.join(frames_dir, f\"frame{frame_number}.png\"), \"wb\") as f:\n                f.write(response.content)\n\n    print(\"All frames downloaded.\")\n    # print(\"List the files names downloaded\")\n    # print(os.listdir(frames_dir))\n\n\ndef lambda_handler(event, context):\n    try:\n        print(\"Starting video creation process...\")\n        download_frames()\n\n        # Paths in the Lambda's writable /tmp directory\n        input_path = os.path.join(TMP_DIR, \"video_frames/frame%02d.png\")\n        output_path = os.path.join(TMP_DIR, \"output.mp4\")\n\n        # Path to the font file packaged with our function\n        font_file = \"./fonts/LiberationSans-Regular.ttf\"\n\n        # When a layer is used, its contents are available in the /opt directory.\n        # Our FFmpeg binary is therefore at /opt/bin/ffmpeg.\n        ffmpeg_cmd = [\n            \"/opt/bin/ffmpeg\",\n            \"-stream_loop\",\n            \"-1\",\n            \"-framerate\",\n            \"1.5\",\n            \"-i\",\n            input_path,\n            \"-vf\",\n            f\"drawtext=fontfile={font_file}:text='The Horse in Motion and FFmpeg Gotchas Part 2':fontcolor=white:fontsize=13:box=1:boxcolor=black@0.8:boxborderw=5:x=(w-text_w)/2:y=(h-text_h)/2:enable='between(t,0,10)'\",\n            \"-c:v\",\n            \"libopenh264\",  # Use the alternate H.264 encoder\n            \"-r\",\n            \"30\",\n            \"-pix_fmt\",\n            \"yuv420p\",\n            \"-t\",\n            \"40\",\n            output_path,\n        ]\n\n        print(f\"Running FFmpeg command: {' '.join(ffmpeg_cmd)}\")\n\n        # Execute the FFmpeg command\n        result = subprocess.run(ffmpeg_cmd, capture_output=True, text=True, check=True)\n\n        print(\"FFmpeg stdout:\", result.stdout)\n        print(\"FFmpeg stderr:\", result.stderr)\n\n        print(f\"FFmpeg command successful. Uploading {output_path} to S3.\")\n\n        s3_client.upload_file(output_path, OUTPUT_BUCKET, \"horse-in-motion.mp4\")\n\n        return {\n            \"statusCode\": 200,\n            \"body\": \"Successfully created and uploaded horse-in-motion.mp4 to S3.\",\n        }\n\n    except subprocess.CalledProcessError as e:\n        print(\"FFmpeg failed to execute.\")\n        print(\"Return code:\", e.returncode)\n        print(\"stdout:\", e.stdout)\n        print(\"stderr:\", e.stderr)\n        raise e\n    except Exception as e:\n        print(e)\n        raise e\n\n\nFinally, make sure your src/requirements.txt file also includes requests:\nboto3\nrequests\n\n\nStep 4: Deploy and Test\nWith all the pieces in place, we can deploy our application.\n# Build the application, including the layer and function code\nsam build\n\n# Deploy the changes to your AWS account\nsam deploy\nOnce the deployment is complete, navigate to the AWS Lambda console, find the function, and invoke it with a test event. Monitor the logs in CloudWatch. If everything works as expected, you will see the logs from the print statements, and a new file named horse-in-motion.mp4 will appear in your S3 bucket!\n\n\n\n\n\n\nTest Lambda function\n\n\n\n\n\n\n\nS3 bucket output file\n\n\n\n\n\n\n\n\n\n\n\nCode checkpoint\n\n\n\nAll the code for this post till this point is available in the Github repo horse-in-motion-ffmpeg-gotchas-part-2 (d756d)"
  },
  {
    "objectID": "posts/2025-07-30-the-horse-in-motion-ffmpeg-gotchas-part-2.html#conclusion",
    "href": "posts/2025-07-30-the-horse-in-motion-ffmpeg-gotchas-part-2.html#conclusion",
    "title": "The Horse in Motion and FFmpeg Gotchas - Part 2 Serverless on AWS Lambda",
    "section": "Conclusion",
    "text": "Conclusion\nOur serverless journey is complete! We successfully migrated our FFmpeg process from a local notebook to a scalable AWS Lambda function. Along the way, we navigated some of the most common real-world gotchas of working with FFmpeg in a constrained environment:\n\nFinding the Right Build: We learned that not all static builds are created equal and that verifying the presence of specific codecs and filters is a critical first step.\nManaging Dependencies: We discovered that a build might have one feature we need (drawtext) but lack another (libx264), forcing us to adapt and use alternatives like libopenh264.\nLambda Environment Constraints: We saw the importance of using the /tmp directory for file operations and learned how to package and access binaries and other assets using Lambda Layers.\n\nBy packaging FFmpeg as a layer, you now have a reusable, serverless video processing engine that you can use to build powerful on-demand media applications."
  },
  {
    "objectID": "posts/2025-02-18-diffusion-model-mnist-part3.html#introduction",
    "href": "posts/2025-02-18-diffusion-model-mnist-part3.html#introduction",
    "title": "From Noise to Numbers: A Hands-On Guide to Generating MNIST Digits with Diffusion Models (Part 3)",
    "section": "Introduction",
    "text": "Introduction\nWelcome back to the final part of our hands-on journey into diffusion models for MNIST digit generation! In Part 1, we laid the groundwork by building a basic Convolutional UNet and training it to directly predict clean MNIST digits from noisy inputs. We then enhanced our UNet architecture in Part 2, leveraging the power of the diffusers library and its UNet2DModel to achieve improved denoising performance.\nWhile our direct image prediction approach showed promising results, we hinted that it was a simplification of true diffusion models. We observed that even with our enhanced UNet and iterative refinement, the generated digits still lacked the crispness and fidelity we might expect from “diffusion model magic.”\nNow, in this final installment, we’re ready to take the leap into the heart of diffusion models. We’ll move beyond directly predicting clean images and embrace the core principles that make diffusion models so powerful: noise prediction and scheduled denoising. Get ready to unlock the true potential of diffusion and witness a significant step-up in image generation quality!\n\nCredits\nThis post is inspired by the Hugging Face Diffusion Course\n\n\nEnvironment Details\nYou can access and run this Jupyter Notebook from the GitHub repository on this link 2025-02-18-diffusion-model-mnist-part3.ipynb\nRun the following cell to install the required packages.\n\nThis notebook can be run with Google Colab T4 GPU runtime.\nI have also tested this notebook with AWS SageMaker Jupyter Notebook running on instance “ml.g5.xlarge” and image “SageMaker Distribution 2.3.0”.\n\n\n%%capture\n!pip install datasets[vision]\n!pip install diffusers\n!pip install watermark\n!pip install torchinfo\n!pip install matplotlib\n\nWaterMark is an IPython magic extension for printing date and time stamps, version numbers, and hardware information. Let’s load this extension and print the environment details.\n\n%load_ext watermark\n\n\n%watermark -v -m -p torch,torchvision,datasets,diffusers,matplotlib,watermark,torchinfo\n\nPython implementation: CPython\nPython version       : 3.11.11\nIPython version      : 7.34.0\n\ntorch      : 2.5.1+cu124\ntorchvision: 0.20.1+cu124\ndatasets   : 3.3.2\ndiffusers  : 0.32.2\nmatplotlib : 3.10.0\nwatermark  : 2.5.0\ntorchinfo  : 1.8.0\n\nCompiler    : GCC 11.4.0\nOS          : Linux\nRelease     : 6.1.85+\nMachine     : x86_64\nProcessor   : x86_64\nCPU cores   : 2\nArchitecture: 64bit"
  },
  {
    "objectID": "posts/2025-02-18-diffusion-model-mnist-part3.html#from-direct-image-prediction-to-noise-prediction-a-paradigm-shift",
    "href": "posts/2025-02-18-diffusion-model-mnist-part3.html#from-direct-image-prediction-to-noise-prediction-a-paradigm-shift",
    "title": "From Noise to Numbers: A Hands-On Guide to Generating MNIST Digits with Diffusion Models (Part 3)",
    "section": "From Direct Image Prediction to Noise Prediction: A Paradigm Shift",
    "text": "From Direct Image Prediction to Noise Prediction: A Paradigm Shift\nIn Parts 1 and 2, we trained our UNet to perform direct image prediction. This meant we fed the model a noisy image and asked it to directly output the estimated clean image. While this approach allowed us to grasp the basic mechanics of UNets and image denoising, it’s important to understand its limitations and why true diffusion models take a different path.\nDirect image prediction, as we implemented it, is essentially a one-step denoising process. It attempts to remove all the noise in a single forward pass through the network. Think of it like trying to un-blur a heavily distorted image in just one go – it’s a difficult task, and the results can often be blurry and lack fine details. Furthermore, this direct approach doesn’t fully capture the essence of the diffusion process, which is inherently gradual and iterative.\nTrue diffusion models, and the approach we’ll adopt now, operate on a different principle: noise prediction. Instead of predicting the clean image directly, we train our model to predict the noise that was added to a slightly noisier version of the image at each step of the reverse diffusion process.\nImagine you’re slowly un-blurring an image, step by step. At each step, instead of trying to guess the entire sharp image, you focus on identifying and removing just a tiny bit of blur. By iteratively removing small amounts of blur (or noise), you gradually reveal the underlying clean image. This is the essence of noise prediction.\nOur model will now learn to estimate the noise present in a slightly noisy image. This predicted noise can then be used to “step back” along the reverse diffusion trajectory, creating a slightly less noisy image. By repeating this process over many steps – a process we call scheduled denoising (which we’ll discuss shortly) – we can generate high-quality images from pure noise.\nThis shift to noise prediction is a crucial paradigm change. It allows for:\n\nMore stable training: Predicting noise at each step is a less ambitious and more manageable task for the model compared to directly predicting the clean image.\nImproved sample quality: The iterative nature of noise prediction, guided by a schedule, leads to the generation of more detailed and visually appealing images.\nAlignment with true diffusion models: Noise prediction is the fundamental building block of modern diffusion models, bringing us closer to state-of-the-art image generation techniques.\n\nIn the following sections, we’ll delve into the code modifications needed to switch to noise prediction and explore the concept of scheduled denoising in detail."
  },
  {
    "objectID": "posts/2025-02-18-diffusion-model-mnist-part3.html#data-preparation-preprocessing-and-unet-model",
    "href": "posts/2025-02-18-diffusion-model-mnist-part3.html#data-preparation-preprocessing-and-unet-model",
    "title": "From Noise to Numbers: A Hands-On Guide to Generating MNIST Digits with Diffusion Models (Part 3)",
    "section": "Data Preparation, Preprocessing, and UNet Model",
    "text": "Data Preparation, Preprocessing, and UNet Model\nAs we are building upon the foundations laid in Part 1, we will reuse the same data preparation and preprocessing steps for the MNIST dataset. For a more in-depth explanation of these steps, please refer back to the first part of this guide. Here, we will quickly outline the process to ensure our data is ready for training.\nWe will train the same UNet2DModel model that we used in the Part 2.\n\n### Load MNIST Dataset\nfrom datasets import load_dataset\ndataset = load_dataset(\"mnist\")\nprint(dataset)\n\nimport torch\nfrom torchvision import transforms\n\nimage_size = 32  # Define the target image size\n\npreprocess = transforms.Compose([\n    transforms.Resize((image_size, image_size)),\n    transforms.ToTensor(),\n])\n\n### Define preprocess pipelein\nimport torch\nfrom torchvision import transforms\n\nimage_size = 32  # Define the target image size\n\npreprocess = transforms.Compose([\n    transforms.Resize((image_size, image_size)),\n    transforms.ToTensor(),\n])\n\n## Define the transform function\ndef transform(examples):\n    examples = [preprocess(image) for image in examples[\"image\"]]\n    return {\"images\": examples}\n\n## Apply the transform to the dataset\ndataset.set_transform(transform)\n\n## Definition of the noise corruption function\ndef corrupt(x, noise, amount):\n    amount = amount.view(-1, 1, 1, 1)  # make sure it's broadcastable\n    return (\n        x * (1 - amount) + noise * amount\n    )  # equivalent to x.lerp(noise, amount)\n\n### Define the UNet Model (Same as Part 2)\nfrom diffusers import UNet2DModel\n\nmodel = UNet2DModel(\n    sample_size=32,\n    in_channels=1,\n    out_channels=1,\n    layers_per_block=2,\n    block_out_channels=(32, 64, 64),\n    down_block_types=(\n        \"DownBlock2D\",\n        \"AttnDownBlock2D\",\n        \"AttnDownBlock2D\",\n    ),\n    up_block_types=(\n        \"AttnUpBlock2D\",\n        \"AttnUpBlock2D\",\n        \"UpBlock2D\",\n    ),\n)\n\n\nfrom torchinfo import summary\nsummary(model)\n\n/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:104: UserWarning: \nError while fetching `HF_TOKEN` secret value from your vault: 'Requesting secret HF_TOKEN timed out. Secrets can only be fetched when running from the Colab UI.'.\nYou are not authenticated with the Hugging Face Hub in this notebook.\nIf the error persists, please let us know by opening an issue on GitHub (https://github.com/huggingface/huggingface_hub/issues/new).\n  warnings.warn(\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDatasetDict({\n    train: Dataset({\n        features: ['image', 'label'],\n        num_rows: 60000\n    })\n    test: Dataset({\n        features: ['image', 'label'],\n        num_rows: 10000\n    })\n})\n\n\nThe cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n\n\n\n\n\n======================================================================\nLayer (type:depth-idx)                        Param #\n======================================================================\nUNet2DModel                                   --\n├─Conv2d: 1-1                                 320\n├─Timesteps: 1-2                              --\n├─TimestepEmbedding: 1-3                      --\n│    └─Linear: 2-1                            4,224\n│    └─SiLU: 2-2                              --\n│    └─Linear: 2-3                            16,512\n├─ModuleList: 1-4                             --\n│    └─DownBlock2D: 2-4                       --\n│    │    └─ModuleList: 3-1                   45,504\n│    │    └─ModuleList: 3-2                   9,248\n│    └─AttnDownBlock2D: 2-5                   --\n│    │    └─ModuleList: 3-3                   33,536\n│    │    └─ModuleList: 3-4                   148,352\n│    │    └─ModuleList: 3-5                   36,928\n│    └─AttnDownBlock2D: 2-6                   --\n│    │    └─ModuleList: 3-6                   33,536\n│    │    └─ModuleList: 3-7                   164,736\n├─ModuleList: 1-5                             --\n│    └─AttnUpBlock2D: 2-7                     --\n│    │    └─ModuleList: 3-8                   50,304\n│    │    └─ModuleList: 3-9                   382,848\n│    │    └─ModuleList: 3-10                  36,928\n│    └─AttnUpBlock2D: 2-8                     --\n│    │    └─ModuleList: 3-11                  50,304\n│    │    └─ModuleList: 3-12                  362,304\n│    │    └─ModuleList: 3-13                  36,928\n│    └─UpBlock2D: 2-9                         --\n│    │    └─ModuleList: 3-14                  112,640\n├─UNetMidBlock2D: 1-6                         --\n│    └─ModuleList: 2-10                       --\n│    │    └─Attention: 3-15                   16,768\n│    └─ModuleList: 2-11                       --\n│    │    └─ResnetBlock2D: 3-16               82,368\n│    │    └─ResnetBlock2D: 3-17               82,368\n├─GroupNorm: 1-7                              64\n├─SiLU: 1-8                                   --\n├─Conv2d: 1-9                                 289\n======================================================================\nTotal params: 1,707,009\nTrainable params: 1,707,009\nNon-trainable params: 0\n======================================================================"
  },
  {
    "objectID": "posts/2025-02-18-diffusion-model-mnist-part3.html#scheduled-denoising-guiding-the-reverse-diffusion",
    "href": "posts/2025-02-18-diffusion-model-mnist-part3.html#scheduled-denoising-guiding-the-reverse-diffusion",
    "title": "From Noise to Numbers: A Hands-On Guide to Generating MNIST Digits with Diffusion Models (Part 3)",
    "section": "Scheduled Denoising: Guiding the Reverse Diffusion",
    "text": "Scheduled Denoising: Guiding the Reverse Diffusion\nThe power of noise prediction truly shines when combined with scheduled denoising. As we discussed, diffusion models work by gradually reversing the noise addition process. Scheduled denoising provides the schedule or the steps for this reverse process, controlling how we iteratively remove noise from an image.\n\nThink of it like carefully peeling layers of an onion. Scheduled denoising defines how many layers we peel back and how much we peel at each step. In diffusion models, these “layers” correspond to different levels of noise.\n\nWhy do we need a schedule?\nInstead of removing all the predicted noise in one go, scheduled denoising breaks down the denoising process into a series of discrete timesteps. This is crucial for several reasons:\n\nControlled Noise Removal: A schedule allows us to gradually remove noise, starting from a highly noisy image (or pure noise) and progressively refining it. This iterative refinement leads to better image quality compared to a one-step approach.\nStability and Guidance: By controlling the denoising steps, we provide a structured path for the reverse diffusion process. This makes the generation process more stable and predictable.\nFlexibility and Control: Different schedules can be designed to influence the generation process. For example, some schedules might prioritize faster generation, while others might focus on higher quality.\n\nTimesteps and the Reverse Process:\nThe following code represents the denoising schedule using timesteps. These timesteps are typically a sequence of numbers going from a large value (representing high noise) down to a small value (representing low noise or a clean image).\n\n# Setup the DDPM scheduler for training\nfrom diffusers import DDPMScheduler\n\nnum_train_timesteps = 1000\n\nscheduler = DDPMScheduler(\n    num_train_timesteps=num_train_timesteps,\n    beta_start=0.0001,\n    beta_end=0.02,\n    beta_schedule=\"linear\",\n)\n\nNotice that we are now using a DDPMScheduler from diffusers. This scheduler is responsible for:\n\nGenerating Timesteps: It creates a schedule of timesteps that guide the reverse diffusion process. We’ve initialized it with num_train_timesteps = 1000. This means the forward diffusion process (noise addition) is simulated over 1000 steps. For the reverse process (denoising), we’ll also use these timesteps, though we might choose to use fewer steps for faster inference.\nAdding Noise (Forward Process Simulation): During training, the scheduler’s add_noise function helps us create noisy versions of clean images at different timesteps. This is what we are using in our training loop:\nnoisy_images = scheduler.add_noise(clean_images, noise, timesteps)\nStepping Backwards (Reverse Process): Crucially, the scheduler also provides a step function that helps us take a denoising step backwards along the diffusion trajectory. This function is used during inference (and could be used in more advanced training schemes).\n\nIn essence, the DDPMScheduler encapsulates the logic for both the forward (noise addition) and reverse (denoising) diffusion processes, providing us with the tools to implement scheduled denoising.\nIn the next section, we’ll modify our training loop to incorporate noise prediction and scheduled denoising using the DDPMScheduler. We’ll see how the step function guides the reverse diffusion and how we train our model to predict the noise at each timestep.\n\nModifying the Training Loop for Noise Prediction\nNow that we understand the concepts of noise prediction and scheduled denoising, let’s adapt our training loop to reflect these changes. We’ll be using the DDPMScheduler and training our UNet2DModel to predict noise instead of directly predicting clean images.\nHere’s how we’ll modify the training loop:\n1. Sampling Timesteps:\nInstead of just generating random noise amounts, we now need to sample timesteps for each image in the batch. These timesteps will be integers between 0 and num_train_timesteps (where, num_train_timesteps = 1000). These timesteps tell the scheduler how much noise to add in the forward process and guide the reverse process.\ntimesteps = torch.randint(0, num_train_timesteps, (batch_size,), device=device).long()\n2. Adding Noise with the Scheduler:\nWe’ll use the scheduler.add_noise function to add noise to our clean images, conditioned on the sampled timesteps. This function takes the clean images, random noise, and the timesteps as input and returns the noisy images.\nnoisy_images = scheduler.add_noise(clean_images, noise, timesteps)\n3. Model Predicts Noise:\nThe crucial change is that we now feed the noisy_images and the timesteps to our UNet2DModel, and we train it to predict the noise that was added. The UNet2DModel in diffusers is designed to be conditioned on timesteps.\nnoise_pred = model(noisy_images, timesteps, return_dict=False)[0]\nNotice that we pass timesteps as the second argument to the model. The return_dict=False)[0] part is just to extract the predicted noise tensor from the output.\n4. Loss Calculation:\nOur loss function remains Mean Squared Error (MSE), but now we calculate the MSE between the predicted noise (noise_pred) and the actual noise (noise) that we used to corrupt the images. This is how we train the model to accurately predict the noise.\nloss = F.mse_loss(noise_pred, noise)\n5. Rest of the Training Loop:\nThe rest of the training loop (optimizer step, loss tracking, etc.) remains largely the same as in Part 2.\nComplete Modified Training Loop Snippet:\nHere’s the complete, modified training loop snippet, incorporating noise prediction and scheduled denoising:\n\nimport torch.nn.functional as F\nfrom torch.optim import Adam\nimport matplotlib.pyplot as plt\n\n# --- Setup (Device, Model, Optimizer, Loss History, Hyperparameters) ---\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = model.to(device) # Our UNet2DModel from diffusers\noptimizer = Adam(model.parameters(), lr=1e-3) # Same learning rate as Part 1\nlosses = []\nnum_epochs = 5 # Same number of epochs as Part 1\nbatch_size = 128 # Same batch size as Part 1\n\ntrain_dataloader = torch.utils.data.DataLoader(\n    dataset[\"train\"], batch_size=batch_size, shuffle=True\n)\n\nprint(device)\n\ncuda\n\n\n\nfor epoch in range(num_epochs):\n    for batch in train_dataloader:\n        # Get the clean images and move to device\n        clean_images = batch[\"images\"].to(device)  # shape: (B, 1, H, W)\n        batch_size = clean_images.shape[0]\n\n        # Sample random noise to add\n        noise = torch.randn_like(clean_images).to(device)\n\n        # Sample a random timestep for each image in the batch\n        timesteps = torch.randint(0, num_train_timesteps, (batch_size,), device=device).long()\n\n        # Add noise to the clean images according to the scheduler's forward process\n        noisy_images = scheduler.add_noise(clean_images, noise, timesteps)\n\n        # Let the model predict the noise component from the noisy images\n        # (Note: The model is conditioned on the timestep)\n        noise_pred = model(noisy_images, timesteps, return_dict=False)[0]\n\n        # Compute the loss between the predicted noise and the actual noise\n        loss = F.mse_loss(noise_pred, noise)\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        losses.append(loss.item())\n\n    # Print the average loss for this epoch\n    avg_loss = sum(losses[-len(train_dataloader):]) / len(train_dataloader)\n    print(f\"Finished epoch {epoch}. Average loss: {avg_loss:.6f}\")\n\nFinished epoch 0. Average loss: 0.029540\nFinished epoch 1. Average loss: 0.015203\nFinished epoch 2. Average loss: 0.013258\nFinished epoch 3. Average loss: 0.012170\nFinished epoch 4. Average loss: 0.011788\n\n\n\n# --- Plotting Loss Curve ---\nplt.figure(figsize=(8, 4))\nplt.plot(losses, label=\"Training Loss\")\nplt.title(\"Training Loss Curve (UNet2DModel - Noise Prediction)\")\nplt.xlabel(\"Iteration\")\nplt.ylabel(\"MSE Loss\")\nplt.legend()\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\nBy making these modifications, we’ve successfully shifted our training paradigm from direct image prediction to noise prediction, leveraging the DDPMScheduler to manage the diffusion process. In the next section, we’ll focus on how to modify the inference process to generate images using scheduled denoising."
  },
  {
    "objectID": "posts/2025-02-18-diffusion-model-mnist-part3.html#inference-with-scheduled-denoising-generating-images-iteratively",
    "href": "posts/2025-02-18-diffusion-model-mnist-part3.html#inference-with-scheduled-denoising-generating-images-iteratively",
    "title": "From Noise to Numbers: A Hands-On Guide to Generating MNIST Digits with Diffusion Models (Part 3)",
    "section": "Inference with Scheduled Denoising: Generating Images Iteratively",
    "text": "Inference with Scheduled Denoising: Generating Images Iteratively\nWith our model now trained to predict noise, we can finally generate images using the true power of diffusion models: iterative denoising guided by a schedule. This is a significant departure from our one-shot denoising approach in Parts 1 and 2.\nHere’s how we’ll modify the inference process.\n1. Start with Pure Noise:\nWe begin the generation process with pure random noise. This noise will be our starting point for the reverse diffusion process.\nsample = torch.randn((num_images, 1, image_size, image_size)).to(device)\n2. Set up the Denoising Loop:\nWe’ll use a loop that iterates through the timesteps provided by our scheduler. Crucially, during inference, we need to use the inference timesteps which are obtained using scheduler.set_timesteps(num_inference_steps). These timesteps are in descending order, going from high noise to low noise.\nfor t in scheduler.timesteps:\n    # ... denoising step ...\n3. Model Predicts Noise at Each Timestep:\nInside the loop, for each timestep t, we feed the current noisy image sample and the timestep t to our UNet2DModel to predict the noise:\nnoise_pred = model(sample, t, return_dict=False)[0]\n4. Scheduler Steps Backwards:\nThis is the core of scheduled denoising! We use the scheduler.step function to take a step backwards along the diffusion trajectory, removing a bit of noise from the current sample. The step function takes the noise_pred, the current timestep t, and the current sample as input and returns a dictionary containing the updated sample in prev_sample.\noutput = scheduler.step(noise_pred, t, sample)\nsample = output.prev_sample\nThe scheduler.step function intelligently uses the predicted noise and the schedule information to determine how much to “denoise” the image at each timestep. This is where the magic of the diffusion schedule comes in!\n5. Iterate and Refine:\nWe repeat steps 3 and 4 for all timesteps in the schedule. In each iteration, the image becomes progressively less noisy and more structured, gradually revealing a coherent MNIST digit.\n6. Visualization:\nAfter the loop completes, the sample tensor will contain the generated (denoised) images. We can then visualize these images as we did in previous parts.\nComplete Inference Loop Snippet:\nHere’s the complete inference loop snippet from your code, demonstrating scheduled denoising:\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport torch\n\n# Set model to evaluation mode\nmodel.eval()\n\n# Number of inference steps (can be the same as used before)\nnum_inference_steps = 50\nscheduler.set_timesteps(num_inference_steps)\n\n# Start with a single image of pure noise\nnoise_image = torch.randn((1, 1, image_size, image_size)).to(device)\nsample = noise_image.clone()\n\nwith torch.no_grad():\n    for t in scheduler.timesteps:\n        # Predict the noise at the current timestep\n        noise_pred = model(sample, t).sample\n        # Use the scheduler to compute the previous (less noisy) sample\n        output = scheduler.step(noise_pred, t, sample)\n        sample = output.prev_sample\n\ndenoised_image = sample.clone()\n\n# Plot both images side by side\nfig, axs = plt.subplots(1, 2, figsize=(8, 4))\n\naxs[0].imshow(noise_image.squeeze().cpu().numpy(), cmap=\"Greys\")\naxs[0].set_title(\"Noisy Image\")\naxs[0].axis(\"off\")\n\naxs[1].imshow(denoised_image.squeeze().cpu().numpy(), cmap=\"Greys\")\naxs[1].set_title(\"Model Prediction (Denoised)\")\naxs[1].axis(\"off\")\n\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "posts/2025-02-18-diffusion-model-mnist-part3.html#iterative-denoising-with-varying-inference-steps",
    "href": "posts/2025-02-18-diffusion-model-mnist-part3.html#iterative-denoising-with-varying-inference-steps",
    "title": "From Noise to Numbers: A Hands-On Guide to Generating MNIST Digits with Diffusion Models (Part 3)",
    "section": "Iterative Denoising with Varying Inference Steps",
    "text": "Iterative Denoising with Varying Inference Steps\nTo visually explore the impact of scheduled denoising, let’s now denoise a batch of MNIST test images using different numbers of inference steps. We’ll use our do_inference function, varying num_inference_steps from 1 to 5. This will allow us to observe how the quality of the denoised digits improves as we increase the number of refinement steps.\nInference with 1 Step:\n\n# multi step inference\n\nimport torch\nimport matplotlib.pyplot as plt\n\ndef do_inference(num_inference_steps):\n    scheduler.set_timesteps(num_inference_steps)\n\n    # Make sure your model is in evaluation mode\n    model.eval()\n\n    # Create a dataloader for the test (or validation) split.\n    # (If you don’t have a separate test split, you can use a subset of the training data.)\n    batch_size=8\n    test_dataloader = torch.utils.data.DataLoader(dataset[\"test\"], batch_size, shuffle=True)\n\n    # Grab one batch of images from the test set\n    batch = next(iter(test_dataloader))\n    clean_images = batch[\"images\"].to(device)  # these are the ground-truth images\n\n    # Sample random noise to add\n    noise = torch.randn_like(clean_images).to(device)\n\n    # Sample a random timestep for each image in the batch\n    timesteps = torch.randint(0, num_train_timesteps, (batch_size,), device=device).long()\n\n    # Add noise to the clean images according to the scheduler's forward process\n    noisy_images = scheduler.add_noise(clean_images, noise, timesteps)\n\n    # Iteratively denoise the sample using the scheduler's timesteps\n    denoised_images = noisy_images.clone()\n    with torch.no_grad():\n        for t in scheduler.timesteps:\n            # The model expects the timestep (here, a scalar or tensor) as input.\n            denoised_sample = model(denoised_images, t).sample\n            # The scheduler returns a dict containing the updated sample in \"prev_sample\"\n            output = scheduler.step(denoised_sample, t, denoised_images)\n            denoised_images = output.prev_sample\n\n    # Move tensors to CPU for plotting\n    clean_images_np = clean_images.cpu().numpy()\n    noisy_images_np = noisy_images.cpu().numpy()\n    denoised_images_np = denoised_images.cpu().numpy()\n\n    # Plot the results:\n    # - Top row: Clean images (ground truth)\n    # - Middle row: Noisy images (input to the model)\n    # - Bottom row: Denoised images (model output)\n    num_images = clean_images_np.shape[0]\n    fig, axes = plt.subplots(3, num_images, figsize=(num_images * 2, 6))\n    for i in range(num_images):\n        # Clean image\n        axes[0, i].imshow(clean_images_np[i].squeeze(), cmap=\"Greys\")\n        axes[0, i].axis(\"off\")\n        if i == 0:\n            axes[0, i].set_title(\"Clean\")\n\n        # Noisy image\n        axes[1, i].imshow(noisy_images_np[i].squeeze(), cmap=\"Greys\")\n        axes[1, i].axis(\"off\")\n        if i == 0:\n            axes[1, i].set_title(\"Noisy\")\n\n        # Denoised image (model output)\n        axes[2, i].imshow(denoised_images_np[i].squeeze(), cmap=\"Greys\")\n        axes[2, i].axis(\"off\")\n        if i == 0:\n            axes[2, i].set_title(\"Denoised\")\n\n    plt.tight_layout()\n    plt.show()\n\n# Set the number of inference steps and update the scheduler's timesteps accordingly.\nnum_inference_steps = 1\ndo_inference(num_inference_steps)\n\n\n\n\n\n\n\n\nWith just a single inference step, the “Denoised” row presents an initial attempt at noise removal. The digits are barely recognizable and remain heavily influenced by the original noise.\nInference with 2 Steps:\n\n# Set the number of inference steps and update the scheduler's timesteps accordingly.\ndo_inference(num_inference_steps=2)\n\n\n\n\n\n\n\n\nIncreasing the inference steps to 2, we observe a noticeable improvement. The digits in the “Denoised” row are becoming more defined. The shapes are clearer, and we can start to discern the intended digits more confidently. However, there’s still a considerable amount of blur and noise remaining, and fine details are lacking.\n\n\n\n\n\n\nNote\n\n\n\nYou might have noticed that the denoised images actually deviate from the original ones. For example, the first clean image is a 3, but the denoised version more closely resembles a 9. Similarly, the third clean image is a 1, yet its denoised version resembles a 2. Why does this happen?\nI’m not entirely sure. I’ve experimented with it and tried to identify the cause but haven’t found a definitive answer. My theory is that the scheduler might be increasing the noise in the first step instead of reducing it. At this stage, the actual image signal could be getting lost in the added noise. As a result, when the second step attempts to remove the noise and recover the original image, it fails.\n\n\nInference with 3 Steps:\n\n# Set the number of inference steps and update the scheduler's timesteps accordingly.\ndo_inference(num_inference_steps=3)\n\n\n\n\n\n\n\n\nAt 3 inference steps, the trend of improvement continues. The “Denoised” digits are now even more recognizable and sharper. The digits like ‘2’, ‘8’, and ‘9’ are becoming quite distinct from the noisy input. The background noise is further reduced, allowing the digit structure to emerge more prominently.\nInference with 4 Steps:\n\n# Set the number of inference steps and update the scheduler's timesteps accordingly.\ndo_inference(num_inference_steps=4)\n\n\n\n\n\n\n\n\nWith 4 inference steps, the quality jumps again. The “Denoised” digits are now quite clear and well-formed. The residual blur is further minimized, and the digits are gaining more crispness.\nInference with 5 Steps:\n\n# Set the number of inference steps and update the scheduler's timesteps accordingly.\ndo_inference(num_inference_steps=5)\n\n\n\n\n\n\n\n\nFinally, at 5 inference steps, we reach a point where the “Denoised” digits are quite impressive. They exhibit a good level of sharpness and clarity. While not perfectly identical to the originals, they are now convincingly denoised MNIST digits. Comparing the “Denoised” row to the “Noisy” row clearly shows the effectiveness of the scheduled denoising process in removing the added noise and recovering the underlying digit structure.\nObservations and Trend:\nBy visually comparing the results across 1 to 5 inference steps, we can clearly observe a positive correlation: increasing the number of inference steps generally leads to improved denoising quality.\nInference with 50 Steps:\n\n# Set the number of inference steps and update the scheduler's timesteps accordingly.\ndo_inference(num_inference_steps=50)\n\n\n\n\n\n\n\n\nThis experiment effectively demonstrates the power of iterative refinement in diffusion models. Scheduled denoising, even with a small number of steps, can significantly improve the quality of generated (or in this case, denoised) images."
  },
  {
    "objectID": "posts/2025-02-18-diffusion-model-mnist-part3.html#iterative-refinement-from-pure-noise",
    "href": "posts/2025-02-18-diffusion-model-mnist-part3.html#iterative-refinement-from-pure-noise",
    "title": "From Noise to Numbers: A Hands-On Guide to Generating MNIST Digits with Diffusion Models (Part 3)",
    "section": "Iterative Refinement from Pure Noise",
    "text": "Iterative Refinement from Pure Noise\nNow, let’s push our enhanced UNet2DModel further and explore its ability to generate MNIST digits directly from pure random noise through iterative refinement. This is a more generative task compared to the denoising experiments in the last section. We’ll start with a batch of pure noise and iteratively refine it over 5 steps, observing how digit-like structures emerge.\n\nimport torchvision\nfrom matplotlib import pyplot as plt\n\nn_steps = 5\nx = torch.rand(8, 1, 32, 32).to(device)  # Start from random\nstep_history = [x.detach().cpu()]\npred_output_history = []\nmodel.to(device)\n\npred = x.clone()\nfor i in range(n_steps):\n    # Predict denoise image\n    with torch.no_grad():\n        for t in scheduler.timesteps:\n            sample = model(pred, t).sample\n            output = scheduler.step(sample, t, pred)\n            pred = output.prev_sample\n\n\n    # Store output for plotting\n    pred_output_history.append(pred.detach().cpu())\n\n    # Move slightly towards that direction\n    mix_factor = 1 / (n_steps - i)\n    x = x * (1 - mix_factor) + pred * mix_factor\n\n    # Store output for plotting\n    step_history.append(x.detach().cpu())\n\nfig, axs = plt.subplots(n_steps, 2, figsize=(9, 4), sharex=True)\naxs[0, 0].set_title(\"x (model input)\")\naxs[0, 1].set_title(\"model prediction\")\nfor i in range(n_steps):\n    axs[i, 0].imshow(\n        torchvision.utils.make_grid(step_history[i])[0].clip(0, 1), cmap=\"Greys\"\n    )\n    axs[i, 1].imshow(\n        torchvision.utils.make_grid(pred_output_history[i])[0].clip(0, 1),\n        cmap=\"Greys\",\n    )\n\n\n\n\n\n\n\n\nThe image above visualizes the iterative refinement process. Let’s examine each column:\n\nLeft Column: “x (model input)” - Image at Each Refinement Step: This column shows the evolving image at each step of the refinement process. Starting from the top row with pure random noise, we can trace the gradual transformation over 5 steps.\nRight Column: “model prediction” - Model’s Direct Prediction at Each Step: This column displays the direct output of the UNet2DModel at each step. It represents what the model predicts should be the next, less noisy image in the sequence. Observing this column, we can see that the model consistently predicts images that are slightly less noisy and more digit-like than the current input image in the left column, guiding the refinement process."
  },
  {
    "objectID": "posts/2025-02-18-diffusion-model-mnist-part3.html#long-term-iterative-generation-50-steps",
    "href": "posts/2025-02-18-diffusion-model-mnist-part3.html#long-term-iterative-generation-50-steps",
    "title": "From Noise to Numbers: A Hands-On Guide to Generating MNIST Digits with Diffusion Models (Part 3)",
    "section": "Long-Term Iterative Generation (50 Steps)",
    "text": "Long-Term Iterative Generation (50 Steps)\nTo fully appreciate the image generation capabilities of our diffusion model, let’s extend the iterative refinement process to a larger number of steps. In this section, we’ll generate MNIST digits from pure noise using num_inference_steps = 50. This extended denoising process should allow the model to refine the images more thoroughly, potentially leading to higher quality and more detailed digits.\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport torch\n\n# Set model to evaluation mode\nmodel.eval()\n\n# Number of inference steps\nnum_inference_steps = 50\nscheduler.set_timesteps(num_inference_steps)\n\n# Start with a single image of pure noise\nsample = torch.randn((1, 1, image_size, image_size)).to(device)\n\n# List to store intermediate images\nprogression = [sample.clone()]\n\nwith torch.no_grad():\n    for t in scheduler.timesteps:\n        # Predict noise and get the previous sample\n        noise_pred = model(sample, t).sample\n        output = scheduler.step(noise_pred, t, sample)\n        sample = output.prev_sample\n        progression.append(sample.clone())\n\n# Convert tensors to NumPy arrays for plotting\nprogression = [img.squeeze().cpu().numpy() for img in progression]\n\n# Select a subset to display\nnum_to_plot = 15  # Change this based on how many you want to display\nindices = np.linspace(0, len(progression) - 1, num_to_plot, dtype=int)\nselected_images = [progression[i] for i in indices]\n\n# Set grid size\nrows = 3  # Adjust based on your preference\ncols = (num_to_plot + rows - 1) // rows  # Compute columns dynamically\n\n# Create subplots\nfig, axes = plt.subplots(rows, cols, figsize=(cols * 2, rows * 2))\naxes = axes.flatten()  # Flatten in case it's a 2D grid\n\n# Plot images\nfor idx, ax in enumerate(axes):\n    if idx &lt; num_to_plot:\n        ax.imshow(selected_images[idx], cmap=\"Greys\")\n        ax.set_title(f\"Step {indices[idx]}\")\n        ax.axis(\"off\")\n    else:\n        ax.axis(\"off\")  # Hide extra empty subplots\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nThe image above visualizes the evolution of a generated MNIST digit over 15 selected steps from the 50-step denoising process. While we are only showing a subset of the steps for clarity, it represents the full 50-step refinement."
  },
  {
    "objectID": "posts/2025-02-18-diffusion-model-mnist-part3.html#conclusion",
    "href": "posts/2025-02-18-diffusion-model-mnist-part3.html#conclusion",
    "title": "From Noise to Numbers: A Hands-On Guide to Generating MNIST Digits with Diffusion Models (Part 3)",
    "section": "Conclusion",
    "text": "Conclusion\nIn this three-part series, we embarked on a hands-on journey to demystify diffusion models and build our own MNIST digit generator from scratch. Starting with a basic Convolutional UNet and direct image prediction, we progressively enhanced our model and delved deeper into the core principles of diffusion.\nKey Milestones of Our Journey:\n\nPart 1: Laying the Foundation: We built a simple BasicUNet and trained it for direct image prediction, establishing a baseline and understanding the fundamentals of UNet architectures for denoising tasks.\nPart 2: Enhancing the Architecture: We upgraded to the diffusers library’s UNet2DModel, incorporating ResNet blocks and attention mechanisms, and observed a noticeable improvement in denoising quality, showcasing the power of more sophisticated architectures.\nPart 3: Embracing True Diffusion: In this final part, we made the crucial shift to noise prediction and scheduled denoising, leveraging the DDPMScheduler. We witnessed firsthand how these core concepts unlock the true potential of diffusion models, enabling us to generate high-quality MNIST digits from pure random noise through iterative refinement.\n\nIn Part 3, we moved beyond simplified approaches and successfully implemented key elements of modern diffusion models. We saw firsthand how a learned noise prediction model and a well-designed denoising schedule can transform pure noise into coherent, recognizable images. With more inference steps, image quality improved significantly.\nI hope this guide has helped you understand and experiment with diffusion models. The code and explanations provided serve as a strong foundation for further exploration. Now, go forth and create your own diffusion model magic!"
  },
  {
    "objectID": "posts/2025-02-10-diffusion-model-mnist-part1.html",
    "href": "posts/2025-02-10-diffusion-model-mnist-part1.html",
    "title": "From Noise to Numbers: A Hands-On Guide to Generating MNIST Digits with Diffusion Models (Part 1)",
    "section": "",
    "text": "image source: https://www.artbreeder.com/image/4740f9c3d7ceac2112e54cd77b5c"
  },
  {
    "objectID": "posts/2025-02-10-diffusion-model-mnist-part1.html#introduction",
    "href": "posts/2025-02-10-diffusion-model-mnist-part1.html#introduction",
    "title": "From Noise to Numbers: A Hands-On Guide to Generating MNIST Digits with Diffusion Models (Part 1)",
    "section": "Introduction",
    "text": "Introduction\nEver been amazed by AI-generated images? Let’s demystify the magic behind them! Have you ever wondered how a machine can generate something coherent from pure randomness? Take a look at the image below. On the left, you see a jumble of random pixels — digital noise. On the right, a crisp, recognizable MNIST digit emerges. This transformation from noise to structure is powered by Diffusion Models, a revolutionary approach in deep learning.\n\n\n\n\n\n\n\n\n\n\nBut how does this noise-to-image process actually work? Imagine an old analog TV with no signal. The screen fills with static — random noise. Diffusion models learn to reverse this process, starting from pure noise and gradually refining it into a meaningful image. Think of it as a sophisticated un-blurring process. If you had an extremely blurry image of a digit, a diffusion model would learn to iteratively “de-blur” it, bringing the digit into sharp focus — step by step — until the final image is clear.\nMNIST digits provide the perfect playground for exploring this concept. Their simplicity allows us to focus on the core mechanics of diffusion without the added complexity of full-color, high-resolution images. In this post, we’ll take a hands-on approach, building our own MNIST digit generator from scratch to truly understand how diffusion models work.\n\nWhat to Expect\nWe’ll break down diffusion models into four key stages, each progressively refining our approach:\n\nBaseline Model: We’ll start with a simple Convolutional UNet trained to directly generate MNIST digits. This serves as our foundation before diving into diffusion.\n\nArchitecture Improvements: Next, we’ll enhance our UNet using a more advanced design from the diffusers library, seeing firsthand how architectural tweaks improve results.\n\nIntroducing Diffusion: Now comes the core idea — rather than predicting full images, we train our model to predict noise, the crucial step that enables real diffusion.\n\nFull Diffusion Process: Finally, we’ll integrate a noise scheduler, allowing our model to iteratively denoise images, unlocking the true power of diffusion models.\n\nBy the end of this journey, you’ll not only understand how diffusion models generate images but also build one yourself! Whether you’re a deep-learning enthusiast or an AI researcher, this hands-on approach will give you both intuitive understanding and practical experience in one of the most exciting areas of machine learning today.\nReady to dive in? Let’s get started!\n\n\nCredits\nThis post is inspired by the Hugging Face Diffusion Course\n\n\nEnvironment Details\nYou can access and run this Jupyter Notebook from the GitHub repository on this link 2025-02-10-diffusion-model-mnist-part1.ipynb\nRun the following cell to install the required packages.\n\nThis notebook can be run with Google Colab T4 GPU runtime.\nI have also tested this notebook with AWS SageMaker Jupyter Notebook running on instance “ml.g5.xlarge” and image “SageMaker Distribution 2.3.0”.\n\n\n%%capture\n!pip install datasets[vision]\n!pip install diffusers\n!pip install watermark\n!pip install torchinfo\n!pip install matplotlib\n\nWaterMark is an IPython magic extension for printing date and time stamps, version numbers, and hardware information. Let’s load this extension and print the environment details.\n\n%load_ext watermark\n\n\n%watermark -v -m -p torch,torchvision,datasets,diffusers,matplotlib,watermark,torchinfo\n\nPython implementation: CPython\nPython version       : 3.11.11\nIPython version      : 7.34.0\n\ntorch      : 2.5.1+cu124\ntorchvision: 0.20.1+cu124\ndatasets   : 3.2.0\ndiffusers  : 0.32.2\nmatplotlib : 3.10.0\nwatermark  : 2.5.0\ntorchinfo  : 1.8.0\n\nCompiler    : GCC 11.4.0\nOS          : Linux\nRelease     : 6.1.85+\nMachine     : x86_64\nProcessor   : x86_64\nCPU cores   : 2\nArchitecture: 64bit"
  },
  {
    "objectID": "posts/2025-02-10-diffusion-model-mnist-part1.html#data-preparation-for-mnist-simple-digits-simple-setup",
    "href": "posts/2025-02-10-diffusion-model-mnist-part1.html#data-preparation-for-mnist-simple-digits-simple-setup",
    "title": "From Noise to Numbers: A Hands-On Guide to Generating MNIST Digits with Diffusion Models (Part 1)",
    "section": "Data Preparation for MNIST: Simple Digits, Simple Setup",
    "text": "Data Preparation for MNIST: Simple Digits, Simple Setup\nBefore training our diffusion model to generate MNIST digits, we first need to prepare the dataset properly. Data preparation is a critical step in any machine learning project, especially for image-based models. Think of it like setting up an artist’s workspace — before painting, an artist ensures that they have the right canvas and paints. Similarly, our model needs well-structured, consistently formatted, and properly scaled data to learn effectively and generate high-quality results.\nIn this section, we’ll go through the key steps required to get our MNIST digits ready for diffusion modeling.\n\nLoading the MNIST Dataset\nLoading the MNIST dataset is incredibly easy, thanks to the Hugging Face datasets library. With just one line of code, we can download and load the dataset:\n\nfrom datasets import load_dataset\ndataset = load_dataset(\"mnist\")\nprint(dataset)\n\n/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \nThe secret `HF_TOKEN` does not exist in your Colab secrets.\nTo authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\nYou will be able to reuse this secret in all of your notebooks.\nPlease note that authentication is recommended but still optional to access public models or datasets.\n  warnings.warn(\n\n\nDatasetDict({\n    train: Dataset({\n        features: ['image', 'label'],\n        num_rows: 60000\n    })\n    test: Dataset({\n        features: ['image', 'label'],\n        num_rows: 10000\n    })\n})\n\n\nHere’s what happens under the hood:\n\nThe load_dataset(\"mnist\") function from the datasets library automatically downloads the MNIST dataset (if not already cached) and prepares it for use.\n\nRunning print(dataset) reveals that it is stored as a DatasetDict with two splits: 'train' and 'test'. Each split contains a Dataset object with 'image' and 'label' features.\n\nAlthough the dataset includes labels, we will not use them for our unconditional image generation task.\n\nThe images are in grayscale with pixel values ranging from 0 to 255.\n\nLet’s display a sample image from the training dataset.\n\nfrom IPython.display import display\n\n# Extract a sample image from the dataset\nsample_image = dataset[\"train\"][\"image\"][0]\n\n# Print details about the image\nprint(\"Data Type:\", type(sample_image))  # Shows the type of object\nprint(\"Image Size:\", sample_image.size)  # Displays dimensions of the image\n\n# Display the image\ndisplay(sample_image)\n\nData Type: &lt;class 'PIL.PngImagePlugin.PngImageFile'&gt;\nImage Size: (28, 28)\n\n\n\n\n\n\n\n\n\nFrom the last cell output, you can see the loaded images are object of Python Image Library (PIL) and are very tiny in their actual size. Let’s enlarge the sample image and view it up close.\n\nimport matplotlib.pyplot as plt\n\n# Display the image with a larger size\nplt.figure(figsize=(5, 5))  # Adjust the figure size as needed\nplt.imshow(sample_image, cmap=\"gray\")  # Use \"gray\" colormap for MNIST images\nplt.axis(\"off\")  # Hide axis for better visibility\nplt.show()\n\n\n\n\n\n\n\n\nEach image is made up of small square pixels, with values ranging from 0 (black) to 255 (white). The grayscale intensity of each pixel determines how bright or dark it appears. By displaying these pixel values, we can understand how the model sees the image.\n\nimport numpy as np\n\n# Convert image to NumPy array\nimage_array = np.array(sample_image)\n\n# Print the pixel values in a structured format\nfor row in image_array:\n    print(\" \".join(f\"{pixel:3}\" for pixel in row))  # Align values properly\n\n  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n  0   0   0   0   0   0   0   0   0   0   0   0   3  18  18  18 126 136 175  26 166 255 247 127   0   0   0   0\n  0   0   0   0   0   0   0   0  30  36  94 154 170 253 253 253 253 253 225 172 253 242 195  64   0   0   0   0\n  0   0   0   0   0   0   0  49 238 253 253 253 253 253 253 253 253 251  93  82  82  56  39   0   0   0   0   0\n  0   0   0   0   0   0   0  18 219 253 253 253 253 253 198 182 247 241   0   0   0   0   0   0   0   0   0   0\n  0   0   0   0   0   0   0   0  80 156 107 253 253 205  11   0  43 154   0   0   0   0   0   0   0   0   0   0\n  0   0   0   0   0   0   0   0   0  14   1 154 253  90   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n  0   0   0   0   0   0   0   0   0   0   0 139 253 190   2   0   0   0   0   0   0   0   0   0   0   0   0   0\n  0   0   0   0   0   0   0   0   0   0   0  11 190 253  70   0   0   0   0   0   0   0   0   0   0   0   0   0\n  0   0   0   0   0   0   0   0   0   0   0   0  35 241 225 160 108   1   0   0   0   0   0   0   0   0   0   0\n  0   0   0   0   0   0   0   0   0   0   0   0   0  81 240 253 253 119  25   0   0   0   0   0   0   0   0   0\n  0   0   0   0   0   0   0   0   0   0   0   0   0   0  45 186 253 253 150  27   0   0   0   0   0   0   0   0\n  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  16  93 252 253 187   0   0   0   0   0   0   0   0\n  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 249 253 249  64   0   0   0   0   0   0   0\n  0   0   0   0   0   0   0   0   0   0   0   0   0   0  46 130 183 253 253 207   2   0   0   0   0   0   0   0\n  0   0   0   0   0   0   0   0   0   0   0   0  39 148 229 253 253 253 250 182   0   0   0   0   0   0   0   0\n  0   0   0   0   0   0   0   0   0   0  24 114 221 253 253 253 253 201  78   0   0   0   0   0   0   0   0   0\n  0   0   0   0   0   0   0   0  23  66 213 253 253 253 253 198  81   2   0   0   0   0   0   0   0   0   0   0\n  0   0   0   0   0   0  18 171 219 253 253 253 253 195  80   9   0   0   0   0   0   0   0   0   0   0   0   0\n  0   0   0   0  55 172 226 253 253 253 253 244 133  11   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n  0   0   0   0 136 253 253 253 212 135 132  16   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n\n\n\n\nPreprocessing for MNIST Images\nOnce the MNIST dataset is loaded, the next crucial step is preprocessing the images. Raw MNIST images, which are grayscale and 28×28 in size, need to be formatted appropriately for deep learning models. Proper preprocessing helps standardize the input data, making it easier for the model to learn effectively.\nIn this pipeline, we will use torchvision.transforms.Compose to apply a series of transformations:\n\nResizing: Ensures all images are a consistent size (e.g., 32×32).\n\nTensor Conversion: Converts images to PyTorch tensors and scales pixel values to [0,1].\n\nBelow is the code for our preprocessing pipeline:\n\nimport torch\nfrom torchvision import transforms\n\nimage_size = 32  # Define the target image size (e.g., 32x32)\n\npreprocess = transforms.Compose([\n    transforms.Resize((image_size, image_size)),  # Resize images to a fixed size\n    transforms.ToTensor(),  # Convert images to tensors & scale pixel values to [0, 1]\n])\n\nEach function in transforms.Compose() plays a crucial role in preparing the images:\n\ntransforms.Resize((image_size, image_size))\n\nResizes images to 32×32 pixels (or any chosen image_size).\n\nEnsures a consistent input size for our UNet model.\n\n\ntransforms.ToTensor()\n\nConverts images from PIL format to PyTorch tensors.\n\nScales pixel values from [0, 255] to [0, 1] for compatibility with deep learning models.\n\n\n\nApplying the Transformations to the Dataset\nWith our preprocessing pipeline defined, we now need to apply it to the dataset. Instead of modifying the dataset in advance, we’ll use on-the-fly transformations — ensuring preprocessing is applied only when data is accessed, keeping memory usage efficient.\nTo achieve this, we define a transformation function and set it for our dataset:\n\n# Define the transform function\ndef transform(examples):\n    examples = [preprocess(image) for image in examples[\"image\"]]\n    return {\"images\": examples}\n\n# Apply the transform to the dataset\ndataset.set_transform(transform)\n\nIn this code:\n\nThe transform function\n\nReceives batches of examples from the dataset.\n\nApplies the preprocess pipeline (resizing, tensor conversion) to the 'image' feature using list comprehension.\n\nReturns a dictionary with the transformed images under the key 'images'.\n\n\ndataset.set_transform(transform)\n\nAssociates the transform function with the dataset.\n\nEnsures that preprocessing is dynamically applied whenever data is accessed, rather than modifying and storing a preprocessed copy.\n\n\n\n\nCreating the DataLoader\nWith our dataset ready, the next step is to create a DataLoader. PyTorch’s DataLoader is an essential utility that efficiently handles data loading during training. It performs key tasks such as:\n\nBatching: Groups images into batches for efficient processing, especially when using a GPU.\n\nShuffling: Randomizes the order of samples to prevent the model from memorizing patterns based on dataset order.\n\nEfficient Loading: Streams data dynamically, preventing memory bottlenecks.\n\nShuffling is particularly important as it enhances the model’s generalization ability, ensuring it learns meaningful patterns rather than sequence-based biases.\nHere’s how to create a DataLoader for the MNIST training set:\n\nimport torch\nfrom torch.utils.data import DataLoader\n\nbatch_size = 8 # Define batch size (adjust as needed based on GPU memory)\n# Using a small batch size for easier visual validation of images.\n# When training, we will use a larger batch size for efficiency.\n\ntrain_dataloader = DataLoader(dataset[\"train\"], batch_size=batch_size, shuffle=True)\n\nIn this code snippet:\n\nbatch_size = 8: Defines the number of images in each batch. Larger batch sizes can speed up training but require more memory.\n\ndataset[\"train\"]: Specifies the training split of the MNIST dataset.\n\nshuffle=True: Ensures that data is randomly shuffled at the beginning of each training epoch, preventing the model from learning sequence-based biases.\n\nWith train_dataloader set up, we are now fully equipped to feed batches of preprocessed MNIST images into our diffusion model during training.\n\n\nVisualizing a Batch of Preprocessed Images\nBefore training a model, it’s important to verify that our data preprocessing pipeline is functioning as expected. One effective way to do this is by visualizing a batch of preprocessed images. This allows us to check whether key transformations — such as resizing and tensor conversion — have been applied correctly.\nWe can use matplotlib along with torchvision.utils.make_grid to display a grid of images from our train_dataloader. This quick visual inspection helps confirm that the images are correctly formatted and ready for model training.\nBelow is a simple implementation to visualize a batch:\n\nimport torchvision\n\n# Get a batch of images from the dataloader\ndata_iter = iter(train_dataloader)\nbatch = next(data_iter)  # Get the first batch\nimgs = batch[\"images\"]  # Get images from the batch\n\nprint(\"Input shape:\", imgs.shape)\n\n# Visualize the batch\nplt.figure(figsize=(8, 8))\nplt.axis(\"off\")\nplt.imshow(torchvision.utils.make_grid(imgs)[0], cmap=\"Greys\")\nplt.show()\n\nInput shape: torch.Size([8, 1, 32, 32])\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhy the color of the images flipped?\n\n\n\nIn the previous plots, MNIST digits appeared as white pixels on a dark background, but in the last plot, they now appear as black pixels on a white background. This change is due to the colormap (cmap) setting used when plotting the images.\nBoth \"gray\" and \"Greys\" are sequential colormaps, but they interpret pixel intensity slightly differently:\n\n\"gray\" : Maps values close to 0 as black and higher values as white.\n\"Greys\": Maps values close to 0 as white and higher values as black.\n\nI switched the colormap to make the MNIST digits sharper and easier to validate visually. You can learn more about Matplotlib colormaps from the official documentation.\n\n\nOur images have been transformed from pixel-based representations to tensors, making them more suitable for training. Now, let’s visualize how a single image appears after these transformations.\n\n# Take the first image from the batch\nimage_tensor = next(iter(train_dataloader))[\"images\"][0]\n\n# Convert tensor to NumPy array\nimage_array = image_tensor.numpy()\n\n# Convert from (C, H, W) to (H, W) since it's grayscale\nimage_array = image_array.squeeze(0)\n\n# Print pixel values\nprint(\"Pixel values after preprocessing:\")\nfor row in image_array:\n    print(\" \".join(f\"{pixel:.1f}\" for pixel in row))\n\n# Display the image for reference\nplt.figure(figsize=(6,6))\nplt.imshow(image_array, cmap=\"gray\", interpolation=\"nearest\")\nplt.title(\"Processed Image\")\nplt.axis(\"off\")\nplt.show()\n\nPixel values after preprocessing:\n0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.2 0.4 0.5 0.3 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.9 0.9 0.6 0.1 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.3 0.8 0.9 0.8 0.5 0.1 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.6 0.9 0.9 0.5 0.1 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.1 0.8 0.9 0.8 0.4 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.1 0.7 1.0 0.6 0.2 0.1 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.1 0.5 0.9 0.8 0.4 0.1 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.1 0.4 0.9 0.9 0.4 0.1 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.2 0.5 0.8 0.8 0.4 0.1 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.4 0.8 0.9 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.1 0.6 0.9 0.7 0.3 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.2 0.7 0.9 0.6 0.0 0.0 0.0 0.0 0.0 0.1 0.1 0.2 0.1 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.1 0.6 0.9 0.5 0.0 0.0 0.0 0.0 0.1 0.3 0.7 0.9 0.7 0.1 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.6 0.9 0.7 0.3 0.0 0.0 0.0 0.4 0.8 0.9 1.0 1.0 0.9 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.6 1.0 0.7 0.2 0.0 0.0 0.3 0.8 1.0 1.0 1.0 1.0 1.0 0.8 0.3 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.6 1.0 0.8 0.3 0.1 0.4 0.8 1.0 1.0 0.7 0.6 0.7 1.0 0.9 0.4 0.1 0.0 0.0 0.0 0.0 0.0 0.0\n0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.2 0.7 1.0 0.9 0.7 0.6 0.8 1.0 0.8 0.5 0.3 0.4 0.7 1.0 0.9 0.2 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.2 0.6 0.9 1.0 1.0 0.9 1.0 1.0 0.8 0.5 0.5 0.6 0.9 1.0 0.8 0.1 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.3 0.6 0.9 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.8 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.2 0.6 1.0 1.0 0.9 0.8 0.9 0.9 0.9 0.9 0.8 0.5 0.1 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.3 0.8 0.9 0.5 0.2 0.4 0.6 0.7 0.7 0.5 0.1 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.3 0.7 0.8 0.2 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.2 0.6 0.8 0.2 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.1 0.2 0.3 0.1 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0"
  },
  {
    "objectID": "posts/2025-02-10-diffusion-model-mnist-part1.html#understanding-the-noise-corruption-function",
    "href": "posts/2025-02-10-diffusion-model-mnist-part1.html#understanding-the-noise-corruption-function",
    "title": "From Noise to Numbers: A Hands-On Guide to Generating MNIST Digits with Diffusion Models (Part 1)",
    "section": "Understanding the Noise Corruption Function",
    "text": "Understanding the Noise Corruption Function\n\nIntroducing the corrupt Function\nIn diffusion models, we need a systematic way to introduce noise into clean images. This controlled noise addition is crucial for simulating the forward diffusion process, where an image gradually transforms from a structured state to pure noise. To achieve this, we use a helper function called corrupt.\nThe corrupt function takes a batch of clean images and progressively degrades them by blending in random noise at varying levels, determined by an amount parameter. This function plays a fundamental role in diffusion models, enabling a structured transition from the original image to noise. While our initial implementation will use a simplified version, understanding its mechanics is essential, as more advanced models will build upon this process.\nNow, let’s examine its implementation:\n\n# Definition of the noise corruption function\ndef corrupt(x, noise, amount):\n    amount = amount.view(-1, 1, 1, 1)  # make sure it's broadcastable\n    return (\n        x * (1 - amount) + noise * amount\n    )  # equivalent to x.lerp(noise, amount)\n\nThe function corrupt(x, noise, amount) takes three input arguments:\n\nx: A batch of clean images, represented as a PyTorch tensor with shape (batch_size, 1, image_size, image_size).\n\nnoise: A tensor of random values, matching the shape of x, representing the noise to be added.\n\namount: A tensor of shape (batch_size,), specifying the level of noise corruption for each image in the batch.\n\nSince amount is a 1D tensor, it must be reshaped to match the dimensions of x for proper broadcasting during element-wise operations. This is achieved with:\namount = amount.view(-1, 1, 1, 1)\nThis transformation ensures that amount is applied consistently across all pixels and channels of each image.\nThe core logic of noise corruption is in the return statement:\nreturn x * (1 - amount) + noise * amount\nThis expression performs a smooth interpolation between the original image x and the noise noise, controlled by amount:\n\n(1 - amount): As amount increases from 0 to 1, this value decreases from 1 to 0.\n\nx * (1 - amount): The clean image is scaled down based on (1 - amount).\n\nIf amount is near 0, the image remains mostly intact.\nIf amount is near 1, its contribution diminishes significantly.\n\n\nnoise * amount: The noise is scaled up in proportion to amount.\n\nAt amount = 0, no noise is added\nAt amount = 1, the image is entirely replaced by noise.\n\nThe sum of these terms creates a seamless blend, where the noise level increases progressively with amount.\n\nBy structuring the noise corruption process this way, we ensure that images degrade in a controlled and predictable manner — an essential property for training diffusion models.\nTo better understand how the corrupt function modifies images, we will apply it to a batch of MNIST digits with varying levels of noise and visualize the results. The code below retrieves a batch of images from train_dataloader, generates a range of noise levels, applies the corrupt function, and then displays both the clean and noisy images.\n\n# --- Visualizing the effect of the corrupt function ---\n# Get a batch of images from the training dataloader\nbatch = next(iter(train_dataloader))\nimgs = batch[\"images\"]\n\n# Prepare noise and noise amounts for visualization\namount = torch.linspace(0, 1, imgs.shape[0]) # Noise amounts from 0 to 1\nnoise = torch.rand_like(imgs)                # Random noise tensor\nnoised_x = corrupt(imgs, noise, amount)      # Apply corrupt function\n\n# --- Plotting the input and corrupted images ---\nfig, axs = plt.subplots(2, 1, figsize=(12, 5)) # Create figure and axes for plotting\n\n# Plotting the original input data\naxs[0].set_title(\"Input data\")\naxs[0].imshow(torchvision.utils.make_grid(imgs)[0], cmap=\"Greys\") # Display original images in top row\n\n# Plotting the corrupted version\naxs[1].set_title(\"Corrupted data, from little corruption to a lot of corrupton\")\naxs[1].imshow(torchvision.utils.make_grid(noised_x)[0], cmap=\"Greys\") # Display corrupted images in bottom row\n\nplt.show() # Show the plot\n\n\n\n\n\n\n\n\nRunning this code generates a visualization consisting of two rows of MNIST digits:\n\nTop row (Input data) – This row displays the original, clean MNIST digits from train_dataloader. These digits are sharp and clearly recognizable. This serves as our reference point before noise is applied.\n\nBottom row (Corrupted data, from little corruption to a lot of corruption) – This row shows the same digits after being processed by the corrupt function. Observing from left to right, we can see an increasing level of noise corruption.\n\n\nLeftmost images: The noise level is minimal, so these images closely resemble the originals in the top row.\n\nGradual progression: Moving toward the right, noise becomes more prominent, making the digits less distinct.\n\nRightmost images: The noise level is at its maximum, significantly obscuring the original digit. In extreme cases, the digit may become unrecognizable.\n\nThis visualization clearly illustrates the behavior of the corrupt function. By adjusting the amount parameter, we can precisely control the degree of noise added to an image — ranging from slight perturbations to near-complete corruption. This capability will be crucial in our exploration of diffusion models, where controlled noise application is fundamental to the learning process."
  },
  {
    "objectID": "posts/2025-02-10-diffusion-model-mnist-part1.html#model-1-simple-convolutional-unet-for-direct-image-prediction",
    "href": "posts/2025-02-10-diffusion-model-mnist-part1.html#model-1-simple-convolutional-unet-for-direct-image-prediction",
    "title": "From Noise to Numbers: A Hands-On Guide to Generating MNIST Digits with Diffusion Models (Part 1)",
    "section": "Model 1: Simple Convolutional UNet for Direct Image Prediction",
    "text": "Model 1: Simple Convolutional UNet for Direct Image Prediction\n\nWhy Use UNets for Diffusion?\nBefore building our first model, the Simple Convolutional UNet, let’s first understand why this architecture is essential for diffusion models.\nIn the context of diffusion models, “diffusion” refers to a process inspired by natural phenomena — like how a drop of ink disperses in water until it is evenly distributed. In machine learning, this concept is mimicked by gradually adding noise to an image until it turns into pure random noise, similar to TV static. This is known as the forward diffusion process. The goal, however, is to reverse this process — step by step removing the noise to recover a meaningful image. This is the reverse diffusion process, and the neural network responsible for this denoising task is often a UNet.\nUNets are widely used for this purpose because they excel at image-to-image tasks, particularly restoring details from noisy images. Their U-shaped architecture consists of two key parts:\n\nThe downsampling path (encoder): This progressively reduces the image size while extracting important features.\nThe upsampling path (decoder): This reconstructs the image while preserving fine details.\n\nA crucial component of UNets is skip connections, which transfer detailed features from the encoder directly to corresponding layers in the decoder. These connections help the network retain high-frequency information, such as edges and textures, which are often lost during downsampling.\n\n\n\nBasic UNet Diagram\n\n\nWhile other architectures could be used, UNets have consistently proven to be highly effective at reversing the diffusion process and generating high-quality images.\nInterestingly, UNets were not originally developed for diffusion models or image generation. They were first introduced for medical image segmentation, where precise localization of structures (such as tumors in MRI scans) is critical. Their success in medical imaging, which requires balancing contextual understanding and fine-detail preservation, makes them an excellent choice for denoising in diffusion models.\nIn our first model, Model 1, we will use a simplified UNet for direct image prediction — a stepping stone toward more advanced diffusion models.\n\n\nOur Direct Image Prediction Approach\nTo ease into the concepts behind diffusion models, Model 1 takes a simplified approach: direct image prediction. Instead of fully implementing a diffusion model that gradually removes noise in multiple steps, we will train a Convolutional UNet to denoise an image in a single forward pass.\nIt’s important to note that this is not a true diffusion model. Unlike traditional diffusion models, which iteratively refine an image by predicting noise at each step, this UNet directly predicts the clean version of a noisy MNIST digit in one go.\nWhy start with this approach?\n\nIt simplifies the learning process. By focusing solely on the UNet’s architecture and training, we can avoid the complexities of diffusion schedulers and iterative sampling.\nIt establishes a useful baseline. This basic denoising UNet allows us to compare performance as we transition to more sophisticated, iterative diffusion models.\n\n\n\n\n\n\n\nAre There U-Net Architectures Without Convolutional Layers?\n\n\n\n\n\nWhen discussing U-Net architectures, we often assume they are convolution-based. But are there U-Net variants that don’t use convolutional layers at all?\nYes, there are. Below are some alternatives that replace convolutions with different computational approaches.\n\n1. Fourier Neural Operator U-Net (FNO-U-Net)\nThis model replaces the usual convolutional layers with Fourier Transforms. Instead of learning patterns using small local filters (like in a CNN), it transforms the image into the frequency domain, where patterns can be processed more efficiently.\n\n\n2. Siren U-Net\nInstead of using convolutions, this model relies on Sinusoidal Representation Networks (SIREN), which use sine functions as activation functions. This allows the network to smoothly model continuous signals.\n\n\n3. MLP-Mixer U-Net\nThis model replaces convolutions with fully connected layers that process information across the entire image. Instead of scanning with filters, it mixes and reshapes the image data in two main steps:\n- First step: Processes information across different image locations (spatial mixing).\n- Second step: Processes information across different color or feature channels (channel mixing).\n- This ensures that both local and global information is captured.\n\n\n\n\n\n\nBuilding a Simple Convolutional UNet\nNow, let’s construct a minimal yet functional UNet that serves as the core of our direct image prediction approach.\n\nNetwork Architecture\nOur Convolutional UNet, implemented as the BasicUNet class, follows the standard U-shape design, consisting of:\n\nA contracting path (encoder): This reduces spatial dimensions while capturing important features through a series of convolutional layers and downsampling operations.\nAn expanding path (decoder): This restores the original image size using convolutional layers and upsampling, reconstructing the denoised image.\nSkip connections: These directly pass feature information from the encoder to the decoder, preserving fine details during the upsampling process.\n\nHere’s the PyTorch code for our BasicUNet class, implementing the simple convolutional UNet architecture we just described:\n\nfrom torch import nn\n\n\nclass BasicUNet(nn.Module):\n    \"\"\"A minimal UNet implementation.\"\"\"\n\n    def __init__(self, in_channels=1, out_channels=1):\n        super().__init__()\n        self.down_layers = nn.ModuleList(\n            [\n                nn.Conv2d(in_channels, 32, kernel_size=5, padding=2),\n                nn.Conv2d(32, 64, kernel_size=5, padding=2),\n                nn.Conv2d(64, 64, kernel_size=5, padding=2),\n            ]\n        )\n        self.up_layers = nn.ModuleList(\n            [\n                nn.Conv2d(64, 64, kernel_size=5, padding=2),\n                nn.Conv2d(64, 32, kernel_size=5, padding=2),\n                nn.Conv2d(32, out_channels, kernel_size=5, padding=2),\n            ]\n        )\n\n        # Use the SiLU activation function, which has been shown to work well\n        # due to different properties (smoothness, non-monotonicity, etc.).\n        self.act = nn.SiLU()\n        self.downscale = nn.MaxPool2d(2)\n        self.upscale = nn.Upsample(scale_factor=2)\n\n    def forward(self, x):\n        h = []\n        for i, l in enumerate(self.down_layers):\n            x = self.act(l(x))\n            if i &lt; 2:  # For all but the third (final) down layer:\n                h.append(x)  # Storing output for skip connection\n                x = self.downscale(x)  # Downscale ready for the next layer\n\n        for i, l in enumerate(self.up_layers):\n            if i &gt; 0:  # For all except the first up layer\n                x = self.upscale(x)  # Upscale\n                x += h.pop()  # Fetching stored output (skip connection)\n            x = self.act(l(x))\n\n        return x\n\n\nfrom torchinfo import summary\n\n# initialize the model\nmodel = BasicUNet()\n\n# print model info\nprint(model)\nsummary(model)\n\nBasicUNet(\n  (down_layers): ModuleList(\n    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n    (1): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n    (2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n  )\n  (up_layers): ModuleList(\n    (0): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n    (1): Conv2d(64, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n    (2): Conv2d(32, 1, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n  )\n  (act): SiLU()\n  (downscale): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  (upscale): Upsample(scale_factor=2.0, mode='nearest')\n)\n\n\n=================================================================\nLayer (type:depth-idx)                   Param #\n=================================================================\nBasicUNet                                --\n├─ModuleList: 1-1                        --\n│    └─Conv2d: 2-1                       832\n│    └─Conv2d: 2-2                       51,264\n│    └─Conv2d: 2-3                       102,464\n├─ModuleList: 1-2                        --\n│    └─Conv2d: 2-4                       102,464\n│    └─Conv2d: 2-5                       51,232\n│    └─Conv2d: 2-6                       801\n├─SiLU: 1-3                              --\n├─MaxPool2d: 1-4                         --\n├─Upsample: 1-5                          --\n=================================================================\nTotal params: 309,057\nTrainable params: 309,057\nNon-trainable params: 0\n=================================================================\n\n\n\n\nExplaining the Simple Conv UNet Code\nLet’s break down the BasicUNet code step-by-step to understand its inner workings.\nWe begin by defining our UNet as a Python class in PyTorch: class BasicUNet(nn.Module):. This line creates a class named BasicUNet that inherits from nn.Module. In PyTorch, nn.Module is the foundational building block for all neural networks. By inheriting from it, BasicUNet gains essential neural network capabilities.\nInside the BasicUNet class, we find the constructor: def __init__(self, in_channels=1, out_channels=1):. This special method, __init__, is executed when you create an instance of BasicUNet. The parameters in_channels=1 and out_channels=1 are set here. They specify that our UNet, by default, is designed to take single-channel grayscale images (like MNIST digits) as input and produce single-channel grayscale images as output.\n\nEncoder Path\nNext, we define the encoder or downsampling path: self.down_layers = nn.ModuleList([...]). self.down_layers is an nn.ModuleList, which acts like a Python list but specifically designed to hold PyTorch neural network layers. Within self.down_layers, we define a sequence of three nn.Conv2d (2D Convolutional) layers:\n\nnn.Conv2d(in_channels, 32, kernel_size=5, padding=2): This is the first convolutional layer of the encoder. in_channels (initially 1 for grayscale images) defines the number of input channels this layer expects. 32 is the number of output channels or feature maps that this layer will generate. kernel_size=5 sets the size of the convolutional filter to 5x5 pixels, and padding=2 adds padding around the input. This padding is used to maintain the spatial dimensions of the output feature map, ensuring it’s the same size as the input.\nnn.Conv2d(32, 64, kernel_size=5, padding=2): The second convolutional layer takes 32 input channels (the output from the previous layer) and outputs 64 channels, further increasing the network’s capacity to learn complex features.\nnn.Conv2d(64, 64, kernel_size=5, padding=2): The third convolutional layer takes 64 input channels and also outputs 64 channels. Notice how the number of feature channels increases in the encoder’s initial layers. This is a common strategy in convolutional networks, allowing them to capture increasingly abstract and complex features as they go deeper.\n\n\n\n\n\n\n\nWhy Conv2d and not Conv3d or Conv1d?\n\n\n\n\n\nIt’s important to understand that nn.Conv2d creates a 2D convolutional layer specifically for two-dimensional data, such as images. In this context, two dimensions refer to height and width, not the number of color channels.\nAll standard images—whether grayscale or RGB—are inherently 2D because they have a height and width. However, the number of channels (e.g., grayscale (1 channel), RGB (3 channels), or RGBA (4 channels)) should not be confused with the concept of dimensionality.\nBut why do we specifically call it a 2D convolutional layer? The reason is that convolutional layers can operate on different types of data:\n\nConv1d (nn.Conv1d) is used for 1D data, where the input has only one spatial dimension (e.g., time series, audio signals, or text embeddings).\n\nConv2d (nn.Conv2d) is for 2D data, where the input has height and width (e.g., grayscale or RGB images).\n\nConv3d (nn.Conv3d) is for 3D data, where the input has height, width, and depth (e.g., CT scans, MRIs, or video frames).\n\nFor example:\n\nConv1d is commonly used for speech processing (e.g., analyzing waveforms).\n\nConv2d is used for standard image processing (e.g., segmentation, classification).\n\nConv3d is useful for volumetric data (e.g., 3D medical scans, motion analysis in videos).\n\n\n\n\n\n\nDecoder Path\nSimilarly, we define the decoder or upsampling path: self.up_layers = nn.ModuleList([...]). self.up_layers, also an nn.ModuleList, holds the convolutional layers for the decoder. It contains three nn.Conv2d layers that, in terms of channel numbers, mirror the encoder in reverse. In the forward pass (explained later), these layers will be used in conjunction with upsampling to reconstruct the image:\n\nnn.Conv2d(64, 64, kernel_size=5, padding=2): The first convolutional layer in the decoder path.\nnn.Conv2d(64, 32, kernel_size=5, padding=2): The second convolutional layer, reducing the number of feature channels back to 32.\nnn.Conv2d(32, out_channels, kernel_size=5, padding=2): The final convolutional layer of the decoder. out_channels (which is 1 by default) ensures that the UNet outputs a single-channel grayscale image, consistent with the input.\n\n\n\nActivation Function\nself.act = nn.SiLU(): This line defines self.act as the SiLU (Sigmoid Linear Unit) activation function. Activation functions are crucial for introducing non-linearity into neural networks, enabling them to learn complex, non-linear relationships in the data. The SiLU activation will be applied after each convolutional layer in the forward pass.\n\n\nDown Sampling\nself.downscale = nn.MaxPool2d(2): Here, self.downscale is defined as a MaxPool2d layer with a kernel size of 2. Max pooling is a downsampling technique. It reduces the spatial dimensions of feature maps by dividing the image into 2x2 blocks and keeping only the maximum value from each block. This effectively halves the height and width of the feature maps, reducing spatial size while retaining important features.\n\n\nUp Sampling\nself.upscale = nn.Upsample(scale_factor=2): This defines self.upscale as an Upsample layer with a scale_factor=2. Upsampling is the inverse of downsampling; it increases the spatial size of feature maps. In this case, it doubles the height and width using nearest-neighbor interpolation by default, effectively reversing the spatial reduction done by max pooling in the encoder.\n\n\nForward Pass\nFinally, the forward(self, x): method defines the forward pass of the network – how input data x flows through the layers to produce an output.\n\nh = []: We initialize an empty list h. This list will serve as storage for intermediate feature maps from the encoder path. These stored feature maps will be used later for skip connections in the decoder.\nThe first for i, l in enumerate(self.down_layers): loop iterates through the encoder’s convolutional layers (self.down_layers).\n\nx = self.act(l(x)): Inside the loop, for each convolutional layer l, we apply the layer to the current input x and then pass the result through the SiLU activation function (self.act). The output becomes the new x.\nif i &lt; 2: h.append(x); x = self.downscale(x): For the first two encoder layers (where i is 0 or 1, meaning not for the last encoder layer), we perform two key actions:\n\nh.append(x): We store the output x of the activated convolutional layer into our skip connection list h. These stored feature maps from the encoder will be added to the decoder path later, enabling skip connections.\nx = self.downscale(x): We apply the MaxPool2d downsampling (self.downscale) to reduce the spatial dimensions of x, preparing it to be processed by the next layer in the encoder path. Downsampling is not performed after the final encoder layer in this architecture.\n\n\nThe second for i, l in enumerate(self.up_layers): loop iterates through the decoder’s convolutional layers (self.up_layers).\n\nif i &gt; 0: x = self.upscale(x); x += h.pop(): For all decoder layers except the first one (i &gt; 0), we perform the following:\n\nx = self.upscale(x): We upsample the feature map x using self.upscale to increase its spatial size, reversing the downsampling from the encoder.\nx += h.pop(): We implement the skip connection here. h.pop() retrieves the last stored feature map from our skip connection list h. We then add this retrieved feature map to the upsampled feature map x. This addition is the core of the skip connection, combining detailed features from the encoder with the upsampled features in the decoder to help preserve fine details during reconstruction.\n\nx = self.act(l(x)): After the optional upsampling and skip connection (for layers after the first in the decoder), we apply the current decoder layer l (a Conv2d layer) and then the SiLU activation function (self.act).\n\nreturn x: Finally, the forward method returns the processed tensor x. This is the output of the UNet, representing the predicted denoised MNIST digit in our case.\n\n\n\n\n\nTraining the Model\nNow that we have defined the BasicUNet architecture, the next step is to train it. We will follow a standard supervised learning approach to train the UNet to predict clean MNIST digits from noisy inputs. This involves defining a loss function, selecting an optimizer, and implementing a training loop.\n\nLoss Function: Mean Squared Error (MSE)\nFor this direct image prediction task, we use the Mean Squared Error (MSE) loss, implemented in PyTorch as F.mse_loss (from torch.nn.functional). MSE measures the average squared difference between the predicted image (output from the UNet) and the ground-truth clean image. By minimizing this loss, the model learns to generate outputs that closely match the original digits, ensuring pixel-wise accuracy.\n\n\nOptimizer: Adam\nWe use the Adam optimizer (torch.optim.Adam) with a learning rate of 1e-3 (0.001). Adam is a widely used optimizer that adapts the learning rate for each parameter, making training more stable and efficient. The learning rate determines how much the model’s weights are adjusted at each step—too high may lead to instability, while too low may slow down convergence.\n\n\nTraining Process\nThe model is trained for 5 epochs, iterating through batches of MNIST images using train_dataloader. Within each epoch, the training loop performs the following steps:\n\nLoad a batch of clean MNIST images from train_dataloader and move them to the computing device (GPU or CPU).\nGenerate noisy images by adding random noise to the clean images. The noise level varies per image, controlled by a random noise_amount scalar.\nFeed the noisy images into the UNet, which predicts the corresponding denoised images.\nCompute the MSE loss, comparing the predicted images with the clean images.\nPerform backpropagation to compute gradients, indicating how each model parameter should be adjusted.\nUpdate the model’s weights using the Adam optimizer to minimize the loss.\nStore the loss value for tracking training progress.\n\nThis process repeats for 5 epochs, allowing the model to progressively improve its ability to denoise MNIST digits.\nAfter training, we plot the loss curve to visualize how the MSE loss evolves over time.\n\nfrom torch.nn import functional as F\nfrom torch.optim import Adam\nimport matplotlib.pyplot as plt\n\n# --- Check GPU Availability ---\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\" # Use GPU if available, else CPU\nprint(\"device: \", device)\n\n# --- Hyperparameters and Setup ---\nmodel = model.to(device)                             # Instantiate BasicUNet and move to device\noptimizer = Adam(model.parameters(), lr=1e-3)             # Initialize Adam optimizer\nlosses = []                                                # Store loss values\nnum_epochs = 5                                             # Number of training epochs\n\nbatch_size = 128                                      # Larger batch size for training\ntrain_dataloader = torch.utils.data.DataLoader(\n    dataset[\"train\"], batch_size=batch_size, shuffle=True\n)\n\ndevice:  cuda\n\n\n\n# --- Training Loop ---\nfor epoch in range(num_epochs):\n    for batch in train_dataloader:                         # Iterate over training batches\n        clean_images = batch[\"images\"].to(device)           # Load clean images\n\n        # Sample noise to add to the images\n        noise = torch.rand_like(clean_images).to(device)\n\n        # Sample amount of noise to add to the images\n        noise_amount = torch.randn(clean_images.shape[0]).to(device)\n\n        # Add noise to the clean images according to the noise magnitude\n        noisy_images = corrupt(clean_images, noise, noise_amount)\n\n        predicted_images = model(noisy_images)             # Model predicts denoised images\n        loss = F.mse_loss(predicted_images, clean_images)  # Compute MSE loss\n\n        optimizer.zero_grad()                              # Clear previous gradients\n        loss.backward()                                    # Backpropagate\n        optimizer.step()                                   # Update model weights\n        losses.append(loss.item())                         # Store loss\n\n    # --- Print average loss per epoch ---\n    avg_loss = sum(losses[-len(train_dataloader):]) / len(train_dataloader)\n    print(f\"Epoch {epoch+1}/{num_epochs} - Average Loss: {avg_loss:.4f}\")\n\nEpoch 1/5 - Average Loss: 0.0173\nEpoch 2/5 - Average Loss: 0.0105\nEpoch 3/5 - Average Loss: 0.0098\nEpoch 4/5 - Average Loss: 0.0093\nEpoch 5/5 - Average Loss: 0.0088\n\n\n\n# --- Plot Loss Curve ---\nplt.figure(figsize=(8, 4))\nplt.plot(losses)\nplt.xlabel(\"Iteration\")\nplt.ylabel(\"MSE Loss\")\nplt.title(\"Training Loss Curve (Model 1 - Direct Image Prediction)\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nInference with Model\nNow that our model is trained, we need to evaluate its ability to denoise images it hasn’t encountered during training. This process, known as inference (or sometimes sampling or generation), involves using the trained model to generate outputs on new data.\nFor Model 1, inference is straightforward because it performs direct image prediction in a single forward pass. Given a noisy image as input, it directly outputs a denoised version.\nTo assess model’s denoising performance, we will use the MNIST test dataset. We’ll set up a DataLoader for the test set, similar to train_dataloader, but using dataset[\"test\"] to load the test split. For each batch of test images, we will:\n\nLoad a batch of clean MNIST test images from the test_dataloader.\n\nGenerate new noisy versions of these test images.\n\nIt’s crucial to use fresh, randomly generated noise—distinct from what was used during training—to properly evaluate the model’s ability to generalize to unseen noisy inputs.\n\nWe will apply the same corrupt function used during training.\n\n\nFeed the noisy test images into our trained BasicUNet model.\n\nThe model will produce its predicted denoised images in a single forward pass.\n\nThis is one-shot denoising—no iterative refinement is involved.\n\n\nTransfer the model’s denoised output to the CPU and convert it to NumPy arrays.\n\nThis step is necessary for visualization with matplotlib, which operates on NumPy arrays.\n\n\nVisualize the results in a plot, displaying:\n\nTop row: Original clean test images (ground truth).\n\nMiddle row: Noisy test images (input to Model 1).\n\nBottom row: Denoised images (Model 1 output).\n\n\nBy analyzing this plot, we can visually assess how effectively Model 1 denoises MNIST digits using its direct image prediction strategy and how well it generalizes to new noisy test data.\nThe PyTorch code for this inference and visualization process is provided in the subsequent code block.\n\n# --- Inference with Model 1 and Visualization ---\n# --- Prepare test dataset and dataloader ---\ntest_dataset = load_dataset(\"mnist\", split=\"test\") # Load MNIST test split dataset\ntest_dataset.set_transform(transform)             # Apply preprocessing transform to test dataset\ntest_dataloader = DataLoader(test_dataset, batch_size=8, shuffle=True) # Create DataLoader for test dataset\n\n# --- Get a batch of test images ---\nbatch = next(iter(test_dataloader))\nclean_images = batch[\"images\"].to(device) # Load clean test images to the device\n\n# --- Generate new random noise for inference ---\nnoise = torch.rand_like(clean_images).to(device) # Generate random noise tensor\nnoise_amount = torch.randn(clean_images.shape[0]).to(device) # Generate noise amount tensor\nnoisy_images = corrupt(clean_images, noise, noise_amount) # Create noisy test images by corruption\n\n# --- Perform inference (get denoised images from Model 1) ---\nmodel.eval()  # Set model to evaluation mode for inference\nwith torch.no_grad():  # Disable gradient calculation during inference\n    denoised_images = model(noisy_images) # Get denoised images from model\n\n# --- Move tensors to CPU and convert to NumPy for visualization ---\nnoisy_images_np = noisy_images.cpu().numpy() # Move noisy images to CPU and convert to NumPy\ndenoised_images_np = denoised_images.cpu().numpy() # Move denoised images to CPU and convert to NumPy\nclean_images_np = clean_images.cpu().numpy() # Move clean images to CPU and convert to NumPy\n\n# --- Plotting the results: Original, Noisy, Denoised ---\nnum_images = 6  # Set number of images to visualize\nplt.figure(figsize=(12, 6)) # Initialize matplotlib figure for plotting\n\nfor i in range(num_images): # Loop through number of images to plot\n    # --- Plot Original (Clean) Images ---\n    plt.subplot(3, num_images, i + 1) # Create subplot for original images (top row)\n    plt.imshow(clean_images_np[i].squeeze(), cmap='Greys') # Display original clean image\n    plt.title(\"Original\") # Set title for original image subplot\n    plt.axis('off') # Hide axes for cleaner image display\n\n    # --- Plot Noisy Images ---\n    plt.subplot(3, num_images, i + num_images + 1) # Create subplot for noisy images (middle row)\n    plt.imshow(noisy_images_np[i].squeeze(), cmap='Greys') # Display noisy image input\n    plt.title(\"Noisy\") # Set title for noisy image subplot\n    plt.axis('off') # Hide axes\n\n    # --- Plot Denoised Images ---\n    plt.subplot(3, num_images, i + 2 * num_images + 1) # Create subplot for denoised images (bottom row)\n    plt.imshow(denoised_images_np[i].squeeze(), cmap='Greys') # Display denoised output image\n    plt.title(\"Denoised\") # Set title for denoised image subplot\n    plt.axis('off') # Hide axes\n\nplt.tight_layout() # Adjust subplot layout for better spacing\nplt.show() # Show the complete plot\n\n\n\n\n\n\n\n\n\n\nResults and Discussion\nLet’s analyze our initial one-shot denoising results to evaluate the baseline performance of our Simple Convolutional UNet.\nIn the “Denoised” row (bottom row), you should notice that the digits are indeed visibly denoised compared to the “Noisy” row (middle row). The random noise has been reduced, and the digits appear less blurred, indicating that our basic Convolutional UNet has learned to remove some noise through its direct image prediction strategy.\nHowever, when comparing the “Denoised” row to the “Original” row (top row, showing clean digits), it becomes clear that the denoised digits are not perfectly restored. Some residual blurriness remains, and they lack the crispness and sharpness of the original images. Despite these imperfections, the denoised outputs remain recognizable as MNIST digits, showing that the model has captured key visual features.\nLet’s run inference one more time, but with a different approach.\nInstead of starting with an original MNIST digit and adding noise to it, we will feed the model pure noise as input and observe the results.\n\nimport torch\nimport matplotlib.pyplot as plt\n\n# Generate a noisy image (random noise)\nnoise_image = torch.rand(1, 32, 32)  # Example: Single-channel (grayscale) 32x32 noise image\n\n# Assume `model` is trained and available\nwith torch.no_grad():\n    denoised_image = model(noise_image.unsqueeze(0).to(device))  # Add batch dimension & move to device\n    denoised_image = denoised_image.squeeze(0).cpu().detach()  # Remove batch dim & move to CPU\n\n# Plot both images side by side\nfig, axs = plt.subplots(1, 2, figsize=(8, 4))\n\naxs[0].imshow(noise_image.squeeze(), cmap=\"Greys\")\naxs[0].set_title(\"Noisy Image\")\naxs[0].axis(\"off\")\n\naxs[1].imshow(denoised_image.squeeze(), cmap=\"Greys\")\naxs[1].set_title(\"Model Prediction (Denoised)\")\naxs[1].axis(\"off\")\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nThis time, the model struggled to generate a clear, crisp image from pure noise. While it did attempt to predict a digit, the result remains blurry.\nIn some of my other runs, the outputs were even worse, indicating that the model has difficulty denoising pure noise in a single step. This suggests that one-shot denoising may not be sufficient for fully reconstructing meaningful digits from random noise.\n\nExploring Iterative Refinement\nTo see if we can further improve upon these one-shot results, we will experiment with a simple iterative refinement approach. Instead of denoising in a single step, we start with pure random noise and iteratively refine it over 5 steps using our trained model. The following code implements this process:\n\n# --- Iterative Refinement Experiment (Model 1) ---\nimport torchvision\nimport matplotlib.pyplot as plt\n\nn_steps = 5\nx = torch.rand(8, 1, 32, 32).to(device) # Start from pure random noise\nstep_history = [x.detach().cpu()] # Store initial noise\npred_output_history = [] # Store model predictions at each step\n\nfor i in range(n_steps):\n    with torch.no_grad():\n        pred = model(x) # Get model's direct prediction\n    pred_output_history.append(pred.detach().cpu()) # Store prediction\n    mix_factor = 1 / (n_steps - i) # Increasing mix factor over steps\n    x = x * (1 - mix_factor) + pred * mix_factor # Linear blend: refine x with prediction\n    step_history.append(x.detach().cpu()) # Store refined x\n\nfig, axs = plt.subplots(n_steps, 2, figsize=(9, 4), sharex=True) # Create 2-column plot\naxs[0, 0].set_title(\"x (model input)\") # Title for 'input' column\naxs[0, 1].set_title(\"model prediction\") # Title for 'prediction' column\nfor i in range(n_steps): # Loop to plot each step\n    axs[i, 0].imshow(torchvision.utils.make_grid(step_history[i])[0].clip(0, 1), cmap=\"Greys\") # Plot refined x\n    axs[i, 1].imshow(torchvision.utils.make_grid(pred_output_history[i])[0].clip(0, 1), cmap=\"Greys\") # Plot prediction\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nRunning this iterative refinement code generates a new plot with two columns:\n\nLeft column (“x (model input)”) Shows images at each step of the refinement process. Starting from pure noise (top row), you can observe how the images gradually become more digit-like over the 5 steps.\n\nRight column (“model prediction”) Displays the direct predictions of Model 1 at each step.\n\nComparing the final results to the one-shot “Denoised” image from the previous experiment, you may notice a subtle but visible improvement in image quality. The iteratively refined digits often appear slightly sharper and more well-formed compared to the one-shot denoised outputs.\nThis suggests that even with a direct prediction model, a basic iterative refinement approach can enhance image quality. However, it’s important to note that this process is still a simplification—it does not represent true diffusion model sampling. The model is still making direct predictions, and we are manually blending them to guide the image toward a cleaner state."
  },
  {
    "objectID": "posts/2025-02-10-diffusion-model-mnist-part1.html#key-takeaways-next-steps",
    "href": "posts/2025-02-10-diffusion-model-mnist-part1.html#key-takeaways-next-steps",
    "title": "From Noise to Numbers: A Hands-On Guide to Generating MNIST Digits with Diffusion Models (Part 1)",
    "section": "Key Takeaways & Next Steps",
    "text": "Key Takeaways & Next Steps\nFrom both one-shot denoising and iterative refinement, we can conclude that even a basic Convolutional UNet trained for direct image prediction can perform some level of denoising on MNIST digits. However, to achieve higher-quality, sharper, and more faithfully reconstructed digits, we need to go beyond this basic approach.\nIn the next section post, we will enhance our UNet architecture using the diffusers library while still employing direct image prediction. This will allow us to examine how architectural improvements can further boost performance. Eventually, we will transition to true noise prediction and scheduled denoising, bringing us closer to the core principles of diffusion models."
  },
  {
    "objectID": "posts/2025-01-26-streamlit-bedrock-nova-app.html",
    "href": "posts/2025-01-26-streamlit-bedrock-nova-app.html",
    "title": "Unleash Your Creativity: A Rapid Prototyping App with Amazon Nova Canvas",
    "section": "",
    "text": "image source: Amazon Nova Canvas. (Prompt: Giant mushroom, stanley artgerm, pointillism, bokeh, sparkles, magic, trees, mountains, lake, thomas kinkaid, norman rockwell)"
  },
  {
    "objectID": "posts/2025-01-26-streamlit-bedrock-nova-app.html#introduction",
    "href": "posts/2025-01-26-streamlit-bedrock-nova-app.html#introduction",
    "title": "Unleash Your Creativity: A Rapid Prototyping App with Amazon Nova Canvas",
    "section": "Introduction",
    "text": "Introduction\nAmazon’s Nova Canvas is a state-of-the-art Amazon foundation model that promises to democratize high-quality image generation. However, the complexities of these models can often be a barrier to entry. This post presents a practical solution: a Streamlit-based application that makes experimenting with Nova Canvas accessible to everyone. I will walk you through the features of this app, which includes five powerful tools, encompassing text-to-image generation, conditional generation, background removal, inpainting, and outpainting. You can seamlessly explore the diverse capabilities of this potent technology and bring your creative visions to life.\n\nEnvironment Details\n\n\nShow the code\nfrom platform import python_version\nimport streamlit\n\nprint(\"python==\" + python_version())\nprint(\"streamlit==\" + streamlit.__version__)\n\n\npython==3.12.8\nstreamlit==1.41.1\n\n\n\n\n\n\n\n\nCode Samples\n\n\n\nAll the code examples used in this post can be found on the GitHub repo 2025-01-26-streamlit-bedrock-nova-app"
  },
  {
    "objectID": "posts/2025-01-26-streamlit-bedrock-nova-app.html#introducing-amazon-nova-canvas",
    "href": "posts/2025-01-26-streamlit-bedrock-nova-app.html#introducing-amazon-nova-canvas",
    "title": "Unleash Your Creativity: A Rapid Prototyping App with Amazon Nova Canvas",
    "section": "Introducing Amazon Nova Canvas",
    "text": "Introducing Amazon Nova Canvas\nAt the heart of my app is Amazon Nova Canvas, a latest foundation model that I’m using to generate realistic, studio-quality images from text prompts. This model is a versatile tool for both text-to-image generation and advanced image editing. Nova Canvas excels at creating images that capture the concepts described in your text prompts.\nBeyond text-to-image (T2I), I’m also leveraging its image editing capabilities, which include inpainting, outpainting, generating variations, and automatic editing without masks. Inpainting allows you to reconstruct masked regions in an image. Outpainting lets you seamlessly extend an image beyond its original boundaries, and you can guide this with masks or let the model determine it. The model also supports image conditioning, which uses a reference image to guide generation, and it also supports subject consistency to preserve a chosen subject like a pet or a shoe in your generated images. It can also automatically remove backgrounds from an image, making the background transparent.\nOverall, this model provides me with a powerful and diverse set of tools to unleash creative possibilities.\n\n\n\n\n\n\nAmazon Nova Canvas Official Documentation\n\n\n\nThe Nova Canvas model id used in this post is amazon.nova-canvas-v1:0.\nYou can read more about Nova Canvas features from its official documentation page. Link Here"
  },
  {
    "objectID": "posts/2025-01-26-streamlit-bedrock-nova-app.html#the-rapid-prototyping-app-a-hands-on-playground",
    "href": "posts/2025-01-26-streamlit-bedrock-nova-app.html#the-rapid-prototyping-app-a-hands-on-playground",
    "title": "Unleash Your Creativity: A Rapid Prototyping App with Amazon Nova Canvas",
    "section": "The Rapid Prototyping App: A Hands-On Playground",
    "text": "The Rapid Prototyping App: A Hands-On Playground\nTo make exploring the power of Amazon Nova Canvas as easy and accessible as possible, I decided to build an application using Streamlit. I chose Streamlit because it is incredibly efficient for quickly creating interactive web apps with minimal coding, making it the perfect tool for this project. My goal was to create a hands-on playground where you could directly experiment with Nova Canvas’s capabilities, transforming your creative ideas into visual reality.\nThis app isn’t just for tech enthusiasts; I wanted it to be user-friendly, allowing anyone to explore AI-powered image generation and editing. The best part is that it’s designed for easy sharing and deployment, so you can effortlessly share your experiments with your friends, or even deploy the app for wider access. I have included some of the app’s screenshots below, so you can have a quick overview.\nYou’ll see a simple login page where you can enter your credentials (or bypass authentication during local testing). After logging in, you are presented with a user-friendly interface, with a sidebar that lets you navigate to each of the five image processing tools. The welcome page gives you a quick overview of each of the tools and how to use them. Each tool provides an intuitive interface with controls to configure the parameters and get the most out of the capabilities of the Amazon Nova Canvas model.\n\n\n\n\n\n\nLogin Page\n\n\n\n\n\n\n\nWelcome Page\n\n\n\n\n\n\n\nText to Image Page\n\n\n\n\n\n\n\n\n\n\n\nText to Image with Condition Page\n\n\n\n\n\n\n\nRemove Image Background Page\n\n\n\n\n\n\n\nImage Inpainting Page\n\n\n\n\n\n\n\n\n\n\n\nImage Outpainting Page\n\n\n\n\n\nWe’ll dive into each of these pages in more detail in the following sections, but this overview should give you a good initial feel of what the app looks and feels like."
  },
  {
    "objectID": "posts/2025-01-26-streamlit-bedrock-nova-app.html#text-to-image",
    "href": "posts/2025-01-26-streamlit-bedrock-nova-app.html#text-to-image",
    "title": "Unleash Your Creativity: A Rapid Prototyping App with Amazon Nova Canvas",
    "section": "Text to Image",
    "text": "Text to Image\nThe first tool in our arsenal is the “Text to Image” generator. This is where the magic begins: It let’s you turn your imagination into stunning visuals with just a few words. By harnessing the advanced capabilities of Nova Canvas, you can generate high-quality images tailored to your specific prompts.\n\nHow to use it:\n\nUse the controls on the page to configure the text-to-image generation process. You can select the number of images you want, the size of each image (with a variety of preset options), and fine-tune the cfgScale parameter.\nThe cfgScale parameter lets you specify how strongly the generated image should adhere to the prompt. A lower value introduces more randomness, while a higher value will stick more closely to the prompt.\nEnter a positive prompt – this is where you describe the image you want to generate. Be as specific as possible to get the best results.\nOptionally, you can also enter a negative prompt. This is a way to tell the model what you don’t want to see in the image, further refining the output.\n\nOnce you are happy with the parameters you have selected, simply click the “Generate Image” button to start the process. The generated images will be displayed below the button.\nBelow is an example of what the app looks like after generating two images. I used the prompt Fantastic celestial night sky with shining stars. Amazing John Dyer art. using the following settings.\n\nI encourage you to experiment with different prompts and parameters to see the amazing results that you can create using this tool. I have provided below more examples of the generated images with different prompts to demonstrate the versatility of the tool.\n\n\n\n\n\n\nTip\n\n\n\nClick on an image to expand it, and view in a higher resolution.\n\n\n\n\n\n\n\n\nA serene forest where the trees are made of glass, sunlight refracting into rainbow colors, with a mystical glowing lake in the center. Style: fantasy realism.\n\n\n\n\n\n\n\nA giant clock tower covered in vines, standing in the middle of a desert, with time flowing like water around it. Style: surreal art, soft light\n\n\n\n\n\n\n\nA massive volcanic mountain surrounded by a swirling galaxy in the night sky, with rivers of lava glowing in the dark. Style: cinematic and dramatic.\n\n\n\n\n\n\n\nAn underwater city glowing with bioluminescent plants and creatures, built into a coral reef, with massive whales swimming above. Style: fantasy underwater.\n\n\n\n\n\n\n\n\n\n\n\nA futuristic Mars colony with advanced architecture, glowing biodomes, and red dunes stretching to the horizon. Style: sci-fi realism.\n\n\n\n\n\n\n\nA warrior elf with glowing silver armor, holding a sword that emits blue flames, standing in an enchanted forest. Style: hyper-detailed fantasy art.\n\n\n\n\n\n\n\nA steampunk inventor with mechanical goggles, surrounded by intricate clockwork machines, in a smoky Victorian-era workshop. Style: steampunk, warm tones.\n\n\n\n\n\n\n\nA robotic wolf with glowing red eyes, prowling through a futuristic cityscape at night. Style: cyberpunk, metallic textures.\n\n\n\n\n\n\n\n\n\n\n\nA giant turtle with a miniature forest growing on its back, walking through the ocean at sunrise. Style: serene, magical realism.\n\n\n\n\n\n\n\nA massive spaceship docked on a floating platform above a gas giant, with bright auroras in the sky. Style: sci-fi cinematic.\n\n\n\n\n\n\n\nA futuristic AI core glowing in the center of a high-tech laboratory, surrounded by floating holograms and robotic arms. Style: sleek and ultra-modern.\n\n\n\n\n\n\n\nAn explosion of colors forming a human face, with geometric patterns and abstract swirls blending into the background. Style: modern art.\n\n\n\n\n\n\n\n\n\n\n\nA dream-like landscape where the sky is made of water and the ground is a glowing galaxy, with floating islands in between. Style: surreal fantasy.\n\n\n\n\n\n\n\nA haunted Victorian mansion on a cliffside, illuminated by lightning strikes, with ghostly figures in the windows. Style: gothic horror.\n\n\n\n\n\n\n\nA dark angel with tattered black wings, standing in a crumbling cathedral, with glowing red eyes and a flaming sword. Style: dark fantasy.\n\n\n\n\n\n\n\nA cursed forest with twisted trees, glowing red mushrooms, and a shadowy figure lurking in the fog. Style: eerie and atmospheric."
  },
  {
    "objectID": "posts/2025-01-26-streamlit-bedrock-nova-app.html#text-to-image-with-condition",
    "href": "posts/2025-01-26-streamlit-bedrock-nova-app.html#text-to-image-with-condition",
    "title": "Unleash Your Creativity: A Rapid Prototyping App with Amazon Nova Canvas",
    "section": "Text to Image with Condition",
    "text": "Text to Image with Condition\nTaking image generation to the next level, my “Text to Image with Condition” tool lets you shape your creations with precision and intention. It goes beyond simple text prompts by using a reference image to guide the generation process, helping you achieve the exact look and feel you desire. This tool uses advanced image conditioning features, offering two powerful methods: Canny Edge Detection and Segmentation Maps.\n\nFeatures:\n\nCanny Edge Detection: By extracting the edges from your reference image. The output will inherit the structural layout of the reference image.\nSegmentation Maps: This allows you to define and control specific areas or objects in the reference image. The model will then use this information to render precise outputs and allow you to manipulate individual objects in the image.\n\n\n\nHow to Use It:\n\nFirst, upload the reference image that you want to use to guide the generation. For best results, ensure the reference image is clear and appropriately sized.\nNext, select the conditioning method that you want to use: Canny Edge or Segmentation Map from the “Control Mode” dropdown.\nYou can adjust the “Control Strength” parameter to determine how closely the generated image adheres to the reference image. Use a lower value to introduce more randomness in the generation.\nThen, enter a text prompt describing the image you want to generate, keeping in mind the selected conditioning method.\nOptionally, you can also enter a negative prompt to further refine the image.\nConfigure other options, such as the number of images, image size and the cfgScale parameter, just like you did in the Text to Image page.\nClick on the “Generate Image” button to view the results. The generated images will be displayed below the button along with the original image used for conditioning.\n\nBelow is an example of what the app looks like after generating two images using the Canny Edge option. I used the reference image (Harry Potter/Daniel Radcliffe) shown in the “Uploaded Image” section and the prompt Reimagine this person in a cyberpunk cityscape with glowing neon lights. The other options were set to the default value.\n\nThis tool allows you to take full control of the image generation process and unleash your creative possibilities. I have shared below some more examples of the generated images with different prompts, and the input image used for the condition.\n\n\n\n\n\n\nInput Condition Image\n\n\n\n\n\n\n\nTransform this wizard into a celestial guardian with a starry cloak, glowing golden staff, and galaxies swirling in his beard. Style: ethereal fantasy.\n\n\n\n\n\nSome more examples.\n\n\n\n\n\n\nInput Condition Image\n\n\n\n\n\n\n\nTransform him into a robotic AI wizard with a metallic body, LED lights in his eyes, and a holographic spellbook floating in front of him.\n\n\n\n\n\n\n\nTurn him into a dark necromancer with a tattered black cloak, glowing green skulls floating around him, and a staff topped with a glowing crystal skull.\n\n\n\n\n\n\n\nMake him an ice wizard with a robe made of frost and snowflakes, a frozen staff, and icy blue magic swirling around his hands.\n\n\n\n\n\n\n\n\n\n\n\nInput Condition Image\n\n\n\n\n\n\n\nEnhance this forest with floating lanterns and mystical creatres like fairies and glowing butterflies.\n\n\n\n\n\n\n\nTurn this forest into a futuristic biome with glass domes and flying drones.\n\n\n\n\n\n\n\nAdd a hidden magical portal surrounded by glowing mushrooms and ancient runes.\n\n\n\n\n\n\n\n\n\n\n\nInput Condition Image\n\n\n\n\n\n\n\nTurn this cat into a futuristic robot with glowing eyes and metal plating.\n\n\n\n\n\n\n\nMake this cat appear as a royal king with a golden crown and cape.\n\n\n\n\n\n\n\nReimagine this cat as a mythical fire spirit surrounded by flames."
  },
  {
    "objectID": "posts/2025-01-26-streamlit-bedrock-nova-app.html#background-removal",
    "href": "posts/2025-01-26-streamlit-bedrock-nova-app.html#background-removal",
    "title": "Unleash Your Creativity: A Rapid Prototyping App with Amazon Nova Canvas",
    "section": "Background Removal",
    "text": "Background Removal\nSometimes, you need a clean subject without any background distractions. That’s where my “Background Removal Tool” comes in. This tool empowers your creative workflow by allowing you to effortlessly remove backgrounds from images in just one step. Whether you want to composite your subject onto a solid color background or seamlessly layer it over another scene, this tool provides a clean and accurate solution.\nLeveraging the power of Nova Canvas, the app intelligently detects and segments multiple foreground objects, ensuring even complex scenes with overlapping elements are isolated with precision.\n\nHow to Use It:\n\nUpload an image (PNG or JPG) using the uploader on the page. The app can handle different image formats for your convenience.\nOnce uploaded, the app automatically processes the image using Amazon Bedrock to remove the background.\nThe processed image, with the background removed, will then be displayed below the uploader.\n\nIt’s that simple! With this tool, you can transform your images with ease. I’ll share some examples of the before and after results of the processed images.\n\nSome more examples.\n\n\n\n\n\n\nInput Image\n\n\n\n\n\n\n\nOutput Image\n\n\n\n\n\n\n\n\n\n\n\nInput Image\n\n\n\n\n\n\n\nOutput Image\n\n\n\n\n\n\n\n\n\n\n\nInput Image\n\n\n\n\n\n\n\nOutput Image"
  },
  {
    "objectID": "posts/2025-01-26-streamlit-bedrock-nova-app.html#image-inpainting",
    "href": "posts/2025-01-26-streamlit-bedrock-nova-app.html#image-inpainting",
    "title": "Unleash Your Creativity: A Rapid Prototyping App with Amazon Nova Canvas",
    "section": "Image Inpainting",
    "text": "Image Inpainting\nSometimes, images have unwanted elements or imperfections. That’s when my “Image Inpainting Tool” becomes invaluable. This tool allows you to enhance, restore, or creatively modify images by masking specific areas and providing detailed prompts to guide the process. Whether you’re fixing imperfections or adding new elements, this tool empowers you with advanced inpainting features.\n\nHow It Works:\n\nUpload an Image: Begin by uploading the image you wish to modify using the uploader. The app supports various image formats for your convenience.\nMask the Area: Use the integrated canvas tool to intuitively draw masks over the regions you want to change. You can adjust the “Canvas Stroke Width” to fine-tune the size of the brush for precise masking.\nProvide a Prompt: Describe the changes you’d like to see (e.g., “replace the masked area with a sunset sky”). Be as specific as possible to guide the inpainting.\nGenerate Results: Once you are happy with the mask and the prompt, simply click the “Edit Image” button to start the process. The generated output will be displayed below along with the original image and the masked image.\n\nBelow is an example of how to use this tool. I have uploaded an image and have masked a section of the image, and using the prompt “Replace the masked area with a honey bee” I was able to generate the image displayed below.\n\nThis tool gives you a high degree of control over the modifications, allowing you to achieve seamless and natural-looking results. I’ll share below some more examples of how you can creatively use this tool.\n\n\n\n\n\n\nInput Image\n\n\n\n\n\n\n\nMasked Image with Prompt: Replace the masked area with a honey bee.\n\n\n\n\n\n\n\nOutput Image\n\n\n\n\n\n\n\n\n\n\n\nInput Image\n\n\n\n\n\n\n\nMasked Image with Prompt: Replace the masked area with a dog.\n\n\n\n\n\n\n\nOutput Image\n\n\n\n\n\n\n\n\n\n\n\nInput Image\n\n\n\n\n\n\n\nMasked Image with Prompt: Remove from the scene.\n\n\n\n\n\n\n\nOutput Image\n\n\n\n\n\n\n\n\n\n\n\nInput Image\n\n\n\n\n\n\n\nMasked Image with Prompt: Remove the tea stains from the masked area. Table cloth should be clean and tidy.\n\n\n\n\n\n\n\nOutput Image"
  },
  {
    "objectID": "posts/2025-01-26-streamlit-bedrock-nova-app.html#image-outpainting",
    "href": "posts/2025-01-26-streamlit-bedrock-nova-app.html#image-outpainting",
    "title": "Unleash Your Creativity: A Rapid Prototyping App with Amazon Nova Canvas",
    "section": "Image Outpainting",
    "text": "Image Outpainting\nMy final tool in this application is the “Image Outpainting Tool.” This tool lets you expand your images beyond their original boundaries, creating seamless extensions that match the original content. Whether you want to extend landscapes, complete partial scenes, or create panoramic views, this tool helps you achieve natural-looking results.\n\nHow It Works:\n\nUpload Image: Start by uploading your image using the uploader. The app can handle different image formats.\nSelect Mask Type:\n\nPrompt: Use text to define the mask area to preserve. You can use a text prompt to specify what part of the image you want to keep unchanged and the model will generate a mask for you.\nImage: Automatically mask the image area to keep unchanged. The model will mask the entire image as the area to preserve.\n\nSet Expanded Size: Choose the new image size: 512x512 or 1024x1024 using the dropdown.\nAdjust Position: Use the “Image Horizontal Position” and “Image Vertical Position” controls to place your image within the expanded canvas.\nAdd a Prompt: Describe how the extended areas should look. Be as specific as possible to get desired results.\nGenerate Results: Click on the “Edit Image” button to create and refine your expanded image. The expanded image and the expanded image mask will be displayed below the image and controls. The output will be displayed below that.\n\nBelow is an example of what the app looks like after outpainting an uploaded image. Here I have used the “Image” mask type and the prompt “forest setting in the background with animals and plants.” I have kept the other options at their default value.\n\nWith this tool, you can easily bring your creative visions to life by expanding images beyond their boundaries!I have provided below more examples of the creative possibilities of this tool.\n\n\n\n\n\n\nInput Image\n\n\n\n\n\n\n\nExpanded Image. Prompt: forest setting in the background with animals and plants. Mask Prompt: a panda\n\n\n\n\n\n\n\nOutput Image\n\n\n\n\n\n\n\n\n\n\n\nInput Image\n\n\n\n\n\n\n\nExpanded Image. Prompt: The cat is surrounded by toys. Mask Prompt: a cat\n\n\n\n\n\n\n\nOutput Image\n\n\n\n\n\n\n\n\n\n\n\nInput Image\n\n\n\n\n\n\n\nExpanded Image. Prompt: The chair is placed in a luxury home. Mask Prompt: a chair\n\n\n\n\n\n\n\nOutput Image\n\n\n\n\n\n\n\n\n\n\n\nInput Image\n\n\n\n\n\n\n\nExpanded Image. Prompt: The car is sitting on the road with trees, houses, shops. and people walking in the background. Mask Prompt: a car\n\n\n\n\n\n\n\nOutput Image"
  },
  {
    "objectID": "posts/2025-01-26-streamlit-bedrock-nova-app.html#running-the-app-locally",
    "href": "posts/2025-01-26-streamlit-bedrock-nova-app.html#running-the-app-locally",
    "title": "Unleash Your Creativity: A Rapid Prototyping App with Amazon Nova Canvas",
    "section": "Running the App Locally",
    "text": "Running the App Locally\nBefore deploying the app, you might want to experiment with it on your local system. Here’s how you can quickly get the app up and running:\n\nPrerequisites\n\nAmazon Bedrock Access: You’ll need access to the Amazon Nova Canvas model. If you don’t have access already, visit the Model Catalog in Amazon Bedrock from your AWS account console and request access to the model (as shown below).\n\n\n\nAWS CLI Configuration: Make sure you have the AWS CLI configured on your local system and that the user has the necessary permissions to invoke the Nova Canvas model.\n\n\n\nIAM Policy\n\n{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Sid\": \"VisualEditor0\",\n            \"Effect\": \"Allow\",\n            \"Action\": \"bedrock:InvokeModel\",\n            \"Resource\": \"*\"\n        }\n    ]\n}\n\n\n\nCode\n\nCode Download: Download the code from the GitHub repository: 2025-01-26-streamlit-bedrock-nova-app.\nCode Structure: The code is organized as follows:\n\n\n\nCode Hierarchy\n\n├── home\n│   └── welcome.py\n├── llm\n│   ├── __init__.py\n│   └── utils.py\n├── tools\n│   ├── inpainting.py\n│   ├── outpainting.py\n│   ├── remove_background.py\n│   ├── text_to_image_condition.py\n│   ├── text_to_image.py\n│   └── __init__.py\n├── .gitignore\n├── app.py\n├── config.py\n├── Dockerfile\n├── README.md\n└── requirements.txt\n\nHere’s a brief explanation of the important files:\nconfig.py: This file is used to set up important configurations for the app. Below is the snippet from the file to give you an overview.\n\n\nconfig.py\n\nclass Config:\n    # The name of the Bedrock model to use\n    BEDROCK_MODEL = {\n        \"id\": \"amazon.nova-canvas-v1:0\",\n        \"name\": \"Amazon Bedrock - Nova Canvas\",\n    }\n\n    # If Bedrock is not activated in us-east-1 in your account, set this value accordingly\n    BEDROCK_REGION = \"us-east-1\"\n\n    # User Auth?\n    LOGIN_REQUIRED = False\n\nIn this file, you can specify the Bedrock model ID and region. Make sure that the BEDROCK_REGION matches the region where you have access to the Nova Canvas model. You can also disable Cognito user authentication for local testing by setting LOGIN_REQUIRED to False.\nrequirements.txt: This file lists the required Python libraries for the app. Make sure these dependencies are installed in your local environment.\n\n\nrequirements.txt\n\nstreamlit==1.41.1\nstreamlit-drawable-canvas==0.8.0\nstreamlit-cognito-auth==1.3.1\npillow==11.1.0\nboto3==1.35.97\nnumpy==2.2.1\n\n\n\nRunning the App\n\napp.py: This is the main file to run the Streamlit application.\nOpen your terminal or command prompt.\nNavigate to the directory where you have downloaded the code.\nRun the command streamlit run app.py. You should see the following output in your terminal:\n\n\nOpen your web browser and visit the URL shown in the output (http://localhost:8501) to access the app."
  },
  {
    "objectID": "posts/2025-01-26-streamlit-bedrock-nova-app.html#deployment",
    "href": "posts/2025-01-26-streamlit-bedrock-nova-app.html#deployment",
    "title": "Unleash Your Creativity: A Rapid Prototyping App with Amazon Nova Canvas",
    "section": "Deployment",
    "text": "Deployment\nNow that you’ve experimented with the app locally, it’s time to deploy it to the cloud. I’ll be focusing on using Amazon Lightsail, which is a cost-effective and easy-to-use service for deploying containerized applications.\n\n\n\n\n\n\nAlternatives to Lightsail?\n\n\n\n\n\nIf you are interested in learning about the alternative approaches in deploying this app, then\n\nRefer to my other post where I have explained some of the alternative: 2025-01-21-streamlit-deployment-aws\nRefer to the AWS blog post for deploying the Streamlit app with AWS CDK: build-and-deploy-a-ui-for-your-generative-ai-applications-with-aws-and-python\n\n\n\n\nHere’s a step-by-step guide for deploying your Streamlit app on Amazon Lightsail:\n\n1. Create an Elastic Container Registry Repository\nHead over to your AWS console and create a new Elastic Container Registry (ECR) repository. This repository will be used to store your application’s Docker container image. I would recommend that you use a descriptive name for your repo, such as streamlit/bedrock-nova-app.\nMake sure you create the ECR in the same AWS region where you have access to the Amazon Bedrock models.\n\n\n\n\n\n\nCan I use a public repo like Docker Hub?\n\n\n\n\n\nI have used AWS private ECR repo in the above step. But you are open to use any other public service of your choice like Docker Hub.\n\n\n\n\n\n\n2. Build and Push the Docker Image\nOn your local machine, build the Docker container image and then push it to your newly created ECR. Use the commands provided by AWS for your specific account and registry. Here is an example of the commands, specific to my account. Remember that the command provided by AWS in the console is very specific to your account and you should use that only:\n\n\n\n3. Create a Lightsail Container Service\nNavigate to the AWS Lightsail service in your console. We’ll be using Lightsail container service.\nLightsail’s interface is very intuitive. Follow the guided steps to create the container service, making sure you select the same region where your ECR repository is located. For a simple Streamlit app, a Nano compute power (512MB RAM, 0.25 vCPU) will be sufficient.\n\n\n4. Give Lightsail Access to ECR\nIn the “Images” tab of your Lightsail container service, select the correct ECR repository and grant Lightsail access to it. This allows Lightsail to pull the container image from your repository.\n\n\n\n\n\n\n\nContainers on AWS — Amazon Lightsail Containers\n\n\n\n\n\nHere is a link to another post that explains the similar step in a very detailed manner: containers-on-aws-amazon-lightsail\n\n\n\n\n\n5. Configure IAM User Permissions\nTo enable your Lightsail container to access Bedrock models, you’ll need to create a new IAM user.\n\nAssign the bedrock:InvokeModel policy to this user.\nCreate an access key for the user and save the AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY for later use.\n\nAmazon Lightsail is a special service and you cannot assign IAM roles/permissions to it like you do for other services. Similarly Lightsail cannot access AWS Secrets Manager to keep the credentails. So we will create a new IAM user with the required persmissions, and assign it to our app.\n\n\n\n\n\n\n\nKeep your IAM access and secret key secure!\n\n\n\n\n\nDo not share your AWS access key ID or secret access key publicly, and keep them secure.\n\n\n\n\n\n6. Configure Cognito User Pool (If Needed)\nIf you have enabled user authentication (LOGIN_REQUIRED = True) in your config.py file, you’ll need to set up a Cognito User Pool.\nCreate a Cognito User Pool and get the POOL_ID, APP_CLIENT_ID, and APP_CLIENT_SECRET.\nNote: Do not share your app client secret publicly and keep it secure!\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n7. Deploy the Container\nIn the “Deployments” tab of your Lightsail container service, provide the following environment variables:\n\nAWS_ACCESS_KEY_ID\nAWS_SECRET_ACCESS_KEY\n\nIf Cognito is enabled\n\nPOOL_ID\nAPP_CLIENT_ID\nAPP_CLIENT_SECRET\n\nSet the container port to 8501 and the protocol to HTTP.\nClick the “Deploy” button.\n\nYour app should now be deployed and accessible via the public domain URL provided by Lightsail! For login you will be required to create a verified cognito user."
  },
  {
    "objectID": "posts/2025-01-26-streamlit-bedrock-nova-app.html#conclusion",
    "href": "posts/2025-01-26-streamlit-bedrock-nova-app.html#conclusion",
    "title": "Unleash Your Creativity: A Rapid Prototyping App with Amazon Nova Canvas",
    "section": "Conclusion",
    "text": "Conclusion\nIn this post, I’ve shared my journey of creating an application using Streamlit and the powerful Amazon Nova Canvas model. From generating images from text to performing advanced editing operations like inpainting and outpainting, this app offers a hands-on experience with some of the latest advancements in AI-powered image processing.\nI’ve shown you how to get started by running the app locally, and I’ve also provided a detailed guide on deploying the application to AWS using Lightsail. Now, you can easily create amazing visuals using the different tools that I have developed and even share it with your friends.\nI encourage you to explore the app, experiment with different prompts and parameters, and unleash your creativity. This is a starting point, and I’m excited to see what you create with these tools. I’d also love to hear your feedback and any ideas you have for future enhancements!\nThank you for joining me on this journey. I hope this app inspires your creative projects and helps you discover the exciting possibilities of AI-powered image generation."
  },
  {
    "objectID": "posts/2025-01-19-streaming-responses-fastapi.html",
    "href": "posts/2025-01-19-streaming-responses-fastapi.html",
    "title": "Streaming Responses in FastAPI",
    "section": "",
    "text": "image source: https://www.artbreeder.com/image/42b4c7b3d50f204a76b86d90e175"
  },
  {
    "objectID": "posts/2025-01-19-streaming-responses-fastapi.html#introduction",
    "href": "posts/2025-01-19-streaming-responses-fastapi.html#introduction",
    "title": "Streaming Responses in FastAPI",
    "section": "Introduction",
    "text": "Introduction\nWhen working with LLMs, you quickly realize that getting a response all at once doesn’t always cut it. For a better user experience, especially with long outputs, streaming the response is the way to go. But standard REST APIs aren’t built for this – they hold the client hostage until the entire response is ready. I’ve been exploring how to overcome this with FastAPI, and in this post, I’ll walk you through creating streaming APIs using StreamingResponse, Server-Sent Events (SSE), and WebSockets. I’ll use a simple dummy LLM to illustrate the concepts, and you can find all the code examples in the linked GitHub repository.\n\nEnvironment Details\n\n\nShow the code\nfrom platform import python_version\nimport fastapi\nimport sse_starlette\n\nprint(\"python==\" + python_version())\nprint(\"fastapi==\" + fastapi.__version__)\nprint(\"sse_starlette==\" + sse_starlette.__version__)\n\n\npython==3.12.8\nfastapi==0.115.6\nsse_starlette==2.2.1\n\n\n\n\n\n\n\n\nCode Samples\n\n\n\nAll the code examples used in this post can be found on the GitHub repo 2025-01-19-streaming-responses-fastapi\n\n\n\n\nSimulating LLM Streaming Output\nAs we dive into implementing streaming APIs, it’s crucial to have a way to mimic how an LLM would generate text. Rather than connecting to a real model for every example, we’ll use a simple, controlled simulation. This allows us to focus on the streaming mechanisms without getting bogged down in the complexities of LLM inference.\nOur simulation is based on a straightforward asynchronous generator function called event_generator. Here’s the Python code:\n# Test messages\nMESSAGES = [\n    \"This is\",\n    \" a large\",\n    \" response\",\n    \" being\",\n    \" streamed\",\n    \" through FastAPI.\",\n    \" Here's the final\",\n    \" chunk!\",\n]\n\nasync def event_generator():\n    for message in MESSAGES:\n        yield message\n        await asyncio.sleep(1)  # Simulate an async delay\nAs you can see, the event_generator iterates through a list of predefined messages. With each iteration, it yields a message chunk and then pauses for 1 second using asyncio.sleep(1). This pause mimics the time it might take for an LLM to generate the next portion of a response. This approach makes the simulation both easy to understand and also representative of the nature of LLM output. We’ll use this event_generator function across all the streaming examples in this post."
  },
  {
    "objectID": "posts/2025-01-19-streaming-responses-fastapi.html#method-1-streaming-with-streamingresponse",
    "href": "posts/2025-01-19-streaming-responses-fastapi.html#method-1-streaming-with-streamingresponse",
    "title": "Streaming Responses in FastAPI",
    "section": "Method 1: Streaming with StreamingResponse",
    "text": "Method 1: Streaming with StreamingResponse\nFastAPI’sStreamingResponse is a powerful tool for handling situations where you need to send large or continuous data to a client. Unlike traditional REST API responses, which require the server to generate the entire response before sending, StreamingResponse allows the server to transmit data in chunks. This is especially useful for use cases such as streaming audio/video, large file downloads, or, as in our case, delivering output from an LLM model.\n\nKey Differences from REST API Responses\n\nREST API: In a typical REST scenario, the entire response body is generated on the server and then sent to the client as a single unit. This works fine for smaller datasets, but it can be inefficient when dealing with large amounts of data. The client has to wait until the whole response is built before it can begin processing.\nStreamingResponse: With StreamingResponse, data is transmitted in a series of chunks over a single HTTP connection. As soon as the first chunk is ready, it’s sent to the client. This allows the client to begin processing data while the server is still generating the rest of the response. This incremental delivery significantly enhances the user experience, particularly when dealing with long processing tasks like those often found in LLM interactions.\n\n\n\nHTTP Protocol Differences\n\nContent-Length: In a traditional REST API response, the Content-Length header specifies the total size of the response in bytes. However, with StreamingResponse, the total size of the data is not always known upfront. As such the Content-Length header might be absent, or more commonly the header Transfer-Encoding: chunked will be used instead.\nTransfer-Encoding: chunked: This HTTP header indicates that the response body is being sent in chunks. Each chunk is prefaced by its size, allowing the client to process data as it arrives without knowing the total size of the response beforehand.\n\n\n\nHow Clients Recognize Incoming Data is Streamed?\nThe client recognizes that it is receiving a streamed response when it encounters the Transfer-Encoding: chunked header. Each chunk is prefaced by its size, and the client waits for the next chunk. This process continues until the stream is closed, indicating that there’s no more data to receive.\nThe server signals the end of the stream in one of the following ways:\n\nEmpty Chunk: The server sends a final chunk that has a size of 0. This indicates that there is no more data to send.\nConnection Closure: If the stream ends naturally (for example, because the generator function has exhausted), FastAPI will automatically close the connection. In essence, the server sends a TCP “FIN” packet to the client signaling the end of transmission.\n\nIn the following example, we’ll see how we can use FastAPI’s StreamingResponse along with our simulated LLM to demonstrate how to construct a streaming API.\n\n\nServer-Side Implementation (FastAPI)\nNow, let’s solidify our understanding with a concrete example using FastAPI’s StreamingResponse. On the server side, our endpoint will use the event_generator we defined earlier to stream data:\n\n\n\nShow the code\n\napp_stream_response.py\n\nfrom fastapi import FastAPI\nfrom fastapi.responses import StreamingResponse\nimport asyncio\n\n# Test messages (same as before)\nMESSAGES = [\n    \"This is\",\n    \" a large\",\n    \" response\",\n    \" being\",\n    \" streamed\",\n    \" through FastAPI.\",\n    \" Here's the final\",\n    \" chunk!\",\n]\n\nasync def event_generator():\n    for message in MESSAGES:\n        yield message\n        await asyncio.sleep(1)  # Simulate an async delay\n\napp = FastAPI()\n\n@app.get(\"/stream\")\nasync def stream_response():\n    return StreamingResponse(\n        event_generator(),\n        media_type=\"text/plain\",)\n\n\n\nIn this code, the /stream endpoint utilizes StreamingResponse. We’re passing in the output of the event_generator, which will be streamed as text/plain. The key here is that the data isn’t collected into a single string first; instead, it is yielded piece-by-piece, and StreamingResponse handles the chunking and transmission.\n\n\nClient-Side Implementation (JavaScript)\nOn the client side, fetching this stream requires a slightly different approach than traditional REST calls. Instead of waiting for a single, complete JSON response, we read from the stream as it becomes available. Here is the JavaScript code:\n\n\n\nShow the code\n\nstream.html\n\nasync function streamResponse() {\n   try {\n      const response = await fetch('/stream');\n      const reader = response.body.getReader();\n      const decoder = new TextDecoder();\n\n      while (true) {\n         const { value, done } = await reader.read();\n         if (done) break;\n\n          const text = decoder.decode(value);\n          const container = document.getElementById('response-container');\n          container.innerText += text;\n      }\n  } catch (error) {\n      console.error('Streaming error:', error);\n  }\n}\n\n// Start streaming when page loads\nstreamResponse();\n\n\n\n\nExplanation of the Client-Side Code\n\nresponse.body.getReader(): This line retrieves a ReadableStreamReader from the response body. The ReadableStream allows us to process incoming data in chunks as they are received, rather than having to wait for the entire response to be downloaded.\nconst { value, done } = await reader.read(): This is the heart of the stream processing. It asynchronously reads the next chunk of data from the stream. The return value is an object that contains two properties:\n\nvalue: This is a Uint8Array (binary data) containing the latest chunk of data.\ndone: A boolean that signals if the stream has completed. If done is true, it indicates that there is no more data to process, and the loop should terminate.\n\nconst text = decoder.decode(value): The binary chunk (value) is converted to a string using TextDecoder, making it usable for display or further processing.\ncontainer.innerText += text: The decoded string is appended to an HTML element with the ID response-container. This provides visible feedback on the incoming data.\n\n\n\nComparison to REST API calls\nIn contrast to this streaming approach, a typical REST API call looks like this:\nconst response = await fetch('/api/data');  // Make the REST API request\nconst data = await response.json(); // Parse the JSON response body\nThe difference is stark. In a REST API call, we wait for the entire response before processing. While with StreamingResponse, we handle each chunk of data as it becomes available. This allows us to display output to the user almost instantly rather than after a long wait.\n\n\n\nA Visual Network Inspection\nTo truly understand what’s happening under the hood with StreamingResponse, let’s take a look at the actual HTTP requests and responses using network inspection tools. We’ll use Chrome DevTools and Wireshark to examine the data being transmitted between the client and the server.\n\n1. Chrome DevTools: Examining HTTP Headers\n\nThe first screenshot is from Chrome’s DevTools, specifically the “Network” tab. We can see the request made to the /stream endpoint. The key thing to notice in the response headers is the Transfer-Encoding: chunked header, highlighted in yellow. This confirms that the server is sending a chunked response, which is essential for streaming. The Content-Type is also set to text/plain, as specified in our server-side code.\n\n\n2. Wireshark: Diving into the Data Packets\n\nThe second screenshot comes from Wireshark, a powerful network protocol analyzer. This tool allows us to inspect the raw packets being transmitted over the network. Here, we can see that multiple HTTP “chunked” responses are being sent from the server to the client as part of a single HTTP connection. This provides a visual confirmation that the server is indeed streaming data in chunks.\n\n\n3. Wireshark: Analyzing Individual Data Chunks\nSixth Data Chunk:\n\nWe’ve opened the sixth data chunk in Wireshark, where we see the chunk has a size of 35 octets. This chunk corresponds to the text ” through FastAPI StreamingResponse.”.\nSeventh Data Chunk:\n\nHere we have the seventh data chunk, indicated by a size of 17 octets. This maps to the message ” Here’s the final”.\nEighth Data Chunk:\n\nIn this screenshot we opened the eighth data chunk. Here the chunk size is 7 octets and it corresponds to the last message ” chunk!“.\nLast Data Chunk:\n\nFinally, we can see the last chunk being sent. It has a size of 0 octets. This zero-size chunk tells the client that the server has finished sending the stream, and it can close the connection."
  },
  {
    "objectID": "posts/2025-01-19-streaming-responses-fastapi.html#method-2-streaming-with-server-sent-events-sse",
    "href": "posts/2025-01-19-streaming-responses-fastapi.html#method-2-streaming-with-server-sent-events-sse",
    "title": "Streaming Responses in FastAPI",
    "section": "Method 2: Streaming with Server-Sent Events (SSE)",
    "text": "Method 2: Streaming with Server-Sent Events (SSE)\nServer-Sent Events (SSE) are another powerful mechanism for pushing data from the server to the client in a stream. While similar in purpose to StreamingResponse, SSE operates at a higher level, utilizing a structured, event-based approach. This method is particularly well-suited for scenarios where the server needs to continuously send updates to the client, such as live notifications or updates to a real-time dashboard.\n\nKey Features of SSE\n\nUnidirectional: SSE is a one-way communication channel. The server pushes data to the client, but the client cannot send data back to the server over the same connection. (For bidirectional communication, we would need to use WebSockets, which we’ll discuss later).\nText-Based: SSE is a text-based protocol. Data is formatted as simple text events, which are easy to parse on the client side.\nAutomatic Reconnection: If the connection between the server and client is interrupted, the client will automatically try to reconnect to the server after a short delay. This is a key benefit of using SSE because it makes sure your connection is always live.\n\n\n\nServer-Side Implementation (FastAPI)\nHere’s how we can implement SSE with FastAPI, using our existing event_generator:\n\n\n\nShow the code\n\napp_ess.py\n\nfrom fastapi import FastAPI\nfrom sse_starlette.sse import EventSourceResponse\nimport asyncio\n\n# Test messages (same as before)\nMESSAGES = [\n    \"This is\",\n    \" a large\",\n    \" response\",\n    \" being\",\n    \" streamed\",\n    \" through FastAPI SSE.\",\n    \" Here's the final\",\n    \" chunk!\",\n]\n\nasync def event_generator():\n    for message in MESSAGES:\n        yield {\"data\": message}\n        await asyncio.sleep(1)  # Simulate an async delay\n\napp = FastAPI()\n\n@app.get(\"/stream\")\nasync def stream_response():\n    return EventSourceResponse(event_generator())\n\n\n\nAs you can see, it’s incredibly straightforward. Instead of StreamingResponse, we use EventSourceResponse from the sse-starlette package (which you’ll need to install). EventSourceResponse takes the asynchronous generator (which produces a dictionary) and formats it as SSE messages that can be consumed by the browser. Note that in order to send raw text we have to format the yield as {\"data\": \"your_text\"}\n\n\nClient-Side Implementation (JavaScript)\nOn the client side, connecting to an SSE stream is also remarkably simple:\n\n\n\nShow the code\n\ness.html\n\nconst eventSource = new EventSource('/stream');\n\neventSource.onmessage = function (event) {\n  const container = document.getElementById('response-container');\n  container.innerText += event.data;\n};\n\n\n\n\nExplanation of the Client-Side Code\n\nconst eventSource = new EventSource(‘/stream’): This line establishes the connection to the SSE endpoint. When the connection is successful, the client begins to receive server-sent events.\neventSource.onmessage = function (event) { … }: This defines a callback function that is executed when a message is received from the server.\n\nThe event object contains the data that the server sent in event.data .\nWe simply append the event.data to an HTML element with the ID response-container.\n\n\n\n\nComparison with StreamingResponse\n\nStructure: StreamingResponse provides raw data that the client must interpret. SSE provides a structured format with an event-driven approach through which you can have multiple event types.\nProtocol Overhead: SSE has slightly more protocol overhead than StreamingResponse due to the additional text-based formatting and event structure.\nClient-Side Ease: The client-side API for SSE is cleaner since the event handling is baked in (EventSource API).\nUse Cases: StreamingResponse might be better if you are trying to stream very large files because it has less overhead. SSE shines in cases where the messages are structured, which allows clients to act on different event types.\n\n\n\n\nDiving Deeper: SSE Specification and Message Fields\nTo gain an even more detailed understanding of Server-Sent Events, let’s refer to the official documentation. The Mozilla Developer Network (MDN) page on Using server-sent events provides a comprehensive overview of the SSE specification and its various features.\nIn particular, the MDN documentation highlights that SSE messages can include various fields, not just the data field we used in our initial example. These fields allow for richer event structures, enabling the client to handle different kinds of events or provide additional context. Here’s a breakdown of the key fields as defined by the SSE specification:\n\ndata: This is the core field containing the event data. It can be any text that is sent from the server. Each message can have multiple data fields, and they will be concatenated together.\nevent: The event field can be used to specify the event type. This allows the client to use a specific callback handler to react to different events originating from the same SSE stream. If no event type is specified, the client uses the default onmessage callback.\nid: The id field sets the event ID. This is crucial for client-side error handling and reconnection. When a connection is interrupted and the client reconnects, the client will include the last seen event ID in the Last-Event-ID header. The server can use this information to determine where to resume the stream.\nretry: The retry field specifies the amount of time (in milliseconds) that the client should wait before trying to reconnect if the connection is lost. This allows the server to control the client-side reconnection behavior.\n\nThese additional fields allow you to build highly structured and robust applications. The server can now send messages such as:\nid: 12345\nevent: user-logged-in\nretry: 10000\ndata: John Doe\ndata: User ID : 123\n\nid: 12346\nevent: message\ndata: hello world\nIn the above message stream we are using id, event, retry and multiple data fields. You can also format the message with data in one line, like the last message example.\nBy understanding these additional message fields, you can design more sophisticated real-time applications leveraging the flexibility of Server-Sent Events.\n\n\nA Visual Network Inspection\nTo fully grasp how Server-Sent Events operate, let’s examine the HTTP requests and responses using network inspection tools. Just as with StreamingResponse, visualizing the data flow helps clarify what’s happening behind the scenes. We’ll again use Chrome DevTools and Wireshark to see the structure and details of the transmitted data.\n\n1. Chrome DevTools: Examining HTTP Headers\n\nThe first screenshot shows the “Network” tab in Chrome DevTools for our SSE endpoint. Here, you can see the Content-Type header is set to text/event-stream, which is the critical header indicating that we are dealing with an SSE stream. This is different from the text/plain we saw when using the StreamingResponse. The Transfer-Encoding is also set to chunked which signals that the content is being sent in parts.\n\n\n2. Chrome DevTools: EventStream Tab\n\nOne of the nice things about Chrome is that when it receives text/event-stream it provides a separate “EventStream” tab to display the events in a more readable format. This view helps in tracing the exact sequence of events sent from the server. Each event includes a timestamp and message data, making debugging straightforward. Here we can also see that each of the messages has a message type of message.\n\n\n3. Chrome DevTools: Response Tab\n\nIn this view we can inspect the raw HTTP response that was received from the server. In this view each of the messages that we received are in a separate line. Each of these lines have the prefix of data.\n\n\n4. Wireshark: Diving into the Data Packets\n\nThis Wireshark screenshot shows the raw packets for the SSE stream. We see the familiar chunked transfer encoding. Notice that, like StreamingResponse, SSE also sends data in chunks. The final chunk of size 0 signals the end of the stream, and all the messages from the server have been received."
  },
  {
    "objectID": "posts/2025-01-19-streaming-responses-fastapi.html#method-3-bidirectional-communication-with-websockets",
    "href": "posts/2025-01-19-streaming-responses-fastapi.html#method-3-bidirectional-communication-with-websockets",
    "title": "Streaming Responses in FastAPI",
    "section": "Method 3: Bidirectional Communication with WebSockets",
    "text": "Method 3: Bidirectional Communication with WebSockets\nWhile StreamingResponse and Server-Sent Events (SSE) are great for sending data from the server to the client, they don’t offer a way for the client to send data back to the server. This is where WebSockets come in. WebSockets provide a persistent, full-duplex communication channel that allows both the server and the client to send and receive data simultaneously over a single TCP connection. This makes WebSockets ideal for real-time applications like chat, live collaboration, and interactive gaming.\n\nKey Features of WebSockets\n\nBidirectional: Unlike SSE, WebSockets enable two-way communication. Both client and server can send messages to each other at any time.\nFull-Duplex: Communication can occur in both directions simultaneously, unlike half-duplex or simplex communication methods.\nPersistent Connection: A WebSocket connection remains open until either the client or server explicitly closes it. This persistent connection eliminates the need for repeated connection establishment, making real-time interaction more efficient.\nLow Overhead: WebSockets provide a lighter-weight communication mechanism compared to repeatedly creating HTTP requests.\n\n\n\nServer-Side Implementation (FastAPI)\nHere’s how to implement a WebSocket endpoint with FastAPI:\n\n\n\nShow the code\n\napp_websocket.py\n\nfrom fastapi import FastAPI, WebSocket\nimport asyncio\n\n# Test messages (same as before)\nMESSAGES = [\n    \"This is\",\n    \" a large\",\n    \" response\",\n    \" being\",\n    \" streamed\",\n    \" through FastAPI WebSocket.\",\n    \" Here's the final\",\n    \" chunk!\",\n]\n\napp = FastAPI()\n\n@app.websocket(\"/ws\")\nasync def websocket_endpoint(websocket: WebSocket):\n    await websocket.accept()\n    try:\n        # Send initial messages\n        for message in MESSAGES:\n            await websocket.send_text(message)\n            await asyncio.sleep(1)\n\n        # Keep connection alive and handle incoming messages\n        while True:\n            try:\n                data = await asyncio.wait_for(\n                    websocket.receive_text(), timeout=60  # 60 second timeout\n                )\n                await websocket.send_text(f\"Response: {data}\")\n            except asyncio.TimeoutError:\n                await websocket.close()\n                break\n\n    except Exception as e:\n        print(f\"Error: {e}\")\n    finally:\n        await websocket.close()\n\n\n\n\nExplanation of the Server-Side Code\n\n@app.websocket(“/ws”): This decorator defines the WebSocket endpoint at the /ws path.\nWebsocket: FastAPI automatically provides a WebSocket object to handle the connection.\nawait websocket.accept(): This line accepts the incoming WebSocket connection, making it active.\nInitial Message Sending: The code first loops through MESSAGES and sends each one to the client as text using await websocket.send_text(message). The await asyncio.sleep(1) simulates a delay between sending messages.\nHandling Incoming Messages: The server waits for new messages from the client with a 60-second timeout, and then it sends a response back. If there is no message received within 60 seconds, it closes the connection.\n\n\n\n\nClient-Side Implementation (JavaScript)\nHere’s the corresponding JavaScript code to connect to the WebSocket endpoint and send messages:\n\n\n\nShow the code\n\nwsocket.html\n\nlet ws;\n\nfunction connectWebSocket() {\n    // Create WebSocket connection\n    ws = new WebSocket(`ws://${window.location.host}/ws`);\n\n    ws.onmessage = function (event) {\n        const container = document.getElementById('response-container');\n        // Add new line for better readability\n        container.innerHTML += event.data + '&lt;br&gt;';\n        // Auto scroll to bottom\n        container.scrollTop = container.scrollHeight;\n    };\n\n    ws.onclose = function (event) {\n        console.log('WebSocket connection closed:', event);\n        // Optionally show connection status to user\n        const container = document.getElementById('response-container');\n        container.innerHTML += '&lt;br&gt;Connection closed&lt;br&gt;';\n    };\n\n    ws.onerror = function (error) {\n        console.error('WebSocket error:', error);\n        // Optionally show error to user\n        const container = document.getElementById('response-container');\n        container.innerHTML += '&lt;br&gt;Error occurred&lt;br&gt;';\n    };\n\n    ws.onopen = function (event) {\n        console.log('WebSocket connection established');\n        // Optionally show connection status to user\n        const container = document.getElementById('response-container');\n        container.innerHTML += 'Connected to server&lt;br&gt;';\n    };\n}\n\nfunction sendMessage() {\n    const input = document.getElementById('messageInput');\n    if (input.value && ws.readyState === WebSocket.OPEN) {\n        ws.send(input.value);\n        // Optionally show sent message\n        const container = document.getElementById('response-container');\n        container.innerHTML += `Sent: ${input.value}&lt;br&gt;`;\n        input.value = '';\n    }\n}\n\n// Add event listener for Enter key\ndocument.getElementById('messageInput').addEventListener('keypress', function (e) {\n    if (e.key === 'Enter') {\n        sendMessage();\n    }\n});\n\n// Connect when page loads\nconnectWebSocket();\n\n\n\n\nExplanation of the Client-Side Code\n\nws = new WebSocket(…): Creates a new WebSocket connection to the specified server endpoint.\nws.onmessage: This event handler is called when the client receives a message from the server.\nws.onclose: This event handler is called when the connection is closed either by the server or client.\nws.onerror: This event handler is called when there is an error in WebSocket communication.\nws.onopen: This event handler is called when connection is successfully created\nsendMessage(): This function sends text that is retrieved from the user from a message input box to the server.\nconnectWebSocket(): Establishes a connection when page loads.\n\n\n\n\nComparison with SSE and StreamingResponse\n\nBidirectional Communication: The primary advantage of WebSockets is the ability for both server and client to send data at will. SSE and StreamingResponse are only for server to client communication.\nComplexity: WebSockets are generally more complex to implement than SSE, as they require more code for handling connection establishment, message processing, and connection closing.\nUse Cases: WebSockets are essential for highly interactive, real-time applications. SSE is sufficient for applications where the server primarily sends updates to the client. StreamingResponse is most suited for cases where raw data or large files are sent from server to client.\n\n\n\nFurther Learning: WebSocket API Reference\nTo deepen your understanding of WebSockets and explore their full capabilities, consult Mozilla Developer Network (MDN) page on the WebSocket API which provides a comprehensive and detailed reference for all aspects of the WebSocket interface.\n\n\nA Visual Network Inspection\nTo understand the real-time communication capabilities of WebSockets, let’s examine how connections are established and messages are exchanged. We’ll use Chrome DevTools and Wireshark to visualize these interactions, gaining insights into the underlying protocol.\n\n1. Chrome DevTools: WebSocket Connection Handshake\n\n\n\n\n\n\n\n\n\n\nThe first two screenshots are from the “Network” tab in Chrome DevTools. They display the initial HTTP GET request made to the /ws endpoint. Observe the following key aspects:\n\nGET Request: The client initiates the connection using a standard HTTP GET request to the /ws endpoint.\nUpgrade and Connection Headers: The request headers include Upgrade: websocket and Connection: Upgrade headers, which instructs the server to switch to the websocket protocol.\nSec-WebSocket-Key and Sec-WebSocket-Version: The request includes a Sec-WebSocket-Key (a random key for security) and Sec-WebSocket-Version (indicating the WebSocket protocol version) headers.\n101 Switching Protocols: The server’s response header indicates a status code of “101 Switching Protocols”. This signals the server’s agreement to upgrade the connection from HTTP to the WebSocket protocol.\nSec-WebSocket-Accept: The server’s response header includes a Sec-WebSocket-Accept, which is derived from the Sec-WebSocket-Key. This confirms that the server has acknowledged the handshake.\n\n\n\n2. Wireshark: Initial Protocol Upgrade\n\nThis Wireshark screenshot shows the raw packets of the initial handshake. Like Chrome, Wireshark also shows that the connection begins with an HTTP request where client requests to switch to the Websocket Protocol. The Server also responds with the status code 101 switching protocols.\n\n\n3. Wireshark: Server-to-Client Messages\nFirst Message: \nThis image shows the first message sent from the server after WebSocket connection. We can see that the server sends “This is” as the first message. The text payload is also displayed in the lower section of the image.\nSecond Message: \nThis image displays the server’s second message to the client which is “a large”.\nSeventh Message: \nThis image shows the server’s message “Here’s the final”.\nEighth Message: \nThis image shows the last message from the server “chunk!”.\n\n\n4. Wireshark: Bidirectional Communication\n\nThis Wireshark screenshot illustrates bidirectional communication. The server echoed “hello” as a response to the client sending a message. The message that is send by client is also shown. Note that Wireshark marks client sent message with [MASKED], which is part of the websocket protocol to prevent proxy attacks.\n\n\n5. Wireshark: Ping/Pong & Connection Close\n\n\n\n\n\n\n\n\n\n\nThe last set of images showcase some of the features of websocket protocol. We can see that the server is sending ping messages to the client and the client is responding back with the pong message. These messages keep the connection alive and also help in detecting connection issues. In the end the server also sends the [FIN] which closes the connection and the client acknowledges it by also sending a [FIN] message."
  },
  {
    "objectID": "posts/2025-01-19-streaming-responses-fastapi.html#conclusion-and-thanks",
    "href": "posts/2025-01-19-streaming-responses-fastapi.html#conclusion-and-thanks",
    "title": "Streaming Responses in FastAPI",
    "section": "Conclusion and Thanks",
    "text": "Conclusion and Thanks\nThis post explored three streaming techniques in FastAPI: StreamingResponse, Server-Sent Events (SSE), and WebSockets. We saw that StreamingResponse is excellent for sending large data chunks, SSE is ideal for server-pushed updates, and WebSockets enable bidirectional, real-time interaction. We covered implementation with code examples and visualized communication using network tools. Understanding these methods lets you build more responsive and efficient applications.\nThank you for taking the time to delve into the world of streaming with FastAPI. I hope you found this guide insightful and that it equips you to build even more powerful applications. Feel free to leave your questions or comments below, and happy coding!"
  },
  {
    "objectID": "posts/2023-05-28-aws-data-analytics-specialty-notes.html",
    "href": "posts/2023-05-28-aws-data-analytics-specialty-notes.html",
    "title": "AWS Certified Data Analytics Specialty DAS-C01 Notes",
    "section": "",
    "text": "On May 27, 2023, I passed the AWS certification exam for the Data Analytics Specialty DAS-C01. I am sharing the notes I used to prepare for this exam."
  },
  {
    "objectID": "posts/2023-05-28-aws-data-analytics-specialty-notes.html#kinesis-data-streams",
    "href": "posts/2023-05-28-aws-data-analytics-specialty-notes.html#kinesis-data-streams",
    "title": "AWS Certified Data Analytics Specialty DAS-C01 Notes",
    "section": "Kinesis Data Streams",
    "text": "Kinesis Data Streams\n\nKinesis Data Streams synchronously replicate data across three Availability Zones in an AWS Region. Kinesis Data Streams is unsuited for data persistence or long-term storage. However, data will be retained for 24 hours, and you can extend the retention period by up to 7 days.\nWindowed queries types\n\nStagger window: Stagger window is a windowing method suited for analyzing groups of data that arrive at inconsistent times.\nTumbling window: When a windowed query processes each window in a non-overlapping manner, the window is referred to as a tumbling window.\nSliding window : time-based window\n\nKinesis shards don’t scale automatically and have to be done manually\nKinesis data streams can’t write to S3 or Redshift directly, use Kinesis firehose instead\nSharding a parent into child shards\n\n\nAfter the reshard has occurred and the stream is again in an ACTIVE state, you could immediately begin to read data from the child shards. However, the parent shards that remain after the reshard could still contain data that you haven’t read yet that was added to the stream before the reshard. If you read data from the child shards before reading all data from the parent shards, you could read data for a particular hash key out of order given by the data records’ sequence numbers. Therefore, assuming that the data order is essential, you should, after a reshard, always continue to read data from the parent shards until it is exhausted. Only then should you begin reading data from the child shards.\n\nEnhanced fan-out is an Amazon Kinesis Data Streams feature that enables consumers to receive records from a data stream with a dedicated throughput of up to 2 MB of data per second per shard. Without the Fanout feature, 2MB read throughput is shared between all consumers of a shard.\nYou can integrate your Kinesis data streams with the AWS Glue schema registry. The AWS Glue schema registry allows you to centrally discover, control, and evolve schemas while ensuring data produced is continuously validated by a registered schema. A schema defines the structure and format of a data record. A schema is a versioned specification for reliable data publication, consumption, or storage.\n\nCurrently, Kinesis Data Streams and AWS Glue schema registry integration is only supported for the Kinesis data streams that use KPL producers implemented in Java. Multi-language support is not provided.\n\nTo manually update the shard count, use ’ UpdateShardCount’ API"
  },
  {
    "objectID": "posts/2023-05-28-aws-data-analytics-specialty-notes.html#amazon-kinesis-data-analytics",
    "href": "posts/2023-05-28-aws-data-analytics-specialty-notes.html#amazon-kinesis-data-analytics",
    "title": "AWS Certified Data Analytics Specialty DAS-C01 Notes",
    "section": "Amazon Kinesis Data Analytics",
    "text": "Amazon Kinesis Data Analytics\n\nAmazon Kinesis Data Analytics is the easiest way to transform and analyze streaming data in real-time with Apache Flink. Apache Flink is an open-source framework and engine for processing data streams. Kinesis Data Analytics reduces the complexity of building, managing, and integrating Apache Flink applications with other AWS services\n\nKinesis Data Analytics takes care of everything required to run streaming applications continuously and scales automatically to match the volume and throughput of your incoming data. With Kinesis Data Analytics, there are no servers to manage, no minimum fee or setup cost, and you only pay for the resources your streaming applications consume.\n\nAn Amazon Kinesis Data Analytics application can receive input from a single streaming source (e.g., Kinesis Data Streams) and, optionally, use one reference data source to enrich the data coming in from streaming sources. You must store reference data as an object in your Amazon S3 bucket. When the application starts, Amazon Kinesis Data Analytics reads the Amazon S3 object and creates an in-application reference table. Your application code can then join it with an in-application stream.\n\nKinesis Data Analytics creates an in-application reference table and then loads the reference data immediately. Suppose that you want to refresh the data after Kinesis Data Analytics creates the in-application reference table, you can explicitly call the ‘UpdateApplication’ API."
  },
  {
    "objectID": "posts/2023-05-28-aws-data-analytics-specialty-notes.html#aws-kinesis-data-firehose",
    "href": "posts/2023-05-28-aws-data-analytics-specialty-notes.html#aws-kinesis-data-firehose",
    "title": "AWS Certified Data Analytics Specialty DAS-C01 Notes",
    "section": "AWS Kinesis Data Firehose",
    "text": "AWS Kinesis Data Firehose\n\nAmazon Kinesis Data Firehose captures, transforms, and loads streaming data from sources such as a Kinesis data stream, the Kinesis Agent, or Amazon CloudWatch Logs into downstream services such as Kinesis Data Analytics or Amazon S3. You can write Lambda functions to request additional, customized processing of the data before it is sent downstream. AWS Lambda can perform data enrichment like looking up data from a DynamoDB table, and then produce the enriched data onto another stream. Lambda is commonly used for preprocessing the analytics app to handle more complicated data formats.\nFirehose automatically takes care of shards resizing\nFirehose is not real time. It is near real time. It buffers messages for a minute before sending them to destination. This service cannot be used for systems that has a real time processing requirement"
  },
  {
    "objectID": "posts/2023-05-28-aws-data-analytics-specialty-notes.html#kinesis-producer-library",
    "href": "posts/2023-05-28-aws-data-analytics-specialty-notes.html#kinesis-producer-library",
    "title": "AWS Certified Data Analytics Specialty DAS-C01 Notes",
    "section": "Kinesis Producer Library",
    "text": "Kinesis Producer Library\n\nThe KPL PutRecords operation often sends multiple records to the stream per request. If a single record fails, it is automatically added to the KPL buffer and retried. The failure of one record does not impact the processing of other records in the request.\nWays to develop Kinesis producers\n\nKPL\nKinesis agent\nKinesis streams API"
  },
  {
    "objectID": "posts/2023-05-28-aws-data-analytics-specialty-notes.html#kinesis-consumer-library",
    "href": "posts/2023-05-28-aws-data-analytics-specialty-notes.html#kinesis-consumer-library",
    "title": "AWS Certified Data Analytics Specialty DAS-C01 Notes",
    "section": "Kinesis Consumer Library",
    "text": "Kinesis Consumer Library\n\nThe KCL consumes and processes the data from a Kinesis data stream. The KCL also offers checkpointing, which means it stores a cursor in DynamoDB to track the records that have been read from a shard in a Kinesis stream. If a KCL worker fails while reading data from the stream, KCL can use that cursor and pick up from where the failed application left off.\n\nTwo essential things to remember:\n\n\n\nSince KCL creates DynamoDB tables on your behalf, it’s essential to use unique application names in KCL for each application.\nYou may run into provisioned throughput exception errors with DynamoDB if your stream has too many shards or if the application does frequent checkpointing.\n\n\n\nEach shard can support up to 5 transactions per second for reads, up to a maximum total data read rate of 2 MB per second, and up to 1,000 records per second for writes, up to a maximum total data write rate of 1 MB per second (including partition keys). The total capacity of the stream is the sum of the capacities of its shards."
  },
  {
    "objectID": "posts/2023-05-28-aws-data-analytics-specialty-notes.html#amazon-redshift",
    "href": "posts/2023-05-28-aws-data-analytics-specialty-notes.html#amazon-redshift",
    "title": "AWS Certified Data Analytics Specialty DAS-C01 Notes",
    "section": "Amazon Redshift",
    "text": "Amazon Redshift\n\nUse the UNLOAD command to move data from Redshift to S3. This command can also be used to move Redshift query results to S3\nUse the COPY command to move data from S3 to Redshift\nAmazon Redshift can automatically sort and perform a VACUUM DELETE operation on tables in the background.\n\nTo clean up tables after a load or a series of incremental updates, you can also run the VACUUM command, either against the entire database or against individual tables.\nAmazon Redshift automatically sorts data in the background to maintain table data in the order of its sort key. Amazon Redshift keeps track of your scan queries to determine which sections of the table will benefit from sorting.\nBy default, VACUUM skips the sort phase for any table where more than 95 percent of the table’s rows are already sorted. Skipping the sort phase can significantly improve VACUUM performance.\n\nRelated to incremental data loading into Redshift: There can be a scenario when data loaded into Redshift using a Glue job results in duplicate rows. To avoid this, first use the Glue job to create a staging table. Load all the data in the staging table. Now make a join between the staging table and Redshift table and perform the UPSERT operation.\nDISTSTYLE Distribution Key\n\nDISTSTYLE defines the data distribution style for the whole table. Amazon Redshift distributes the rows of a table to the compute nodes according to the distribution style specified for the table. The distribution style you select for tables affects the overall performance of your database.\nALL\n\nFull replication on all nodes.\nNot suitable for small tables\nGood for slow-changing tables (less frequently updated)\n\nEVEN\n\nData (rows) evenly distributed on all nodes. Each node has a small portion\nGood for tables NOT used in joins\nUse when not sure about KEY and ALL\n\nKEY\n\nData (rows) distributed but similar data (based on a specified column) kept together on nodes\nGood for tables used in joins\n\n\nIf the workload require high I/O performance then use dense computer nodes (DC2)\nAmazon Redshift workload management (WLM) queues enable users to flexibly manage priorities within workloads so that short, fast-running queries won’t get stuck in queues behind long-running queries. You can use workload management (WLM) to define multiple query queues and to route queries to the appropriate queues at runtime.\nTo encrypt data on Redshift storage nodes you can use AWS KMS or HMS (hardware security module). Redshift can automatically connect with AWS KMS but not with HMS. For HMS we must use client and server certification to establish a trusted connection between them. We cannot enable encryption by modifying an existing cluster. For this we have to create a new cluster and migrate data to it.\nTo create an external table in Redshift (i.e. a table with data on S3) use command “CREATE EXTERNAL TABLE”\nTo increase the COPY command data transfer processing power, make sure that the files are multiple of the number of slices in the redshift cluster. Optimal file size is between 100MB and 1GB\nif your data has a fixed retention period, you can organize your data as a sequence of time-series tables. In such a sequence, each table is identical but contains data for different time ranges. You can easily remove old data simply by running a DROP TABLE command on the corresponding tables. This approach is much faster than running a large-scale DELETE process and saves you from having to run a subsequent VACUUM process to reclaim space. Time series tables are best for IoT devices data in Redshift. This is very much similar to the MySql table with partitions applied on the datetime column.\nAmazon Redshift is more cost-effective than Amazon Athena for frequently accessed reports i.e. the reports that are frequently accessed throughout the day\nYou can take automatic snapshots of Redshift cluster BUT it takes a snapshot every 8 hours. For more frequent, use manul snapshots."
  },
  {
    "objectID": "posts/2023-05-28-aws-data-analytics-specialty-notes.html#aws-glue",
    "href": "posts/2023-05-28-aws-data-analytics-specialty-notes.html#aws-glue",
    "title": "AWS Certified Data Analytics Specialty DAS-C01 Notes",
    "section": "AWS Glue",
    "text": "AWS Glue\n\nGlue uses DPU (Data Processing Unit) as a relative measure of processing power\nTo improve performance, you can enable job metrics to estimate the number of DPUs to scale out Glue job\nGlue bookmarks from maintaining their state and prevent Glue from reprocessing old data. They are helpful in incremental data processing. Job bookmarks help AWS Glue maintain state information and prevent the reprocessing of old data. With job bookmarks, you can process new data when rerunning on a scheduled interval. The job bookmark implementation for the Amazon S3 source saves information so that when the job runs again, it can filter only the new objects using the saved information and recompute the state for the next job run. A timestamp is used to filter the new files.\nGlue also supports PySpark Scala dialect to write ETL scripts\nAWS Glue can be used to crawl and catalog data stored in S3 in different regions. Once cataloged, Athena can query the data from S3 in different regions\nAWS Glue does not directly support Apache Hive\nA classifier (used by Glue Crawler) reads the data in a data store. If it recognizes the format of the data, it generates a schema. The classifier also returns a certainty number to indicate how certain the format recognition was.\n\nAWS Glue provides a set of built-in classifiers, but you can also create custom classifiers. AWS Glue invokes custom classifiers first, in the order that you specify in your crawler definition. Depending on the results that are returned from custom classifiers, AWS Glue might also invoke built-in classifiers. If a classifier returns certainty=1.0 during processing, it indicates that it’s 100 percent certain that it can create the correct schema. AWS Glue then uses the output of that classifier.\nIf no classifier returns certainty=1.0, AWS Glue uses the output of the classifier that has the highest certainty. If no classifier returns a certainty greater than 0.0, AWS Glue returns the default classification string of UNKNOWN.\n\nGlue PySpark transformation Relationize: Flattens a nested schema in a DynamicFrame and pivots out array columns from the flattened frame.\nSome of your organization’s complex extract, transform, and load (ETL) processes might best be implemented by using multiple, dependent AWS Glue jobs and crawlers. Using AWS Glue workflows , you can design a complex multi-job, multi-crawler ETL process that AWS Glue can run and track as a single entity. After you create a workflow and specify the jobs, crawlers, and triggers in the workflow, you can run the workflow on demand or on a schedule."
  },
  {
    "objectID": "posts/2023-05-28-aws-data-analytics-specialty-notes.html#aws-iam",
    "href": "posts/2023-05-28-aws-data-analytics-specialty-notes.html#aws-iam",
    "title": "AWS Certified Data Analytics Specialty DAS-C01 Notes",
    "section": "AWS IAM",
    "text": "AWS IAM\n\nA resource policy is a policy that is attached to a resource rather than to an IAM identity. For example, in Amazon Simple Storage Service (Amazon S3), a resource policy is attached to an Amazon S3 bucket. AWS Glue supports using resource policies to control access to Data Catalog resources. These resources include databases, tables, connections, and user-defined functions, along with the Data Catalog APIs that interact with these resources."
  },
  {
    "objectID": "posts/2023-05-28-aws-data-analytics-specialty-notes.html#aws-athena",
    "href": "posts/2023-05-28-aws-data-analytics-specialty-notes.html#aws-athena",
    "title": "AWS Certified Data Analytics Specialty DAS-C01 Notes",
    "section": "AWS Athena",
    "text": "AWS Athena\n\nImportant AWS Blog Post: Top 10 Performance Tuning Tips for Amazon Athena\nWe can connect to Amazon Athena using ODBC and JDBC drivers. This allows you to report and visualize all of your data in S3 with the tools of your choice.\nAthena cannot update the data. It can only perform “insert” and “select” operations. Athena writes files to source data locations in Amazon S3 as a result of the INSERT command. Each INSERT operation creates a new file, rather than appending to an existing file.\nAmazon Athena allows you to set two types of cost controls:\n\nper-query limit\nper-workgroup limit (a.k.a workgroup-wide data usage control limit)\n\nWorkgroups allow you to set data usage control limits per query or per workgroup, set up alarms when those limits are exceeded, and publish query metrics to CloudWatch. The per-query control limit specifies the total amount of data scanned per query. If any query that runs in the workgroup exceeds the limit, it is canceled.\n\n\nAthena can only query the latest version of data on a versioned Amazon S3 bucket and cannot query previous versions of the data.\nAthena supports querying objects that are stored with multiple storage classes in the same bucket specified by the LOCATION clause. For example, you can query data in objects that are stored in different Storage classes (Standard, Standard-IA and Intelligent-Tiering) in Amazon S3. Athena does not support querying the data in the S3 Glacier.\nSuppose I have an Athena table with data on S3. I can query the table and it returns the results. Suppose that I partition the data on S3, then if I query the table again it will not return any data. This is because partitions information is not updated in Glue catalog. To load the information use query ‘MSCK REPAIR TABLE’"
  },
  {
    "objectID": "posts/2023-05-28-aws-data-analytics-specialty-notes.html#aws-emr",
    "href": "posts/2023-05-28-aws-data-analytics-specialty-notes.html#aws-emr",
    "title": "AWS Certified Data Analytics Specialty DAS-C01 Notes",
    "section": "AWS EMR",
    "text": "AWS EMR\n\nEMR File System (EMRFS) extends Hadoop to directly access data stored in S3 as if it is a file system\nAmazon EMR supports multiple master nodes to enable high availability for EMR applications. Launch an EMR cluster with three master nodes and support high availability. EMR clusters with multiple master nodes are not tolerant of Availability Zone failures. In the case of an Availability Zone outage, you lose access to the EMR cluster. Using the Amazon EMR version 5.7.0 or later, you can set up a read-replica cluster in a different availability zone, which allows you to maintain read-only copies of data in Amazon S3. If the primary cluster becomes unavailable, you can access the data from the read-replica cluster to perform read operations\nEMR cluster scaling is done in two ways:\n\nFor older versions: Use “automatic scaling” with a custom policy\n\nTwo commonly used metrics for “automatic scaling”:\n\nYarnMemoryAvailablePercentage : This is the percentage of remaining memory available for YARN.\nContainerPendingRatio\n\n\nFor the latest versions: Use “Amazon EMR-managed scaling”. Or, use automatic scaling with a custom policy\nThe difference between the two options is that in automatic scaling you have to choose the metric for scaling control, whereas in managed scaling, this is done automatically\n\nResize of the cluster is done in two ways\n\nElastic resize : this is the fastest method and does not create a new cluster. While resizing, the cluster is put into read-only mode. Resizing takes around 10-15 mins\nClassic resize : this creates a new cluster. The old cluster is put into read-only mode till the new cluster is ready, and all the data is transferred to it. It can take hours to resize.\n\nEMR cluster can be created using RunJobFlow API call (e.g. from a Lambda function)\nAudit logs for an EMR cluster are enabled by default, and are stored on the master node. EMR also has the option to store these logs in an S3 bucket. To encrypt the logs use AWS KMS. EMR can automatically transfer logs to S3. This is a builtin functionality, and we don’t need Lambda or some other service to transfer logs from EMS master nodes to S3\nBy default, all security configurations are disabled. You must re-create the cluster and enable them.\nEMR on Spot Instances can significantly reduce the computing costs for time-flexible, interruption-tolerant tasks. Always use on-demand instances for core nodes and spot instances for task nodes. In case of SLAs compliance, use “Provisioned Timeout” for spot instances.\n\nFor each fleet, you can define a provisioning timeout. The timeout applies when the cluster is provisioning capacity and does not have enough Spot Instances to fulfill the target capacity according to the provided specifications. With the provisioning timeout, you can specify the timeout period and choose to switch to On-Demand capacity to fulfill the remaining Spot capacity and comply with the SLA.\n\nUsers often forget to terminate the clusters after their work is done. This leads to the idle running of the clusters and in turn, adds up to unnecessary costs. To avoid this overhead, you must track the idleness of the EMR cluster and terminate it if it is running idle for long hours. There is the Amazon EMR native IsIdle Amazon CloudWatch metric, which determines the idleness of the cluster by checking whether there’s a YARN job running.\nBlock Public Access configuration is an account-level configuration that helps you centrally manage public network access to EMR clusters in a region. You can enable this configuration in a region and block your account users from launching EMR clusters that allow unrestricted inbound traffic from the public IP address (source set to 0.0.0.0/0 for IPv4 and ::/0 for IPv6) through its ports.\n\nIf your applications require specific ports to be open to the Internet, you can configure these ports (or port ranges) in the “block public access” configuration as exceptions to allow public access before you launch the clusters. Port 22 is an exception by default. You can configure exceptions to allow public access on a port or range of ports. In addition, you can enable or disable block public access. It is recommended that you enable it.\n\n\nA transient EMR cluster is designed to terminate as soon as the job is complete or if any error occurs. A transient cluster provides cost savings because it runs only during the computation time, and it provides scalability and flexibility in a cloud environment\nNode Type – Each node has a role within the cluster, referred to as the node type. The node types are:\n\nMaster node : A node that manages the cluster by running software components to coordinate the distribution of data and tasks among other nodes for processing. The master node tracks the status of tasks and monitors the health of the cluster. Every cluster has a master node, and it’s possible to create a single-node cluster with only the master node. Does not support automatic failover.\nCore node : A node with software components that run tasks and store data in the Hadoop Distributed File System (HDFS) on your cluster. Multi-node clusters have at least one core node. EMR is fault tolerant for slave failures and continues job execution if a slave node goes down.\nTask node : A node with software components that only runs tasks and does not store data in HDFS. Task nodes are optional."
  },
  {
    "objectID": "posts/2023-05-28-aws-data-analytics-specialty-notes.html#aws-elasticsearch-opensearch",
    "href": "posts/2023-05-28-aws-data-analytics-specialty-notes.html#aws-elasticsearch-opensearch",
    "title": "AWS Certified Data Analytics Specialty DAS-C01 Notes",
    "section": "AWS ElasticSearch (OpenSearch)",
    "text": "AWS ElasticSearch (OpenSearch)\n\nAmazon OpenSearch lets you search, analyze, and visualize your data in real-time.\nBefore you can search for data, you must index it. Indexing is the method by which search engines organize data for fast retrieval.\nIn Elasticsearch, the basic unit of data is a JSON document."
  },
  {
    "objectID": "posts/2023-05-28-aws-data-analytics-specialty-notes.html#aws-quicksight",
    "href": "posts/2023-05-28-aws-data-analytics-specialty-notes.html#aws-quicksight",
    "title": "AWS Certified Data Analytics Specialty DAS-C01 Notes",
    "section": "AWS QuickSight",
    "text": "AWS QuickSight\n\nFor Quicksight to access Redshift, you need to add inbound rules for the security group of Redshift cluster nodes.\nTo successfully connect Amazon QuickSight to the Amazon S3 buckets used by Athena, make sure that you authorized Amazon QuickSight to access the S3 account.\nData encryption at rest is only available in the enterprise edition. It sports only AWS KMS. Active Directory connector (AD connector) is only available in the enterprise edition.\nQuicksight can’t visualize data in real-time or near real-time. Use OpenSearch and Kibana to achieve this.\nWhen you create a dataset using Amazon S3, the file data is automatically imported into SPICE. Data in SPICE can be updated at scheduled intervals"
  },
  {
    "objectID": "posts/2023-05-28-aws-data-analytics-specialty-notes.html#aws-s3",
    "href": "posts/2023-05-28-aws-data-analytics-specialty-notes.html#aws-s3",
    "title": "AWS Certified Data Analytics Specialty DAS-C01 Notes",
    "section": "AWS S3",
    "text": "AWS S3\n\nS3 bucket throughput (# of read-write requests) is controlled at the prefix level (or folder). To increase the read throughput of data from S3, put the data under multiple prefixes\nBoth Data Migration Service (DMS) and DataSync can be used to migrate data from on-prem to S3. DMS is suitable when migrating data from a database system, and datasync is suitable when migrating data from a file system\n” S3Distcp” Apache DistCp is an open-source tool you can use to copy large amounts of data. S3DistCp is similar to DistCp, but optimized to work with AWS, notably Amazon S3.\n\nUsing S3DistCp, copy data between different S3 buckets\nCopy data from HDFS to S3 and vice versa\nIt cannot be used to move data to Redshift. For this, use COPY command\n\n_Athena can’t query S3 Glacier; you need to use Glacier select. S3 Glacier select cannot select compressed files\nImportant file types"
  },
  {
    "objectID": "posts/2023-05-28-aws-data-analytics-specialty-notes.html#aws-lambda",
    "href": "posts/2023-05-28-aws-data-analytics-specialty-notes.html#aws-lambda",
    "title": "AWS Certified Data Analytics Specialty DAS-C01 Notes",
    "section": "AWS Lambda",
    "text": "AWS Lambda\n\nAWS Lambda now supports Parallelization Factor, a feature that allows you to process one shard of a Kinesis or DynamoDB data stream with more than one Lambda invocation simultaneously. This new feature allows you to build more agile stream processing applications on volatile data traffic.\n\nBy default, Lambda invokes a function with one batch of data records from one shard at a time. For a single event source mapping, the maximum number of concurrent Lambda invocations equals the number of Kinesis or DynamoDB shards.\nNow you can specify the number of concurrent batches that Lambda polls from a shard via a Parallelization Factor from 1 (default) to 10. For example, when Parallelization Factor is set to 2, you can have 200 concurrent Lambda invocations at maximum to process 100 Kinesis data shards. This helps scale up the processing throughput when the data volume is volatile and the IteratorAge is high."
  },
  {
    "objectID": "posts/2023-05-28-aws-data-analytics-specialty-notes.html#aws-iot",
    "href": "posts/2023-05-28-aws-data-analytics-specialty-notes.html#aws-iot",
    "title": "AWS Certified Data Analytics Specialty DAS-C01 Notes",
    "section": "AWS IoT",
    "text": "AWS IoT\n\nAWS IoT provides cloud services that connect your IoT devices to other devices and AWS cloud services. AWS IoT provides device software that can help you integrate your IoT devices into AWS IoT-based solutions. If your devices can connect to AWS IoT, AWS IoT can join them in the AWS cloud services.\n\nAWS IoT Analytics automates the steps required to analyze data from IoT devices. AWS IoT Analytics filters, transforms, and enriches IoT data before storing it in a time-series data store for analysis. You can set up the service to collect only the data you need from your devices, apply mathematical transforms to process the data, and enrich it with device-specific metadata such as device type and location before storing it. You can then analyze your data by running queries using the built-in SQL query engine or perform more complex analytics and machine learning inference."
  },
  {
    "objectID": "posts/2023-05-28-aws-data-analytics-specialty-notes.html#aws-cloudwatch",
    "href": "posts/2023-05-28-aws-data-analytics-specialty-notes.html#aws-cloudwatch",
    "title": "AWS Certified Data Analytics Specialty DAS-C01 Notes",
    "section": "AWS CloudWatch",
    "text": "AWS CloudWatch\n\nEC2 detailed monitoring is not real-time. This is because detailed monitoring can at most send logs to CloudWatch in 1-minute intervals. For real-time logs collection, use KPL. Also note the CloudWatch filter subscription only works on data that is already available in CloudWatch.\nOpenSearch JVMMemoryPressure\n\nJVMMemoryPressure is a metric that indicates how much memory is being used by the Java Virtual Machine (JVM) in OpenSearch. A high JVMMemoryPressure value can indicate that the JVM is running out of memory, which can lead to performance problems and even outages.\nThere are a number of reasons why JVMMemoryPressure can increase, including:\n\nToo many shards: If there are too many shards in your OpenSearch cluster, the JVM will need to allocate more memory to store the data.\nLarge documents: If your documents are large, they will require more memory to store.\nComplex queries: Complex queries can require more memory to execute.\nToo many concurrent requests: If there are too many concurrent requests to your OpenSearch cluster, the JVM will need to allocate more memory to handle the load.\n\n\nKinesis ProvisionedThroughputExceededException\n\nThis error occurs when GetRecords calls are throttled by Kinesis Data Streams over a duration of time. This can happen if you are trying to read data from a stream faster than the provisioned throughput allows.\nThere are a few things you can do to troubleshoot this error:\n\nCheck your provisioned throughput. Make sure that you have enough provisioned throughput for the amount of data you are trying to read. You can increase the provisioned throughput for a stream by using the UpdateShardCount API.\nUse a smaller batch size. The GetRecords API allows you to specify a batch size, which is the number of records that will be returned in each call. If you are getting this error, try using a smaller batch size.\nReduce the frequency of your requests. If you are getting this error, try reducing the frequency of your requests. You can do this by increasing the idle time between reads.\n\n\nKinesis Data Analytics MillisBehindLatest\n\nThe MillisBehindLatest metric indicates how far behind an application is reading from the streaming source. A value of 0 indicates that record processing is caught up, and there are no new records to process at this moment.\nThere are a few reasons why the MillisBehindLatest metric might increase:\n\nIncreased input data rate: If the input data rate increases, the application might not be able to keep up with the rate of data ingestion. This can cause the MillisBehindLatest metric to increase.\nIncreased application latency: If the application latency increases, it can take longer for the application to process records. This can also cause the MillisBehindLatest metric to increase.\nApplication errors: If the application encounters errors, it might not be able to process records. This can also cause the MillisBehindLatest metric to increase.\n\n\nEMR YarnMemoryAvailablePercentage\n\nThe EMR YarnMemoryAvailablePercentage metric indicates the percentage of available memory on the cluster. A value of 100% indicates that all of the memory on the cluster is available. A value of less than 100% indicates that some of the memory on the cluster is being used.\nThere are a few reasons why the EMR YarnMemoryAvailablePercentage metric might decrease:\n\nIncreased memory usage: If the memory usage on the cluster increases, the YarnMemoryAvailablePercentage metric will decrease. This can happen if there are more applications running on the cluster or if the applications are using more memory.\nReduced cluster size: If the cluster size is reduced, the YarnMemoryAvailablePercentage metric will decrease. This can happen if you terminate nodes from the cluster or if you create a new cluster with fewer nodes.\nMemory leaks: If there are memory leaks in the applications running on the cluster, the YarnMemoryAvailablePercentage metric will decrease. This can happen if the applications are not properly releasing memory that they are no longer using."
  },
  {
    "objectID": "posts/2023-05-28-aws-data-analytics-specialty-notes.html#amazon-msk",
    "href": "posts/2023-05-28-aws-data-analytics-specialty-notes.html#amazon-msk",
    "title": "AWS Certified Data Analytics Specialty DAS-C01 Notes",
    "section": "Amazon MSK",
    "text": "Amazon MSK\nHere is a table that summarizes the key differences between Amazon Kinesis and Amazon MSK:\n\n\n\nFeature\nAmazon Kinesis\nAmazon MSK\n\n\n\n\nManaged service\nYes\nNo\n\n\nOpen source\nNo\nYes\n\n\nFlexibility\nLess\nMore\n\n\nScalability\nLess\nMore\n\n\nCommunity\nSmaller\nLarger\n\n\nCost\nLower\nHigher\n\n\n\n\nMaximum size of a payload in Kinesis is 1MB. For streaming objects having a size greater than 1MB consider using MSK"
  },
  {
    "objectID": "posts/2023-04-26-amazon-rekognition-custom-labels.html",
    "href": "posts/2023-04-26-amazon-rekognition-custom-labels.html",
    "title": "Creating an Object Detection Model using Amazon Rekognition Custom Labels",
    "section": "",
    "text": "image source: https://openart.ai/discovery/md-438e164b-250d-4dca-8989-cda9e5dda6ec"
  },
  {
    "objectID": "posts/2023-04-26-amazon-rekognition-custom-labels.html#credits",
    "href": "posts/2023-04-26-amazon-rekognition-custom-labels.html#credits",
    "title": "Creating an Object Detection Model using Amazon Rekognition Custom Labels",
    "section": "Credits",
    "text": "Credits\nThis post takes inspiration from the book Computer Vision on AWS. Chapter 3 of the book dives into Amazon Rekognition and covers many more details than this post. The book used Packt logos as an example for Rekognition Custom Labels. However, I have used the Raspberry Pi logo instead to make it more interesting. To download the Raspberry Pi images from the internet, I have relied on FastAI and Duckduckgo_search libraries."
  },
  {
    "objectID": "posts/2023-04-26-amazon-rekognition-custom-labels.html#introduction",
    "href": "posts/2023-04-26-amazon-rekognition-custom-labels.html#introduction",
    "title": "Creating an Object Detection Model using Amazon Rekognition Custom Labels",
    "section": "Introduction",
    "text": "Introduction\nAmazon Rekognition Custom Labels is a fully managed computer vision service that allows developers to build custom models to classify and identify objects in images that are specific and unique to your business. Rekognition Custom Labels doesn’t require you to have any prior computer vision expertise. For example, you can find your logo in social media posts, identify your products on store shelves, classify machine parts in an assembly line, distinguish healthy and infected plants, or detect animated characters in videos.\nDeveloping a custom model to analyze images is a significant undertaking that requires time, expertise, and resources, often taking months to complete. Additionally, it often requires thousands or tens of thousands of hand-labeled images to provide the model with enough data to accurately make decisions. Generating this data can take months to gather and requires large teams of labelers to prepare it for use in machine learning (ML).\nRekognition Custom Labels builds off of the existing capabilities of Amazon Rekognition, which are already trained on tens of millions of images across many categories. Instead of thousands of images, you simply need to upload a small set of training images (typically a few hundred images or less) that are specific to your use case using the Amazon Rekognition console. If the images are already labeled, you can begin training a model in just a few clicks. If not, you can label them directly on the Rekognition Custom Labels console, or use Amazon SageMaker Ground Truth to label them. Rekognition Custom Labels uses transfer learning and AutoML to automatically inspect the training data, select the right model framework and algorithm, optimize the hyperparameters, and train the model. When you’re satisfied with the model accuracy, you can start hosting the trained model with just one click.\n\n\n\n\n\n\nNote\n\n\n\nThis introduction is adapted from AWS Machine Learning Blog post Announcing the launch of the model copy feature for Amazon Rekognition Custom Labels\n\n\nIn this post, I have explained how to create a custom object detection model using the Reckognition custom labels service. Our goal will be to create a model that can analyze images and locate the Raspberry Pi logo on its boards."
  },
  {
    "objectID": "posts/2023-04-26-amazon-rekognition-custom-labels.html#environment",
    "href": "posts/2023-04-26-amazon-rekognition-custom-labels.html#environment",
    "title": "Creating an Object Detection Model using Amazon Rekognition Custom Labels",
    "section": "Environment",
    "text": "Environment\nThis notebook is created with Amazon SageMaker Studio envionment. Details of the environment are given below\n\nGitHub: 2022-10-10-pytorch-linear-regression.ipynb\nOpen In Colab:"
  },
  {
    "objectID": "posts/2023-04-26-amazon-rekognition-custom-labels.html#step-1-download-and-clean-raspberry-pi-images",
    "href": "posts/2023-04-26-amazon-rekognition-custom-labels.html#step-1-download-and-clean-raspberry-pi-images",
    "title": "Creating an Object Detection Model using Amazon Rekognition Custom Labels",
    "section": "Step 1: Download and clean Raspberry Pi images",
    "text": "Step 1: Download and clean Raspberry Pi images\nWe need to collect relevant images to successfully train a model that can detect Raspberry Pi logos on computer boards. We can use the Google Images search to find such images. But downloading many pictures from Google search can take much work to automate. So instead, we can use a different search engine, DuckDuckGo.com, that provides a more straightforward interface duckduckgo_search for search. For downloading and resizing images, we will use additional libraries from the FastAI ecosystem fastai and fastdownload.\n\n%%writefile requirements.txt\nduckduckgo-search==2.8.6\nfastai==2.7.12 \nfastdownload==0.0.7 \nfastcore==1.5.29\n\nOverwriting requirements.txt\n\n\n\n%%capture\n!pip install -r requirements.txt \n\n# Install required libraries\n# 1. `duckduckgo_search` to search for words, documents, images, news, maps and text translation using the DuckDuckGo.com search engine.\n# 2. `fastdownload` to easily download, verify, and extract archives\n# 3. `fastai` to open, visualize, and transform images\n# 4. `fastcore` extends Python list functionality\n\n\nfrom duckduckgo_search import ddg_images\nfrom fastcore.all import *\n\n# Define a function to search for images using DuckDuckGo.com search engine for the provided term. It returns the URL of the searched image.\n# By default, it will try to find 200 images matching the searched word.\ndef search_images(term, max_images=200):\n    print(f\"Searching for '{term}'\")\n    return L(ddg_images(term, max_results=max_images)).itemgot(\"image\")\n\n\n# Define search term. In our case it is \"raspberry pi board\"\nsearches = {\n    \"pi\": \"raspberry pi board\",\n}\n\n# Test the search function and display URLs returned\nurls = search_images(searches[\"pi\"], max_images=1)\nurls[0]\n\nSearching for 'raspberry pi board'\n\n\n'https://www.watelectronics.com/wp-content/uploads/2019/07/Model-A-Raspberry-Pi-Board.jpg'\n\n\nAbove, we have defined a function that can be used to search images and return their URLs. Next, we can use these URLs to download and save images to a local directory.\n\nfrom pathlib import Path\n\n# Define a local path to store downloaded images\nlocal_dir = \"./assets/2023-04-26-amazon-rekognition-custom-labels/\"\n\n# Create the directory if it does not exist\nlocal_path = Path(local_dir)\nlocal_path.mkdir(parents=True, exist_ok=True)\n\n\nfrom fastai.vision.all import *\nfrom fastdownload import download_url\n\n# `local_path_raw` is the local directory to store raw downloaded images\nlocal_path_raw = Path(f\"{local_path}/raw/\")\ndest = f\"{local_path_raw}/sample-image.jpg\"\n\n# Download a sample Raspberry Pi board image\ndownload_url(urls[0], dest, show_progress=True)\n\n# Display the downloaded image\nim = Image.open(dest)\nim.to_thumb(512, 512)\n\n\n\n\n\n\n    \n      \n      101.17% [524288/518218 00:00&lt;00:00]\n    \n    \n\n\n\n\n\n\n\n\n\nIn the above image, you can see the Raspberry Pi logo in the center of the board. That is our target and we want our model to locate it automatically. This sample image shows that our search string is correct, and we can proceed to download similar images.\n\n# Use each search string to search and download images\nmax_images = 200  # total number of images to search for\n\nfor key, value in searches.items():\n    # create a separate folder for each searched term\n    dest = local_path_raw / key\n    dest.mkdir(exist_ok=True, parents=True)\n\n    # download and store the images for provided searched term\n    download_images(dest, urls=search_images(value, max_images=max_images))\n    resize_images(local_path_raw / key, max_size=800, dest=local_path_raw / key)\n\nSearching for 'raspberry pi board'\n\n\nAt this point, the search and download step is complete. Next, let’s count the number of images successfully downloaded to our local directory.\n\n# Check the count of downloaded images\n(local_path_raw / \"pi\").ls()\n\n(#156) [Path('assets/2023-04-26-amazon-rekognition-custom-labels/raw/pi/e3a144d9-5730-4def-9581-c2cc6f26140a.jpg'),Path('assets/2023-04-26-amazon-rekognition-custom-labels/raw/pi/195d40ea-0a1c-4875-a7eb-6f06fc0855cd.jpg'),Path('assets/2023-04-26-amazon-rekognition-custom-labels/raw/pi/5d10c219-ae38-4cfe-b432-96f951e0bb3f.jpg'),Path('assets/2023-04-26-amazon-rekognition-custom-labels/raw/pi/2d0e1737-a72d-4513-bd74-432fae126891.png'),Path('assets/2023-04-26-amazon-rekognition-custom-labels/raw/pi/338f84b5-e583-4925-b3bb-aebc4bc18bd1.jpg'),Path('assets/2023-04-26-amazon-rekognition-custom-labels/raw/pi/c114204b-b97a-455b-bdcc-0e3ee835ad11.png'),Path('assets/2023-04-26-amazon-rekognition-custom-labels/raw/pi/f3a698c6-25b7-4c5f-b17b-95ade57afee0.jpg'),Path('assets/2023-04-26-amazon-rekognition-custom-labels/raw/pi/2c950136-cb8e-4809-ae0b-f115a00e2f2d.jpg'),Path('assets/2023-04-26-amazon-rekognition-custom-labels/raw/pi/5a574d21-5bbb-4214-b9c2-513e014bf348.jpg'),Path('assets/2023-04-26-amazon-rekognition-custom-labels/raw/pi/c3d979de-648c-4e98-8fe4-3e617e12ec32.jpg')...]\n\n\nWe have downloaded the required images (#156 in total), but there is no guarantee that all our photos contain a Raspberry Pi logo. An image without a proper logo is useless for training. So we need to manually verify all the pictures and remove any that does not meet our requirements. This is a manual step and has to be performed very patiently. After cleaning all the images, I uploaded them under the /clean directory.\n\n# After downloading the images, next step is to manually clean all the images\n# After cleaning, check the count of images\nlocal_path_clean = local_path / \"clean\"\nlocal_path_clean.ls()\n\n(#112) [Path('assets/2023-04-26-amazon-rekognition-custom-labels/clean/d14e3413-0c2f-4a8d-9361-e842633a7fb6.jpg'),Path('assets/2023-04-26-amazon-rekognition-custom-labels/clean/1a623864-2b33-4bce-98e9-5cbc2551afae.jpg'),Path('assets/2023-04-26-amazon-rekognition-custom-labels/clean/228d560d-ab50-4bf5-8a3b-510dcdf89e9a.jpg'),Path('assets/2023-04-26-amazon-rekognition-custom-labels/clean/babf3bb6-fa72-4a53-989d-a34326422e19.jpg'),Path('assets/2023-04-26-amazon-rekognition-custom-labels/clean/95660e55-1566-47cb-912c-2c683b790dcd.jpg'),Path('assets/2023-04-26-amazon-rekognition-custom-labels/clean/9ee061b2-c492-4691-9557-4b848cd16f10.jpg'),Path('assets/2023-04-26-amazon-rekognition-custom-labels/clean/5e0490d6-646d-470e-8257-f3c6742c3a48.jpg'),Path('assets/2023-04-26-amazon-rekognition-custom-labels/clean/a3f1aaa0-3188-414a-b6b6-0e5ce308daf9.jpg'),Path('assets/2023-04-26-amazon-rekognition-custom-labels/clean/3ae6fd96-22fa-404c-a867-48ca3a181f05.jpg'),Path('assets/2023-04-26-amazon-rekognition-custom-labels/clean/51be4baa-f930-4816-9383-119e89bd0270.png')...]\n\n\nLet’s display a sample of the cleaned images.\n\n# Define a function to display the extention of the image.\n# Note that only JPG and PNG images can be used for training in Rekognition custom labels\ndef label_func(f):\n    return f[-3:].upper()\n\n\n# Load images from the folder. This function act as a filter to omit files that are not images.\n# It does not read images at this point and only return their paths.\nfiles = get_image_files(local_path_clean)\n\n# Use FastAI DataLoader class to read images from the provided paths\ndls = ImageDataLoaders.from_name_func(\n    local_path_clean, files, label_func, item_tfms=Resize(224)\n)\n\n# Display a subset of images\ndls.show_batch(max_n=15)"
  },
  {
    "objectID": "posts/2023-04-26-amazon-rekognition-custom-labels.html#step-2-upload-images-to-s3-bucket",
    "href": "posts/2023-04-26-amazon-rekognition-custom-labels.html#step-2-upload-images-to-s3-bucket",
    "title": "Creating an Object Detection Model using Amazon Rekognition Custom Labels",
    "section": "Step 2: Upload images to S3 bucket",
    "text": "Step 2: Upload images to S3 bucket\nThe next step is to upload our finalized training images to AWS S3 bucket. I have created a bucket with the name 2023-04-26-amazon-rekognition-custom-labels. You may use a unique name of your own choice to create an S3 bucket.\n\n# Define S3 bucket name, bucket URL and images S3 path\n\ns3_bucket_name = '2023-04-26-amazon-rekognition-custom-labels'\ns3_bucket_url = f's3://{s3_bucket_name}'\ns3_images_path = f'{s3_bucket_url}/images'\nregion = 'us-east-2'\n\nprint(\n's3_bucket_url: ', s3_buket_url, '\\n'\n's3_images_path: ', s3_images_path\n)\n\ns3_bucket_url:  s3://2023-04-26-amazon-rekognition-custom-labels \ns3_images_path:  s3://2023-04-26-amazon-rekognition-custom-labels/images\n\n\n\n## Create an S3 bucket using AWS CLI MakeBucket (mb) command. \n# https://awscli.amazonaws.com/v2/documentation/api/latest/reference/s3/mb.html\n!aws s3 mb $s3_bucket_url --region $region\n\n# !aws s3 mb s3://2023-04-26-amazon-rekognition-custom-labels --region us-east-2\n\nmake_bucket failed: s3://2023-04-26-amazon-rekognition-custom-labels An error occurred (BucketAlreadyOwnedByYou) when calling the CreateBucket operation: Your previous request to create the named bucket succeeded and you already own it.\n\n\n\n# Upload images to s3 bucket\n!aws s3 sync ./$local_path_clean $s3_images_path --region $region\n\n# !aws s3 sync ./assets/2023-04-26-amazon-rekognition-custom-labels/clean s3://2023-04-26-amazon-rekognition-custom-labels/images --region us-east-2\n\n\n# Display the list of images from S3\n!aws s3 ls $s3_images_path --recursive\n\n2023-05-01 09:11:23      39999 images/029ae3b8-b647-43f2-92b0-36ecbc7ae551.jpg\n2023-05-01 09:11:23     527439 images/034cb4ef-b7b1-48ab-8839-515878aefe72.png\n2023-05-01 09:11:23     506785 images/050e16bf-ef67-4791-8b44-da1b7f903972.png\n2023-05-01 09:11:23      50558 images/075d8ea9-ccd2-4cd7-b9b5-f499ff7d0f09.jpg\n2023-05-01 09:11:23      73841 images/07b07c52-69ce-4eef-9887-d2b795913b33.jpeg\n2023-05-01 09:11:23      66759 images/09caf2a1-3958-47f0-b2b8-61905a6337ef.jpg\n2023-05-01 09:11:23     104966 images/09f566df-128b-428d-b514-a7cfe47bccca.jpg\n2023-05-01 09:11:23      68640 images/0a3b17b6-a4d5-4252-9628-5b857fe9ce25.jpg\n2023-05-01 09:11:23      64107 images/0dc491c7-f1c0-4d54-bf60-bf986ff4340a.jpg\n2023-05-01 09:11:23      61480 images/0f1fcd8d-dbb1-4926-9d2a-55e18deed3ec.jpg\n2023-05-01 09:11:23      91267 images/172ed24a-c58b-4186-bb66-c5bf48c125f8.png\n2023-05-01 09:11:23      45327 images/1a623864-2b33-4bce-98e9-5cbc2551afae.jpg\n2023-05-01 09:11:23      62962 images/1b8b90a9-b8cb-4c85-a3fb-cd9a865baa5d.jpg\n2023-05-01 09:11:23     108271 images/20f1ec83-13c3-4517-b9e7-3d7cb8f1d4dd.jpg\n2023-05-01 09:11:23      71131 images/21b40f53-63a7-47f7-984b-d413ad254781.jpg\n2023-05-01 09:11:23      75492 images/21e48d81-57d7-4da5-9d95-a038d0c285c1.jpg\n2023-05-01 09:11:23      52485 images/228d560d-ab50-4bf5-8a3b-510dcdf89e9a.jpg\n2023-05-01 09:11:23      52315 images/23e65a8d-e3e2-4d8c-9aab-9f03c0f62868.jpg\n2023-05-01 09:11:23      47489 images/267bd98b-4001-4610-8af8-ca60595029c0.jpg\n2023-05-01 09:11:23      38170 images/2bbd80f8-9a0e-45a2-8f76-1f5f3f5f4095.jpg\n2023-05-01 09:11:23      39310 images/2c96018d-5501-4226-ae2b-a4ed84a75243.jpg\n2023-05-01 09:11:23      39218 images/3260abbe-f3f4-4acc-8a26-244a9bfe60da.jpeg\n2023-05-01 09:11:23      48423 images/3714d169-4839-44fc-8664-04cefbfd5376.jpg\n2023-05-01 09:11:23     323750 images/38092e95-5aa3-4a1e-80c8-b8bb39d269cd.png\n2023-05-01 09:11:23      12804 images/38395e14-56ea-4bb0-980f-fc33a98221ce.jpg\n2023-05-01 09:11:24      66244 images/39e4317c-0ee7-4d0e-95d7-cec439f4c7e5.jpg\n2023-05-01 09:11:24      76993 images/3ae6fd96-22fa-404c-a867-48ca3a181f05.jpg\n2023-05-01 09:11:24      68135 images/3fdf6f5a-bbe2-43bd-a0ff-b60c263cb360.jpg\n2023-05-01 09:11:24     112907 images/41f7b9f1-f12f-47a1-9c0a-20566fb141f4.jpg\n2023-05-01 09:11:24      56593 images/42a1cc88-09f0-449e-80f5-014c141c0bd7.jpg\n2023-05-01 09:11:24     147903 images/46fd265c-45ba-4d0f-9a1e-3d7c1211f6ee.jpg\n2023-05-01 09:11:24      72134 images/499518b6-37b9-4fe0-b3e9-86e5330ad45b.jpg\n2023-05-01 09:11:24      53768 images/4b3221ee-4e26-4cf9-965a-0eadc0fa6228.jpg\n2023-05-01 09:11:24     113101 images/4ca55c47-a818-4186-b730-eda0e866b81f.jpg\n2023-05-01 09:11:24      81604 images/4fa1edaf-9194-4d60-8552-8c120e73e440.png\n2023-05-01 09:11:24     327371 images/51be4baa-f930-4816-9383-119e89bd0270.png\n2023-05-01 09:11:24      60700 images/575ed94f-7228-4b2b-8e22-6c1aa63ede18.jpg\n2023-05-01 09:11:24      64617 images/5e0490d6-646d-470e-8257-f3c6742c3a48.jpg\n2023-05-01 09:11:24      76236 images/5fbfce6f-1c46-4ee4-bba7-159558992d2f.jpg\n2023-05-01 09:11:24      55074 images/618177b5-2ed9-4bd6-9580-8dcfd025865b.jpg\n2023-05-01 09:11:24      39436 images/633d8365-40c3-4374-a216-6a7778d43f03.jpg\n2023-05-01 09:11:24      49081 images/64119679-dab3-47c2-b7f5-4001a58700ec.jpg\n2023-05-01 09:11:24      77339 images/65418732-0c4e-4f24-8d50-ec3151fa5d6a.jpg\n2023-05-01 09:11:24      75945 images/672afbf2-de03-4bf5-aa57-d40b7d31d541.jpeg\n2023-05-01 09:11:24      59209 images/68966e2e-bd6b-4d23-bf49-679713065f95.jpg\n2023-05-01 09:11:24      48855 images/6da5aaae-1be1-4316-ae14-8de75789414a.jpg\n2023-05-01 09:11:24     623433 images/7339b3ae-86d8-4181-8055-866fe5f22b0c.png\n2023-05-01 09:11:24     366856 images/749fb305-cf38-4576-b749-1296408b6712.png\n2023-05-01 09:11:24     226675 images/75f95913-8fe0-4c4a-80d1-44a73c29ae2d.jpg\n2023-05-01 09:11:24      42959 images/78d8228e-e49b-47e1-a167-6cd559ab9a73.jpg\n2023-05-01 09:11:24      72061 images/79d1f3fe-9470-41d1-b0b0-1e295f7d562b.jpg\n2023-05-01 09:11:24      52261 images/7ac4b88d-d078-4772-b436-6fcb93f4055f.jpg\n2023-05-01 09:11:24      96002 images/7e182025-7977-4f71-b1e8-f07788758598.jpg\n2023-05-01 09:11:24     105369 images/825a4024-df99-42ac-9496-ea04d1b1c7bd.jpg\n2023-05-01 09:11:24      93159 images/83204d18-cdf0-466f-ba71-a62be9244b8a.jpg\n2023-05-01 09:11:24      93044 images/8627d03b-1dfa-436b-9178-d427d181fb7a.jpg\n2023-05-01 09:11:24     133550 images/8630cdac-946f-4153-838b-56e687922e6c.png\n2023-05-01 09:11:24     251448 images/8738fe36-29b7-4eb7-919c-d9c6afdf75a5.png\n2023-05-01 09:11:24      20703 images/8880357a-d253-4048-9cdb-f096f233e00a.jpg\n2023-05-01 09:11:24     394736 images/897f9b22-dbfe-46ad-adef-abc89f81e637.PNG\n2023-05-01 09:11:24      45704 images/8a445c99-8f7b-4dcd-973c-173d7207a9eb.jpg\n2023-05-01 09:11:24      65416 images/8fdad490-0d0e-4878-bf16-9804dec79874.jpg\n2023-05-01 09:11:24      34553 images/8fee7840-b28d-41b3-83df-92273f550203.jpg\n2023-05-01 09:11:24      52397 images/92c4d6d7-4927-4fd2-91bf-9b2aa2ef2882.jpg\n2023-05-01 09:11:24      76263 images/95660e55-1566-47cb-912c-2c683b790dcd.jpg\n2023-05-01 09:11:24      41339 images/958f88c2-49fa-4fc4-8cc0-1f7801494a5b.jpg\n2023-05-01 09:11:24      96875 images/971c86ee-4956-470a-9464-40298017a4a0.jpg\n2023-05-01 09:11:24      82668 images/990c3934-9530-4eef-98ea-06a09c2b6855.jpg\n2023-05-01 09:11:24      74400 images/9968f3b0-c79e-4796-bf39-14ff13f2f7ad.jpg\n2023-05-01 09:11:24      90824 images/9ca1913e-545b-4404-92ce-49da444a63c4.jpg\n2023-05-01 09:11:24      76967 images/9ccd7dd1-0ca4-444f-bc1a-af9db96a51e5.jpg\n2023-05-01 09:11:24      68486 images/9d3851ae-2f6c-453e-a023-a7bf513438c7.jpg\n2023-05-01 09:11:24      67379 images/9ee061b2-c492-4691-9557-4b848cd16f10.jpg\n2023-05-01 09:11:24      97003 images/9f77c47f-52af-4be9-acd2-28aeb8ec53e9.jpg\n2023-05-01 09:11:24      69616 images/9fb73562-02a1-4c1e-9c02-a6189674db9e.jpg\n2023-05-01 09:11:24      35460 images/a0105005-96b7-41d5-8e3f-28bb45129fbe.jpg\n2023-05-01 09:11:24      65774 images/a3f1aaa0-3188-414a-b6b6-0e5ce308daf9.jpg\n2023-05-01 09:11:24     569945 images/a85aeaa4-244f-45fc-826b-5fb9b26235bf.png\n2023-05-01 09:11:24      72757 images/a9588d35-98ed-417d-a620-01800e4ff831.jpg\n2023-05-01 09:11:24      73227 images/af307494-b8ce-422f-9011-f3eeb0e15879.jpg\n2023-05-01 09:11:24      36802 images/b3312eec-43a5-475c-b5b4-4722e1aa8fec.jpg\n2023-05-01 09:11:24      47581 images/b620148b-86c5-4f18-abcd-548ec88fefe8.jpg\n2023-05-01 09:11:24      36604 images/b6b220b5-bfe7-4647-acca-c0d8fa98e02d.png\n2023-05-01 09:11:24      57179 images/b8af0031-fa79-4aa7-9405-cd9825624db3.jpg\n2023-05-01 09:11:24      41915 images/ba4a4c20-a35f-4627-8d7a-6e4204cfe3e8.jpg\n2023-05-01 09:11:24      44096 images/babf3bb6-fa72-4a53-989d-a34326422e19.jpg\n2023-05-01 09:11:24      85721 images/be5c7e07-735b-46e6-a4d1-10b94d8b5092.jpg\n2023-05-01 09:11:24     225141 images/bea1c670-4f1b-46be-939b-be1b6d0d79bc.jpg\n2023-05-01 09:11:24      28111 images/c1ab05ea-9ec7-4d70-ba48-e59c899a54cd.jpg\n2023-05-01 09:11:24      53330 images/c2615222-51f3-4799-a8f7-e1797c7ed1a5.jpg\n2023-05-01 09:11:24      72484 images/c4b37d79-82f0-4455-a61c-d6d36b85efb3.jpg\n2023-05-01 09:11:24     131996 images/c7c95ad8-407d-4fa1-85d4-d642c3f2f154.jpg\n2023-05-01 09:11:24      82042 images/c959d9f2-392b-4e8d-bffe-60f1c73df2d4.jpg\n2023-05-01 09:11:24      54427 images/c9f005fe-601d-436d-86de-3378a3f97d61.JPG\n2023-05-01 09:11:24      70177 images/cc7111c1-cc40-4ad7-9e88-b8d3c8dbbbc2.jpg\n2023-05-01 09:11:24     166650 images/cc8db196-e8a0-431d-9da8-8dabaa22ea24.png\n2023-05-01 09:11:24      63822 images/d14e3413-0c2f-4a8d-9361-e842633a7fb6.jpg\n2023-05-01 09:11:24     487367 images/d2c711fa-f922-413f-aadc-ae0e9e10af2f.png\n2023-05-01 09:11:24      70247 images/d3f80782-ea64-40a6-89c0-0590cc16528e.jpg\n2023-05-01 09:11:24      64264 images/e1091468-29c6-4d32-b0c9-2f08ec2bfa3c.jpg\n2023-05-01 09:11:24      49633 images/e3e17e8e-3c59-4ebe-8516-1dd2aa79f38e.jpg\n2023-05-01 09:11:24      98993 images/e4965e99-67b5-4681-8477-fce7cec39608.jpg\n2023-05-01 09:11:24     176557 images/e50a84c0-0c05-4d00-a80d-f8727e682955.png\n2023-05-01 09:11:24      46807 images/e52c99cf-6db7-44c8-9402-8ba0e1690e68.jpg\n2023-05-01 09:11:24      51590 images/e71c9747-6dab-4c15-a7a5-b0667d14994c.jpg\n2023-05-01 09:11:24      74435 images/e964abee-0aa3-4d74-95ef-7c4650be6e68.jpg\n2023-05-01 09:11:24      68261 images/ed2da15b-bbf0-4d8d-8543-cffb293a6321.jpg\n2023-05-01 09:11:24     103909 images/edeaf6bf-59ea-4132-a6f7-ce6011c2cba6.jpg\n2023-05-01 09:11:24      48948 images/f4552f5f-3d4f-4e27-9430-52a4c0d82b6c.jpg\n2023-05-01 09:11:24      67209 images/fa5d6d7d-156e-4971-87d2-eb0f3ea2baa2.jpg\n2023-05-01 09:11:24      70342 images/fb5b1519-2f9e-4e65-a2f7-2d66a0f5cd61.jpg"
  },
  {
    "objectID": "posts/2023-04-26-amazon-rekognition-custom-labels.html#step-3-create-a-project",
    "href": "posts/2023-04-26-amazon-rekognition-custom-labels.html#step-3-create-a-project",
    "title": "Creating an Object Detection Model using Amazon Rekognition Custom Labels",
    "section": "Step 3: Create a Project",
    "text": "Step 3: Create a Project\nTo work with Rekognition Custom Labels, we first have to create a project. A Project is like a logical workspace where you place and organize your project assets like datasets and models.\nTo create a Project go to Amazon Rekognition Custom Labels &gt; Projects &gt; Create Project\n\nUnder Project details give it’s Project name as raspberry-pi-logos, and click Create project."
  },
  {
    "objectID": "posts/2023-04-26-amazon-rekognition-custom-labels.html#step-4-create-a-dataset",
    "href": "posts/2023-04-26-amazon-rekognition-custom-labels.html#step-4-create-a-dataset",
    "title": "Creating an Object Detection Model using Amazon Rekognition Custom Labels",
    "section": "Step 4: Create a Dataset",
    "text": "Step 4: Create a Dataset\nNext we will create a dataset under the project. This dataset will hold our training and test images. For this go to Custom Labels &gt; Projects &gt; raspberry-pi-logos &gt; Create dataset.\n\n\n\n\n\n\nDataset\n\n\n\nA dataset is a collection of images, and image labels, that you use to train or test a model.\n\n\n\nOn the next pane, provide the details of the dataset. Under Starting configuration, select Start with a single dataset for Configuation options.\n\nUnder Training dataset details, select Import images from S3 bucket for Import images\n\nNext, we will provide the S3 bucket URI s3://2023-04-26-amazon-rekognition-custom-labels/images/ from where it will import the images. Keep the Automatic Labelling checkbox unchecked.\n\nOnce S3 URI is given, it will provide some permissions we need to configure on the S3 bucket containing our images. The Custom Labels service cannot import the images from the bucket without these permissions. So let’s configure them.\n\nCopy the provided permissions, go to the S3 bucket, edit the bucket policy and paste the permissions there and save it.\n\nOnce all done, go back to the Create Dataset page and click Create Dataset."
  },
  {
    "objectID": "posts/2023-04-26-amazon-rekognition-custom-labels.html#step-5-add-labels",
    "href": "posts/2023-04-26-amazon-rekognition-custom-labels.html#step-5-add-labels",
    "title": "Creating an Object Detection Model using Amazon Rekognition Custom Labels",
    "section": "Step 5: Add Labels",
    "text": "Step 5: Add Labels\nNext step is to add labels. For this go to Custom Labels &gt; Projects &gt; raspberry-pi-logos &gt; Add labels\n\n\n\n\n\n\nLabels\n\n\n\nLabels identify objects, scenes, or concepts on an entire image, or they identify object locations on an image.\n\n\n\nClick the Start Labeling button on the next page to start the labeling activity.\n\nIt is easy to label in our case as we only have a single label: Raspberry Pi Logo. Let’s add that label by clicking on Labels &gt; Add Labels. On the Manage labels pane select “Add Labels” and add the name of a single label raspberry-pi-logo, and click Save."
  },
  {
    "objectID": "posts/2023-04-26-amazon-rekognition-custom-labels.html#step-6-draw-bounding-boxes",
    "href": "posts/2023-04-26-amazon-rekognition-custom-labels.html#step-6-draw-bounding-boxes",
    "title": "Creating an Object Detection Model using Amazon Rekognition Custom Labels",
    "section": "Step 6: Draw Bounding Boxes",
    "text": "Step 6: Draw Bounding Boxes\nThe next step is to draw Bounding Boxes around the Raspberry Pi logos in all the images. This is again a manual step and has to be done very patiently. To start this activity, select unlabelled images and click Draw bounding boxes.\n\nFrom the next pane, use the cursor to draw bounding boxes around the Raspberry Pi logos, as shown below.\n\nOnce completed, click Done and Save changes."
  },
  {
    "objectID": "posts/2023-04-26-amazon-rekognition-custom-labels.html#step-7-train-a-model",
    "href": "posts/2023-04-26-amazon-rekognition-custom-labels.html#step-7-train-a-model",
    "title": "Creating an Object Detection Model using Amazon Rekognition Custom Labels",
    "section": "Step 7: Train a model",
    "text": "Step 7: Train a model\nThe next step in our process is to train a model. Click Train model to start the model training.\n\nThis will open a Train model configuration page. Keep all the defaults on this page, and click Train model. This will bring a popup saying\n\nOur single dataset will be split into training dataset (80%) and test dataset (20%)\nAnd training may take about 30 minutes to complete\n\n\nStart the training by clicking Train model.\nOnce the training starts, a model entry will appear under Custom Labels &gt; Projects &gt; raspberry-pi-logos with the status TRAINING_IN_PROGRESS. After that, we have to wait till the status changes to TRAINING_COMPLETED."
  },
  {
    "objectID": "posts/2023-04-26-amazon-rekognition-custom-labels.html#step-8-evaluate-the-model",
    "href": "posts/2023-04-26-amazon-rekognition-custom-labels.html#step-8-evaluate-the-model",
    "title": "Creating an Object Detection Model using Amazon Rekognition Custom Labels",
    "section": "Step 8: Evaluate the model",
    "text": "Step 8: Evaluate the model\nOnce the training is complete, we can evaluate the model’s performance by clicking the model name.\n\nThe Evaluation tab shows the testing results for our trained model. This helps to understand the overall performance of our model.\n\nWe may dig deeper into the model’s performance on the test set by clicking on the View test results. In our case, the trained model gave wrong predictions (False Negatives) for two test images. When looked closely, it showed that even though the model could pick the presence of the Raspberry Pi logo, it was not confident enough to accurately draw the bounding box around it."
  },
  {
    "objectID": "posts/2023-04-26-amazon-rekognition-custom-labels.html#step-9-get-inference-from-the-model",
    "href": "posts/2023-04-26-amazon-rekognition-custom-labels.html#step-9-get-inference-from-the-model",
    "title": "Creating an Object Detection Model using Amazon Rekognition Custom Labels",
    "section": "Step 9: Get inference from the model",
    "text": "Step 9: Get inference from the model\nAll right, we have trained a model, and we are satisfied with the model’s performance. We may now deploy this model and put it to some actual use. Amazon Rekognition makes the model deployment part a breeze and can be done by clicking the button Start. When you start your model, specify the number of compute resources, known as an inference unit, that the model uses. To read more about the model deployment, compute resources and cost, refer to the documentation page Running a trained Amazon Rekognition Custom Labels model\n\nThe following cell defines two functions:\n\ndisplay_image This function reads an image from the S3 bucket, and draws a bounding box around the Raspberry Pi logo based on the coordinates received from the model API.\nshow_custom_labels This function uses the Boto3 library to initiate Amazon Rekognition service client, and invoke its APIs. For custom label detection, it uses the interface detect_custom_labels. To read more about this API and the available features, refer to the documentation page Rekognition.Client.detect_custom_labels\n\n\n\nShow the code\nimport boto3\nimport io\nfrom PIL import Image, ImageDraw, ExifTags, ImageColor, ImageFont\n\ndef display_image(bucket,photo,response):\n    # Load image from S3 bucket\n    s3_connection = boto3.resource('s3')\n\n    s3_object = s3_connection.Object(bucket,photo)\n    s3_response = s3_object.get()\n\n    stream = io.BytesIO(s3_response['Body'].read())\n    image=Image.open(stream)\n\n    # Ready image to draw bounding boxes on it.\n    imgWidth, imgHeight = image.size\n    draw = ImageDraw.Draw(image)\n\n    # calculate and display bounding boxes for each detected custom label\n    print('Detected custom labels for ' + photo)\n    for customLabel in response['CustomLabels']:\n        print('Label ' + str(customLabel['Name']))\n        print('Confidence ' + str(customLabel['Confidence']))\n        if 'Geometry' in customLabel:\n            box = customLabel['Geometry']['BoundingBox']\n            left = imgWidth * box['Left']\n            top = imgHeight * box['Top']\n            width = imgWidth * box['Width']\n            height = imgHeight * box['Height']\n\n            draw.text((left,top), customLabel['Name'], fill='#00d400')\n\n            print('Left: ' + '{0:.0f}'.format(left))\n            print('Top: ' + '{0:.0f}'.format(top))\n            print('Label Width: ' + \"{0:.0f}\".format(width))\n            print('Label Height: ' + \"{0:.0f}\".format(height))\n\n            points = (\n                (left,top),\n                (left + width, top),\n                (left + width, top + height),\n                (left , top + height),\n                (left, top))\n            draw.line(points, fill='#00d400', width=5)\n    \n    return image\n\ndef show_custom_labels(model,bucket,photo, min_confidence):\n    client=boto3.client('rekognition')\n\n    #Call DetectCustomLabels\n    response = client.detect_custom_labels(Image={'S3Object': {'Bucket': bucket, 'Name': photo}},\n        MinConfidence=min_confidence,\n        ProjectVersionArn=model)\n\n    print(\"Response from the API: \\n\", response, \"\\n\")\n    # For object detection use case, uncomment below code to display image.\n    image_with_labels = display_image(bucket,photo,response)\n\n    return image_with_labels\n\n\n\nbucket='2023-04-26-amazon-rekognition-custom-labels'\nphoto='images/029ae3b8-b647-43f2-92b0-36ecbc7ae551.jpg'\nmodel='arn:aws:rekognition:us-east-2:801598032724:project/raspberry-pi-logos/version/raspberry-pi-logos.2023-05-01T18.56.34/1682949462653'\nmin_confidence=20\n\nimage_with_labels=show_custom_labels(model,bucket,photo, min_confidence)\n\nResponse from the API: \n {'CustomLabels': [{'Name': 'raspberry-pi-logo', 'Confidence': 21.43899917602539, 'Geometry': {'BoundingBox': {'Width': 0.08699999749660492, 'Height': 0.0675399973988533, 'Left': 0.34002000093460083, 'Top': 0.30324000120162964}}}], 'ResponseMetadata': {'RequestId': '45c1b1cd-162f-4ebf-8603-957648440d41', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amzn-requestid': '45c1b1cd-162f-4ebf-8603-957648440d41', 'content-type': 'application/x-amz-json-1.1', 'content-length': '216', 'date': 'Tue, 02 May 2023 14:30:22 GMT'}, 'RetryAttempts': 0}} \n\nDetected custom labels for images/029ae3b8-b647-43f2-92b0-36ecbc7ae551.jpg\nLabel raspberry-pi-logo\nConfidence 21.43899917602539\nLeft: 272\nTop: 182\nLabel Width: 70\nLabel Height: 41\n\n\n\n# Let's display the image with labels\nimage_with_labels"
  },
  {
    "objectID": "posts/2023-04-26-amazon-rekognition-custom-labels.html#step-10-stop-the-model",
    "href": "posts/2023-04-26-amazon-rekognition-custom-labels.html#step-10-stop-the-model",
    "title": "Creating an Object Detection Model using Amazon Rekognition Custom Labels",
    "section": "Step 10: Stop the model",
    "text": "Step 10: Stop the model\nRekognition Custom Labels service will charge us as long as the model is in the Running state. Therefore, we should stop it once it in no more being used. To do that, click the Stop button on the model."
  },
  {
    "objectID": "posts/2022-12-14-pytorch-autoencoder.html#credits",
    "href": "posts/2022-12-14-pytorch-autoencoder.html#credits",
    "title": "Implementing AutoEncoder with PyTorch",
    "section": "Credits",
    "text": "Credits\nThis notebook takes inspiration and ideas from the following sources.\n\nIntroduction to Deep Learning course by Sebastian Raschka. Sebastian is one of my favorite instructors. You can find all his lectures free on his YouTube channel, with tons of practice material he regularly shares on his blog. Lectures I find helpful on this topic include L16 AutoEncoder. Parts of the code in this notebook are taken from his course material.\n\nL16.0 Introduction to Autoencoders – Lecture Overview\nL16.1 Dimensionality Reduction\nL16.2 A Fully-Connected Autoencoder\nL16.3 Convolutional Autoencoders & Transposed Convolutions\nL16.4 A Convolutional Autoencoder in PyTorch\nL16.5 Other Types of Autoencoders\nNotebook: 1_VAE_mnist_sigmoid_mse.ipynb\n\nModern Computer Vision with PyTorch book published by Packt has tons of useful material on its GitHub repository. Chapter 11 from this book is related to AutoEncoder. Parts of the code you see in this notebook are taken from the following notebooks.\n\nNotebook: simple_auto_encoder_with_different_latent_size.ipynb\nNotebook: conv_auto_encoder.ipynb"
  },
  {
    "objectID": "posts/2022-12-14-pytorch-autoencoder.html#environment",
    "href": "posts/2022-12-14-pytorch-autoencoder.html#environment",
    "title": "Implementing AutoEncoder with PyTorch",
    "section": "Environment",
    "text": "Environment\nThis notebook is prepared with Google Colab.\n\nGitHub: 2022-12-14-pytorch-autoencoder.ipynb\nOpen In Colab: \n\n\n\nCode\nfrom platform import python_version\nimport numpy, matplotlib, pandas, torch\n\nprint(\"python==\" + python_version())\nprint(\"numpy==\" + numpy.__version__)\nprint(\"torch==\" + torch.__version__)\nprint(\"matplotlib==\" + matplotlib.__version__)\n\n\npython==3.8.16\nnumpy==1.21.6\ntorch==1.13.0+cu116\nmatplotlib==3.2.2\n\n\nLet’s also set the device and seed for results reproducibility.\n\n\nCode\nimport torch, os, random\nimport numpy as np\n\n# https://wandb.ai/sauravmaheshkar/RSNA-MICCAI/reports/How-to-Set-Random-Seeds-in-PyTorch-and-Tensorflow--VmlldzoxMDA2MDQy\ndef set_seed(seed: int = 42) -&gt; None:\n    np.random.seed(seed)\n    random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    # When running on the CuDNN backend, two further options must be set\n    if torch.cuda.is_available():\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\n    # Set a fixed value for the hash seed\n\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    print(f\"Random seed set as {seed}\")\n\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Device:\", device)\nset_seed()\n\n\nDevice: cuda\nRandom seed set as 42"
  },
  {
    "objectID": "posts/2022-12-14-pytorch-autoencoder.html#introduction",
    "href": "posts/2022-12-14-pytorch-autoencoder.html#introduction",
    "title": "Implementing AutoEncoder with PyTorch",
    "section": "Introduction",
    "text": "Introduction\n\nFollowing is the AutoEncoder explanation provided by OpenAI ChatGPT. If unsatisfied, I suggest using the links provided in the “Credits” section (L16.0 Introduction to Autoencoders by Sebastian Raschka)\n\nprompt &gt; introduction to autoencoder\nAutoEncoder\nAn autoencoder is a type of artificial neural network that is used to learn efficient data codings in an unsupervised manner. Autoencoders are composed of two parts: an encoder and a decoder. The encoder takes in an input and converts it into a compressed representation, typically in the form of a lower-dimensional code. The decoder then takes this code and converts it back into a reconstruction of the original input.\nThe goal of an autoencoder is to learn a representation of the input data that is more efficient than the original input. This is achieved by training the network to reconstruct the input from the lower-dimensional code, which forces the encoder to learn a compact representation of the data. Autoencoders can be used for dimensionality reduction, feature learning, and generating new data samples.\n\n\n\nFigure 1: A basic fully connected AutoEncoder\n\n\nFrom these descriptions, we can deduce the following points about an autoencoder working.\n\nAn autoencoder can learn a compact representation of the data. This representation is stored in the center hidden layer of the network. We have multiple names for this center hidden layer, including bottleneck, latent space, embedded space, and hidden units.\nThe latent space has a dimension less than the input data dimension.\nAutoencoder can be used for dimensionality reduction. In fact, if we don’t use any non-linearity (e.g., ReLU) then autoencoder will function similarly to PCA since PCA is a linear dimensionality reduction method.\nEncoder part of the autoencoder model compresses the data to a latent space. The decoder can use the latent space to reconstruct (or decode) the original image.\n\nSummary of the steps followed in this notebook\n\nDownload MNIST handwritten digit dataset\nTrain an autoencoder with 2-dimensional (2 pixels) latent space. Use 2 pixels to deconstruct the whole MNIST digit image. Visualize the latent space learned by the model. Use random latent space points to decode the images\nCreate multiple autoencoders with varying latent space [2, 5, 10, 20, 50] and use them to decode the images. Compare the results to analyze the effect of latent dimension in storing information and decoded image quality.\nVisualize the latent space of the model with a latent space of 50 dimensions. Use random points from the latent space to decode (or construct new) images.\nFinally, discuss the limitations of autoencoders."
  },
  {
    "objectID": "posts/2022-12-14-pytorch-autoencoder.html#data-preparation",
    "href": "posts/2022-12-14-pytorch-autoencoder.html#data-preparation",
    "title": "Implementing AutoEncoder with PyTorch",
    "section": "Data Preparation",
    "text": "Data Preparation\n\nLoad MNIST Dataset\nIn the next cell, I downloaded the MNIST dataset and created a DataLoader with a batch size of 256.\n\nfrom torchvision import datasets\nfrom torch.utils.data import DataLoader\nfrom torchvision import transforms\n\nbatch_size = 256\n\ntrain_dataset = datasets.MNIST(\n    root=\"data\", train=True, transform=transforms.ToTensor(), download=True\n)\n\ntest_dataset = datasets.MNIST(root=\"data\", train=False, transform=transforms.ToTensor())\n\ntrain_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=False)\n\ntest_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n\nDownloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\nDownloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to data/MNIST/raw/train-images-idx3-ubyte.gz\n\n\n\n\n\nExtracting data/MNIST/raw/train-images-idx3-ubyte.gz to data/MNIST/raw\n\nDownloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\nDownloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to data/MNIST/raw/train-labels-idx1-ubyte.gz\n\n\n\n\n\nExtracting data/MNIST/raw/train-labels-idx1-ubyte.gz to data/MNIST/raw\n\nDownloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\nDownloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to data/MNIST/raw/t10k-images-idx3-ubyte.gz\n\n\n\n\n\nExtracting data/MNIST/raw/t10k-images-idx3-ubyte.gz to data/MNIST/raw\n\nDownloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\nDownloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n\n\n\n\n\nExtracting data/MNIST/raw/t10k-labels-idx1-ubyte.gz to data/MNIST/raw\n\n\n\nNotice that while creating Datasets, I used a transformer transform=transforms.ToTensor(). torchvision.transforms.ToTensor.html\n\n[transforms.ToTensor] Convert a PIL Image or numpy.ndarray to tensor … Converts a PIL Image or numpy.ndarray (H x W x C) in the range [0, 255] to a torch.FloatTensor of shape (C x H x W) in the range [0.0, 1.0].\n\nSo it manipulates the MNSIT images in two ways\n\nConverts image dimensions from (H x W x C) to (C x H x W)\nScales image pixel values from the range [0, 255] to range [0.0, 1.0]\n\nLet’s check the dimension of our dataset.\n\nfor images, labels in train_loader:\n    print(\n        \"Image batch dimensions:\", images.shape\n    )  # [batch size, channels, img height, img width]\n    print(\"Image label dimensions:\", labels.shape)  # [batch size]\n    break\n\nImage batch dimensions: torch.Size([256, 1, 28, 28])\nImage label dimensions: torch.Size([256])"
  },
  {
    "objectID": "posts/2022-12-14-pytorch-autoencoder.html#model-configuration",
    "href": "posts/2022-12-14-pytorch-autoencoder.html#model-configuration",
    "title": "Implementing AutoEncoder with PyTorch",
    "section": "Model configuration",
    "text": "Model configuration\nLet’s configure a simple AutoEncoder model. It is made of two fully connected multilayer perceptrons. The first perceptron will gradually decrease the dimensions of the input data till it reaches the latent dimension size. The second perceptron will gradually increase the dimensions of data obtained from latent space till it reaches the input size. Notice that\n\nLatent space dimension (latent_dim) is kept configurable so we can use the same model class to configure autoencoder having different latent spaces\nI have used torch.sigmoid() in the forward pass. This is to squash the data into the range [0, 1]. This is done because the input image received by the model is assumed to be in this range (remember transforms.ToTensor() function while creating datasets). So we want the image created (or returned) by the model to be in the same distribution range.\n\n\nimport torch.nn as nn\n\nclass AutoEncoder(nn.Module):\n    def __init__(self, num_features, latent_dim):\n        super().__init__()\n        self.latent_dim = latent_dim\n\n        self.encoder = nn.Sequential(\n            nn.Linear(num_features, 128),\n            nn.ReLU(True),\n            nn.Linear(128, 64),\n            nn.ReLU(True),\n            nn.Linear(64, latent_dim),\n        )\n\n        self.decoder = nn.Sequential(\n            nn.Linear(latent_dim, 64),\n            nn.ReLU(True),\n            nn.Linear(64, 128),\n            nn.ReLU(True),\n            nn.Linear(128, num_features),\n        )\n\n    def forward(self, x):\n        x = self.encoder(x)\n        x = self.decoder(x)\n        x = torch.sigmoid(x)\n        return x"
  },
  {
    "objectID": "posts/2022-12-14-pytorch-autoencoder.html#autoencoder-with-latent-space-dimension2",
    "href": "posts/2022-12-14-pytorch-autoencoder.html#autoencoder-with-latent-space-dimension2",
    "title": "Implementing AutoEncoder with PyTorch",
    "section": "AutoEncoder with latent space dimension=2",
    "text": "AutoEncoder with latent space dimension=2\nLet’s create our first autoencoder with a latent space of 2 features. This means we will compress our input image of 28*28=784 features to only two in the latent space. Then we will use the information stored in these two features to reconstruct (or decode) the full image having 784 features. Why only two features for the latent space?\n\nWe want to try an extreme case, and you will be surprised to see that even with two features in latent space, we can construct the whole image with acceptable quality.\nWe want to visualize the 2D latent space. Visualizing (plotting) latent space with higher dimensions will not be possible.\n\n\nlearning_rate = 0.005\nnum_features = 28 * 28  # image height X image width = 784\nlatent_dim = 2\n\nmodel = AutoEncoder(num_features, latent_dim)\nmodel = model.to(device)\noptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n\nWe have configured our model and optimizer, and we can start training our model.\n\nimport time\nimport torch.nn.functional as F\n\nnum_epochs = 10\nstart_time = time.time()\n\nfor epoch in range(num_epochs):\n    for batch_idx, (features, targets) in enumerate(train_loader):\n        # don't need labels, only the images (features)\n        features = features.view(-1, num_features).to(device)\n\n        ### FORWARD AND BACK PROP\n        decoded = model(features)\n        loss = F.mse_loss(decoded, features)\n        optimizer.zero_grad()\n\n        loss.backward()\n\n        ### UPDATE MODEL PARAMETERS\n        optimizer.step()\n\n        ### LOGGING\n        if not batch_idx % 50:\n            print(\n                \"Epoch: %03d/%03d | Batch %03d/%03d | Training Loss: %.4f\"\n                % (epoch + 1, num_epochs, batch_idx, len(train_loader), loss)\n            )\n\nprint(\"Total Training Time: %.2f min\" % ((time.time() - start_time) / 60))\n\nEpoch: 001/010 | Batch 000/235 | Training Loss: 0.2315\nEpoch: 001/010 | Batch 050/235 | Training Loss: 0.0655\nEpoch: 001/010 | Batch 100/235 | Training Loss: 0.0523\nEpoch: 001/010 | Batch 150/235 | Training Loss: 0.0517\nEpoch: 001/010 | Batch 200/235 | Training Loss: 0.0511\nEpoch: 002/010 | Batch 000/235 | Training Loss: 0.0479\nEpoch: 002/010 | Batch 050/235 | Training Loss: 0.0486\nEpoch: 002/010 | Batch 100/235 | Training Loss: 0.0441\nEpoch: 002/010 | Batch 150/235 | Training Loss: 0.0456\nEpoch: 002/010 | Batch 200/235 | Training Loss: 0.0461\nEpoch: 003/010 | Batch 000/235 | Training Loss: 0.0438\nEpoch: 003/010 | Batch 050/235 | Training Loss: 0.0460\nEpoch: 003/010 | Batch 100/235 | Training Loss: 0.0423\nEpoch: 003/010 | Batch 150/235 | Training Loss: 0.0442\nEpoch: 003/010 | Batch 200/235 | Training Loss: 0.0439\nEpoch: 004/010 | Batch 000/235 | Training Loss: 0.0415\nEpoch: 004/010 | Batch 050/235 | Training Loss: 0.0447\nEpoch: 004/010 | Batch 100/235 | Training Loss: 0.0411\nEpoch: 004/010 | Batch 150/235 | Training Loss: 0.0423\nEpoch: 004/010 | Batch 200/235 | Training Loss: 0.0425\nEpoch: 005/010 | Batch 000/235 | Training Loss: 0.0406\nEpoch: 005/010 | Batch 050/235 | Training Loss: 0.0434\nEpoch: 005/010 | Batch 100/235 | Training Loss: 0.0404\nEpoch: 005/010 | Batch 150/235 | Training Loss: 0.0419\nEpoch: 005/010 | Batch 200/235 | Training Loss: 0.0415\nEpoch: 006/010 | Batch 000/235 | Training Loss: 0.0397\nEpoch: 006/010 | Batch 050/235 | Training Loss: 0.0435\nEpoch: 006/010 | Batch 100/235 | Training Loss: 0.0399\nEpoch: 006/010 | Batch 150/235 | Training Loss: 0.0412\nEpoch: 006/010 | Batch 200/235 | Training Loss: 0.0405\nEpoch: 007/010 | Batch 000/235 | Training Loss: 0.0391\nEpoch: 007/010 | Batch 050/235 | Training Loss: 0.0426\nEpoch: 007/010 | Batch 100/235 | Training Loss: 0.0394\nEpoch: 007/010 | Batch 150/235 | Training Loss: 0.0409\nEpoch: 007/010 | Batch 200/235 | Training Loss: 0.0398\nEpoch: 008/010 | Batch 000/235 | Training Loss: 0.0389\nEpoch: 008/010 | Batch 050/235 | Training Loss: 0.0423\nEpoch: 008/010 | Batch 100/235 | Training Loss: 0.0392\nEpoch: 008/010 | Batch 150/235 | Training Loss: 0.0405\nEpoch: 008/010 | Batch 200/235 | Training Loss: 0.0398\nEpoch: 009/010 | Batch 000/235 | Training Loss: 0.0377\nEpoch: 009/010 | Batch 050/235 | Training Loss: 0.0429\nEpoch: 009/010 | Batch 100/235 | Training Loss: 0.0386\nEpoch: 009/010 | Batch 150/235 | Training Loss: 0.0403\nEpoch: 009/010 | Batch 200/235 | Training Loss: 0.0392\nEpoch: 010/010 | Batch 000/235 | Training Loss: 0.0377\nEpoch: 010/010 | Batch 050/235 | Training Loss: 0.0413\nEpoch: 010/010 | Batch 100/235 | Training Loss: 0.0384\nEpoch: 010/010 | Batch 150/235 | Training Loss: 0.0404\nEpoch: 010/010 | Batch 200/235 | Training Loss: 0.0388\nTotal Training Time: 0.96 min\n\n\nNote that in the above training loop.\n\nfeatures = input images\ndecoded = decoded or reconstructed images from latent space. This is because it runs the forward pass when we predict from our model (decoded = model(features)). And we know that during the forward pass, we will first encode the input image, then use the output (latent representation) to reconstruct the image using the decoder.\n\nLet’s plot these original images and decoded ones to see how much we are successful in doing that.\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nn_images = 15\nimage_width = 28\n\nfig, axes = plt.subplots(nrows=2, ncols=n_images, sharex=True, sharey=True, figsize=(20, 2.5))\n\norig_images = features[:n_images]\ndecoded_images = decoded[:n_images]\nlabel_images = targets[:n_images]\n\nfor i in range(n_images):\n    for ax, img in zip(axes, [orig_images, decoded_images]):\n        curr_img = img[i].detach().to(torch.device('cpu'))\n        ax[i].imshow(curr_img.view((image_width, image_width)), cmap='binary')\n\n\n\n\n\n\n\n\nThese results are remarkable, considering we only use information from two pixels to generate a complete image of 784 pixels. So what information is stored in those two pixels (embedding)? Let’s also print embeddings for the above generated images.\n\nfor feature, target in zip(orig_images, label_images):\n    feature = feature.view(-1, num_features).to(device)\n    with torch.no_grad():\n        embedding = model.encoder(feature)\n\n    print(f\"Label: {target}, Embedding: {embedding.cpu().numpy()}\")\n\nLabel: 3, Embedding: [[-1.6004068  -0.96022624]]\nLabel: 4, Embedding: [[3.5773783 1.7578685]]\nLabel: 5, Embedding: [[ 9.145074 -2.872149]]\nLabel: 6, Embedding: [[ 1.7183754 -2.08906  ]]\nLabel: 7, Embedding: [[10.19044   9.111174]]\nLabel: 8, Embedding: [[-2.115856   1.4071143]]\nLabel: 9, Embedding: [[6.411824  5.6826334]]\nLabel: 0, Embedding: [[  8.0015   -13.730832]]\nLabel: 1, Embedding: [[-7.0476646  6.571097 ]]\nLabel: 2, Embedding: [[-5.4590693  2.0496612]]\nLabel: 3, Embedding: [[-1.0255219 -0.9620053]]\nLabel: 4, Embedding: [[14.75902   7.228238]]\nLabel: 8, Embedding: [[14.938157   -0.26024085]]\nLabel: 9, Embedding: [[8.782047 6.697904]]\nLabel: 0, Embedding: [[ 13.757708 -11.047596]]\n\n\nBy analyzing these values, we can say that our autoencoder has learned to use slightly different ranges for embeddings each of the digits. Next, let’s try to visualize the distribution of latent space for the complete training set. For this, I have used a helper function. This function creates scatter plots of the latent space for all the digits.\n\nimport numpy as np\nimport matplotlib.colors as mcolors\n\n# https://github.com/rasbt/stat453-deep-learning-ss21/blob/main/L17/helper_plotting.py\ndef plot_latent_space_with_labels(num_classes, data_loader, encoding_fn, device):\n    d = {i: [] for i in range(num_classes)}\n\n    with torch.no_grad():\n        for i, (features, targets) in enumerate(data_loader):\n\n            features = features.view(-1, num_features).to(device)\n            targets = targets.to(device)\n\n            embedding = encoding_fn(features)\n\n            for i in range(num_classes):\n                if i in targets:\n                    mask = targets == i\n                    d[i].append(embedding[mask].to(\"cpu\").numpy())\n\n    colors = list(mcolors.TABLEAU_COLORS.items())\n    plt.figure(figsize=(8, 8))\n    for i in range(num_classes):\n        d[i] = np.concatenate(d[i])\n\n        plt.scatter(d[i][:, 0], d[i][:, 1], color=colors[i][1], label=f\"{i}\", alpha=0.5)\n\n    plt.legend()\n\nLet’s create the plot.\n\nplot_latent_space_with_labels(\n    num_classes=len(train_dataset.classes),  # 10\n    data_loader=train_loader,\n    encoding_fn=model.encoder,\n    device=device,\n)\n\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nThis plot tells us different digits are occupying different areas in the latent space. If we want to draw a sample for a number (e.g., 0), we must take the point from its range. It is a 2D plot, so If I take a sample from it, I will get two points (x, y), and then I can use that as an embedding. I can then reconstruct an image using the decoder of my model from this embedding.\nIn the next cell, I have created a helper function that can take the embeddings list and decode an image from it.\n\ndef decode_images(model, embedding_list):\n    fig, axs = plt.subplots(1, len(embedding_list), figsize=(20, 2.5))\n\n    for i, (label, embedding) in enumerate(embedding_list):\n        with torch.no_grad():\n            new_image = model.decoder(torch.tensor(embedding).to(device))\n\n        new_image = torch.sigmoid(new_image)\n        new_image = new_image.view(28, 28)\n        new_image = new_image.detach().to(torch.device(\"cpu\"))\n\n        axs[i].set_title(label)\n        axs[i].imshow(new_image, cmap=\"binary\")\n\n    plt.show()\n\nNext, I created some embeddings using the above plot.\n\nembedding_list = [\n    (\"0\", [5.0, -10.0]),\n    (\"1\", [-4.0, 13.0]),\n    (\"2\", [-5.4, 2.0]),\n    (\"2 overlap with 3\", [-4.0, 0.0]),\n    (\"3 ouside sample space\", [-5.0, -5.0]),\n    (\"7\", [6.0, 11.0]),\n    (\"9 overlap with 8\", [0.0, 0.0]),\n    (\"5\", [9.0, -3.0]),\n]\n\nLet’s use these embeddings to generate some images. Note a few points.\n\nThere are no clear boundaries between the digits’ latent space\nIf you select a point that is on the edge or at the boundary of the space, then you may get a more distorted image\n\nDigit 2 image that has some overlap with digit 3\nDigit 9 image that has some overlap with 8\n\nThere is no definite range of the latent space\n\nDigit 3 image generated from the embedding range not visible in the plot\n\n\n\ndecode_images(model, embedding_list)"
  },
  {
    "objectID": "posts/2022-12-14-pytorch-autoencoder.html#autoencoder-with-latent-space-dimension5-10-20-50",
    "href": "posts/2022-12-14-pytorch-autoencoder.html#autoencoder-with-latent-space-dimension5-10-20-50",
    "title": "Implementing AutoEncoder with PyTorch",
    "section": "AutoEncoder with latent space dimension=[5, 10, 20, 50]",
    "text": "AutoEncoder with latent space dimension=[5, 10, 20, 50]\nWe have seen the quality of decoded images from 2d latent space. Our understanding is that if we increase the dimension, it will retain more information and improve the image quality.\nNext, I have created a function that can take a model to train it. We will iteratively create models with increasing dimensions and use this function to train them.\n\ndef training_loop(train_loader, model, optimizer, device, num_epochs=10):\n    start_time = time.time()\n    for epoch in range(num_epochs):\n        for batch_idx, (features, targets) in enumerate(train_loader):\n            # don't need labels, only the images (features)\n            features = features.view(-1, num_features).to(device)\n\n            ### FORWARD AND BACK PROP\n            decoded = model(features)\n            loss = F.mse_loss(decoded, features)\n            optimizer.zero_grad()\n\n            loss.backward()\n\n            ### UPDATE MODEL PARAMETERS\n            optimizer.step()\n\n        ### LOGGING\n        print(\"Epoch: %03d | Training Loss: %.4f\" % (epoch + 1, loss))\n\n    print(\"Total Training Time: %.2f min\" % ((time.time() - start_time) / 60))\n\n\nmodels_list = [model]  # model with latent_dim=2\nlatent_dimensions = [5, 10, 20, 50]\n\nfor latent_dim in latent_dimensions:\n    model_n = AutoEncoder(num_features, latent_dim)\n    model_n = model_n.to(device)\n    optimizer_n = torch.optim.Adam(model_n.parameters(), lr=learning_rate)\n    models_list.append(model_n)\n\n    print(f\"\\n*** Training AutoEncoder with latent_dim={latent_dim} ***\\n\")\n    training_loop(train_loader, model_n, optimizer_n, device)\n\n\n*** Training AutoEncoder with latent_dim=5 ***\n\nEpoch: 001 | Training Loss: 0.0396\nEpoch: 002 | Training Loss: 0.0315\nEpoch: 003 | Training Loss: 0.0292\nEpoch: 004 | Training Loss: 0.0278\nEpoch: 005 | Training Loss: 0.0270\nEpoch: 006 | Training Loss: 0.0262\nEpoch: 007 | Training Loss: 0.0256\nEpoch: 008 | Training Loss: 0.0251\nEpoch: 009 | Training Loss: 0.0248\nEpoch: 010 | Training Loss: 0.0247\nTotal Training Time: 0.92 min\n\n*** Training AutoEncoder with latent_dim=10 ***\n\nEpoch: 001 | Training Loss: 0.0372\nEpoch: 002 | Training Loss: 0.0279\nEpoch: 003 | Training Loss: 0.0235\nEpoch: 004 | Training Loss: 0.0209\nEpoch: 005 | Training Loss: 0.0194\nEpoch: 006 | Training Loss: 0.0186\nEpoch: 007 | Training Loss: 0.0180\nEpoch: 008 | Training Loss: 0.0176\nEpoch: 009 | Training Loss: 0.0175\nEpoch: 010 | Training Loss: 0.0170\nTotal Training Time: 0.90 min\n\n*** Training AutoEncoder with latent_dim=20 ***\n\nEpoch: 001 | Training Loss: 0.0392\nEpoch: 002 | Training Loss: 0.0290\nEpoch: 003 | Training Loss: 0.0244\nEpoch: 004 | Training Loss: 0.0212\nEpoch: 005 | Training Loss: 0.0193\nEpoch: 006 | Training Loss: 0.0181\nEpoch: 007 | Training Loss: 0.0168\nEpoch: 008 | Training Loss: 0.0158\nEpoch: 009 | Training Loss: 0.0150\nEpoch: 010 | Training Loss: 0.0144\nTotal Training Time: 0.92 min\n\n*** Training AutoEncoder with latent_dim=50 ***\n\nEpoch: 001 | Training Loss: 0.0383\nEpoch: 002 | Training Loss: 0.0273\nEpoch: 003 | Training Loss: 0.0225\nEpoch: 004 | Training Loss: 0.0197\nEpoch: 005 | Training Loss: 0.0172\nEpoch: 006 | Training Loss: 0.0159\nEpoch: 007 | Training Loss: 0.0145\nEpoch: 008 | Training Loss: 0.0137\nEpoch: 009 | Training Loss: 0.0130\nEpoch: 010 | Training Loss: 0.0120\nTotal Training Time: 0.91 min\n\n\nVerify the latent space dimension of our models.\n\nfor model_n in models_list:\n    print(model_n.latent_dim)\n\n2\n5\n10\n20\n50\n\n\nNext, we will compare the results from these models. This time I have used the test dataset that our model has not seen. From the results, you may note that the results from the models with latent_dim 2 and 5 are blurry and make mistakes while decoding the input image. In comparison, the results from the rest are more accurate and of better quality.\nIt also tells us that we can significantly compress the images without losing much information. For example, our model_50 (model with latent_dim=50) can generate an original image with 16x less information. But this performance also depends on the type of images used. For example, we have black-and-white digit images with large white spaces, so we were able to compress them significantly. On the other hand, we may get different performances with images having more patterns.\n\nimg_sample_count = 25\n\nfor idx in range(img_sample_count):\n    # get an image from test_dataset\n    input_img, _ = test_dataset[idx]\n    # flatten the image\n    input_img = input_img.view(-1, num_features).to(device)\n\n    fig, ax = plt.subplots(1, len(models_list) + 1, figsize=(10, 4))\n\n    # iterate over all the models list\n    for i, model_n in enumerate(models_list):\n        # put the model in eval mode to stop accumulating gradients\n        model_n.eval()\n        # make a prediction\n        decoded_img = model_n(input_img)\n        # squach the prediction between [0,1]\n        decoded_img = torch.sigmoid(decoded_img)\n        # detach the prediction and convert back to 28,28 shape\n        decoded_img = decoded_img.detach().to(torch.device(\"cpu\"))\n        decoded_img = decoded_img.view(28, 28)\n\n        ax[i + 1].set_title(f\"prediction\\nlatent-dim:{model_n.latent_dim}\")\n        ax[i + 1].imshow(decoded_img, cmap=\"binary\")\n\n    # plot the input image\n    ax[0].set_title(f\"input: {idx}\")\n    input_img = input_img.detach().to(torch.device(\"cpu\"))\n    input_img = input_img.view(28, 28)\n    ax[0].imshow(input_img, cmap=\"binary\")\n\n    plt.tight_layout()\n    plt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFrom model_50, we are getting considerably good results compared to model_2 and model_5. The last time we visualized the embeddings for model_2, we also found that the latent space of digits was overlapping. Since the output quality of model_50 has improved, the overlap in the latent space should be less. Let’s confirm this assumption.\nFor this, we will first gather the embeddings of the test images. Remember that the output from the model.encoder is the embedding of an image (or compressed representation).\n\nlatent_vectors = []\nclasses = []\nmodel_50 = models_list[-1]\n\nprint(\"model_50 latent_dim: \", model_50.latent_dim)\n\nmodel_50 latent_dim:  50\n\n\nIn the next cell, I am collecting the embeddings. Note that each fetch from the DataLoader iterator returns a batch of images (batch_size=256).\n\nfor images_batch,labels_batch in test_loader:\n    # images_batch.shape = torch.Size([256, 1, 28, 28]\n    # labels_batch.shape = torch.Size([256])\n    # images_batch.view(-1, num_features).shape = torch.Size([256, 784])\n    # encoded_batch.shape =torch.Size([256, 50])\n\n    images_batch = images_batch.view(-1, num_features).to(device)\n    encoded_batch = model_50.encoder(images_batch)\n\n    latent_vectors.append(encoded_batch)\n    classes.extend(labels_batch)    \n\nLet’s confirm the dimensions.\n\nlen(latent_vectors) = 40. It means we have 40 latent vectors. Each vector holds the embeddings for a batch of images\nlatent_vectors[0].shape = torch.Size([256, 50]. It means that each latent vector holds embeddings of size 50 for a batch of 256 images\ntorch.cat(latent_vectors).shape = torch.Size([10000, 50]). It means that if we concatenate the embeddings of all the images, we will have a tensor of 10,000 rows with 50 dimensions (or features/columns)\n\n\nlen(latent_vectors), latent_vectors[0].shape, torch.cat(latent_vectors).shape\n\n(40, torch.Size([256, 50]), torch.Size([10000, 50]))\n\n\n\n# concatenate all test images embeddings\nlatent_vectors_cat = torch.cat(latent_vectors).cpu().detach().numpy()\nlatent_vectors_cat.shape\n\n(10000, 50)\n\n\nNow that we have all the embeddings, we can visualize them too. But how can we visualize embeddings with 50 features? We can use sklearn.manifold.TSNE.html.\nchatGPT prompt &gt; explain T-distributed Stochastic Neighbor Embedding\nT-distributed Stochastic Neighbor Embedding (t-SNE) is a machine learning algorithm that is used to reduce the dimensionality of high-dimensional data and visualize it in a lower-dimensional space. It works by measuring the similarity between the data points and representing them as points in a lower-dimensional space, such that similar data points are grouped together, and dissimilar ones are separated. This allows for better visualization and understanding of the structure of the data.\n\nfrom sklearn.manifold import TSNE\n\ntsne = TSNE(2)\nclustered = tsne.fit_transform(latent_vectors_cat)\n\nfig = plt.figure(figsize=(12, 10))\ncmap = plt.get_cmap(\"Spectral\", 10)\nplt.scatter(*zip(*clustered), c=classes, cmap=cmap)\nplt.colorbar(drawedges=True)\n\n/usr/local/lib/python3.8/dist-packages/sklearn/manifold/_t_sne.py:780: FutureWarning: The default initialization in TSNE will change from 'random' to 'pca' in 1.2.\n  warnings.warn(\n/usr/local/lib/python3.8/dist-packages/sklearn/manifold/_t_sne.py:790: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n  warnings.warn(\n\n\n\n\n\n\n\n\n\nWhat does this plot tell us?\n\nIt shows that using a latent space of higher dimension, there is less overlap in digits’ latent space\nIt also tells us that these digits’ distribution of latent space is discreet and has no boundary. Meaning that the range of values of these embeddings is not continuous\n\nembedding values for 0 share a range between 0 to 20\nembedding values for 1 share a range between -40 to -60\nthere can be areas in between that choosing a sample from it may result in a distorted image that is neither 1 or 0\n\n\nWe can prove this by adding some noise to these embeddings and trying to regenerate images from them.\n\n# step 1: transpose to get 50 embeddings with 10000 features\n# len(latent_vectors_transpose) = 50\n\nlatent_vectors_transpose = latent_vectors_cat.transpose(1, 0)\nlatent_vectors_cat.shape, latent_vectors_transpose.shape\n\n((10000, 50), (50, 10000))\n\n\n\n# step 2: add some noise.\n# take each row, calucate mean and std\n# use mean and std to generate 100 new features. each time add some noise to std\n\nrand_vectors = []  # randomized latent vectors\nfor col in latent_vectors_transpose:\n    mu, sigma = col.mean(), col.std()\n    rand_vectors.append(sigma * torch.randn(1, 100) + mu)\n\n\n# step 3: verify dimensions\n\nlen(rand_vectors), rand_vectors[0].shape, torch.cat(rand_vectors).shape\n\n(50, torch.Size([1, 100]), torch.Size([50, 100]))\n\n\n\n# step 4: concat 100 features\n\nrand_vectors_cat = torch.cat(rand_vectors)\nrand_vectors_cat.shape\n\ntorch.Size([50, 100])\n\n\n\n# step 5: transpose back to have 100 embeddings of dimension 50\n\nrand_vectors_transpose = rand_vectors_cat.transpose(1, 0)\nrand_vectors_cat.shape, rand_vectors_transpose.shape\n\n(torch.Size([50, 100]), torch.Size([100, 50]))\n\n\nWe have generated 100 new (random) embeddings by adding some noise to the original embeddings. We have done it in a way so that these embeddings do not represent any digit. They are like a mix of embeddings of all the numbers. So when we try to use them, they will generate distorted images. Similar to decoded images generated by taking a sample from an overlapping latent space.\n\nrand_vectors_transpose = rand_vectors_transpose.to(device)\n\nfig, ax = plt.subplots(10, 10, figsize=(20, 20))\nax = iter(ax.flat)\nfor idx, rand_latent_vector in enumerate(rand_vectors_transpose):\n    decoded_img = model_50.decoder(rand_latent_vector)\n    decoded_img = torch.sigmoid(decoded_img)\n    decoded_img = decoded_img.view(28, 28)\n    decoded_img = decoded_img.detach().to(torch.device(\"cpu\"))\n    ax[idx].imshow(decoded_img, cmap=\"binary\")\n\n\n\n\n\n\n\n\nWhat does this plot tell us? It shows that the latent space of the (vanilla) autoencoder is restricted to specific ranges and has discrete values. Therefore, we cannot use any value from latent space to generate an image. It can be a problem since it is difficult to know an autoencoder’s range of latent space beforehand, making them less useful for image generation.\nAn improvement on this is variational autoencoders (VAE), where the latent space is continuous and follows normal distribution making them more useful for image generation.\nchatGPT prompt &gt; compare the latent space of VAE with traditional autoencoder\nThe latent space of a VAE is continuous, whereas the latent space of a traditional autoencoder is typically discrete. This means that in a VAE, the latent representation of the data can take on any value in a continuous range, whereas in a traditional autoencoder, the latent representation is restricted to a set of discrete values.\nThis has several implications:\n\nA continuous latent space allows a VAE to capture more fine-grained variations in the data, which can be useful for tasks such as image generation.\nIt allows the VAE to produce more diverse outputs, which can be beneficial for tasks such as anomaly detection.\nIt makes the VAE more flexible and easier to train, since it can capture complex distributions in the data.\n\nOverall, the continuous latent space of a VAE is one of its key advantages over traditional autoencoders."
  },
  {
    "objectID": "posts/22022-11-19-pytorch-lstm-text-generation.html#credits",
    "href": "posts/22022-11-19-pytorch-lstm-text-generation.html#credits",
    "title": "Generating Text with Recurrent Neural Networks in PyTorch",
    "section": "Credits",
    "text": "Credits\nThis notebook takes inspiration and ideas from the following sources.\n\n“Machine learning with PyTorch and Scikit-Learn” by “Sebastian Raschka, Yuxi (Hayden) Liu, and Vahid Mirjalili”. You can get the book from its website: Machine learning with PyTorch and Scikit-Learn. In addition, the GitHub repository for this book has valuable notebooks: github.com/rasbt/machine-learning-book. Parts of the code you see in this notebook are taken from chapter 15 notebook of the same book.\n“Intro to Deep Learning and Generative Models Course” lecture series from “Sebastian Raschka”. Course website: stat453-ss2021. YouTube Link: Intro to Deep Learning and Generative Models Course. Lectures that are related to this post are L15.5 Long Short-Term Memory and L15.7 An RNN Sentiment Classifier in PyTorch\n“Andrej Karpathy” blog post The Unreasonable Effectiveness of Recurrent Neural Networks."
  },
  {
    "objectID": "posts/22022-11-19-pytorch-lstm-text-generation.html#environment",
    "href": "posts/22022-11-19-pytorch-lstm-text-generation.html#environment",
    "title": "Generating Text with Recurrent Neural Networks in PyTorch",
    "section": "Environment",
    "text": "Environment\nThis notebook is prepared with Google Colab.\n\nGitHub: 2022-11-19-pytorch-lstm-text-generation.ipynb\nOpen In Colab: \n\n\n\nCode\nfrom platform import python_version\nimport numpy, matplotlib, pandas, torch\n\nprint(\"python==\" + python_version())\nprint(\"numpy==\" + numpy.__version__)\nprint(\"torch==\" + torch.__version__)\nprint(\"matplotlib==\" + matplotlib.__version__)\n\n\npython==3.7.15\nnumpy==1.21.6\ntorch==1.12.1+cu113\nmatplotlib==3.2.2"
  },
  {
    "objectID": "posts/22022-11-19-pytorch-lstm-text-generation.html#introduction",
    "href": "posts/22022-11-19-pytorch-lstm-text-generation.html#introduction",
    "title": "Generating Text with Recurrent Neural Networks in PyTorch",
    "section": "Introduction",
    "text": "Introduction\nRecurrent Neural Network (RNN) works well for sequence problems, i.e., predicting the next sequence item. Stock prices, for example, are a type of sequence data more commonly known as time-series data. A similar notion can be applied to the NLP domain to build a character-level language model. Here language textual data becomes the sequence data, and from our model, we try to predict the next character in the input text. For training, the input text is broken into a sequence of characters and fed to the model one character at a time. The network will process the new character in relation to previously seen characters and use this information to predict the next alphabet."
  },
  {
    "objectID": "posts/22022-11-19-pytorch-lstm-text-generation.html#data-preparation",
    "href": "posts/22022-11-19-pytorch-lstm-text-generation.html#data-preparation",
    "title": "Generating Text with Recurrent Neural Networks in PyTorch",
    "section": "Data Preparation",
    "text": "Data Preparation\n\nDownload data\nFor input text, we will use a famous English folk story (though any other text will work equally well) with the name Cinderella. To download the story text, you may use Project Gutenberg site or Archive.org.\n\ndownload_link = \"https://ia600204.us.archive.org/30/items/cinderella10830gut/10830.txt\"\n\n## alternate download link\n# download_link = \"https://www.gutenberg.org/cache/epub/10830/pg10830.txt\"\n\nfile_name = 'input.txt'\n\n\n##\n# download the story text and save it as {file_name}\n! curl {download_link} -o {file_name}\n\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n100 45278  100 45278    0     0  38865      0  0:00:01  0:00:01 --:--:-- 38831\n\n\nThe download is complete. We can now open the file and read its contents.\n\n##\n# Reading and processing text\nwith open(file_name, \"r\", encoding=\"utf8\") as fp:\n    text = fp.read()\n\n\n\nPreprocess data\nThe downloaded text has been published as a volunteer effort under Project Gutenberg. They have added some project and license information after the original story text as part of the project requirements. We are not interested in that text (boilerplate text), so let’s omit that and limit our input text to the folk story.\n\n##\n# truncate text till story start and end\nstart_indx = text.find(\n    \"There once lived a gentleman and his wife, who were the parents of a\\nlovely little daughter.\"\n)\nend_indx = text.find(\"*       *       *       *       *\")\n\ntext = text[start_indx:end_indx]\n\n# total length of the text\nprint(\"Total Length (character count):\", len(text))\n\nTotal Length (character count): 21831\n\n\n\n\nHow does the data look?\nLet’s view the first 500 characters from the story text.\n\n# view the text start\ntext[:500]\n\n'There once lived a gentleman and his wife, who were the parents of a\\nlovely little daughter.\\n\\nWhen this child was only nine years of age, her mother fell sick.\\nFinding her death coming on, she called her child to her and said to\\nher, \"My child, always be good; bear every thing that happens to you\\nwith patience, and whatever evil and troubles you may suffer, you will\\nbe happy in the end if you are so.\" Then the poor lady died, and her\\ndaughter was full of great grief at the loss of a mother so go'\n\n\nAnd the last 500 characters.\n\n# view the text end\ntext[-500:]\n\n'their affection.\\nShe was then taken to the palace of the young prince, in whose eyes she\\nappeared yet more lovely than before, and who married her shortly after.\\n\\nCinderella, who was as good as she was beautiful, allowed her sisters to\\nlodge in the palace, and gave them in marriage, that same day, to two\\nlords belonging to the court.\\n\\n[Illustration: MARRIAGE OF THE PRINCE AND CINDERELLA.]\\n\\nThe amiable qualities of Cinderella were as conspicuous after as they\\nhad been before marriage.\\n\\n\\n\\n\\n       '\n\n\n\n\nPreparing data dictionary\nOur data is a string and can’t be used to train a model. So instead, we have to convert it into integers. For this encoding, we will use a simple methodology where each unique character in the text is assigned an integer and then replaced with all occurrences of that character in the text with that integer value.\nFor this, let’s first create a set of all the unique characters in the text.\n\nimport numpy as np\n\n# find unique chars from text\nchar_set = set(text)\nprint(\"Unique Characters:\", len(char_set))\n\n# sort char set\nchars_sorted = sorted(char_set)\nprint(chars_sorted)\n\nUnique Characters: 65\n['\\n', ' ', '!', '\"', \"'\", ',', '-', '.', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'Y', 'Z', '[', ']', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n\n\nWe now know all the unique characters in our input text. Accordingly, we can create a dictionary and assign each character in char_set a unique integer.\n\n# encode chars\nchar2int = {ch: i for i, ch in enumerate(chars_sorted)}\n\n# `char2int` dictionary for char -&gt; int\nprint(char2int)\n\n{'\\n': 0, ' ': 1, '!': 2, '\"': 3, \"'\": 4, ',': 5, '-': 6, '.': 7, ':': 8, ';': 9, '?': 10, 'A': 11, 'B': 12, 'C': 13, 'D': 14, 'E': 15, 'F': 16, 'G': 17, 'H': 18, 'I': 19, 'J': 20, 'K': 21, 'L': 22, 'M': 23, 'N': 24, 'O': 25, 'P': 26, 'Q': 27, 'R': 28, 'S': 29, 'T': 30, 'U': 31, 'V': 32, 'W': 33, 'Y': 34, 'Z': 35, '[': 36, ']': 37, '_': 38, 'a': 39, 'b': 40, 'c': 41, 'd': 42, 'e': 43, 'f': 44, 'g': 45, 'h': 46, 'i': 47, 'j': 48, 'k': 49, 'l': 50, 'm': 51, 'n': 52, 'o': 53, 'p': 54, 'q': 55, 'r': 56, 's': 57, 't': 58, 'u': 59, 'v': 60, 'w': 61, 'x': 62, 'y': 63, 'z': 64}\n\n\nBut more than just the encoding, we also need a way to convert the encoded characters back to the original form. For this, we will use a separate array that will hold the index of each char in the dictionary. Together with char2int and int2char we can move back and forth between encoded and decoded characters.\n\nint2char = np.array(chars_sorted)\n\n# `int2char` for int -&gt; char\nprint(int2char)\n\n['\\n' ' ' '!' '\"' \"'\" ',' '-' '.' ':' ';' '?' 'A' 'B' 'C' 'D' 'E' 'F' 'G'\n 'H' 'I' 'J' 'K' 'L' 'M' 'N' 'O' 'P' 'Q' 'R' 'S' 'T' 'U' 'V' 'W' 'Y' 'Z'\n '[' ']' '_' 'a' 'b' 'c' 'd' 'e' 'f' 'g' 'h' 'i' 'j' 'k' 'l' 'm' 'n' 'o'\n 'p' 'q' 'r' 's' 't' 'u' 'v' 'w' 'x' 'y' 'z']\n\n\n\n\nEncode input text\nIn this step, we will use the char2int dictionary to encode our story text. The encoded version of text is called text_encoded.\n\n##\n# encode original text\ntext_encoded = np.array([char2int[ch] for ch in text], dtype=np.int32)\n\nprint(\"Text encoded shape: \", text_encoded.shape)\n\nText encoded shape:  (21831,)\n\n\nLet’s use int2char to decode and return the original text.\n\n##\n# decoding original text\nfor ex in text_encoded[:5]:\n    print(\"{} -&gt; {}\".format(ex, int2char[ex]))\n\n30 -&gt; T\n46 -&gt; h\n43 -&gt; e\n56 -&gt; r\n43 -&gt; e\n\n\nAnother example of encoding and decoding. This time I used multiple words together.\n\nprint(text[:18], \"     == Encoding ==&gt; \", text_encoded[:18])\nprint(text_encoded[19:41], \" == Reverse  ==&gt; \", \"\".join(int2char[text_encoded[19:41]]))\n\nThere once lived a      == Encoding ==&gt;  [30 46 43 56 43  1 53 52 41 43  1 50 47 60 43 42  1 39]\n[45 43 52 58 50 43 51 39 52  1 39 52 42  1 46 47 57  1 61 47 44 43]  == Reverse  ==&gt;  gentleman and his wife\n\n\n\n\nPrepare data sequences\nWe have our encoded data ready. Next, we will convert it into sequences of fixed length. The last sequence element will act as a target, and the remaining elements will be the input. For sequencing, we will use length 41.\n\nThe first 40 characters in sequence form the input\nThe last character in sequence (41) represents the output\n\n\n##\n# make sequences of encoded text as `text_chunks`\nseq_length = 40\nchunk_size = seq_length + 1\n\ntext_chunks = [\n    text_encoded[i : i + chunk_size] for i in range(len(text_encoded) - chunk_size + 1)\n]\n\n\n##\n# inspect the first chuck\nfor seq in text_chunks[:1]:\n    input_seq = seq[:-1]\n    target = seq[-1]\n\n    print(input_seq, \" -&gt; \", target)\n    print(repr(\"\".join(int2char[input_seq])), \" -&gt; \", repr(\"\".join(int2char[target])))\n\n[30 46 43 56 43  1 53 52 41 43  1 50 47 60 43 42  1 39  1 45 43 52 58 50\n 43 51 39 52  1 39 52 42  1 46 47 57  1 61 47 44]  -&gt;  43\n'There once lived a gentleman and his wif'  -&gt;  'e'\n\n\n\n##\n# inspect the second chuck\nfor seq in text_chunks[1:2]:\n    input_seq = seq[:-1]\n    target = seq[-1]\n\n    print(input_seq, \" -&gt; \", target)\n    print(repr(\"\".join(int2char[input_seq])), \" -&gt; \", repr(\"\".join(int2char[target])))\n\n[46 43 56 43  1 53 52 41 43  1 50 47 60 43 42  1 39  1 45 43 52 58 50 43\n 51 39 52  1 39 52 42  1 46 47 57  1 61 47 44 43]  -&gt;  5\n'here once lived a gentleman and his wife'  -&gt;  ','"
  },
  {
    "objectID": "posts/22022-11-19-pytorch-lstm-text-generation.html#load-data-into-dataset-and-dataloader-class",
    "href": "posts/22022-11-19-pytorch-lstm-text-generation.html#load-data-into-dataset-and-dataloader-class",
    "title": "Generating Text with Recurrent Neural Networks in PyTorch",
    "section": "Load Data into Dataset and DataLoader class",
    "text": "Load Data into Dataset and DataLoader class\nIn this section, we will load our encoded data sequences into Dataset and DataLoader class to prepare batches for model training.\n\nLoad data into Dataset class\nclass TextDataset is derived from PyTorch Dataset. When we get a sequence using this class, it will return the sequence as a tuple of input and target.\n\n\nCode\nimport torch\nfrom torch.utils.data import Dataset\n\n\nclass TextDataset(Dataset):\n    def __init__(self, text_chunks):\n        self.text_chunks = text_chunks\n\n    def __len__(self):\n        return len(self.text_chunks)\n\n    def __getitem__(self, idx):\n        text_chunk = self.text_chunks[idx]\n        return text_chunk[:-1].long(), text_chunk[1:].long()  # return input, target\n\n\nseq_dataset = TextDataset(torch.tensor(text_chunks))\n\n\n/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:15: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:201.)\n  from ipykernel import kernelapp as app\n\n\nEach element from the seq_dataset consists of\n\ninput data that we will feed to the model for training\ntarget data that we will use to compare the model output\n\nRemember that both input and target sequences are derived from the same encoded text. We train our model to predict the next character from the given input. One character is given as an input to the model, and one character output comes out of the model. In an ideal case, the model output character should represent the next character in a sequence. And our target sequence is just that: one next character from the input sequence.\n\nfor i, (seq, target) in enumerate(seq_dataset):\n    print(\" Input (x):\", repr(\"\".join(int2char[seq])))\n    print(\"Target (y):\", repr(\"\".join(int2char[target])))\n    print()\n    if i == 1:\n        break\n\n Input (x): 'There once lived a gentleman and his wif'\nTarget (y): 'here once lived a gentleman and his wife'\n\n Input (x): 'here once lived a gentleman and his wife'\nTarget (y): 'ere once lived a gentleman and his wife,'\n\n\n\n\n\nLoad data into DataLoader class to prepare batches\nIn this step, we have prepared training batches using the PyTorch DataLoader class.\n\nfrom torch.utils.data import DataLoader\n\nbatch_size = 64\n\ntorch.manual_seed(1)\nseq_dl = DataLoader(seq_dataset, batch_size=batch_size, shuffle=True, drop_last=True)"
  },
  {
    "objectID": "posts/22022-11-19-pytorch-lstm-text-generation.html#model-configuration-and-training",
    "href": "posts/22022-11-19-pytorch-lstm-text-generation.html#model-configuration-and-training",
    "title": "Generating Text with Recurrent Neural Networks in PyTorch",
    "section": "Model Configuration and Training",
    "text": "Model Configuration and Training\nIn this section, we will configure a model for character-level language modeling. This model will have an Embedding layer at the start. Next, output from the embedding layer will be passed to the LSTM layer. Finally, at the output, we have a fully connected linear layer.\nFor an in-depth analysis of the working of an Embedding layer, I recommend this article Embeddings in Machine Learning: Everything You Need to Know\n\n\nCode\nimport torch.nn as nn\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\nclass RNN(nn.Module):\n    def __init__(self, vocab_size, embed_dim, rnn_hidden_size):\n        super().__init__()\n        self.embedding = nn.Embedding(vocab_size, embed_dim)\n        self.rnn_hidden_size = rnn_hidden_size\n        self.rnn = nn.LSTM(embed_dim, rnn_hidden_size, batch_first=True)\n        self.fc = nn.Linear(rnn_hidden_size, vocab_size)\n\n    def forward(self, x, hidden, cell):\n        out = self.embedding(x).unsqueeze(1)\n        out, (hidden, cell) = self.rnn(out, (hidden, cell))\n        out = self.fc(out).reshape(out.size(0), -1)\n        return out, hidden, cell\n\n    def init_hidden(self, batch_size):\n        hidden = torch.zeros(1, batch_size, self.rnn_hidden_size)\n        cell = torch.zeros(1, batch_size, self.rnn_hidden_size)\n        return hidden.to(device), cell.to(device)\n\n\n\ntorch.manual_seed(1)\n\n# define model dimensions\nvocab_size = len(int2char)\nembed_dim = 256\nrnn_hidden_size = 512\n\n# initialize model\nmodel = RNN(vocab_size, embed_dim, rnn_hidden_size)\nmodel = model.to(device)\nmodel\n\nRNN(\n  (embedding): Embedding(65, 256)\n  (rnn): LSTM(256, 512, batch_first=True)\n  (fc): Linear(in_features=512, out_features=65, bias=True)\n)\n\n\n\nConfigure loss function and optimizer\n\nFor the loss function, we will use CrossEntropyLoss. This is because we are dealing with a classification problem, and our model has to predict the next character from vocab_size of 65 classes.\nFor optimization, we will use torch.optim.Adam\n\n\nloss_fn = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.005)\n\n\n\nModel training\nAll parts are ready so let’s start the training. Google Colab “CPU” runtime can take significantly longer to train. I would suggest using “GPU” runtime instead.\n\n\nCode\n# for execution time measurement\nfrom timeit import default_timer as timer\n\nnum_epochs = 10000\nmodel.train()\n\nstart = timer()  # timer start\nfor epoch in range(num_epochs):\n    hidden, cell = model.init_hidden(batch_size)\n\n    seq_batch, target_batch = next(iter(seq_dl))\n    seq_batch = seq_batch.to(device)\n    target_batch = target_batch.to(device)\n\n    optimizer.zero_grad()\n    loss = 0\n\n    for c in range(seq_length):\n        pred, hidden, cell = model(seq_batch[:, c], hidden, cell)\n        loss += loss_fn(pred, target_batch[:, c])\n\n    loss.backward()\n    optimizer.step()\n\n    loss = loss.item() / seq_length\n    if epoch % 500 == 0:\n        print(f\"Epoch {epoch} loss: {loss:.4f}\")\n\nend = timer()  # timer end\nprint(\"Total execution time in seconds: \", \"%.2f\" % (end - start))\nprint(\"Device type: \", device)\n\n\nEpoch 0 loss: 2.6252\nEpoch 500 loss: 0.3377\nEpoch 1000 loss: 0.2502\nEpoch 1500 loss: 0.2403\nEpoch 2000 loss: 0.2501\nEpoch 2500 loss: 0.2374\nEpoch 3000 loss: 0.2368\nEpoch 3500 loss: 0.2499\nEpoch 4000 loss: 0.2643\nEpoch 4500 loss: 0.2555\nEpoch 5000 loss: 0.3854\nEpoch 5500 loss: 0.2326\nEpoch 6000 loss: 0.2390\nEpoch 6500 loss: 0.2270\nEpoch 7000 loss: 0.2663\nEpoch 7500 loss: 0.3403\nEpoch 8000 loss: 0.2475\nEpoch 8500 loss: 0.2370\nEpoch 9000 loss: 0.2126\nEpoch 9500 loss: 0.2308\nTotal execution time in seconds:  378.14\nDevice type:  cuda"
  },
  {
    "objectID": "posts/22022-11-19-pytorch-lstm-text-generation.html#process-output-from-the-model",
    "href": "posts/22022-11-19-pytorch-lstm-text-generation.html#process-output-from-the-model",
    "title": "Generating Text with Recurrent Neural Networks in PyTorch",
    "section": "Process output from the model",
    "text": "Process output from the model\nGetting a prediction (text generation) from the model takes some extra work. Since the model is trained on encoded text, the output generated from the model is also encoded. Further, any input used for prediction itself needs to be encoded using the same encoding dictionary model it is trained with. For this, we have defined a helper function.\n\nThis function will take the input text and encode it before passing it to the model\nIt will take the output from the model and decode it before returning\nNote that LSTM model output has logits, hidden state, and cell state . Logits give us the next predicted character. Hidden state and cell state are for keeping the context (or memory) of characters processed so far and are supplied to the model for the next prediction.\nFor the output logits, we can predict the next character using the index of the highest logit value. This will make our model predict the exact text on the same input each time. To introduce some randomness, we take help from PyTorch class torch.distributions.categorical.Categorical. This is how it works\n\nWe obtain output probabilities by applying softmax to logits and pass them to a Categorical object to create a distribution.\nGenerate a sample from a Categorical object. Samples generated from the same distribution may be different. This way, we get different outputs with the same input text.\nThis way, we can also control the predictability of the model output by controlling the probability distribution (calculated from logits) passed to the Categorical object. If we can make probabilities a lot more similar (through scaling), the sample generated by Categorical will also be mostly the same. On the other hand, if we can make the probabilities further apart, then we can also increase the randomness of the output from the Categorical class.\n\n\n\n\nCode\nfrom torch.distributions.categorical import Categorical\n\ndef sample(model, starting_str, len_generated_text=500, scale_factor=1.0):\n\n    encoded_input = torch.tensor([char2int[s] for s in starting_str])\n    encoded_input = torch.reshape(encoded_input, (1, -1))\n\n    generated_str = starting_str\n\n    model.eval()\n    hidden, cell = model.init_hidden(1)\n    hidden = hidden.to(\"cpu\")\n    cell = cell.to(\"cpu\")\n    for c in range(len(starting_str) - 1):\n        _, hidden, cell = model(encoded_input[:, c].view(1), hidden, cell)\n\n    last_char = encoded_input[:, -1]\n    for i in range(len_generated_text):\n        logits, hidden, cell = model(last_char.view(1), hidden, cell)\n        logits = torch.squeeze(logits, 0)\n        scaled_logits = logits * scale_factor\n        m = Categorical(logits=scaled_logits)\n        last_char = m.sample()\n        generated_str += str(int2char[last_char])\n\n    return generated_str"
  },
  {
    "objectID": "posts/22022-11-19-pytorch-lstm-text-generation.html#generating-new-text-passages",
    "href": "posts/22022-11-19-pytorch-lstm-text-generation.html#generating-new-text-passages",
    "title": "Generating Text with Recurrent Neural Networks in PyTorch",
    "section": "Generating new text passages",
    "text": "Generating new text passages\nWe are processing text and model output on the ‘CPU’ device in the ‘sample’ function. So let’s also move the model to the same device.\n\n##\n# move model to cpu\nmodel.to('cpu')\n\nRNN(\n  (embedding): Embedding(65, 256)\n  (rnn): LSTM(256, 512, batch_first=True)\n  (fc): Linear(in_features=512, out_features=65, bias=True)\n)\n\n\nBefore generating some lengthy text, let’s experiment with simple words and see if our model can complete them.\nAt first, I used the string “fat” and asked the model to generate the following three characters to complete this word. But at the same time, I have passed a tiny scaling factor meaning I have decreased the model’s predictability.\n\nprint(sample(model, starting_str=\"fat\", len_generated_text=3, scale_factor=0.1))\n\nfat, i\n\n\nNext, I asked the model to use the same input and predict the following three characters, but I increased the model’s predictability ten times. So let’s see the output this time.\n\nprint(sample(model, starting_str='fat', len_generated_text=3, scale_factor=1.0))\n\nfather\n\n\nThe second time model generated the correct word “father” it had seen before in the training text. So let’s now generate some lengthy texts.\n\n##\n# text generation example 1\nprint(sample(model, starting_str=\"The father\"))\n\nThe father too was she was one of those good faeries who protect children. Her\nspirits revived, and she wiped away her tears.\n\nThe faery took Cinderella by the hand, and old woman, assuming her character of Queen of the\nFaeries, that only jumped up behind the\ncarriage as nimbly as if they had been footmen and laced so tight, touched Cinderella's clothes with her wand, and said, \"Now, my dear good child,\" said the faery, \"here you have a coach and\nhorses, much handsomer than your sisters', to say the least\n\n\n\n##\n# text generation example 2\nprint(sample(model, starting_str=\"The mother\"))\n\nThe mother so good crust. But\nif you like to give the household. It was she who washed the dishes, and\nscrubbed down the step-sisters were very cruel to Cinderella,\nthat he did not eat one morsel of the supper.\n\nCinderella drew the fellow slipper\nout of her godmother\nwould do with it. Her godmother took the pumpkin, and scooped out the\ninside of it, leaving nothing but rind; she then struck it with her\ngodmother then said, \"My dear Cinderella,\nthat he did not eat one morsel of the supper.\n\nCinderella drew\n\n\n\n##\n# text generation example 3\nprint(sample(model, starting_str=\"The three sisters\"))\n\nThe three sisters were very cruel to Cinderella,\nthat he delicacies which she had\nreceived from the prince:  but they did not eat one morsel for a\ncouple of days. They spent their whole time before a looking-glass, and\nthey would be laced so tight, tossing her head disdainfully, \"that I\nshould lend my clothes to a dirty Cinderella like you!\"\n\nCinderella quite amazed; but their\nastonishment at her dancing was still greater.\n\nGracefulness seemed to play in the attempt.\n\nThe long-wished-for evening came at last, an\n\n\n\n##\n# text generation example 4\nprint(sample(model, starting_str=\"The lovely prince\"))\n\nThe lovely prince\nimmediately jumped up behind the\ncarriage as nimbly as conspicuous after as they\nhad been before mocking me,\" replied the poor girl to do all the\ndrudgery of the household. It was she who washed the dishes, and\nscrubbed down the stairs, who tried with all their might to force their unwould stration: CINDERELLA IS PRESENTED BY THE PRINCE TO THE KING AND\nQUEEN, WHO WELCOME HER WITH THE HONORS DUE TO A GREAT PRINCESS, AND IS\nTHEN LED INTO THE ROYAL BY THE HER WITH THE HONORS DUE TO A GREAT PRINCES"
  },
  {
    "objectID": "posts/2022-11-07-timeseries-rnn-gru-lstm-cnn-pytorch.html#credits",
    "href": "posts/2022-11-07-timeseries-rnn-gru-lstm-cnn-pytorch.html#credits",
    "title": "Build Temporal Models for Univariate Time series Data with RNN, GRU, LSTM, CNN using PyTorch",
    "section": "Credits",
    "text": "Credits\nThis notebook takes inspiration and ideas from the following sources.\n\nThe outstanding book “Deep Learning with PyTorch Step-by-Step” by “Daniel Voigt Godoy”. You can get the book from its website: pytorchstepbystep. In addition, the GitHub repository for this book has valuable notebooks: github.com/dvgodoy/PyTorchStepByStep. Parts of the code you see in this notebook are taken from chapter 3 and chapter 8 notebooks of the same book.\nVery helpful Kaggle notebook from ‘TARON ZAKARYAN’ to predict stock prices using LSTM. Link here"
  },
  {
    "objectID": "posts/2022-11-07-timeseries-rnn-gru-lstm-cnn-pytorch.html#environment",
    "href": "posts/2022-11-07-timeseries-rnn-gru-lstm-cnn-pytorch.html#environment",
    "title": "Build Temporal Models for Univariate Time series Data with RNN, GRU, LSTM, CNN using PyTorch",
    "section": "Environment",
    "text": "Environment\nThis notebook is prepared with Google Colab.\n\nGitHub: 2022-11-07-timeseries-rnn-gru-lstm-cnn-pytorch.ipynb\nOpen In Colab: \n\n\n\nCode\nfrom platform import python_version\nimport numpy, matplotlib, pandas, torch, seaborn\n\nprint(\"python==\" + python_version())\nprint(\"numpy==\" + numpy.__version__)\nprint(\"torch==\" + torch.__version__)\nprint(\"matplotlib==\" + matplotlib.__version__)\nprint(\"seaborn==\" + seaborn.__version__)\n\n\npython==3.7.15\nnumpy==1.21.6\ntorch==1.12.1+cu113\nmatplotlib==3.2.2\nseaborn==0.11.2"
  },
  {
    "objectID": "posts/2022-11-07-timeseries-rnn-gru-lstm-cnn-pytorch.html#introduction",
    "href": "posts/2022-11-07-timeseries-rnn-gru-lstm-cnn-pytorch.html#introduction",
    "title": "Build Temporal Models for Univariate Time series Data with RNN, GRU, LSTM, CNN using PyTorch",
    "section": "Introduction",
    "text": "Introduction\nRecurrent Neural Network (RNN) is great for exploiting data that involves one-dimensional (1D) ordered structures. We call these 1D-ordered structures sequences. Two main sequence problems are Time series and Natural Language Processing (NLP). RNN and its variants are developed to work for both types of sequence problems, but in this notebook we will only deal with time series sequences.\nI have divided this notebook into two sections. In the first section, our focus will be on understanding the structure of sequences and generating training sets and batches from them. We will develop a simple (synthetic) sequence data and then create its training set. Next, we will make batches using PyTorch DataLoaders and write a training pipeline. We will end this section by training an RNN on this data.\nIn the next section, our focus will be more on the internals of different neural architectures for sequence data problems. We will use stock price data and train multiple networks (RNN, GRU, LSTM, CNN) on it while understanding their features and behavior."
  },
  {
    "objectID": "posts/2022-11-07-timeseries-rnn-gru-lstm-cnn-pytorch.html#data-generation",
    "href": "posts/2022-11-07-timeseries-rnn-gru-lstm-cnn-pytorch.html#data-generation",
    "title": "Build Temporal Models for Univariate Time series Data with RNN, GRU, LSTM, CNN using PyTorch",
    "section": "Data generation",
    "text": "Data generation\nLet’s generate some one dimensional ordered sequence data.\n\n# from numpy.ma.core import size\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n# generate 1000 data points\nn_points = 1000\nnoise = 0.04\n\nX_synth = np.arange(1, n_points + 1, 1)\ny_synth = np.sin(X_synth * np.pi / 180) + np.random.randn(n_points) * noise\n\ndf_synth = pd.DataFrame(y_synth, index=X_synth)\n\n# plot timeseries data\ndf_synth.plot(figsize=(15, 6))\nplt.ylabel(\"value\")\nplt.xlabel(\"step\")\nplt.title(\"Synthetic time series data\")\nplt.show()\n\n\n\n\n\n\n\n\nIn the above plot, X dimension represents the time or steps. And y dimension represents the measurements. In actual data, these measurements could represent price stocks, temperature, population, etc. If we print our DataFrame, it has only one column which shows the measurements. The DataFrame index represents the time dimension.\n\ndf_synth.head()\n\n\n    \n      \n\n\n\n\n\n\n0\n\n\n\n\n1\n0.031713\n\n\n2\n-0.000675\n\n\n3\n0.026890\n\n\n4\n0.087844\n\n\n5\n0.057978"
  },
  {
    "objectID": "posts/2022-11-07-timeseries-rnn-gru-lstm-cnn-pytorch.html#data-preparation",
    "href": "posts/2022-11-07-timeseries-rnn-gru-lstm-cnn-pytorch.html#data-preparation",
    "title": "Build Temporal Models for Univariate Time series Data with RNN, GRU, LSTM, CNN using PyTorch",
    "section": "Data preparation",
    "text": "Data preparation\nNotice that our data does not have any labels. We usually have features and labels to train our model in supervised learning problems. However, sequence data is unique as we try to predict the next value from the sequence data itself. Therefore, we don’t have to provide labels with our data separately but can generate them from the sequence itself.\nLet’s use a simple ordered sequence of 15 integers to understand how the training set is created from it.\n\n##\n# generate a simple sequential data of 15 integers\ndata_dummy = np.arange(15)\nprint(data_dummy)\n\n# create a DataFrame of this sequence\ndf_dummy = pd.DataFrame(data_dummy)\n\n[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14]\n\n\nWe can split this long sequence into multiple smaller sequences (as training and test sets). The earlier part of the sequence will contain training features, and the last element acts as a label.\nI have created a helper function that will take a sequence DataFrame, and split it into training features and labels.\n\n##\n# create a function to generate multiple sequences for training and testing\n# look_back = size of the generated sets\ndef generate_sequences(df, test_size=0.3, look_back=10):\n    data = []\n    df_raw = df.values\n\n    for index in range(len(df_raw) - look_back):\n        data.append(df_raw[index : index + look_back])\n\n    data = np.array(data)\n\n    test_set_size = int(np.round(test_size * data.shape[0]))\n    train_set_size = data.shape[0] - (test_set_size)\n\n    x_train = data[:train_set_size, :-1, :]\n    y_train = data[:train_set_size, -1, :]\n    x_test = data[train_set_size:, :-1]\n    y_test = data[train_set_size:, -1, :]\n\n    return [x_train, y_train, x_test, y_test]\n\nLet’s apply this function to our sequence and check the output.\n\n##\n# generate test and train sequences\n# x = features\n# y = labels\nx_train_dummy, y_train_dummy, x_test_dummy, y_test_dummy = generate_sequences(df_dummy)\n\n# view the training data. features and labels together\n# feature 't' = labels\ndf_train_dummy = pd.DataFrame(np.squeeze(x_train_dummy))\ndf_train_dummy[\"t\"] = np.squeeze(y_train_dummy)\ndf_train_dummy.head(10)\n\n\n    \n      \n\n\n\n\n\n\n0\n1\n2\n3\n4\n5\n6\n7\n8\nt\n\n\n\n\n0\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n1\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n2\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n\n\nNotice that our training set has smaller sequences, with the last element acting as a label denoted by column ‘t’. This is because our generate_sequences function acts as a moving window where earlier elements become features and the last element in the window acts as a label.\nLet’s also check the generated testing set.\n\n##\n# view the testing data. features and labels together\n# feature 't' = labels\ndf_test_dummy = pd.DataFrame(np.squeeze(x_test_dummy))\ndf_test_dummy[\"t\"] = np.squeeze(y_test_dummy)\ndf_test_dummy.head(10)\n\n\n    \n      \n\n\n\n\n\n\n0\n1\n2\n3\n4\n5\n6\n7\n8\nt\n\n\n\n\n0\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\n1\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n\n\nFrom this sequence example, we have learned that we can generate training and test sets of different sizes using the same sequence data. The features and label aren’t provided separately but can be produced by splitting the sequence data into smaller chunks. The last element in the chunks acts as the label.\nLet’s apply this understanding to our synthetic data and generate training and test samples.\n\n##\n# generate training and test data for synthetic sequence data\nx_train_synth, y_train_synth, x_test_synth, y_test_synth = generate_sequences(df_synth)"
  },
  {
    "objectID": "posts/2022-11-07-timeseries-rnn-gru-lstm-cnn-pytorch.html#load-generated-data-into-pytorch-dataset-and-dataloader-class",
    "href": "posts/2022-11-07-timeseries-rnn-gru-lstm-cnn-pytorch.html#load-generated-data-into-pytorch-dataset-and-dataloader-class",
    "title": "Build Temporal Models for Univariate Time series Data with RNN, GRU, LSTM, CNN using PyTorch",
    "section": "Load generated data into PyTorch Dataset and DataLoader class",
    "text": "Load generated data into PyTorch Dataset and DataLoader class\nNow let’s load our data into Dataset and DataLoader classes. PyTorch Dataset is a helper class that converts data and labels into a list of tuples. DataLoader is another helper class to create batches from Dataset tuples. batch_size means the number of tuples we want in a single batch. We have used 16 here, so each fetch from DataLoader will give us a list of 16 tuples.\n\nimport torch\nfrom torch.utils.data import DataLoader, TensorDataset\n\ntrain_dataset_synth = TensorDataset(\n    torch.as_tensor(x_train_synth).float(), torch.as_tensor(y_train_synth).float()\n)\ntest_dataset_synth = TensorDataset(\n    torch.as_tensor(x_test_synth).float(), torch.as_tensor(y_test_synth).float()\n)\n\nbatch_size = 16\n\ntrain_loader_synth = DataLoader(\n    train_dataset_synth, batch_size=batch_size, shuffle=True\n)\ntest_loader_synth = DataLoader(test_dataset_synth, batch_size=batch_size)"
  },
  {
    "objectID": "posts/2022-11-07-timeseries-rnn-gru-lstm-cnn-pytorch.html#define-a-class-to-implement-training-validation-and-mini-batch-processing-pipeline",
    "href": "posts/2022-11-07-timeseries-rnn-gru-lstm-cnn-pytorch.html#define-a-class-to-implement-training-validation-and-mini-batch-processing-pipeline",
    "title": "Build Temporal Models for Univariate Time series Data with RNN, GRU, LSTM, CNN using PyTorch",
    "section": "Define a class to implement training, validation, and mini-batch processing pipeline",
    "text": "Define a class to implement training, validation, and mini-batch processing pipeline\nIn this section we will implement a class that encapsulates all the usual steps required in training a PyTorch model. This way we can focus more on the model architecture and performance, and less concerned about the boilerplate training loop. Important parts of this class are\n\n__init__: Class constructor to define the main actors in a training cycle including model, optimizer, loss function, training and validation DataLoaders\n_make_train_step_fn: Training pipeline is usually called “training step” which includes the following steps\n\nCompute our model’s predicted output - the forward pass\nCompute the loss\nCompute gradients i.e., find the direction and scale to update the weights to reduce the loss\nUpdate weight parameters using gradients and the learning rate\n\n_make_val_step_fn: Validation pipeline is usually called the “validation step” which includes the following steps\n\nCompute our model’s predicted output - the forward pass\nCompute the loss\nNote that during validation, we are only concerned about the loss, i.e., how well our model performs on the validation dataset. Therefore, we don’t use it to calculate the gradients.\n\n_mini_batch: It defines the steps to process a single minibatch in a helper function. For a mini-batch processing, we want to\n\nGet the next batch of data and labels (x, y) from the DataLoader iterator\nPerform a step on the batch. A step can be either training or validation\nCompute the average batch loss\n\ntrain: Execute training and validation steps for given number of epoch\npredict: Make a prediction from model on provided data\n\n\n\nCode\nimport numpy as np\nimport datetime\n\nclass DeepLearningPipeline(object):\n    def __init__(self, model, loss_fn, optimizer):\n        # Here we define the attributes of our class\n\n        # We start by storing the arguments as attributes\n        # to use them later\n        self.model = model\n        self.loss_fn = loss_fn\n        self.optimizer = optimizer\n        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n        # Let's send the model to the specified device right away\n        self.model.to(self.device)\n\n        # These attributes are defined here, but since they are\n        # not informed at the moment of creation, we keep them None\n        self.train_loader = None\n        self.val_loader = None\n        self.writer = None\n\n        # These attributes are going to be computed internally\n        self.losses = []\n        self.val_losses = []\n        self.total_epochs = 0\n\n        # Creates the train_step function for our model,\n        # loss function and optimizer\n        # Note: there are NO ARGS there! It makes use of the class\n        # attributes directly\n        self.train_step_fn = self._make_train_step_fn()\n        # Creates the val_step function for our model and loss\n        self.val_step_fn = self._make_val_step_fn()\n\n    def set_loaders(self, train_loader, val_loader=None):\n        # This method allows the user to define which train_loader (and val_loader, optionally) to use\n        # Both loaders are then assigned to attributes of the class\n        # So they can be referred to later\n        self.train_loader = train_loader\n        self.val_loader = val_loader\n\n    def _make_train_step_fn(self):\n        # This method does not need ARGS... it can refer to\n        # the attributes: self.model, self.loss_fn and self.optimizer\n\n        # Builds function that performs a step in the train loop\n        def perform_train_step_fn(x, y):\n            # Sets model to TRAIN mode\n            self.model.train()\n\n            # Step 1 - Computes our model's predicted output - forward pass\n            yhat = self.model(x)\n            # Step 2 - Computes the loss\n            loss = self.loss_fn(yhat, y)\n            # Step 3 - Computes gradients for both \"a\" and \"b\" parameters\n            loss.backward()\n            # Step 4 - Updates parameters using gradients and the learning rate\n            self.optimizer.step()\n            self.optimizer.zero_grad()\n\n            # Returns the loss\n            return loss.item()\n\n        # Returns the function that will be called inside the train loop\n        return perform_train_step_fn\n\n    def _make_val_step_fn(self):\n        # Builds function that performs a step in the validation loop\n        def perform_val_step_fn(x, y):\n            # Sets model to EVAL mode\n            self.model.eval()\n\n            # Step 1 - Computes our model's predicted output - forward pass\n            yhat = self.model(x)\n            # Step 2 - Computes the loss\n            loss = self.loss_fn(yhat, y)\n            # There is no need to compute Steps 3 and 4,\n            # since we don't update parameters during evaluation\n            return loss.item()\n\n        return perform_val_step_fn\n\n    def _mini_batch(self, validation=False):\n        # The mini-batch can be used with both loaders\n        # The argument `validation`defines which loader and\n        # corresponding step function is going to be used\n        if validation:\n            data_loader = self.val_loader\n            step_fn = self.val_step_fn\n        else:\n            data_loader = self.train_loader\n            step_fn = self.train_step_fn\n\n        if data_loader is None:\n            return None\n\n        # Once the data loader and step function, this is the\n        # same mini-batch loop we had before\n        mini_batch_losses = []\n        for x_batch, y_batch in data_loader:\n            x_batch = x_batch.to(self.device)\n            y_batch = y_batch.to(self.device)\n\n            mini_batch_loss = step_fn(x_batch, y_batch)\n            mini_batch_losses.append(mini_batch_loss)\n\n        loss = np.mean(mini_batch_losses)\n        return loss\n\n    def set_seed(self, seed=42):\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\n        torch.manual_seed(seed)\n        np.random.seed(seed)\n\n    def train(self, n_epochs, seed=42, print_loss=False):\n        # To ensure reproducibility of the training process\n        self.set_seed(seed)\n\n        for epoch in range(n_epochs):\n            # Keeps track of the numbers of epochs\n            # by updating the corresponding attribute\n            self.total_epochs += 1\n\n            # inner loop\n            # Performs training using mini-batches\n            loss = self._mini_batch(validation=False)\n            self.losses.append(loss)\n\n            if print_loss:\n                if epoch % 10 == 0 and epoch != 0:\n                    print(\"Epoch \", epoch, \"MSE: \", loss)\n\n            # VALIDATION\n            # no gradients in validation!\n            with torch.no_grad():\n                # Performs evaluation using mini-batches\n                val_loss = self._mini_batch(validation=True)\n                self.val_losses.append(val_loss)\n\n            # If a SummaryWriter has been set...\n            if self.writer:\n                scalars = {\"training\": loss}\n                if val_loss is not None:\n                    scalars.update({\"validation\": val_loss})\n                # Records both losses for each epoch under the main tag \"loss\"\n                self.writer.add_scalars(\n                    main_tag=\"loss\", tag_scalar_dict=scalars, global_step=epoch\n                )\n\n        if self.writer:\n            # Closes the writer\n            self.writer.close()\n\n    def predict(self, x):\n        # Set is to evaluation mode for predictions\n        self.model.eval()\n        # Takes aNumpy input and make it a float tensor\n        x_tensor = torch.as_tensor(x).float()\n        # Send input to device and uses model for prediction\n        y_hat_tensor = self.model(x_tensor.to(self.device))\n        # Set it back to train mode\n        self.model.train()\n        # Detaches it, brings it to CPU and back to Numpy\n        return y_hat_tensor.detach().cpu().numpy()\n\n    def plot_losses(self):\n        fig = plt.figure(figsize=(10, 4))\n        plt.plot(self.losses, label=\"Training Loss\", c=\"b\")\n        plt.plot(self.val_losses, label=\"Validation Loss\", c=\"r\")\n        plt.yscale(\"log\")\n        plt.xlabel(\"Epochs\")\n        plt.ylabel(\"Loss\")\n        plt.legend()\n        plt.tight_layout()\n        return fig"
  },
  {
    "objectID": "posts/2022-11-07-timeseries-rnn-gru-lstm-cnn-pytorch.html#model-configuration-and-training",
    "href": "posts/2022-11-07-timeseries-rnn-gru-lstm-cnn-pytorch.html#model-configuration-and-training",
    "title": "Build Temporal Models for Univariate Time series Data with RNN, GRU, LSTM, CNN using PyTorch",
    "section": "Model configuration and training",
    "text": "Model configuration and training\nWe have all the pieces ready to train a neural network on the ordered sequence data. So here, I will train an RNN model on the generated data. At this point, I will not go into the details of the structure and working of RNN. But in the next section, we will discuss it in much more detail.\n\n\nCode\n# configure an RNN model\nimport torch.nn as nn\nimport torch.optim as optim\n\nclass RnnModel(nn.Module):\n    def __init__(self, n_features, hidden_dim, n_outputs, n_layers):\n        super(RnnModel, self).__init__()\n        self.hidden_dim = hidden_dim\n        self.n_features = n_features\n        self.n_outputs = n_outputs\n        self.n_layers = n_layers\n        self.hidden = None\n\n        # Simple RNN\n        self.basic_rnn = nn.RNN(\n            self.n_features, self.hidden_dim, self.n_layers, batch_first=True\n        )\n        # Classifier to produce as many logits as outputs\n        self.classifier = nn.Linear(self.hidden_dim, self.n_outputs)\n\n    def forward(self, X):\n        # X is batch first (N, L, F)\n        # output is (N, L, H)\n        # final hidden state is (1, N, H)\n        # print(X.shape)\n        batch_first_output, self.hidden = self.basic_rnn(X)\n\n        # print(\"check1\")\n        # only last item in sequence (N, 1, H)\n        last_output = batch_first_output[:, -1]\n        # classifier will output (N, 1, n_outputs)\n        out = self.classifier(last_output)\n\n        # final output is (N, n_outputs)\n        return out.view(-1, self.n_outputs)\n\n\nConfigure model loss and optimizer.\n\ntorch.manual_seed(21)\nrnn_model = RnnModel(n_features=1, hidden_dim=10, n_outputs=1, n_layers=1)\nrnn_loss = nn.MSELoss()\nrnn_optimizer = optim.Adam(rnn_model.parameters(), lr=0.01)\n\nRun the training pipeline.\n\ndlp_rnn = DeepLearningPipeline(rnn_model, rnn_loss, rnn_optimizer)\ndlp_rnn.set_loaders(train_loader_synth, test_loader_synth)\ndlp_rnn.train(100, print_loss=True)\n\nEpoch  10 MSE:  0.0043875276698434554\nEpoch  20 MSE:  0.003170915104088966\nEpoch  30 MSE:  0.0032213201127226716\nEpoch  40 MSE:  0.003209590242477134\nEpoch  50 MSE:  0.0030302550162146376\nEpoch  60 MSE:  0.0031480757964097643\nEpoch  70 MSE:  0.002840602589210241\nEpoch  80 MSE:  0.0030571757948068394\nEpoch  90 MSE:  0.0031562208208594134\n\n\nPlot the model loss.\n\nfig = dlp_rnn.plot_losses()\n\n\n\n\n\n\n\n\nGet predictions on the test data.\n\ny_test_pred_synth = dlp_rnn.predict(x_test_synth)\ny_train_pred_synth = dlp_rnn.predict(x_train_synth)\n\nCalculate mean squared error.\n\nimport math\nfrom sklearn.metrics import mean_squared_error\nfrom math import sqrt\n\n# calculate root mean squared error\ntrainScore = math.sqrt(\n    mean_squared_error(y_train_synth[:, 0], y_train_pred_synth[:, 0])\n)\nprint(\"Train Score: %.2f RMSE\" % (trainScore))\ntestScore = math.sqrt(mean_squared_error(y_test_synth[:, 0], y_test_pred_synth[:, 0]))\nprint(\"Test Score: %.2f RMSE\" % (testScore))\n\nTrain Score: 0.05 RMSE\nTest Score: 0.05 RMSE\n\n\nPlot the predicted values along with true values on the test data.\n\ndef plot_predictions(y, y_pred, model_name=\"\"):\n    plt.figure(figsize=(15, 7))\n\n    x = np.arange(len(y))\n    plt.plot(x, y, color=\"red\", label=\"True values\")\n    plt.plot(x, y_pred, color=\"blue\", label=\"Predicted values\")\n\n    title = \"Comparison of true and predicted values\"\n    if len(model_name):\n        title = model_name + \": \" + title\n\n    plt.title(title)\n    plt.xlabel(\"Steps\")\n    plt.ylabel(\"Values\")\n    plt.legend()\n    plt.show()\n\n\nplot_predictions(y_test_synth, y_test_pred_synth)\n\n\n\n\n\n\n\n\nThat is the end of Section I. We have successfully trained a recurrent neural network on ordered sequence data, and our predicted values are very close to the actual values. We have also learned to use ordered sequences to generate training and test data sets with features and labels."
  },
  {
    "objectID": "posts/2022-11-07-timeseries-rnn-gru-lstm-cnn-pytorch.html#data-preparation-1",
    "href": "posts/2022-11-07-timeseries-rnn-gru-lstm-cnn-pytorch.html#data-preparation-1",
    "title": "Build Temporal Models for Univariate Time series Data with RNN, GRU, LSTM, CNN using PyTorch",
    "section": "Data preparation",
    "text": "Data preparation\nLet’s load this data and view the stock prices as a plot.\n\nfolder = \"./datasets/2022-11-07-timeseries-rnn-gru-lstm-cnn-pytorch/\"\nfile_name = \"MSFT_2006-01-01_to_2018-01-01.csv\"\ndf_msft = pd.read_csv(folder + file_name, parse_dates=True, index_col=0)\ndf_msft[[\"Close\"]].plot(figsize=(15, 6))\nplt.ylabel(\"stock_price\")\nplt.title(\"MSFT Stock\")\nplt.show()\n\n\n\n\n\n\n\n\n\n##\n# Range of the stock price\nprint(\"Minimum stock price: \", min(df_msft['Close'].values))\nprint(\"Maximum stock price: \", max(df_msft['Close'].values)) \n\nMinimum stock price:  15.15\nMaximum stock price:  86.85\n\n\nFrom the above plot, we can see that the price value continuously increases over time, and the range of prices is roughly between 15 to 87 USD. This scale is not good news for neural networks as they work best when they get data on a scale closer to zero. Preferably -1 to 1. So in the next cell, we will convert our data to a much smaller scale.\n\nfrom sklearn.preprocessing import MinMaxScaler\n\nscaler = MinMaxScaler(feature_range=(-1, 1))\n\ndf_msft = df_msft[[\"Close\"]]\n# fill any missing values as a precaution\ndf_msft = df_msft.fillna(method=\"ffill\")\n\n# create a copy for scaling and keep original data\ndf_msft_scaled = df_msft.copy(deep=True)\ndf_msft_scaled[\"Close\"] = scaler.fit_transform(df_msft[\"Close\"].values.reshape(-1, 1))\n\nprint(\"*** Before scaling ***\\n\", df_msft.tail())\nprint(\"\\n*** After scaling ***\\n\", df_msft_scaled.tail())\n\n*** Before scaling ***\n             Close\nDate             \n2017-12-22  85.51\n2017-12-26  85.40\n2017-12-27  85.71\n2017-12-28  85.72\n2017-12-29  85.54\n\n*** After scaling ***\n                Close\nDate                \n2017-12-22  0.962622\n2017-12-26  0.959554\n2017-12-27  0.968201\n2017-12-28  0.968480\n2017-12-29  0.963459\n\n\nIn the next step we will generate training and test sets for our data.\n\n# look_back = size of a sequence in training and test set\nlook_back = 30\nx_train_scaled, y_train_scaled, x_test_scaled, y_test_scaled = generate_sequences(\n    df_msft_scaled, look_back=look_back\n)\n\nNow let’s load this data into PyTorch Dataset and DataLoader class.\n\nimport torch\nfrom torch.utils.data import DataLoader, Dataset, random_split, TensorDataset\n\ntrain_data = TensorDataset(\n    torch.as_tensor(x_train_scaled).float(), torch.as_tensor(y_train_scaled).float()\n)\ntest_data = TensorDataset(\n    torch.as_tensor(x_test_scaled).float(), torch.as_tensor(y_test_scaled).float()\n)\n\nbatch_size = 32\ntrain_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\ntest_loader = DataLoader(test_data, batch_size=batch_size)"
  },
  {
    "objectID": "posts/2022-11-07-timeseries-rnn-gru-lstm-cnn-pytorch.html#recurrent-neural-network-rnn",
    "href": "posts/2022-11-07-timeseries-rnn-gru-lstm-cnn-pytorch.html#recurrent-neural-network-rnn",
    "title": "Build Temporal Models for Univariate Time series Data with RNN, GRU, LSTM, CNN using PyTorch",
    "section": "Recurrent Neural Network (RNN)",
    "text": "Recurrent Neural Network (RNN)\nLet’s train the same RNN we built in section 1 on stock prices data, and check it’s performance.\n\n##\n# configure model, its loss and optimizer\ntorch.manual_seed(21)\nmodel = RnnModel(n_features=1, hidden_dim=32, n_outputs=1, n_layers=1)\nloss = nn.MSELoss()\noptimizer = optim.Adam(model.parameters(), lr=0.01)\n\n\n##\n# check the dimension for one batch\ntemp = next(iter(train_loader))\nlen(temp[0]), len(temp[0][0]), len(temp[0][0][0]) # N (batch_size), L (seq len), F (n_features)\n\n(32, 29, 1)\n\n\n\n##\n# start training pipeline\ndlp_rnn = DeepLearningPipeline(model, loss, optimizer)\ndlp_rnn.set_loaders(train_loader, test_loader)\ndlp_rnn.train(100, print_loss=True)\n\nEpoch  10 MSE:  0.00026397304452874585\nEpoch  20 MSE:  0.00022604088944993265\nEpoch  30 MSE:  0.00023406338930891997\nEpoch  40 MSE:  0.00029436370514855355\nEpoch  50 MSE:  0.000270840552151309\nEpoch  60 MSE:  0.000291384112201878\nEpoch  70 MSE:  0.00026263904168665636\nEpoch  80 MSE:  0.00022778069747304968\nEpoch  90 MSE:  0.00023518060378123528\n\n\nPlot training and validation loss.\n\nfig = dlp_rnn.plot_losses()\n\n\n\n\n\n\n\n\nIn the next cell, we will make predictions on the test data. After that, we will invert (or rescale) predicted and actual values to their original scale. Once that is done, we will use predicted and actual values to calculate RMSE (Root Mean Squared Error).\n\n##\n# make predictions on the test data\ny_test_pred_scaled = dlp_rnn.predict(x_test_scaled)\ny_train_pred_scaled = dlp_rnn.predict(x_train_scaled)\n\n# invert predictions and true values\ny_train_pred = scaler.inverse_transform(y_train_pred_scaled)\ny_train = scaler.inverse_transform(y_train_scaled)\ny_test_pred = scaler.inverse_transform(y_test_pred_scaled)\ny_test = scaler.inverse_transform(y_test_scaled)\n\n# calculate root mean squared error\ntrainScore = math.sqrt(mean_squared_error(y_train[:, 0], y_train_pred[:, 0]))\ntestScore = math.sqrt(mean_squared_error(y_test[:, 0], y_test_pred[:, 0]))\n\nprint(\"Train Score: %.2f RMSE\" % (trainScore))\nprint(\"Test Score: %.2f RMSE\" % (testScore))\n\nTrain Score: 0.52 RMSE\nTest Score: 2.37 RMSE\n\n\nPlot the True and Predicted values.\n\nplot_predictions(y_test, y_test_pred)\n\n\n\n\n\n\n\n\nThe above plot shows that the RNN model can correctly predict values till about 500 steps, but after that predictions start to diverge, and the gap keeps increasing as time passes.\n\nRNN cell in detail\nIf you revisit section 1 topic ‘Model configuration and training’, we have built an RNN model using PyTorch nn.RNN class.\nself.basic_rnn = nn.RNN(self.n_features, self.hidden_dim, self.n_layers, batch_first=True)\nFrom PyTorch documentation, we don’t get much information on the internal working of this class as it only gives a short description. (Link here)\n\nApplies a multi-layer Elman RNN with tanh or ReLU non-linearity to an input sequence.\n\nPyTorch Step By Step Chapter 8 does a great job of explaining the internal working of an RNN Cell. I have taken the following image from the book’s official GitHub repo.\n Image Source: dvgodoy/PyTorchStepByStep/blob/master/Chapter08\nFrom this image we can reason the following\n\nThere are two types of weight layers\n\nFirst weight layer (Wi) which processes the input. Let’s call it as ‘input linear layer’ or in short linear_input\nSecond weight layer (Wh) which processes the hidden state. Let’s call it as hidden linear layer or in short linear_hidden\n\nRNN cell processes two types of input at the same time\n\nSequence data input or X\nand the Hidden state. Hidden state is also the output of the RNN cell at each step, and it is returned as an input for the next step\n\n\nThe processing of an RNN cell can be described in the following steps\n\nPass input (X=[x0, x1]) to input linear layer (linear_input) and get the output (tx=[t0, t1])\nPass the last “hidden state” to the hidden linear layer (linear_hidden), and get the output (th=[h0, h1]). Since at the start we don’t have a hidden state from the last step, we can manually assign hidden state as zeros and pass it to hidden linear layer.\nAdd both outputs tx and th. Let’s call it adding\npass the ‘adding’ to activation function tanh. The output is the new “hidden state” and will be used in the next step.\n\nNow that we have learned how an RNN cell works let’s build it ourselves without relying on PyTorch nn.RNN class. To ensure that our custom RNN cell produces the same output as nn.RNN, we will do the following test\n\nCreate two linear layers that will represent input and hidden layers (linear_input and linear_hidden respectively)\nCreate an nn.RNN cell\nCopy and assign the weights from nn.RNN cell to input and hidden linear layers\nTrain both linear layers and nn.RNN cell on an input data point\nCompare the weight states of both. If they match, then we have successfully replicated the internal functionality of an.RNN cell.\n\n\n##\n# create input and hidden linear layers\ntorch.manual_seed(19)\nn_features = 2\nhidden_dim = 2\n\nlinear_input = nn.Linear(n_features, hidden_dim)\nlinear_hidden = nn.Linear(hidden_dim, hidden_dim)\n\n\n##\n# create nn.RNN cell from PyTorch class\nrnn_cell = nn.RNNCell(input_size=n_features, hidden_size=hidden_dim)\nrnn_state = rnn_cell.state_dict()\nrnn_state\n\nOrderedDict([('weight_ih', tensor([[-0.6701, -0.5811],\n                      [-0.0170, -0.5856]])),\n             ('weight_hh', tensor([[ 0.1159, -0.6978],\n                      [ 0.3241, -0.0983]])),\n             ('bias_ih', tensor([-0.3163, -0.2153])),\n             ('bias_hh', tensor([ 0.0722, -0.3242]))])\n\n\nIn the last two cells, we have created two linear layers for our custom RNN cell and an instance of PyTorch nn.RNN class. In the next step, we will assign a copy of weights from nn.RNN to linear layers. This way both will have the same initial weights.\n\n##\n# assgin weight from nn.RNN to linear layers\nwith torch.no_grad():\n    linear_input.weight = nn.Parameter(rnn_state[\"weight_ih\"])\n    linear_input.bias = nn.Parameter(rnn_state[\"bias_ih\"])\n    linear_hidden.weight = nn.Parameter(rnn_state[\"weight_hh\"])\n    linear_hidden.bias = nn.Parameter(rnn_state[\"bias_hh\"])\n\nLet’s create an input data point X with two dimensions x0 and x1 * X = [x0, x1] * x0 = 1.0349 * x1 = 0.9661\n\nX = torch.as_tensor(np.array([1.0349, 0.9661])).float()\nX\n\ntensor([1.0349, 0.9661])\n\n\nNow let’s follow the steps we have defined for working of an RNN cell.\n\n##\n# 1. Pass input (`X=[x0, x1]`) to input linear layer (linear_input) and get the output (`tx=[t0, t1`])\ntx = linear_input(X)\ntx\n\ntensor([-1.5712, -0.7985], grad_fn=&lt;AddBackward0&gt;)\n\n\n\n##\n# 2. Pass the last \"hidden state\" to the hidden linear layer (linear_hidden), and get the output (`th=[h0, h1`]). \n# Since this is the first step, and we don't have a hidden state from the last step, \n# we can manually assign hidden state as zeros and pass it to hidden linear layer.\ninitial_hidden = torch.zeros(1, hidden_dim)\n\nth = linear_hidden(initial_hidden)\nth\n\ntensor([[ 0.0722, -0.3242]], grad_fn=&lt;AddmmBackward0&gt;)\n\n\n\n##\n# 3. Add both outputs `tx` and `th`. Let's call it `adding`\nt_hx = th + tx\nt_hx\n\ntensor([[-1.4991, -1.1227]], grad_fn=&lt;AddBackward0&gt;)\n\n\n\n##\n# 4. pass the 'adding' to activation function `tanh`. The output is the new \"hidden state\" and will be used for upcoming inputs\nnew_hidden_state = torch.tanh(t_hx)\nnew_hidden_state\n\ntensor([[-0.9050, -0.8085]], grad_fn=&lt;TanhBackward0&gt;)\n\n\nWe have an output from our custom RNN cell. This is the new hidden state that will be passed to the linear_hidden layer in the next step.\nNow time to compare this output with that of nn.RNN to see if they match or not.\n\nrnn_cell(X)\n\ntensor([-0.9050, -0.8085], grad_fn=&lt;SqueezeBackward1&gt;)\n\n\nNotice that the output from both the custom RNN cell and nn.RNN match. This means that we are successful in replicating the internal working of nn.RNN class."
  },
  {
    "objectID": "posts/2022-11-07-timeseries-rnn-gru-lstm-cnn-pytorch.html#gated-recurrent-units-gru",
    "href": "posts/2022-11-07-timeseries-rnn-gru-lstm-cnn-pytorch.html#gated-recurrent-units-gru",
    "title": "Build Temporal Models for Univariate Time series Data with RNN, GRU, LSTM, CNN using PyTorch",
    "section": "Gated Recurrent Units (GRU)",
    "text": "Gated Recurrent Units (GRU)\nIn this section we will apply gated recurrent units on the stock price data, and compare its performance with simple RNNs.\n\n##\n# GRU model configuration\nclass GruModel(nn.Module):\n    def __init__(self, n_features, hidden_dim, n_outputs, n_layers):\n        super(GruModel, self).__init__()\n        self.n_features = n_features\n        self.hidden_dim = hidden_dim\n        self.n_outputs = n_outputs\n        self.n_layers = n_layers\n        self.hidden = None\n        # Simple GRU\n        self.basic_rnn = nn.GRU(\n            self.n_features, self.hidden_dim, self.n_layers, batch_first=True\n        )\n        # Classifier to produce as many logits as outputs\n        self.classifier = nn.Linear(self.hidden_dim, self.n_outputs)\n\n    def forward(self, X):\n        # X is batch first (N, L, F)\n        # output is (N, L, H)\n        # final hidden state is (1, N, H)\n        batch_first_output, self.hidden = self.basic_rnn(X)\n\n        # only last item in sequence (N, 1, H)\n        last_output = batch_first_output[:, -1]\n        # classifier will output (N, 1, n_outputs)\n        out = self.classifier(last_output)\n\n        # final output is (N, n_outputs)\n        return out.view(-1, self.n_outputs)\n\nConfigure model loss and optimizer.\n\ntorch.manual_seed(21)\ngru_model = GruModel(n_features=1, hidden_dim=32, n_outputs=1, n_layers=1)\ngru_loss = nn.MSELoss()\ngru_optimizer = optim.Adam(gru_model.parameters(), lr=0.01)\n\nRun the training pipeline for 100 epochs.\n\ndlp_gru = DeepLearningPipeline(gru_model, gru_loss, gru_optimizer)\ndlp_gru.set_loaders(train_loader, test_loader)\ndlp_gru.train(100, print_loss=True)\n\nEpoch  10 MSE:  0.00022010822761909697\nEpoch  20 MSE:  0.00020518084370864514\nEpoch  30 MSE:  0.00020595710090922446\nEpoch  40 MSE:  0.00020482327377204925\nEpoch  50 MSE:  0.00022252999384143163\nEpoch  60 MSE:  0.0002140117964396874\nEpoch  70 MSE:  0.00023651681564815314\nEpoch  80 MSE:  0.00020522110384208094\nEpoch  90 MSE:  0.0002454350946980853\n\n\nPlot the training and validation loss.\n\nfig = dlp_gru.plot_losses()\n\n\n\n\n\n\n\n\nMake prediction on test data and calculate the loss.\n\n# make predictions\ny_test_pred_scaled = dlp_gru.predict(x_test_scaled)\ny_train_pred_scaled = dlp_gru.predict(x_train_scaled)\n\n# invert predictions\ny_train_pred = scaler.inverse_transform(y_train_pred_scaled)\ny_train = scaler.inverse_transform(y_train_scaled)\ny_test_pred = scaler.inverse_transform(y_test_pred_scaled)\ny_test = scaler.inverse_transform(y_test_scaled)\n\n# calculate root mean squared error\ntrainScore = math.sqrt(mean_squared_error(y_train[:, 0], y_train_pred[:, 0]))\ntestScore = math.sqrt(mean_squared_error(y_test[:, 0], y_test_pred[:, 0]))\n\nprint(\"Train Score: %.2f RMSE\" % (trainScore))\nprint(\"Test Score: %.2f RMSE\" % (testScore))\n\nTrain Score: 0.48 RMSE\nTest Score: 2.68 RMSE\n\n\nPlot predictions along with actual data.\n\nplot_predictions(y_test, y_test_pred)\n\n\n\n\n\n\n\n\nGRU seems to be on par with RNN. It has a slightly better training score, but at the same time, it performed somewhat poorly on the validation data.\n\nGRU cell in detail\n Image Source: dvgodoy/PyTorchStepByStep/blob/master/Chapter08\nFrom the above image we can see that GRU cell is more advanced than a simple RNN cell. It has two more weight layers commonly referred as gates\n\nReset gate: This weight layer is used to control how much of the past is needed to neglect or forget. This gate has a control r which is learned during training.\n\nIf we decrease r to 0, then the current state of cell n is less and less influenced by the old hidden state\nIf we increase r all the way to 1, then the current state will have maximum affect of the last hidden state.\n\nUpdate gate: This weight layer is used to control how much of the past information is needed to be passed on to the next step. This gate has a control z which is also learned during training.\n\nIf we decrease z all the way to 0, then the new hidden state h` is closer and closer to current state of the cell. In the figure the current state is n\nIf we increase z all the way to 1, then new hidden state h` is simply a copy of last hidden state h\n\nIf we decrease both r and z to 0, then GRU is simply a linear layer followed by an activation layer.\n\nHow does having two extra learnable weight layers and their controls make GRU better than RNN?\n\nIt is like giving more control to a neural network to decide which information it wants to retain and which to forget as time passes.\nIt may seem like both layers are trying to achieve the same thing: What information to keep or forget? But there is more to it. Suppose that we have a single weight layer as in RNN. For RNN, if the neural network has decided to forget something, then that information is gone. If it is needed in future steps, then the network will have to relearn it. In the case of GRU, the network has the luxury that the information it wants to forget can be parked in a separate layer (forget layer). If, in the future, that information is needed again, then it can simply change the gate control and make that information available.\n\nWhy learning and forgetting are important for recurrent neural networks?\nPatterns change over time for ordered sequence data like stock prices, and we want our networks to be sensitive to such changes. A repeating hidden state helps the network connect the dots between new information it has received and the past it has learned. If the network finds the new information it received has changed from past learning, it will try to unlearn some of the past experiences and learn the new pattern. Different variants of RNN are designed to give more and more such controls to the network and make it efficient in deciding which information to learn or forget."
  },
  {
    "objectID": "posts/2022-11-07-timeseries-rnn-gru-lstm-cnn-pytorch.html#long-short-term-memory-lstm",
    "href": "posts/2022-11-07-timeseries-rnn-gru-lstm-cnn-pytorch.html#long-short-term-memory-lstm",
    "title": "Build Temporal Models for Univariate Time series Data with RNN, GRU, LSTM, CNN using PyTorch",
    "section": "Long Short Term Memory (LSTM)",
    "text": "Long Short Term Memory (LSTM)\nIn this section we will apply lstm on the stock price data, and compare its performance with RNN and GRU.\n\n##\n# LSTM model configuration\nclass LstmModel(nn.Module):\n    def __init__(self, n_features, hidden_dim, n_outputs, n_layers):\n        super(LstmModel, self).__init__()\n        self.hidden_dim = hidden_dim\n        self.n_features = n_features\n        self.n_outputs = n_outputs\n        self.n_layers = n_layers\n\n        self.hidden = None\n        self.cell = None\n        # Simple LSTM\n        self.basic_rnn = nn.LSTM(\n            self.n_features, self.hidden_dim, self.n_layers, batch_first=True\n        )\n        # Classifier to produce as many logits as outputs\n        self.classifier = nn.Linear(self.hidden_dim, self.n_outputs)\n\n    def forward(self, X):\n        # X is batch first (N, L, F)\n        # output is (N, L, H)\n        # final hidden state is (1, N, H)\n        # final cell state is (1, N, H)\n        batch_first_output, (self.hidden, self.cell) = self.basic_rnn(X)\n\n        # only last item in sequence (N, 1, H)\n        last_output = batch_first_output[:, -1]\n        # classifier will output (N, 1, n_outputs)\n        out = self.classifier(last_output)\n\n        # final output is (N, n_outputs)\n        return out.view(-1, self.n_outputs)\n\nDefine model loss and optimizer.\n\ntorch.manual_seed(21)\nlstm_model = LstmModel(n_features=1, hidden_dim=32, n_outputs=1, n_layers=1)\nlstm_loss = nn.MSELoss()\nlstm_optimizer = optim.Adam(lstm_model.parameters(), lr=0.01)\n\nRun the training pipeline.\n\ndlp_lstm = DeepLearningPipeline(lstm_model, lstm_loss, lstm_optimizer)\ndlp_lstm.set_loaders(train_loader, test_loader)\ndlp_lstm.train(100, print_loss=True)\n\nEpoch  10 MSE:  0.0003263879698351957\nEpoch  20 MSE:  0.000262940919569563\nEpoch  30 MSE:  0.0002264043755668822\nEpoch  40 MSE:  0.000254558740076997\nEpoch  50 MSE:  0.0002543745165784265\nEpoch  60 MSE:  0.00028126772259852396\nEpoch  70 MSE:  0.00025442599127762315\nEpoch  80 MSE:  0.00020528354511814982\nEpoch  90 MSE:  0.00022827486629301512\n\n\nPrint the training and validation loss.\n\nfig = dlp_lstm.plot_losses()\n\n\n\n\n\n\n\n\nMake predictions on the test data and calculate the error.\n\n# make predictions\ny_test_pred_scaled = dlp_lstm.predict(x_test_scaled)\ny_train_pred_scaled = dlp_lstm.predict(x_train_scaled)\n\n# invert predictions\ny_train_pred = scaler.inverse_transform(y_train_pred_scaled)\ny_train = scaler.inverse_transform(y_train_scaled)\ny_test_pred = scaler.inverse_transform(y_test_pred_scaled)\ny_test = scaler.inverse_transform(y_test_scaled)\n\n# calculate root mean squared error\ntrainScore = math.sqrt(mean_squared_error(y_train[:, 0], y_train_pred[:, 0]))\ntestScore = math.sqrt(mean_squared_error(y_test[:, 0], y_test_pred[:, 0]))\n\nprint(\"Train Score: %.2f RMSE\" % (trainScore))\nprint(\"Test Score: %.2f RMSE\" % (testScore))\n\nTrain Score: 0.53 RMSE\nTest Score: 0.93 RMSE\n\n\nPlot predicted values along with true values.\n\nplot_predictions(y_test, y_test_pred)\n\n\n\n\n\n\n\n\nLSTM has performed much better on the test data compared to both RNN and GRU. But it has also taken more time to get trained.\n\nLSTM cell in detail\n Image Source: dvgodoy/PyTorchStepByStep/blob/master/Chapter08\nThe above image shows that LSTM network has more learnable parameters and controls compared to RNN and GRU. There is\n\nForget gate: Similar to the reset gate in GRU, it controls which information needs attention and which can be ignored.\nInput gate and Cell state: LSTM is unique in that besides the hidden state, it also maintains a separate state called cell state. Cell state acts as a long-term memory, while the hidden state acts like a working or short-term memory. Input gate controls how to update the cell state based on past hidden state, past cell state, and new input.\nUpdate gate: Update gate controls how to update the hidden state to generate a new hidden state value. It gets influenced by past hidden states and new input. Cell state does not affect this gate."
  },
  {
    "objectID": "posts/2022-11-07-timeseries-rnn-gru-lstm-cnn-pytorch.html#one-dimensional-convolutional-neural-network-1d-convnet",
    "href": "posts/2022-11-07-timeseries-rnn-gru-lstm-cnn-pytorch.html#one-dimensional-convolutional-neural-network-1d-convnet",
    "title": "Build Temporal Models for Univariate Time series Data with RNN, GRU, LSTM, CNN using PyTorch",
    "section": "One Dimensional Convolutional Neural Network (1D ConvNet)",
    "text": "One Dimensional Convolutional Neural Network (1D ConvNet)\nIn this section we will take an alternate approach and apply a type of CNN on stock price data.\n\n##\n# 1D CNN model configuration\nclass CNNmodel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.c1 = nn.Conv1d(1, 32, 2)\n        self.p1 = nn.AvgPool1d(2)\n        self.c2 = nn.Conv1d(32, 64, 1)\n        self.p2 = nn.AvgPool1d(2)\n        self.tanh = nn.Tanh()\n        self.fc1 = nn.Linear(448, 64)\n        self.fc2 = nn.Linear(64, 1)\n\n    def forward(self, x):\n        \"\"\"\n        x1:  torch.Size([32, 32, 28])\n        x2:  torch.Size([32, 32, 14])\n        x3:  torch.Size([32, 64, 14])\n        x4:  torch.Size([32, 64, 7])\n        x5:  torch.Size([32, 448])\n        x6:  torch.Size([32, 64])\n        x7:  torch.Size([32, 1])\n        \"\"\"\n\n        x1 = self.c1(x)\n        x2 = self.p1(x1)\n        x3 = self.c2(x2)\n        x4 = self.p2(x3)\n\n        x4 = self.tanh(x4)\n        x5 = x4.reshape(x4.shape[0], -1)\n\n        x6 = self.fc1(x5)\n        x7 = self.fc2(x6)\n\n        return x7\n\n\n##\n# configure model loss and optimizer\nmodel = CNNmodel()\nloss = nn.MSELoss()\noptimizer = optim.Adam(model.parameters(), lr=0.01)\n\n\n##\n# change the dimension of dataset\n# for CNN use: N (batch_size), F (n_features), L (seq len)\ntrain_data_1d = TensorDataset(\n    torch.as_tensor(x_train_scaled).float().permute(0, 2, 1),\n    torch.as_tensor(y_train_scaled).float(),\n)\ntest_data_1d = TensorDataset(\n    torch.as_tensor(x_test_scaled).float().permute(0, 2, 1),\n    torch.as_tensor(y_test_scaled).float(),\n)\n\ntrain_loader = DataLoader(train_data_1d, batch_size=32, shuffle=True)\ntest_loader = DataLoader(test_data_1d, batch_size=32)\n\n\n##\n# check the dimensions of one batch\ntemp = next(iter(train_loader))\nlen(temp[0]), len(temp[0][0]), len(temp[0][0][0]) # N (batch_size), F (n_features), L (seq len)\n\n(32, 1, 29)\n\n\n\n##\n# run the training pipeline\ndlp_conv1d = DeepLearningPipeline(model, loss, optimizer)\ndlp_conv1d.set_loaders(train_loader, test_loader)\ndlp_conv1d.train(100, print_loss=True)\n\nEpoch  10 MSE:  0.000601009127973212\nEpoch  20 MSE:  0.0005247063511915533\nEpoch  30 MSE:  0.0004160549495171643\nEpoch  40 MSE:  0.00038349507733735004\nEpoch  50 MSE:  0.0005176076665520668\nEpoch  60 MSE:  0.00043436023538974536\nEpoch  70 MSE:  0.0004034905033415381\nEpoch  80 MSE:  0.00036779195050423203\nEpoch  90 MSE:  0.00027141175329840433\n\n\n\n##\n# plot the training and validation loss\nfig = dlp_conv1d.plot_losses()\n\n\n\n\n\n\n\n\n\n##\n# make predictions\ny_test_pred_scaled = dlp_conv1d.predict(\n    torch.as_tensor(x_test_scaled).float().permute(0, 2, 1)\n)\ny_train_pred_scaled = dlp_conv1d.predict(\n    torch.as_tensor(x_train_scaled).float().permute(0, 2, 1)\n)\n\n# invert predictions\ny_train_pred = scaler.inverse_transform(y_train_pred_scaled)\ny_train = scaler.inverse_transform(y_train_scaled)\ny_test_pred = scaler.inverse_transform(y_test_pred_scaled)\ny_test = scaler.inverse_transform(y_test_scaled)\n\n# calculate root mean squared error\ntrainScore = math.sqrt(mean_squared_error(y_train[:, 0], y_train_pred[:, 0]))\ntestScore = math.sqrt(mean_squared_error(y_test[:, 0], y_test_pred[:, 0]))\n\nprint(\"Train Score: %.2f RMSE\" % (trainScore))\nprint(\"Test Score: %.2f RMSE\" % (testScore))\n\nTrain Score: 0.61 RMSE\nTest Score: 2.07 RMSE\n\n\n\n##\n# plot true and predicted values\nplot_predictions(y_test, y_test_pred)\n\n\n\n\n\n\n\n\n1D ConvNet results stand between GRU and LSTM. It is performing better than RNN and GRU but less than LSTM.\nHow does 1D ConvNet compare to RNN?\n1D ConvNets have 1D convolutions, i.e. they move the filter in one dimension from left to right like a moving window. These kernels (or filters) behave similarly to the hidden state in RNN\n\nIn RNN, we process one data point at a step and move forward, incrementing the steps till we reach the end of the sequence\nIn 1D-ConvNet, we process one sequence at a time and move the filter along the entire length of the sequence"
  },
  {
    "objectID": "posts/2022-10-18-pytorch-mnist-convolutional-neural-networks.html#introduction",
    "href": "posts/2022-10-18-pytorch-mnist-convolutional-neural-networks.html#introduction",
    "title": "Convolutional Neural Networks Filters and Feature Maps with PyTorch",
    "section": "Introduction",
    "text": "Introduction\nWe will train a LeNet-5 CNN model with PyTorch on the MNIST dataset in this notebook. Given below is the summary of the steps followed in this notebook.\n\nDownload the MNIST dataset. Split the data into Train and Validation datasets. Then convert them into mini-batches using PyTorch DataLoader class\nCreate a Neural Net model configuration, an SGD optimizer, and a loss function\nCreate a pipeline that will train the model on given data and update the weights based on the loss\nVisualize filters and feature maps from the trained model"
  },
  {
    "objectID": "posts/2022-10-18-pytorch-mnist-convolutional-neural-networks.html#environment",
    "href": "posts/2022-10-18-pytorch-mnist-convolutional-neural-networks.html#environment",
    "title": "Convolutional Neural Networks Filters and Feature Maps with PyTorch",
    "section": "Environment",
    "text": "Environment\nThis notebook is prepared with Google Colab.\n\nGitHub: 2022-10-18-pytorch-mnist-convolutional-neural-networks.ipynb\nOpen In Colab: \n\n\n\nCode\nfrom platform import python_version\nimport numpy, matplotlib, pandas, torch\n\nprint(\"python==\" + python_version())\nprint(\"numpy==\" + numpy.__version__)\nprint(\"torch==\" + torch.__version__)\nprint(\"matplotlib==\" + matplotlib.__version__)\n\n\npython==3.7.15\nnumpy==1.21.6\ntorch==1.12.1+cu113\nmatplotlib==3.2.2"
  },
  {
    "objectID": "posts/2022-10-18-pytorch-mnist-convolutional-neural-networks.html#credits",
    "href": "posts/2022-10-18-pytorch-mnist-convolutional-neural-networks.html#credits",
    "title": "Convolutional Neural Networks Filters and Feature Maps with PyTorch",
    "section": "Credits",
    "text": "Credits\nThis notebook takes inspiration and ideas from the following two sources.\n\nThe outstanding book “Deep Learning with PyTorch Step-by-Step” by “Daniel Voigt Godoy”. You can get the book from its website: pytorchstepbystep. In addition, the GitHub repository for this book has valuable notebooks and can be used independently: github.com/dvgodoy/PyTorchStepByStep. Parts of the code you see in this notebook are taken from chapter 5 notebook of the same book.\nA great post by “Eugenia Anello” with the title Visualizing the Feature Maps and Filters by Convolutional Neural Networks. You can click the title to reach the post on the medium.com platform. Eugenia Anello can also be found on her linkedin profile."
  },
  {
    "objectID": "posts/2022-10-18-pytorch-mnist-convolutional-neural-networks.html#download-mnist-dataset",
    "href": "posts/2022-10-18-pytorch-mnist-convolutional-neural-networks.html#download-mnist-dataset",
    "title": "Convolutional Neural Networks Filters and Feature Maps with PyTorch",
    "section": "Download MNIST Dataset",
    "text": "Download MNIST Dataset\nMNIST dataset can be downloaded easily from PyTorch built-in datasets provided under torchvision.datasets. In this section, we will download it, split it into train and test datasets, and then convert it into PyTorch tensors.\n\nRead more about the PyTorch MNIST dataset here\ntorchvision.transforms.Compose is like a container to hold a list of transformations you intend to apply. Read more about it here\ntorchvision.transforms.ToTensor converts a PIL Image or numpy.ndarray to tensor. It converts a PIL Image or numpy.ndarray (H x W x C) in the range [0, 255] to a torch.FloatTensor of shape (C x H x W) in the range [0.0, 1.0]. Here C=Channel, H=Height, W=Width. Read more about this transformation here\n\n\n#collapse-output\nimport torchvision\nimport numpy as np\n\ntrain_dataset = torchvision.datasets.MNIST('classifier_data', train=True, download=True)\ntest_dataset  = torchvision.datasets.MNIST('classifier_data', train=False, download=True)\n\ntransform = torchvision.transforms.Compose([\n    torchvision.transforms.ToTensor()\n])\n\ntrain_dataset.transform=transform\ntest_dataset.transform=transform\n\nprint(f\"Total training images: {len(train_dataset)}\")\nprint(f\"Shape of an image: {np.shape(train_dataset.data[7])}\")\nprint(f\"Values of an image: \\n{train_dataset.data[7]}\")\n\nTotal training images: 60000\nShape of an image: torch.Size([28, 28])\nValues of an image: \ntensor([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  38,  43, 105,\n         255, 253, 253, 253, 253, 253, 174,   6,   0,   0,   0,   0,   0,   0],\n        [  0,   0,   0,   0,   0,   0,   0,   0,   0,  43, 139, 224, 226, 252,\n         253, 252, 252, 252, 252, 252, 252, 158,  14,   0,   0,   0,   0,   0],\n        [  0,   0,   0,   0,   0,   0,   0,   0,   0, 178, 252, 252, 252, 252,\n         253, 252, 252, 252, 252, 252, 252, 252,  59,   0,   0,   0,   0,   0],\n        [  0,   0,   0,   0,   0,   0,   0,   0,   0, 109, 252, 252, 230, 132,\n         133, 132, 132, 189, 252, 252, 252, 252,  59,   0,   0,   0,   0,   0],\n        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   4,  29,  29,  24,   0,\n           0,   0,   0,  14, 226, 252, 252, 172,   7,   0,   0,   0,   0,   0],\n        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n           0,   0,   0,  85, 243, 252, 252, 144,   0,   0,   0,   0,   0,   0],\n        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n           0,   0,  88, 189, 252, 252, 252,  14,   0,   0,   0,   0,   0,   0],\n        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n          91, 212, 247, 252, 252, 252, 204,   9,   0,   0,   0,   0,   0,   0],\n        [  0,   0,   0,   0,   0,   0,   0,   0,   0,  32, 125, 193, 193, 193,\n         253, 252, 252, 252, 238, 102,  28,   0,   0,   0,   0,   0,   0,   0],\n        [  0,   0,   0,   0,   0,   0,   0,   0,  45, 222, 252, 252, 252, 252,\n         253, 252, 252, 252, 177,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n        [  0,   0,   0,   0,   0,   0,   0,   0,  45, 223, 253, 253, 253, 253,\n         255, 253, 253, 253, 253,  74,   0,   0,   0,   0,   0,   0,   0,   0],\n        [  0,   0,   0,   0,   0,   0,   0,   0,   0,  31, 123,  52,  44,  44,\n          44,  44, 143, 252, 252,  74,   0,   0,   0,   0,   0,   0,   0,   0],\n        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n           0,   0,  15, 252, 252,  74,   0,   0,   0,   0,   0,   0,   0,   0],\n        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n           0,   0,  86, 252, 252,  74,   0,   0,   0,   0,   0,   0,   0,   0],\n        [  0,   0,   0,   0,   0,   0,   5,  75,   9,   0,   0,   0,   0,   0,\n           0,  98, 242, 252, 252,  74,   0,   0,   0,   0,   0,   0,   0,   0],\n        [  0,   0,   0,   0,   0,  61, 183, 252,  29,   0,   0,   0,   0,  18,\n          92, 239, 252, 252, 243,  65,   0,   0,   0,   0,   0,   0,   0,   0],\n        [  0,   0,   0,   0,   0, 208, 252, 252, 147, 134, 134, 134, 134, 203,\n         253, 252, 252, 188,  83,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n        [  0,   0,   0,   0,   0, 208, 252, 252, 252, 252, 252, 252, 252, 252,\n         253, 230, 153,   8,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n        [  0,   0,   0,   0,   0,  49, 157, 252, 252, 252, 252, 252, 217, 207,\n         146,  45,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n        [  0,   0,   0,   0,   0,   0,   7, 103, 235, 252, 172, 103,  24,   0,\n           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0]],\n       dtype=torch.uint8)\n\n\nFrom the above cell output, there are 60,000 training images. The shape of each image is 28 x 28, which means it is a 2D matrix. We have also printed the values of one image, but they don’t make much sense unless we view them as an image. So let’s do that.\n\nimport matplotlib.pyplot as plt\n\nplt.imshow(train_dataset.data[7], cmap='gray')"
  },
  {
    "objectID": "posts/2022-10-18-pytorch-mnist-convolutional-neural-networks.html#load-generated-data-into-pytorch-dataset-and-dataloader-class",
    "href": "posts/2022-10-18-pytorch-mnist-convolutional-neural-networks.html#load-generated-data-into-pytorch-dataset-and-dataloader-class",
    "title": "Convolutional Neural Networks Filters and Feature Maps with PyTorch",
    "section": "Load generated data into PyTorch Dataset and DataLoader class",
    "text": "Load generated data into PyTorch Dataset and DataLoader class\nNow let’s load our data into Dataset and DataLoader classes. PyTorch Dataset is a helper class that converts data and labels into a list of tuples. DataLoader is another helper class to create batches from Dataset tuples. batch_size means the number of tuples we want in a single batch. We have used 128 here, so each fetch from DataLoader will give us a list of 128 tuples.\n\nimport torch\nfrom torch.utils.data import Dataset, DataLoader, random_split\n\ntrain_size=len(train_dataset)\n\n# Randomly split the data into non-overlapping train and validation set\n# train size = 70% and validation size = 30%\ntrain_data, val_data = random_split(train_dataset, [int(train_size*0.7), int(train_size - train_size*0.7)])\n\nbatch_size=128\n\n# Load data into DataLoader class\ntrain_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size)\nvalid_loader = torch.utils.data.DataLoader(val_data, batch_size=batch_size)\n\nprint(f\"Batches in Train Loader: {len(train_loader)}\")\nprint(f\"Batches in Valid Loader: {len(valid_loader)}\")\n\nprint(f\"Examples in Train Loader: {len(train_loader.sampler)}\")\nprint(f\"Examples in Valid Loader: {len(valid_loader.sampler)}\")\n\nBatches in Train Loader: 469\nBatches in Valid Loader: 141\nExamples in Train Loader: 60000\nExamples in Valid Loader: 18000\n\n\n\n##\n# Helper function to plot images from DataLoader\ndef plot_images(images, targets, n_plot=30):\n    n_rows = n_plot // 10 + ((n_plot % 10) &gt; 0)\n    fig, axes = plt.subplots(n_rows, 10, figsize=(15, 1.5 * n_rows))\n    axes = np.atleast_2d(axes)\n\n    for i, (image, target) in enumerate(zip(images[:n_plot], targets[:n_plot])):\n        row, col = i // 10, i % 10    \n        ax = axes[row, col]\n        ax.set_title('#{} - Label:{}'.format(i, target), {'size': 12})\n        # plot filter channel in grayscale\n        ax.imshow(image.squeeze(), cmap='gray', vmin=0, vmax=1)\n\n    for ax in axes.flat:\n        ax.set_xticks([])\n        ax.set_yticks([])\n        ax.label_outer()\n\n    plt.tight_layout()\n    return fig\n\n## Code taken from https://github.com/dvgodoy/PyTorchStepByStep/blob/master/plots/chapter5.py\n\nLet’s plot some dataset images along with their labels from a batch.\n\nimages, labels = next(iter(train_loader))\nfig = plot_images(images, labels, n_plot=40)"
  },
  {
    "objectID": "posts/2022-10-18-pytorch-mnist-convolutional-neural-networks.html#define-a-class-to-implement-training-validation-and-mini-batch-processing-pipeline",
    "href": "posts/2022-10-18-pytorch-mnist-convolutional-neural-networks.html#define-a-class-to-implement-training-validation-and-mini-batch-processing-pipeline",
    "title": "Convolutional Neural Networks Filters and Feature Maps with PyTorch",
    "section": "Define a class to implement training, validation, and mini-batch processing pipeline",
    "text": "Define a class to implement training, validation, and mini-batch processing pipeline\nIn this section we will implement a class that encapsulates all the usual steps required in training a PyTorch model. This way we can focus more on the model architecture and performance, and less concerned about the boilerplate training loop. Important parts of this class are\n\n__init__: Class constructor to define the main actors in a training cycle including model, optimizer, loss function, training and validation DataLoaders\n_make_train_step_fn: Training pipeline is usually called “training step” which includes the following steps\n\nCompute our model’s predicted output - the forward pass\nCompute the loss\nCompute gradients i.e., find the direction and scale to update the weights to reduce the loss\nUpdate weight parameters using gradients and the learning rate\n\n_make_val_step_fn: Validation pipeline is usually called the “validation step” which includes the following steps\n\nCompute our model’s predicted output - the forward pass\nCompute the loss\nNote that during validation, we are only concerned about the loss, i.e., how well our model performs on the validation dataset. Therefore, we don’t use it to calculate the gradients.\n\n_mini_batch: It defines the steps to process a single minibatch in a helper function. For a mini-batch processing, we want to\n\nGet the next batch of data and labels (x, y) from the DataLoader iterator\nPerform a step on the batch. A step can be either training or validation\nCompute the average batch loss\n\ntrain: Execute training and validation steps for given number of epoch\npredict: Make a prediction from model on provided data\n\n\n\nCode\nclass DeepLearningPipeline(object):\n    def __init__(self, model, loss_fn, optimizer):\n        # Here we define the attributes of our class\n        \n        # We start by storing the arguments as attributes \n        # to use them later\n        self.model = model\n        self.loss_fn = loss_fn\n        self.optimizer = optimizer\n        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n        # Let's send the model to the specified device right away\n        self.model.to(self.device)\n\n        # These attributes are defined here, but since they are\n        # not informed at the moment of creation, we keep them None\n        self.train_loader = None\n        self.val_loader = None\n        self.writer = None\n        \n        # These attributes are going to be computed internally\n        self.losses = []\n        self.val_losses = []\n        self.total_epochs = 0\n\n        # Creates the train_step function for our model, \n        # loss function and optimizer\n        # Note: there are NO ARGS there! It makes use of the class\n        # attributes directly\n        self.train_step_fn = self._make_train_step_fn()\n        # Creates the val_step function for our model and loss\n        self.val_step_fn = self._make_val_step_fn()\n\n    def set_loaders(self, train_loader, val_loader=None):\n        # This method allows the user to define which train_loader (and val_loader, optionally) to use\n        # Both loaders are then assigned to attributes of the class\n        # So they can be referred to later\n        self.train_loader = train_loader\n        self.val_loader = val_loader\n\n    def _make_train_step_fn(self):\n        # This method does not need ARGS... it can refer to\n        # the attributes: self.model, self.loss_fn and self.optimizer\n        \n        # Builds function that performs a step in the train loop\n        def perform_train_step_fn(x, y):\n            # Sets model to TRAIN mode\n            self.model.train()\n\n            # Step 1 - Computes our model's predicted output - forward pass\n            yhat = self.model(x)\n            # Step 2 - Computes the loss\n            loss = self.loss_fn(yhat, y)\n            # Step 3 - Computes gradients for both \"a\" and \"b\" parameters\n            loss.backward()\n            # Step 4 - Updates parameters using gradients and the learning rate\n            self.optimizer.step()\n            self.optimizer.zero_grad()\n\n            # Returns the loss\n            return loss.item()\n\n        # Returns the function that will be called inside the train loop\n        return perform_train_step_fn\n    \n    def _make_val_step_fn(self):\n        # Builds function that performs a step in the validation loop\n        def perform_val_step_fn(x, y):\n            # Sets model to EVAL mode\n            self.model.eval()\n\n            # Step 1 - Computes our model's predicted output - forward pass\n            yhat = self.model(x)\n            # Step 2 - Computes the loss\n            loss = self.loss_fn(yhat, y)\n            # There is no need to compute Steps 3 and 4, \n            # since we don't update parameters during evaluation\n            return loss.item()\n\n        return perform_val_step_fn\n            \n    def _mini_batch(self, validation=False):\n        # The mini-batch can be used with both loaders\n        # The argument `validation`defines which loader and \n        # corresponding step function is going to be used\n        if validation:\n            data_loader = self.val_loader\n            step_fn = self.val_step_fn\n        else:\n            data_loader = self.train_loader\n            step_fn = self.train_step_fn\n\n        if data_loader is None:\n            return None\n            \n        # Once the data loader and step function, this is the \n        # same mini-batch loop we had before\n        mini_batch_losses = []\n        for x_batch, y_batch in data_loader:\n            x_batch = x_batch.to(self.device)\n            y_batch = y_batch.to(self.device)\n\n            mini_batch_loss = step_fn(x_batch, y_batch)\n            mini_batch_losses.append(mini_batch_loss)\n\n        loss = np.mean(mini_batch_losses)\n        return loss\n\n    def set_seed(self, seed=42):\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False    \n        torch.manual_seed(seed)\n        np.random.seed(seed)\n    \n    def train(self, n_epochs, seed=42):\n        # To ensure reproducibility of the training process\n        self.set_seed(seed)\n\n        for epoch in range(n_epochs):\n            # Keeps track of the numbers of epochs\n            # by updating the corresponding attribute\n            self.total_epochs += 1\n\n            # inner loop\n            # Performs training using mini-batches\n            loss = self._mini_batch(validation=False)\n            self.losses.append(loss)\n\n            # VALIDATION\n            # no gradients in validation!\n            with torch.no_grad():\n                # Performs evaluation using mini-batches\n                val_loss = self._mini_batch(validation=True)\n                self.val_losses.append(val_loss)\n\n            # If a SummaryWriter has been set...\n            if self.writer:\n                scalars = {'training': loss}\n                if val_loss is not None:\n                    scalars.update({'validation': val_loss})\n                # Records both losses for each epoch under the main tag \"loss\"\n                self.writer.add_scalars(main_tag='loss',\n                                        tag_scalar_dict=scalars,\n                                        global_step=epoch)\n            \n            print(f\"epoch: {epoch:3}, train loss: {loss:.5f}, valid loss: {val_loss:.5f}\")\n\n        if self.writer:\n            # Closes the writer\n            self.writer.close()\n\n    def predict(self, x):\n        # Set is to evaluation mode for predictions\n        self.model.eval() \n        # Takes aNumpy input and make it a float tensor\n        x_tensor = torch.as_tensor(x).float()\n        # Send input to device and uses model for prediction\n        y_hat_tensor = self.model(x_tensor.to(self.device))\n        # Set it back to train mode\n        self.model.train()\n        # Detaches it, brings it to CPU and back to Numpy\n        return y_hat_tensor.detach().cpu().numpy()\n\n    def plot_losses(self):\n        fig = plt.figure(figsize=(10, 4))\n        plt.plot(self.losses, label='Training Loss', c='b')\n        plt.plot(self.val_losses, label='Validation Loss', c='r')\n        plt.yscale('log')\n        plt.xlabel('Epochs')\n        plt.ylabel('Loss')\n        plt.legend()\n        plt.tight_layout()\n        return fig"
  },
  {
    "objectID": "posts/2022-10-18-pytorch-mnist-convolutional-neural-networks.html#create-lenet-5-model-configuration",
    "href": "posts/2022-10-18-pytorch-mnist-convolutional-neural-networks.html#create-lenet-5-model-configuration",
    "title": "Convolutional Neural Networks Filters and Feature Maps with PyTorch",
    "section": "Create LeNet-5 model configuration",
    "text": "Create LeNet-5 model configuration\n\n\n\narchitecture_lenet.png\n\n\nSource: Generated using Alexander Lenail’s NN-SVG and adapted by the author [“Daniel Voigt Godoy”]. For more details, see LeCun, Y., et al (1998). Gradient-based learning applied to document recognition. Proceedings of the IEEE,86(11), 2278–2324\nImage taken from dvgodoy/PyTorchStepByStep\n\nimport torch.nn as nn\n\nlenet = nn.Sequential()\n\n# Featurizer\n# Block 1: 1@28x28 -&gt; 6@28x28 -&gt; 6@14x14\nlenet.add_module('C1', nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5, padding=2))\nlenet.add_module('func1', nn.ReLU())\nlenet.add_module('S2', nn.MaxPool2d(kernel_size=2))\n# Block 2: 6@14x14 -&gt; 16@10x10 -&gt; 16@5x5\nlenet.add_module('C3', nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5))\nlenet.add_module('func2', nn.ReLU())\nlenet.add_module('S4', nn.MaxPool2d(kernel_size=2))\n# Block 3: 16@5x5 -&gt; 120@1x1\nlenet.add_module('C5', nn.Conv2d(in_channels=16, out_channels=120, kernel_size=5))\nlenet.add_module('func2', nn.ReLU())\n# Flattening\nlenet.add_module('flatten', nn.Flatten())\n\n# Classification\n# Hidden Layer\nlenet.add_module('F6', nn.Linear(in_features=120, out_features=84))\nlenet.add_module('func3', nn.ReLU())\n# Output Layer\nlenet.add_module('OUTPUT', nn.Linear(in_features=84, out_features=10))\n\nNow let’s create optimizer and a loss function.\n\nimport torch.optim as optim\n\nlr = 0.003\ntorch.manual_seed(42)\n\nmodel = lenet\n\n# Defines a SGD optimizer to update the parameters\noptimizer = optim.SGD(model.parameters(), lr=lr)\n\n# Defines a BCE loss function\nloss_fn = nn.CrossEntropyLoss()\n\nNow let’s train our model for 20 epochs.\n\nn_epochs = 20\n\ndlp = DeepLearningPipeline(model, loss_fn, optimizer)\ndlp.set_loaders(train_loader, valid_loader)\ndlp.train(n_epochs)\n\nepoch:   0, train loss: 2.29349, valid loss: 2.28632\nepoch:   1, train loss: 2.27442, valid loss: 2.25871\nepoch:   2, train loss: 2.22077, valid loss: 2.15503\nepoch:   3, train loss: 1.85172, valid loss: 1.27419\nepoch:   4, train loss: 0.82061, valid loss: 0.59110\nepoch:   5, train loss: 0.50765, valid loss: 0.45872\nepoch:   6, train loss: 0.41486, valid loss: 0.39295\nepoch:   7, train loss: 0.36466, valid loss: 0.35130\nepoch:   8, train loss: 0.33108, valid loss: 0.32108\nepoch:   9, train loss: 0.30512, valid loss: 0.29665\nepoch:  10, train loss: 0.28347, valid loss: 0.27550\nepoch:  11, train loss: 0.26473, valid loss: 0.25711\nepoch:  12, train loss: 0.24807, valid loss: 0.24056\nepoch:  13, train loss: 0.23314, valid loss: 0.22590\nepoch:  14, train loss: 0.21989, valid loss: 0.21282\nepoch:  15, train loss: 0.20804, valid loss: 0.20097\nepoch:  16, train loss: 0.19743, valid loss: 0.19028\nepoch:  17, train loss: 0.18791, valid loss: 0.18065\nepoch:  18, train loss: 0.17929, valid loss: 0.17199\nepoch:  19, train loss: 0.17142, valid loss: 0.16408\n\n\nLet’s see how our training and validation loss looks like.\n\nfig = dlp.plot_losses()"
  },
  {
    "objectID": "posts/2022-10-18-pytorch-mnist-convolutional-neural-networks.html#visualize-model-filters",
    "href": "posts/2022-10-18-pytorch-mnist-convolutional-neural-networks.html#visualize-model-filters",
    "title": "Convolutional Neural Networks Filters and Feature Maps with PyTorch",
    "section": "Visualize model filters",
    "text": "Visualize model filters\nNeural network convolutional layers are a stack of square matrices. We repeatedly apply these matrices or filters on images, and the output of this operation is called convolutions. These convolutions act as intermediate (or new) datasets generated from the images (a kind of dynamic feature engineering). We try to learn from these convolutions and calculate a loss for them. Initially, the value for these filters is randomly selected. If the loss is high, we try slowly changing values or weights for these filters. Changing filter values also create changes in outputs or convolutions. If a convolution results in a slight decrease in loss, we take it as a good sign and try to move in that direction (make more similar changes). If we try these steps repeatedly, we might find good convolutions (or good weights in the filter, as they are the source of convolutions). By “good weights”, we mean that the final loss is significantly lower than a random value.\nThese convolutional filters are created in PyTorch using class nn.Conv2d. A filter can have one matrix or a stack of matrices under it. Matrices under a filter are sometimes called kernels, but I will stick to the filter and matrix terms to avoid confusion.\nSo If we look at the LeNet-5 model configuration, we will find that we created three convolutional filters or nn.Conv2d layers. Let’s print the dimension of these layers.\n\nmodel_weights = [] \nconv_layers = [] \nmodel_children = list(model.children())\n\n# counter to keep count of the conv layers\ncounter = 0 \n# append all the conv layers and their respective weights to the list\nfor i in range(len(model_children)):\n    if type(model_children[i]) == nn.Conv2d:\n        counter += 1\n        model_weights.append(model_children[i].weight)\n        conv_layers.append(model_children[i])\n\nprint(f\"Total convolutional layers: {counter}\")\n\nfor weight, conv in zip(model_weights, conv_layers):\n    print(f\"CONV: {conv} ====&gt; SHAPE: {weight.shape}\")\n\nTotal convolutional layers: 3\nCONV: Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2)) ====&gt; SHAPE: torch.Size([6, 1, 5, 5])\nCONV: Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1)) ====&gt; SHAPE: torch.Size([16, 6, 5, 5])\nCONV: Conv2d(16, 120, kernel_size=(5, 5), stride=(1, 1)) ====&gt; SHAPE: torch.Size([120, 16, 5, 5])\n\n\nThe output of the above cell tells us that\n\nThere are three convolutional layers\nFirst layer has a dimension of [6, 1, 5, 5]. It means we have 6 filters in this layer. Each filter has 1 matrix of dimension 5x5 under it.\nSecond layer has a dimension of [16, 6, 5, 5]. It means we have 16 filters. Each filter has 6 matrices of size 5x5 under it.\nThird layer has a dimension of [120, 16, 5, 5]. It means we have 120 filters. Each filter has 16 matrices of size 5x5 under it.\n\nNote that the learned weights from these matrices are stored in model_weights list, which we will visualize in the next section.\n\n##\n# Helper function to visualize filters\ndef visualize_filters(layer, n_plots=30):\n    # Get layer dimensions. e.g. `[6, 1, 5, 5]` where\n    # filters=6, kernels=1, kernel_height=5, kernel_weight=5\n    filters, kernels, kernel_height, kernel_weight = layer.shape\n    # total plots = total number of matrices present in a layer. \n    # Each matrix weights can be plotted as an image\n    total_plots = filters * kernels\n    # total_plots can be too many. So let's create an upper limit on them as 'MAX_PLOTS'\n    MAX_PLOTS = min(n_plots, total_plots)\n    # number of columns for our plots 'MAX_COL_PLOTS'\n    MAX_COL_PLOTS = 10\n    # number of rows for our plots 'MAX_ROW_PLOTS'\n    MAX_ROW_PLOTS = max(MAX_PLOTS // MAX_COL_PLOTS, MAX_COL_PLOTS)\n\n    # specify some size of each plot image\n    plt.figure(figsize=(20, 2.5*MAX_ROW_PLOTS)) # width, height\n    plt.tight_layout()\n    \n    plot_count = 1\n    # interate filters\n    for f, filter in enumerate(layer):\n        # iterate kernels under each filter\n        for k, kernel in enumerate(filter):\n            # plot a single kernel or a matrix\n            plt.subplot(MAX_ROW_PLOTS, MAX_COL_PLOTS, plot_count)\n            plt.imshow(kernel[:, :].detach().cpu().numpy(), cmap='gray')\n            plt.title('#F:{} - K:{}'.format(f, k), {'size': 12})\n            plt.axis('off')\n            plot_count += 1\n            \n            # terminate on `MAX_PLOTS` \n            if plot_count &gt; MAX_PLOTS:\n                return plt.show()\n    \n    plt.show()\n\n\nVisualize weights for first ‘Conv2d’ layer\n\nvisualize_filters(model_weights[0], 50)\n\n\n\n\n\n\n\n\n\n\nVisualize weights for second ‘Conv2d’ layer\n\nvisualize_filters(model_weights[1], 50)\n\n\n\n\n\n\n\n\n\n\nVisualize weights for third ‘Conv2d’ layer\n\nvisualize_filters(model_weights[2], 50)\n\n\n\n\n\n\n\n\n\n\nWhat do these filter images tell us?\nThese learned filter (2d matrix) images seem very random. How can these filters create an output (convolution or intermediate dataset) that can help our model to learn and classify an image to its correct class? Filters usually learn to find edges and curves from images. When these filters are applied to images, they amplify certain aspects of these images, like edges, curves, lines, or other patterns.\nLooking just at the filters does not give us much information. So in the next section, we will visualize the outputs (or convolutions) produced by these filters."
  },
  {
    "objectID": "posts/2022-10-18-pytorch-mnist-convolutional-neural-networks.html#visualize-feature-maps-for-convolutional-layers",
    "href": "posts/2022-10-18-pytorch-mnist-convolutional-neural-networks.html#visualize-feature-maps-for-convolutional-layers",
    "title": "Convolutional Neural Networks Filters and Feature Maps with PyTorch",
    "section": "Visualize feature maps for convolutional layers",
    "text": "Visualize feature maps for convolutional layers\nThe output produced by a neural net layer is called its feature map. These layers can be convolutional, flattening, or linear (fully connected). For example, in our LeNet-5 model, we have three convolutional layers, and each of these layers produces a feature map. In this section, we will visualize them.\n\nDefine a hook\nA hook is simply a function we can give to our model to execute after its forward or backward pass. While attaching (or registering) a hook to a model, we provide the layer name on which we want to connect it. A hook function takes three arguments.\n\na layer or a model\na tensor representing the inputs to the layer or model\na tensor representing the outputs from the layer or model\n\nLet’s define a function that will serve as our hook.\n\n##\n# 'activation' is a dictionary to store the output from the layer\n# It should be defined outside the function, otherwise we will not be able to access it.\nactivation = {}\ndef get_activation(name):\n    def hook(model, input, output):\n        activation[name] = output.detach()\n    return hook\n\nLet’s also define a function to visualize the feature maps.\n\n##\n# Helper funtion to visualize the feature maps\ndef visualize_feature_map(layer, n_plots=30, cmap='gray', repeats=5, figsize=(5, 5)):\n    # get feature map values and store them as 'act'\n    act = layer.squeeze()\n\n    # if feature map has three dimension\n    if len(act.shape) == 3:\n        total_plots, plot_width, plot_height = act.shape\n\n        # total_plots can be too many so let's create an upper limit on them as 'MAX_PLOT'\n        MAX_PLOT = min(total_plots, n_plots)\n        # number of columns for our plots as 'MAX_COL_PLOTS'\n        MAX_COL_PLOTS = 6\n        # number of rows for our plots as 'MAX_ROW_PLOTs'\n        MAX_ROW_PLOTs = max(MAX_PLOT // MAX_COL_PLOTS, MAX_COL_PLOTS)\n\n        # specify some size for each plot image\n        plt.figure(figsize=(20, 3.5*MAX_ROW_PLOTs)) # width, height\n        plt.tight_layout()\n\n        plot_count = 0\n        for i in range(MAX_ROW_PLOTs):\n            for j in range(MAX_COL_PLOTS):\n                plt.subplot(MAX_ROW_PLOTs, MAX_COL_PLOTS, plot_count+1)\n                plt.imshow(act[plot_count].detach().cpu().numpy(), cmap=cmap)\n                plt.title('#R:{} - C:{}'.format(i, j), {'size': 12})\n                plt.axis('off')\n                plot_count += 1\n\n                # terminate if plot_count reaches MAX_PLOT\n                if plot_count &gt;= MAX_PLOT:\n                    return plt.show()\n    else: # len(act.shape) == 3\n    # if feature map has two dimension\n        arr_r = np.repeat(act.reshape(1,-1), repeats=repeats, axis=0)\n        plt.figure(figsize=figsize) # width, height\n        plt.tight_layout()\n        plt.imshow(arr_r, cmap=cmap)\n        plt.axis('off')\n\n        return plt.show()\n\n\n\nFeature map from first ‘Conv2d’ layer\nLet’s register our hook to first convolutional layer and visualize its feature map.\n\n##\n# empty 'activation' as a precaution\nactivation = {}\n\n# first 'Conv2d' layer is named as 'C1'\nhandle = model.C1.register_forward_hook(get_activation('C1'))\n\n# take any dataset image. image '7' is for three number\ndata, label = train_dataset[7]\ndata.unsqueeze_(0)\noutput = model(data)\n\n# remove hook\nhandle.remove()\n\nprint(f\"Dimensions for C1 feature map: {activation['C1'].squeeze().shape}\")\n\nDimensions for C1 feature map: torch.Size([6, 28, 28])\n\n\n\n##\n# visualize the feature map\n# dimensions: 6@28x28\nvisualize_feature_map(activation['C1'])\n\n\n\n\n\n\n\n\n\n\nFeature map from second ‘Conv2d’ layer\nLet’s register our hook to second convolutional layer and visualize its feature map.\n\n##\n# empty 'activation' as a precaution\nactivation = {}\n\n# second 'Conv2d' layer is named as 'C3'\nhandle = model.C3.register_forward_hook(get_activation('C3'))\n\n# take any dataset image. image '7' is for three number\ndata, label = train_dataset[7]\ndata.unsqueeze_(0)\noutput = model(data)\n\n# remove hook\nhandle.remove()\n\nprint(f\"Dimensions for C3 feature map: {activation['C3'].squeeze().shape}\")\n\nDimensions for C3 feature map: torch.Size([16, 10, 10])\n\n\n\n##\n# visualize the feature map\n# dimensions: 16@10x10\nvisualize_feature_map(activation['C3'])\n\n\n\n\n\n\n\n\n\n\nFeature maps from the first and second ‘Conv2d’ layers together\nFeature maps from the first layer show that they are sharper. Feature maps from the second layer show that they are more spread out or convolved. By spreading out or blurring effect, it seems like only the most significant features remain in the output, and the rest slowly disappear. For example, in the case of ‘3’ in the second feature map, only the horizontal edge signal remains, and any other signal gets dissolved.\nTo get more intuition of what is happening here, let’s visualize both feature maps together and for multiple images.\n\n##\n# Visualize feature maps for C1 and C3 together\n# Visualize them for first 5 train images\nfor i in range(5):\n    # just a separator.\n    print(f\"{'*'*30} IMAGE {i} {'*'*30}\")\n\n    # empty 'activation' as a precaution\n    activation = {}\n\n    # create hooks for C1 and C3\n    handle1 = model.C1.register_forward_hook(get_activation('C1'))\n    handle2 = model.C3.register_forward_hook(get_activation('C3'))\n    \n    data, _ = train_dataset[i]\n    data.unsqueeze_(0)\n    output = model(data)\n\n    # remove hooks\n    handle1.remove()\n    handle2.remove()\n\n    # visualize feature maps\n    # I have chaged the colors of output to sharpen the differences\n    visualize_feature_map(activation['C1'], cmap='viridis')\n    visualize_feature_map(activation['C3'], cmap='viridis')\n\n****************************** IMAGE 0 ******************************\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n****************************** IMAGE 1 ******************************\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n****************************** IMAGE 2 ******************************\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n****************************** IMAGE 3 ******************************\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n****************************** IMAGE 4 ******************************\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFeature map from third ‘Conv2d’ layer\nLet’s register our hook to third convolutional layer and visualize its feature map.\n\n##\n# empty 'activation' as a precaution\nactivation = {}\n\n# third 'Conv2d' layer is named as 'C5'\nhandle = model.C5.register_forward_hook(get_activation('C5'))\n\n# take any dataset image. image '7' is for three number\ndata, label = train_dataset[7]\ndata.unsqueeze_(0)\noutput = model(data)\n\n# remove hook\nhandle.remove()\n\nprint(f\"Dimensions for C5 feature map: {activation['C5'].squeeze().shape}\")\n\nDimensions for C5 feature map: torch.Size([120])\n\n\n\n##\n# visualize the feature map\n# dimensions: 120@1x1\nvisualize_feature_map(activation['C5'], figsize=(20,15))\n\n\n\n\n\n\n\n\n\n\nFeature map from third ‘Conv2d’ layer for multiple images\nThe output from the third ‘Conv2d’ layer C5 looks like a signature bar code that the model learns to associate with a particular target class. Let’s visualize more of them for a set of 1, 3, and 5 images together.\n\n##\n# create a collection of 1, 3, and 5 images\nbucket = {'1':[], '3':[], '5':[]}\n\n# iterate through the dataset till we have 5 images for each class\ncount_1, count_3, count_5 = 0,0,0\nfor idx in range(100):\n    _, label = train_dataset[idx]\n\n    if label == 1 and count_1 &lt; 5:\n        bucket[str(label)].append(idx)\n        count_1 +=1\n    elif label == 3 and count_3 &lt; 5:\n        bucket[str(label)].append(idx)\n        count_3 +=1\n    elif label == 5 and count_5 &lt; 5:\n        bucket[str(label)].append(idx)\n        count_5 +=1\n\n    if count_1 + count_3 + count_5 == 15:\n        break\n\n\n##\n# visualize feature maps for a set of images from class 1, 3 and 5\nfor key in bucket:\n    print(f\"{'*'*30} LAYER C5, LABEL {key} {'*'*30}\")\n\n    for i in range(len(bucket[key])):\n        activation = {}\n        \n        # attach hook\n        handle = model.C5.register_forward_hook(get_activation('C5'))\n        idx = bucket[key][i]\n        data, label = train_dataset[idx]\n        data.unsqueeze_(0)\n        output = model(data)\n        \n        # remove hook\n        handle.remove()\n        \n        # visualize feature map\n        # i have changed the output colormap to sharpen the differences\n        visualize_feature_map(activation['C5'], cmap='hsv', figsize=(20,15))\n\n****************************** LAYER C5, LABEL 1 ******************************\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n****************************** LAYER C5, LABEL 3 ******************************\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n****************************** LAYER C5, LABEL 5 ******************************"
  },
  {
    "objectID": "posts/2022-10-18-pytorch-mnist-convolutional-neural-networks.html#visualize-feature-maps-for-a-classifier-or-hidden-layer",
    "href": "posts/2022-10-18-pytorch-mnist-convolutional-neural-networks.html#visualize-feature-maps-for-a-classifier-or-hidden-layer",
    "title": "Convolutional Neural Networks Filters and Feature Maps with PyTorch",
    "section": "Visualize feature maps for a classifier or hidden layer",
    "text": "Visualize feature maps for a classifier or hidden layer\nWe have two hidden layers or ‘nn.Linear’ in the classification part of our model. They are also called fully connected layers. Let’s visualize the feature maps for them.\n\nVisualize feature map for first ‘Linear’ layer\nLet’s visualize the feature maps for the first hidden layer.\n\n# visualize differences for images 1,3 and 5\nfor key in bucket:\n    print(f\"{'*'*30} LAYER F6, LABEL {key} {'*'*30}\")\n    for i in range(len(bucket[key])):\n\n        activation = {}\n        handle1 = model.F6.register_forward_hook(get_activation('F6'))\n        idx = bucket[key][i]\n        data, label = train_dataset[idx]\n        \n        data.unsqueeze_(0)\n        output = model(data)\n        handle1.remove()\n        \n        visualize_feature_map(activation['F6'], cmap='hsv', figsize=(20,15))\n\n****************************** LAYER F6, LABEL 1 ******************************\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n****************************** LAYER F6, LABEL 3 ******************************\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n****************************** LAYER F6, LABEL 5 ******************************\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVisualize feature map for second ‘Linear’ layer\nLet’s visualize the feature maps for the second hidden layer.\n\n# visualize differences for images 1,3 and 5\nfor key in bucket:\n    print(f\"{'*'*30} LAYER OUTPUT, LABEL {key} {'*'*30}\")\n    for i in range(len(bucket[key])):\n\n        activation = {}\n        handle1 = model.OUTPUT.register_forward_hook(get_activation('OUTPUT'))\n        idx = bucket[key][i]\n        data, label = train_dataset[idx]\n        \n        data.unsqueeze_(0)\n        output = model(data)\n        handle1.remove()\n        \n        visualize_feature_map(activation['OUTPUT'], repeats=1,cmap='hsv', figsize=(10,5))\n\n****************************** LAYER OUTPUT, LABEL 1 ******************************\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n****************************** LAYER OUTPUT, LABEL 3 ******************************\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n****************************** LAYER OUTPUT, LABEL 5 ******************************\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhat do these OUTPUT feature maps tell us?\nWe can see that output feature maps of the same numbers are very similar. They show that our model has learned some hidden patterns that distinguish these images. For example\n\nFor label 1, all output feature maps have a RED bar at the extreme left and a BLUE bar at the right side\nFor label 3, there are two RED bars around the middle and a light BLUE bar running along them\nFor label 5, RED bars are in the middle and at the extreme right side.\n\nIf we look at the first feature map from label 5, it is a bit different. A kind of outlier from the rest of label 5 feature maps. What could be the reason for this?\nLet’s plot these label 5 images to check their actual shape.\n\n##\n# label 5 image indexes\nbucket['5']\n\n[0, 11, 35, 47, 65]\n\n\n\n##\n# Plot label 5 images\nplt.figure(figsize=(20, 10))\nfor i, idx in enumerate(bucket['5']):\n    plt.subplot(1, 5, i+1)\n    plt.imshow(train_dataset.data[idx], cmap='gray')\n    plt.title(f\"#Index:{idx} \")\n    plt.axis('off')\n\n\n\n\n\n\n\n\nNow we can see the reason for the output feature map of the first label ‘5’ image being quite different from the rest. The first image from the left side is slightly different from the rest. So it is a bit weird number five image, and that weirdness got reflected in the feature map generated by it."
  },
  {
    "objectID": "posts/2022-10-10-pytorch-linear-regression.html#introduction",
    "href": "posts/2022-10-10-pytorch-linear-regression.html#introduction",
    "title": "Linear Regression with PyTorch",
    "section": "Introduction",
    "text": "Introduction\nIn this notebook, we will train a linear regression model using PyTorch. Given below is the summary of the steps followed in this notebook.\n\nCreate a synthetic linear dataset\nSplit the data into Train and Validation datasets. Then convert them into mini-batches using PyTorch DataLoader class\nCreate a Linear Neural Net model configuration, an SGD optimizer, and a loss function\nCreate a pipeline that will train the model on given data and update the weights based on the loss\nView the training results using TensorBoard graphical view"
  },
  {
    "objectID": "posts/2022-10-10-pytorch-linear-regression.html#environment",
    "href": "posts/2022-10-10-pytorch-linear-regression.html#environment",
    "title": "Linear Regression with PyTorch",
    "section": "Environment",
    "text": "Environment\nThis notebook is prepared with Google Colab.\n\nGitHub: 2022-10-10-pytorch-linear-regression.ipynb\nOpen In Colab: \n\n\n\nCode\nfrom platform import python_version\nimport sklearn, numpy, matplotlib, pandas, torch\n\nprint(\"python==\" + python_version())\nprint(\"sklearn==\" + sklearn.__version__)\nprint(\"numpy==\" + numpy.__version__)\nprint(\"torch==\" + torch.__version__)\nprint(\"matplotlib==\" + matplotlib.__version__)\n\n\npython==3.7.14\nsklearn==1.0.2\nnumpy==1.21.6\ntorch==1.12.1+cu113\nmatplotlib==3.2.2"
  },
  {
    "objectID": "posts/2022-10-10-pytorch-linear-regression.html#credits",
    "href": "posts/2022-10-10-pytorch-linear-regression.html#credits",
    "title": "Linear Regression with PyTorch",
    "section": "Credits",
    "text": "Credits\nThis notebook takes inspiration from the book “Deep Learning with PyTorch Step-by-Step” by “Daniel Voigt Godoy”. You can get the book from its website: pytorchstepbystep. In addition, the GitHub repository for this book has valuable notebooks and can be used independently: github.com/dvgodoy/PyTorchStepByStep. Parts of the code you see in this notebook are taken from chapter 2 notebook of the same book."
  },
  {
    "objectID": "posts/2022-10-10-pytorch-linear-regression.html#generate-synthetic-linear-data",
    "href": "posts/2022-10-10-pytorch-linear-regression.html#generate-synthetic-linear-data",
    "title": "Linear Regression with PyTorch",
    "section": "Generate synthetic linear data",
    "text": "Generate synthetic linear data\nIn this section, we will generate some data representing a line using equation “y = mx + b”. y = mx + b is the slope intercept form of writing the equation of a straight line. In the equation ‘b’ is the point where the line intersects the ‘y axis’ and ‘m’ denotes the slope of the line. If you want to read more about this equation then follow this post: cuemath.com/geometry/y-mx-b\n\nimport numpy as np\nnp.random.seed(0)\n\ndef generate_linear_data(n_data_points=100, true_m=1, true_b=1):\n    \"\"\"\n    Generate linear data using equation: y = mx + b + e\n    where 'e' is some random noise added\n    \"\"\"\n    x = np.random.rand(n_data_points, 1)\n    y = true_m * x + true_b + (.1 * np.random.randn(n_data_points, 1))\n    return x, y\n\n# Let's generate 100 data points\nn_data_points = 100\ntrue_m = 2 # this is 'm' from slope-intercept line equation\ntrue_b = 1 # this is 'b' from slope-intercept line equation\nx, y = generate_linear_data(n_data_points, true_m, true_b)\n\nLet’s plot our generated data to see how it looks.\n\nimport matplotlib.pyplot as plt\n\nfig, ax = plt.subplots(1, 1, figsize=(6, 6))\nax.scatter(x, y)\nax.set_xlabel('x')\nax.set_ylabel('y')\nax.set_title('Generated Full Dataset')\nplt.show()"
  },
  {
    "objectID": "posts/2022-10-10-pytorch-linear-regression.html#load-generated-data-into-pytorch-dataset-and-dataloader-class",
    "href": "posts/2022-10-10-pytorch-linear-regression.html#load-generated-data-into-pytorch-dataset-and-dataloader-class",
    "title": "Linear Regression with PyTorch",
    "section": "Load generated data into PyTorch Dataset and DataLoader class",
    "text": "Load generated data into PyTorch Dataset and DataLoader class\nIn this section, we will load our data in PyTorch helper classes Dataset and DataLoader. PyTorch documentation defines them as: [see basics/data_tutorial]\n\nCode for processing data samples can get messy and hard to maintain; we ideally want our dataset code to be decoupled from our model training code for better readability and modularity. PyTorch provides two data primitives: torch.utils.data.DataLoader and torch.utils.data.Dataset that allow you to use pre-loaded datasets as well as your own data. Dataset stores the samples and their corresponding labels, and DataLoader wraps an iterable around the Dataset to enable easy access to the samples.\n\nFor this, we first need to convert NumPy data arrays to PyTorch tensors.\n\n## \n# Convert NumPy array to PyTorch tensors\nimport torch\n\nx_tensor = torch.as_tensor(x).float()\ny_tensor = torch.as_tensor(y).float()\n\nNow load the tensors into Dataset and DataLoader class. PyTorch Dataset is a helper class that converts data and labels into a list of tuples. DataLoader is another helper class to create batches from Dataset tuples. batch_size means the number of tuples we want in a single batch. We have used 16 here since our data is small. So each fetch from DataLoader will give us a list of 16 tuples.\n\n## \n# Load tensors into Dataset and DataLoader\nfrom torch.utils.data import DataLoader, TensorDataset, random_split\n\ndataset = TensorDataset(x_tensor, y_tensor)\n\n# Performs the 80-20% train-valid split\nratio = .8\nn_total = len(dataset)\nn_train = int(n_total * ratio)\nn_val = n_total - n_train\n\ntrain_data, val_data = random_split(dataset, [n_train, n_val])\n\n# Builds a loader of each set\n# Use batch_size = 16 as data size is small\ntrain_loader = DataLoader(\n    dataset=train_data,\n    batch_size=16,\n    shuffle=True\n)\n\nval_loader = DataLoader(dataset=val_data, batch_size=16)\n\nWe have our DataLoaders ready for training and validation set. DataLoader objects are iterators, and let’s extract data from them to plot.\n\n## \n# Visualize training and validation data\n\n# extrat train and validation sets from DataLoader as a list of tuples\ntrain_data_list = list(iter(train_data))\nval_data_list = list(iter(val_data))\n\n# get data and labels (x, y) from extracted tuples list\nx_train = [e[0].numpy() for e in train_data_list]\ny_train = [e[1].numpy() for e in train_data_list]\n\nx_val = [e[0].numpy() for e in val_data_list]\ny_val = [e[1].numpy() for e in val_data_list]\n\n# plot the data\nfigure, axes = plt.subplots(1, 3, figsize=(10,5))\nfigure.suptitle('Train and Validation Dataset')\n\naxes[0].set_title('Training Data')\naxes[0].scatter(x_train, y_train)\n\naxes[1].set_title('Validation Data')\naxes[1].scatter(x_val, y_val)\n\naxes[2].set_title('Combined Data')\naxes[2].scatter(x_train, y_train)\naxes[2].scatter(x_val, y_val)\nplt.show()"
  },
  {
    "objectID": "posts/2022-10-10-pytorch-linear-regression.html#create-model-configuration",
    "href": "posts/2022-10-10-pytorch-linear-regression.html#create-model-configuration",
    "title": "Linear Regression with PyTorch",
    "section": "Create model configuration",
    "text": "Create model configuration\nIn this section, we will configure the linear model for training, define a loss function, and an optimizer to update the weights.\n\n## \n# Model configuration\nimport torch.nn as nn\nimport torch.optim as optim\n\ntorch.manual_seed(0)\n\n# check gpu availability\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\n\n# Sets learning rate\nlr = 0.1\n\n# Now we can create a model and send it at once to the device\nmodel = nn.Linear(1, 1).to(device)\n\n# Defines a SGD optimizer to update the parameters (now retrieved directly from the model)\noptimizer = optim.SGD(model.parameters(), lr=lr)\n\n# Defines a MSE loss function\nloss_fn = nn.MSELoss(reduction='mean')\n\nWe have initialized a model with default weights. Let’s view them. Note that “weight” denotes m and “bias” denotes b from our line equation. At this point they are very random but once we have trained our model they will be much closer to true_m and true_b which we used to generate the data.\n\n## \n# model weights before training\nprint(model.state_dict())\n\nOrderedDict([('weight', tensor([[-0.0075]])), ('bias', tensor([0.5364]))])"
  },
  {
    "objectID": "posts/2022-10-10-pytorch-linear-regression.html#define-training-validation-and-mini-batch-processing-pipeline",
    "href": "posts/2022-10-10-pytorch-linear-regression.html#define-training-validation-and-mini-batch-processing-pipeline",
    "title": "Linear Regression with PyTorch",
    "section": "Define training, validation and mini-batch processing pipeline",
    "text": "Define training, validation and mini-batch processing pipeline\nIn this section, we will define our pipelines for training and validation.\n\nTraining pipeline is usually called “training step” which includes the following steps\n\nComputes our model’s predicted output - the forward pass\nComputes the loss\nComputes gradients i.e., find the direction and scale to update the weight to reduce the loss\nUpdates parameters using gradients and the learning rate\n\nValidation pipeline is usually called the “validation step” which includes the following steps\n\nComputes our model’s predicted output - the forward pass\nComputes the loss\n\n\nNote that during validation, we are only concerned about the loss, i.e., how well our model performs on the validation dataset. Therefore, we don’t use it to calculate the gradients.\nLet’s configure our training pipeline steps in a helper function.\n\n## \n# Training pipeline - training step\n\n# helper function for training\ndef make_train_step_fn(model, loss_fn, optimizer):\n    # Builds function that performs a step in the train loop\n    def perform_train_step_fn(x, y):\n        # Sets model to TRAIN mode\n        model.train() \n        \n        # Step 1 - Computes our model's predicted output - forward pass\n        yhat = model(x)\n        # Step 2 - Computes the loss\n        loss = loss_fn(yhat, y)\n        # Step 3 - Computes gradients\n        loss.backward()\n        # Step 4 - Updates parameters using gradients and the learning rate\n        optimizer.step()\n        optimizer.zero_grad()\n        \n        # Returns the loss\n        return loss.item()\n    \n    # Returns the function that will be called inside the train loop\n    return perform_train_step_fn\n\n# Creates the train_step function for our model, loss function and optimizer\ntrain_step_fn = make_train_step_fn(model, loss_fn, optimizer)\n\nLet’s now configure our validation pipeline steps in a helper function.\n\n## \n# Validation pipeline - validation step.\n\n# helper function for validation\ndef make_val_step_fn(model, loss_fn):\n    # Builds function that performs a step in the validation loop\n    def perform_val_step_fn(x, y):\n        # Sets model to EVAL mode\n        model.eval()\n        \n        # Step 1 - Computes our model's predicted output - forward pass\n        yhat = model(x)\n        # Step 2 - Computes the loss\n        loss = loss_fn(yhat, y)\n        # There is no need to compute Steps 3 and 4, since we don't update parameters during evaluation\n        return loss.item()\n    \n    return perform_val_step_fn\n\n# Creates the val_step function for our model and loss function\nval_step_fn = make_val_step_fn(model, loss_fn)\n\nNow let’s define the steps to process a single minibatch in a helper function. For a mini-batch processing, we want to\n\nGet the next batch of data and labels (x, y) from the DataLoader iterator\nPerform a step on the batch. A step can be either training or validation\nCompute the average batch loss\n\n\n## \n# Helper function for minibatch processing\ndef mini_batch(device, data_loader, step_fn):\n    mini_batch_losses = []\n    for x_batch, y_batch in data_loader:\n        x_batch = x_batch.to(device)\n        y_batch = y_batch.to(device)\n\n        mini_batch_loss = step_fn(x_batch, y_batch)\n        mini_batch_losses.append(mini_batch_loss)\n\n    loss = np.mean(mini_batch_losses)\n    return loss"
  },
  {
    "objectID": "posts/2022-10-10-pytorch-linear-regression.html#configure-tensorboard-to-visualize-loss-logs",
    "href": "posts/2022-10-10-pytorch-linear-regression.html#configure-tensorboard-to-visualize-loss-logs",
    "title": "Linear Regression with PyTorch",
    "section": "Configure TensorBoard to visualize loss logs",
    "text": "Configure TensorBoard to visualize loss logs\nIn this section we will configure TensorBoard to track and visualize training and validation loss.\n\nfrom torch.utils.tensorboard import SummaryWriter\nimport datetime\n\n# Creates a Summary Writer to interface with TensorBoard\ntimestamp = datetime.datetime.utcnow().strftime('%Y-%m-%d-%H.%M.%S')\nwriter = SummaryWriter(f'runs/simple_linear_regression/{timestamp}')\n\n# Fetches a single mini-batch so we can use add_graph\nx_sample, y_sample = next(iter(train_loader))\nwriter.add_graph(model, x_sample.to(device))"
  },
  {
    "objectID": "posts/2022-10-10-pytorch-linear-regression.html#execute-model-training-and-validation-pipeline",
    "href": "posts/2022-10-10-pytorch-linear-regression.html#execute-model-training-and-validation-pipeline",
    "title": "Linear Regression with PyTorch",
    "section": "Execute model training and validation pipeline",
    "text": "Execute model training and validation pipeline\nNow we are ready to execute our training pipeline. We will train our model for 200 epochs. An epoch is one cycle when the model has seen all the training data to compute loss, and we want our model to do it 200 times (200 epochs).\n\n#collapse-output\n# Execute pipeline with training and validation steps\nn_epochs = 200\n\nlosses = []\nval_losses = []\n\nfor epoch in range(n_epochs):\n    # training step\n    loss = mini_batch(device, train_loader, train_step_fn)\n    losses.append(loss)\n    \n    # validation step\n    # no gradients in validation!\n    with torch.no_grad():\n        val_loss = mini_batch(device, val_loader, val_step_fn)\n        val_losses.append(val_loss)\n    \n    # Records both losses for each epoch under the main tag \"loss\"\n    writer.add_scalars(main_tag='loss',\n                       tag_scalar_dict={'training': loss, 'validation': val_loss},\n                       global_step=epoch)\n    \n    print(f\"epoch: {epoch:3}, train loss: {loss:.5f}, valid loss: {val_loss:.5f}\")\n\n# Closes the writer\nwriter.close()\n\nepoch:   0, train loss: 1.17219, valid loss: 0.23260\nepoch:   1, train loss: 0.20757, valid loss: 0.11046\nepoch:   2, train loss: 0.13150, valid loss: 0.09627\nepoch:   3, train loss: 0.11224, valid loss: 0.08610\nepoch:   4, train loss: 0.09926, valid loss: 0.07644\nepoch:   5, train loss: 0.08905, valid loss: 0.06739\nepoch:   6, train loss: 0.07843, valid loss: 0.06012\nepoch:   7, train loss: 0.06852, valid loss: 0.05478\nepoch:   8, train loss: 0.06251, valid loss: 0.04898\nepoch:   9, train loss: 0.05494, valid loss: 0.04451\nepoch:  10, train loss: 0.04878, valid loss: 0.04031\nepoch:  11, train loss: 0.04447, valid loss: 0.03616\nepoch:  12, train loss: 0.03953, valid loss: 0.03173\nepoch:  13, train loss: 0.03562, valid loss: 0.02867\nepoch:  14, train loss: 0.03217, valid loss: 0.02642\nepoch:  15, train loss: 0.02985, valid loss: 0.02395\nepoch:  16, train loss: 0.02688, valid loss: 0.02295\nepoch:  17, train loss: 0.02487, valid loss: 0.02103\nepoch:  18, train loss: 0.02344, valid loss: 0.01901\nepoch:  19, train loss: 0.02130, valid loss: 0.01751\nepoch:  20, train loss: 0.01974, valid loss: 0.01663\nepoch:  21, train loss: 0.01851, valid loss: 0.01555\nepoch:  22, train loss: 0.01764, valid loss: 0.01392\nepoch:  23, train loss: 0.01664, valid loss: 0.01410\nepoch:  24, train loss: 0.01585, valid loss: 0.01355\nepoch:  25, train loss: 0.01513, valid loss: 0.01283\nepoch:  26, train loss: 0.01448, valid loss: 0.01217\nepoch:  27, train loss: 0.01387, valid loss: 0.01166\nepoch:  28, train loss: 0.01328, valid loss: 0.01125\nepoch:  29, train loss: 0.01293, valid loss: 0.01146\nepoch:  30, train loss: 0.01253, valid loss: 0.01059\nepoch:  31, train loss: 0.01227, valid loss: 0.01087\nepoch:  32, train loss: 0.01199, valid loss: 0.01042\nepoch:  33, train loss: 0.01176, valid loss: 0.00927\nepoch:  34, train loss: 0.01160, valid loss: 0.00912\nepoch:  35, train loss: 0.01164, valid loss: 0.00951\nepoch:  36, train loss: 0.01119, valid loss: 0.00945\nepoch:  37, train loss: 0.01131, valid loss: 0.00946\nepoch:  38, train loss: 0.01095, valid loss: 0.00918\nepoch:  39, train loss: 0.01084, valid loss: 0.00888\nepoch:  40, train loss: 0.01099, valid loss: 0.00943\nepoch:  41, train loss: 0.01072, valid loss: 0.00863\nepoch:  42, train loss: 0.01063, valid loss: 0.00899\nepoch:  43, train loss: 0.01063, valid loss: 0.00843\nepoch:  44, train loss: 0.01063, valid loss: 0.00809\nepoch:  45, train loss: 0.01066, valid loss: 0.00779\nepoch:  46, train loss: 0.01046, valid loss: 0.00831\nepoch:  47, train loss: 0.01051, valid loss: 0.00780\nepoch:  48, train loss: 0.01043, valid loss: 0.00790\nepoch:  49, train loss: 0.01035, valid loss: 0.00813\nepoch:  50, train loss: 0.01048, valid loss: 0.00796\nepoch:  51, train loss: 0.01032, valid loss: 0.00859\nepoch:  52, train loss: 0.01065, valid loss: 0.00835\nepoch:  53, train loss: 0.01059, valid loss: 0.00790\nepoch:  54, train loss: 0.01039, valid loss: 0.00782\nepoch:  55, train loss: 0.01033, valid loss: 0.00821\nepoch:  56, train loss: 0.01027, valid loss: 0.00832\nepoch:  57, train loss: 0.01024, valid loss: 0.00856\nepoch:  58, train loss: 0.01034, valid loss: 0.00853\nepoch:  59, train loss: 0.01027, valid loss: 0.00846\nepoch:  60, train loss: 0.01019, valid loss: 0.00783\nepoch:  61, train loss: 0.01051, valid loss: 0.00831\nepoch:  62, train loss: 0.01032, valid loss: 0.00790\nepoch:  63, train loss: 0.01022, valid loss: 0.00815\nepoch:  64, train loss: 0.01040, valid loss: 0.00749\nepoch:  65, train loss: 0.01020, valid loss: 0.00768\nepoch:  66, train loss: 0.01025, valid loss: 0.00783\nepoch:  67, train loss: 0.01040, valid loss: 0.00861\nepoch:  68, train loss: 0.01025, valid loss: 0.00764\nepoch:  69, train loss: 0.01018, valid loss: 0.00818\nepoch:  70, train loss: 0.01030, valid loss: 0.00771\nepoch:  71, train loss: 0.01033, valid loss: 0.00809\nepoch:  72, train loss: 0.01034, valid loss: 0.00747\nepoch:  73, train loss: 0.01032, valid loss: 0.00855\nepoch:  74, train loss: 0.01023, valid loss: 0.00852\nepoch:  75, train loss: 0.01019, valid loss: 0.00785\nepoch:  76, train loss: 0.01027, valid loss: 0.00751\nepoch:  77, train loss: 0.01027, valid loss: 0.00742\nepoch:  78, train loss: 0.01031, valid loss: 0.00723\nepoch:  79, train loss: 0.01021, valid loss: 0.00816\nepoch:  80, train loss: 0.01023, valid loss: 0.00829\nepoch:  81, train loss: 0.01028, valid loss: 0.00803\nepoch:  82, train loss: 0.01040, valid loss: 0.00823\nepoch:  83, train loss: 0.01024, valid loss: 0.00815\nepoch:  84, train loss: 0.01020, valid loss: 0.00818\nepoch:  85, train loss: 0.01024, valid loss: 0.00835\nepoch:  86, train loss: 0.01029, valid loss: 0.00797\nepoch:  87, train loss: 0.01021, valid loss: 0.00776\nepoch:  88, train loss: 0.01025, valid loss: 0.00859\nepoch:  89, train loss: 0.01021, valid loss: 0.00840\nepoch:  90, train loss: 0.01026, valid loss: 0.00804\nepoch:  91, train loss: 0.01020, valid loss: 0.00803\nepoch:  92, train loss: 0.01031, valid loss: 0.00883\nepoch:  93, train loss: 0.01028, valid loss: 0.00808\nepoch:  94, train loss: 0.01034, valid loss: 0.00778\nepoch:  95, train loss: 0.01023, valid loss: 0.00797\nepoch:  96, train loss: 0.01019, valid loss: 0.00826\nepoch:  97, train loss: 0.01033, valid loss: 0.00747\nepoch:  98, train loss: 0.01022, valid loss: 0.00785\nepoch:  99, train loss: 0.01021, valid loss: 0.00778\nepoch: 100, train loss: 0.01025, valid loss: 0.00782\nepoch: 101, train loss: 0.01022, valid loss: 0.00807\nepoch: 102, train loss: 0.01032, valid loss: 0.00796\nepoch: 103, train loss: 0.01017, valid loss: 0.00770\nepoch: 104, train loss: 0.01019, valid loss: 0.00778\nepoch: 105, train loss: 0.01017, valid loss: 0.00776\nepoch: 106, train loss: 0.01018, valid loss: 0.00766\nepoch: 107, train loss: 0.01027, valid loss: 0.00823\nepoch: 108, train loss: 0.01021, valid loss: 0.00783\nepoch: 109, train loss: 0.01037, valid loss: 0.00753\nepoch: 110, train loss: 0.01017, valid loss: 0.00747\nepoch: 111, train loss: 0.01045, valid loss: 0.00805\nepoch: 112, train loss: 0.01020, valid loss: 0.00815\nepoch: 113, train loss: 0.01027, valid loss: 0.00811\nepoch: 114, train loss: 0.01016, valid loss: 0.00790\nepoch: 115, train loss: 0.01016, valid loss: 0.00776\nepoch: 116, train loss: 0.01018, valid loss: 0.00758\nepoch: 117, train loss: 0.01020, valid loss: 0.00743\nepoch: 118, train loss: 0.01021, valid loss: 0.00791\nepoch: 119, train loss: 0.01032, valid loss: 0.00731\nepoch: 120, train loss: 0.01019, valid loss: 0.00788\nepoch: 121, train loss: 0.01025, valid loss: 0.00819\nepoch: 122, train loss: 0.01039, valid loss: 0.00786\nepoch: 123, train loss: 0.01032, valid loss: 0.00791\nepoch: 124, train loss: 0.01026, valid loss: 0.00745\nepoch: 125, train loss: 0.01021, valid loss: 0.00786\nepoch: 126, train loss: 0.01026, valid loss: 0.00747\nepoch: 127, train loss: 0.01028, valid loss: 0.00794\nepoch: 128, train loss: 0.01037, valid loss: 0.00768\nepoch: 129, train loss: 0.01029, valid loss: 0.00775\nepoch: 130, train loss: 0.01027, valid loss: 0.00805\nepoch: 131, train loss: 0.01019, valid loss: 0.00828\nepoch: 132, train loss: 0.01024, valid loss: 0.00804\nepoch: 133, train loss: 0.01033, valid loss: 0.00801\nepoch: 134, train loss: 0.01022, valid loss: 0.00773\nepoch: 135, train loss: 0.01034, valid loss: 0.00868\nepoch: 136, train loss: 0.01031, valid loss: 0.00792\nepoch: 137, train loss: 0.01045, valid loss: 0.00862\nepoch: 138, train loss: 0.01031, valid loss: 0.00853\nepoch: 139, train loss: 0.01034, valid loss: 0.00832\nepoch: 140, train loss: 0.01022, valid loss: 0.00793\nepoch: 141, train loss: 0.01019, valid loss: 0.00754\nepoch: 142, train loss: 0.01017, valid loss: 0.00781\nepoch: 143, train loss: 0.01025, valid loss: 0.00809\nepoch: 144, train loss: 0.01022, valid loss: 0.00810\nepoch: 145, train loss: 0.01020, valid loss: 0.00822\nepoch: 146, train loss: 0.01016, valid loss: 0.00778\nepoch: 147, train loss: 0.01042, valid loss: 0.00790\nepoch: 148, train loss: 0.01027, valid loss: 0.00781\nepoch: 149, train loss: 0.01032, valid loss: 0.00742\nepoch: 150, train loss: 0.01018, valid loss: 0.00779\nepoch: 151, train loss: 0.01032, valid loss: 0.00830\nepoch: 152, train loss: 0.01028, valid loss: 0.00748\nepoch: 153, train loss: 0.01045, valid loss: 0.00763\nepoch: 154, train loss: 0.01025, valid loss: 0.00754\nepoch: 155, train loss: 0.01020, valid loss: 0.00739\nepoch: 156, train loss: 0.01022, valid loss: 0.00768\nepoch: 157, train loss: 0.01021, valid loss: 0.00727\nepoch: 158, train loss: 0.01021, valid loss: 0.00834\nepoch: 159, train loss: 0.01026, valid loss: 0.00809\nepoch: 160, train loss: 0.01027, valid loss: 0.00814\nepoch: 161, train loss: 0.01041, valid loss: 0.00773\nepoch: 162, train loss: 0.01028, valid loss: 0.00737\nepoch: 163, train loss: 0.01017, valid loss: 0.00785\nepoch: 164, train loss: 0.01015, valid loss: 0.00795\nepoch: 165, train loss: 0.01021, valid loss: 0.00808\nepoch: 166, train loss: 0.01023, valid loss: 0.00769\nepoch: 167, train loss: 0.01027, valid loss: 0.00792\nepoch: 168, train loss: 0.01031, valid loss: 0.00753\nepoch: 169, train loss: 0.01026, valid loss: 0.00753\nepoch: 170, train loss: 0.01020, valid loss: 0.00774\nepoch: 171, train loss: 0.01027, valid loss: 0.00768\nepoch: 172, train loss: 0.01025, valid loss: 0.00806\nepoch: 173, train loss: 0.01019, valid loss: 0.00826\nepoch: 174, train loss: 0.01034, valid loss: 0.00841\nepoch: 175, train loss: 0.01025, valid loss: 0.00751\nepoch: 176, train loss: 0.01025, valid loss: 0.00740\nepoch: 177, train loss: 0.01026, valid loss: 0.00800\nepoch: 178, train loss: 0.01045, valid loss: 0.00810\nepoch: 179, train loss: 0.01028, valid loss: 0.00799\nepoch: 180, train loss: 0.01039, valid loss: 0.00827\nepoch: 181, train loss: 0.01020, valid loss: 0.00768\nepoch: 182, train loss: 0.01031, valid loss: 0.00794\nepoch: 183, train loss: 0.01027, valid loss: 0.00806\nepoch: 184, train loss: 0.01019, valid loss: 0.00821\nepoch: 185, train loss: 0.01035, valid loss: 0.00847\nepoch: 186, train loss: 0.01026, valid loss: 0.00766\nepoch: 187, train loss: 0.01032, valid loss: 0.00747\nepoch: 188, train loss: 0.01025, valid loss: 0.00788\nepoch: 189, train loss: 0.01025, valid loss: 0.00774\nepoch: 190, train loss: 0.01027, valid loss: 0.00853\nepoch: 191, train loss: 0.01024, valid loss: 0.00778\nepoch: 192, train loss: 0.01026, valid loss: 0.00717\nepoch: 193, train loss: 0.01019, valid loss: 0.00781\nepoch: 194, train loss: 0.01017, valid loss: 0.00754\nepoch: 195, train loss: 0.01024, valid loss: 0.00798\nepoch: 196, train loss: 0.01019, valid loss: 0.00760\nepoch: 197, train loss: 0.01025, valid loss: 0.00762\nepoch: 198, train loss: 0.01025, valid loss: 0.00773\nepoch: 199, train loss: 0.01018, valid loss: 0.00771\n\n\nThat is it! We have trained our linear model and can view its learned weight and bias. Note that after training, they are much closer to true_m and true_b, which we used to generate the data. We can say that our model has learned the inherent pattern from within the data.\n\n## \n# model weights after training\nprint(model.state_dict())\n\nOrderedDict([('weight', tensor([[1.9914]])), ('bias', tensor([1.0314]))])"
  },
  {
    "objectID": "posts/2022-10-10-pytorch-linear-regression.html#visualize-loss-logs-from-tensorboard",
    "href": "posts/2022-10-10-pytorch-linear-regression.html#visualize-loss-logs-from-tensorboard",
    "title": "Linear Regression with PyTorch",
    "section": "Visualize loss logs from TensorBoard",
    "text": "Visualize loss logs from TensorBoard\nLet’s view how our training and validation loss has progressed throughout the training using TensorBoard.\n\nf'runs/simple_linear_regression/{timestamp}'\n\n'runs/simple_linear_regression/2022-10-10-12.02.43'\n\n\n\n%load_ext tensorboard\n%tensorboard --logdir runs\n\n\n\n\n\n\n\ntensorboard.PNG"
  },
  {
    "objectID": "posts/2022-08-05-sagemaker-feature-store.html#data-source",
    "href": "posts/2022-08-05-sagemaker-feature-store.html#data-source",
    "title": "Building a Feature Repository with SageMaker Feature Store",
    "section": "Data source",
    "text": "Data source\n[Moro et al., 2014] S. Moro, P. Cortez and P. Rita. A Data-Driven Approach to Predict the Success of Bank Telemarketing. Decision Support Systems, Elsevier, 62:22-31, June 2014"
  },
  {
    "objectID": "posts/2022-08-05-sagemaker-feature-store.html#data-link",
    "href": "posts/2022-08-05-sagemaker-feature-store.html#data-link",
    "title": "Building a Feature Repository with SageMaker Feature Store",
    "section": "Data link",
    "text": "Data link\nUCI Machine Learning Repository: https://archive.ics.uci.edu/ml/datasets/bank+marketing. Dataset has multiple files. we will use bank-additional-full.csv file that has all examples (41188) and 20 inputs, ordered by date."
  },
  {
    "objectID": "posts/2022-08-05-sagemaker-feature-store.html#data-classification-goaltarget",
    "href": "posts/2022-08-05-sagemaker-feature-store.html#data-classification-goaltarget",
    "title": "Building a Feature Repository with SageMaker Feature Store",
    "section": "Data classification goal/target",
    "text": "Data classification goal/target\nThe classification goal is to predict if the client will subscribe (yes/no) a term deposit (variable y)"
  },
  {
    "objectID": "posts/2022-08-05-sagemaker-feature-store.html#data-attributes-information",
    "href": "posts/2022-08-05-sagemaker-feature-store.html#data-attributes-information",
    "title": "Building a Feature Repository with SageMaker Feature Store",
    "section": "Data attributes information",
    "text": "Data attributes information\n\nInput variables\nattributes from bank client data 1. age (numeric) 2. job : type of job (categorical: ‘admin.’, ‘blue-collar’, ‘entrepreneur’, ‘housemaid’, ‘management’, ‘retired’, ‘self-employed’, ‘services’,‘student’, ‘technician’, ‘unemployed’, ‘unknown’) 3. marital : marital status (categorical: ‘divorced’, ‘married’, ‘single’, ‘unknown’; note: ‘divorced’ means divorced or widowed) 4. education (categorical: ‘basic.4y’, ‘basic.6y’, ‘basic.9y’, ‘high.school’, ‘illiterate’,‘professional.course’, ‘university.degree’,‘unknown’) 5. default: has credit in default? (categorical: ‘no’,‘yes’,‘unknown’) 6. housing: has housing loan? (categorical: ‘no’,‘yes’,‘unknown’) 7. loan: has personal loan? (categorical: ‘no’,‘yes’,‘unknown’)\nattributes related with the last contact of the current campaign\n\ncontact: contact communication type (categorical: ‘cellular’,‘telephone’)\nmonth: last contact month of year (categorical: ‘jan’, ‘feb’, ‘mar’, …, ‘nov’, ‘dec’)\nday_of_week: last contact day of the week (categorical: ‘mon’,‘tue’,‘wed’,‘thu’,‘fri’)\nduration: last contact duration, in seconds (numeric). Important note: this attribute highly affects the output target (e.g., if duration=0 then y=‘no’). Yet, the duration is not known before a call is performed. Also, after the end of the call y is obviously known. Thus, this input should only be included for benchmark purposes and should be discarded if the intention is to have a realistic predictive model.\n\nother attributes 12. campaign: number of contacts performed during this campaign and for this client (numeric, includes last contact) 13. pdays: number of days that passed by after the client was last contacted from a previous campaign (numeric; 999 means client was not previously contacted) 14. previous: number of contacts performed before this campaign and for this client (numeric) 15. poutcome: outcome of the previous marketing campaign (categorical: ‘failure’,‘nonexistent’,‘success’)\nsocial and economic context attributes 16. emp.var.rate: employment variation rate - quarterly indicator (numeric) 17. cons.price.idx: consumer price index - monthly indicator (numeric) 18. cons.conf.idx: consumer confidence index - monthly indicator (numeric) 19. euribor3m: euribor 3 month rate - daily indicator (numeric) 20. nr.employed: number of employees - quarterly indicator (numeric)\n\n\nOutput variable (desired target)\n\ny: has the client subscribed a term deposit? (binary: ‘yes’,‘no’)"
  },
  {
    "objectID": "posts/2022-08-05-sagemaker-feature-store.html#considerations-for-creating-a-feature-group",
    "href": "posts/2022-08-05-sagemaker-feature-store.html#considerations-for-creating-a-feature-group",
    "title": "Building a Feature Repository with SageMaker Feature Store",
    "section": "Considerations for creating a feature group",
    "text": "Considerations for creating a feature group\n\nSupported data types in feature group are: string, integral, and fractional\nThere should be a feature that can uniquely identify each row\nThere should be a feature that defines event time (event_time). This feature is required for versioning and time travel. Excepted data types for this feature are string or fractional.\n\nFor String type event time has to be ISO-8601 format in UTC time with the yyyy-MM-dd'T'HH:mm:ssZ or yyyy-MM-dd'T'HH:mm:ss.SSSZ patterns\nFor Fractional type, the values are expected to be in seconds from Unix epoch time with millisecond precision\n\n\nOur dataset does not have a feature that can uniquely identify each row. So let’s create one.\n\n##\n# 'FS_id' defines unique id for each row\ndf['FS_id'] = df.index\n\nSimilarly, we also need to create an event time feature. For this, we will use string type with yyyy-MM-dd'T'HH:mm:ss.SSSZ pattern.\n\nfrom datetime import datetime, timezone, date\n\ndef generate_event_timestamp():\n    # naive datetime representing local time\n    naive_dt = datetime.now()\n    # take timezone into account\n    aware_dt = naive_dt.astimezone()\n    # time in UTC\n    utc_dt = aware_dt.astimezone(timezone.utc)\n    # transform to ISO-8601 format\n    event_time = utc_dt.isoformat(timespec=\"milliseconds\")\n    event_time = event_time.replace(\"+00:00\", \"Z\")\n    return event_time\n\n\ngenerate_event_timestamp()\n\n'2022-08-08T06:06:07.059Z'\n\n\n\n##\n# `FS_event_time` contains event timestamps\ndf['FS_event_time'] = [generate_event_timestamp() for _ in range(len(df))]\n\nLet’s check our dataset with two new features.\n\ndf.head()\n\n\n\n\n\n\n\n\nage\njob\nmarital\neducation\ndefault\nhousing\nloan\ncontact\nmonth\nday_of_week\nduration\ncampaign\npdays\nprevious\npoutcome\nemp.var.rate\ncons.price.idx\ncons.conf.idx\neuribor3m\nnr.employed\ny\nFS_id\nFS_event_time\n\n\n\n\n0\n56\nhousemaid\nmarried\nbasic.4y\nno\nno\nno\ntelephone\nmay\nmon\n261\n1\n999\n0\nnonexistent\n1.1\n93.994\n-36.4\n4.857\n5191.0\nno\n0\n2022-08-08T06:06:07.524Z\n\n\n1\n57\nservices\nmarried\nhigh.school\nunknown\nno\nno\ntelephone\nmay\nmon\n149\n1\n999\n0\nnonexistent\n1.1\n93.994\n-36.4\n4.857\n5191.0\nno\n1\n2022-08-08T06:06:07.524Z\n\n\n2\n37\nservices\nmarried\nhigh.school\nno\nyes\nno\ntelephone\nmay\nmon\n226\n1\n999\n0\nnonexistent\n1.1\n93.994\n-36.4\n4.857\n5191.0\nno\n2\n2022-08-08T06:06:07.524Z\n\n\n3\n40\nadmin.\nmarried\nbasic.6y\nno\nno\nno\ntelephone\nmay\nmon\n151\n1\n999\n0\nnonexistent\n1.1\n93.994\n-36.4\n4.857\n5191.0\nno\n3\n2022-08-08T06:06:07.524Z\n\n\n4\n56\nservices\nmarried\nhigh.school\nno\nno\nyes\ntelephone\nmay\nmon\n307\n1\n999\n0\nnonexistent\n1.1\n93.994\n-36.4\n4.857\n5191.0\nno\n4\n2022-08-08T06:06:07.524Z\n\n\n\n\n\n\n\nInitialize SageMaker session.\n\nimport sagemaker\n\nsession = sagemaker.Session()\nrole = sagemaker.get_execution_role()\nbucket = session.default_bucket()\nregion = session.boto_region_name\n\nprint(\"sagemaker.__version__: \", sagemaker.__version__)\nprint(\"Session: \", session)\nprint(\"Role: \", role)\nprint(\"Bucket: \", bucket)\nprint(\"Region: \", region)\n\nsagemaker.__version__:  2.99.0\nSession:  &lt;sagemaker.session.Session object at 0x7fb40934c890&gt;\nRole:  arn:aws:iam::801598032724:role/service-role/AmazonSageMaker-ExecutionRole-20220804T174502\nBucket:  sagemaker-us-east-1-801598032724\nRegion:  us-east-1\n\n\n\n# You may choose any other prefix for your bucket.\n# All the data related to this post will be under this prefix.\nbucket_prefix = \"2022-08-05-sagemaker-feature-store\"\n\nFeature store requires an S3 location for storing the ingested data. Let’s define it as well.\n\nfs_offline_bucket_studio = f\"s3://{bucket}/{bucket_prefix}/fs_offline/studio\"\nfs_offline_bucket_studio\n\n's3://sagemaker-us-east-1-801598032724/2022-08-05-sagemaker-feature-store/fs_offline/studio'"
  },
  {
    "objectID": "posts/2022-08-05-sagemaker-feature-store.html#create-feature-group-from-sagemaker-studio-ide",
    "href": "posts/2022-08-05-sagemaker-feature-store.html#create-feature-group-from-sagemaker-studio-ide",
    "title": "Building a Feature Repository with SageMaker Feature Store",
    "section": "Create feature group from SageMaker studio IDE",
    "text": "Create feature group from SageMaker studio IDE\nLet’s see how we can create a feature group using SageMaker Studio IDE. You don’t need to write any code while creating a feature group using Studio IDE. From the left sidebar, use the SageMaker Resources menu to open the Feature Group pane, and click the create feature group option. This will open a new tab in IDE to create a feature group.\n\n\n\ncreate-feature-group.PNG\n\n\nOn the create feature group tab, define the following settings: - Feature group name : “bank-marketing-studio” - Description (optional) : “The data is related to direct marketing campaigns (phone calls) of a Portuguese banking institution.” - Feature group storage configurations - Enable online store : Check this box. Note that for the online store there is no S3 bucket requirement. - Enable offline store : Check this box too. - Enter S3 location from fs_offline_bucket_studio - IAM Role ARN : Default SageMaker role. - Enable Data Catalog for offline store - Select continue\nOn the next page, you will be asked to specify feature definitions. There are two ways to define them. - Using Table, and manually fill each feature and its type - Using JSON. We will use this option to define the features and their types.\nRemember that the feature group only supports three data types: string, integral, and fractional. So we need to create a mapping between Pandas Dataframe data types and that of a feature store. - “object” -&gt; “String” - “int64” -&gt; “Integral” - “float64” -&gt; “Fractional”\n\n##\n# map DataFrame types to feature group.\ndef get_mapping(dt):\n\n    feature_store_dtype_mapping = {\n        \"object\": \"String\",\n        \"int64\": \"Integral\",\n        \"float64\": \"Fractional\",\n    }\n\n    return feature_store_dtype_mapping[str(dt)]\n\n\n##\n# DataFrame feature data types\ndf.dtypes\n\nage                 int64\njob                object\nmarital            object\neducation          object\ndefault            object\nhousing            object\nloan               object\ncontact            object\nmonth              object\nday_of_week        object\nduration            int64\ncampaign            int64\npdays               int64\nprevious            int64\npoutcome           object\nemp.var.rate      float64\ncons.price.idx    float64\ncons.conf.idx     float64\neuribor3m         float64\nnr.employed       float64\ny                  object\nFS_id               int64\nFS_event_time      object\ndtype: object\n\n\n\n##\n# prepare list of feature names and correct data types\nfeature_names = df.columns.tolist()\nfeature_types = [get_mapping(dt) for dt in df.dtypes]\n\nFeature names allow alphanumeric characters including dashes and underscores. So let’s remove the “.” character from the feature names.\n\n##\n# fix feature names\nfor indx in range(len(feature_names)):\n    feature_names[indx] = feature_names[indx].replace(\".\", \"_\")\n\n\n##\n# corrected feature names\nfeature_names\n\n['age',\n 'job',\n 'marital',\n 'education',\n 'default',\n 'housing',\n 'loan',\n 'contact',\n 'month',\n 'day_of_week',\n 'duration',\n 'campaign',\n 'pdays',\n 'previous',\n 'poutcome',\n 'emp_var_rate',\n 'cons_price_idx',\n 'cons_conf_idx',\n 'euribor3m',\n 'nr_employed',\n 'y',\n 'FS_id',\n 'FS_event_time']\n\n\nNow we are ready to prepare JSON for feature definitions. JSON created should be of the following format.\n[\n    {\n        \"FeatureName\": \"age\",\n        \"FeatureType\": \"Integral\"\n    }\n]\nLet’s prepare it.\n\ndf_features = pd.DataFrame({\"FeatureName\": feature_names, \"FeatureType\": feature_types})\n\nprint(df_features.to_json(orient=\"records\"))\n\n[{\"FeatureName\":\"age\",\"FeatureType\":\"Integral\"},{\"FeatureName\":\"job\",\"FeatureType\":\"String\"},{\"FeatureName\":\"marital\",\"FeatureType\":\"String\"},{\"FeatureName\":\"education\",\"FeatureType\":\"String\"},{\"FeatureName\":\"default\",\"FeatureType\":\"String\"},{\"FeatureName\":\"housing\",\"FeatureType\":\"String\"},{\"FeatureName\":\"loan\",\"FeatureType\":\"String\"},{\"FeatureName\":\"contact\",\"FeatureType\":\"String\"},{\"FeatureName\":\"month\",\"FeatureType\":\"String\"},{\"FeatureName\":\"day_of_week\",\"FeatureType\":\"String\"},{\"FeatureName\":\"duration\",\"FeatureType\":\"Integral\"},{\"FeatureName\":\"campaign\",\"FeatureType\":\"Integral\"},{\"FeatureName\":\"pdays\",\"FeatureType\":\"Integral\"},{\"FeatureName\":\"previous\",\"FeatureType\":\"Integral\"},{\"FeatureName\":\"poutcome\",\"FeatureType\":\"String\"},{\"FeatureName\":\"emp_var_rate\",\"FeatureType\":\"Fractional\"},{\"FeatureName\":\"cons_price_idx\",\"FeatureType\":\"Fractional\"},{\"FeatureName\":\"cons_conf_idx\",\"FeatureType\":\"Fractional\"},{\"FeatureName\":\"euribor3m\",\"FeatureType\":\"Fractional\"},{\"FeatureName\":\"nr_employed\",\"FeatureType\":\"Fractional\"},{\"FeatureName\":\"y\",\"FeatureType\":\"String\"},{\"FeatureName\":\"FS_id\",\"FeatureType\":\"Integral\"},{\"FeatureName\":\"FS_event_time\",\"FeatureType\":\"String\"}]\n\n\nCopy the JSON from the last cell output and past it in feature definition JSON input. Click continue\n\n\n\nfeature-definition.PNG\n\n\nOn the next page, it will ask for the required features. - record identifier feature name : select FS_ID - event type feature name : select FS_event_time\nClick continue and create the feature group.\n\n\n\nfeature-group-created.PNG"
  },
  {
    "objectID": "posts/2022-08-05-sagemaker-feature-store.html#create-feature-group-from-sagemaker-sdk",
    "href": "posts/2022-08-05-sagemaker-feature-store.html#create-feature-group-from-sagemaker-sdk",
    "title": "Building a Feature Repository with SageMaker Feature Store",
    "section": "Create feature group from SageMaker SDK",
    "text": "Create feature group from SageMaker SDK\nWe have seen how we can create a feature group from SageMaker studio IDE. Let’s also see how to create it using SageMaker SDK.\n\n##\n# define a feature group\nfrom sagemaker.feature_store.feature_group import FeatureGroup\n\nfeature_group_name = \"bank-marketing-sdk\"\nfeature_group = FeatureGroup(name=feature_group_name, sagemaker_session=session)\n\nI have created a FeatureGroup, now we need to define its schema (FeatureDefinitions). When I check the SageMaker Python SDK Feature Store APIs reference, I could not find any method to provide FeatureDefinitions to a feature group. But feature store documentation examples amazon_sagemaker_featurestore mention that we can use feature_group.load_feature_definitions() method to load the feature definitions from Pandas dataframe. When I checked the sagemaker-python-sdk GitHub page there is an open issue that says “The documentation does not include the load_feature_definitions() method for the FeatureGroup class”, and is still open.\nTo get more understanding of this method we can check the source code for sagemaker feature group class github.com/aws/sagemaker-python-sdk/blob/master/src/sagemaker/feature_store/feature_group.py. If we check the signature and documentation for this method it says:\ndef load_feature_definitions(\n        self,\n        data_frame: DataFrame,\n    ) -&gt; Sequence[FeatureDefinition]:\n    \n        \"\"\"Load feature definitions from a Pandas DataFrame.\n        Column name is used as feature name. Feature type is inferred from the dtype\n        of the column. Dtype int_, int8, int16, int32, int64, uint8, uint16, uint32\n        and uint64 are mapped to Integral feature type. Dtype float_, float16, float32\n        and float64 are mapped to Fractional feature type. string dtype is mapped to\n        String feature type.\n        No feature definitions will be loaded if the given data_frame contains\n        unsupported dtypes.\n        Args:\n            data_frame (DataFrame):\n        Returns:\n            list of FeatureDefinition\n        \"\"\"\nThat is * It loads feature definitions from a Pandas DataFrame * DataFrame column names are used as feature names * Feature types are inferred from the dtype of columns * Dtype int_, int8, int16, int32, int64, uint8, uint16, uint32 and uint64 are mapped to Integral feature type * Dtype float_, float16, float32 and float64 are mapped to Fractional feature type * Dtype string is mapped to String feature type * No feature definitions will be loaded if the given data_frame contains unsupported dtypes\nIn the last section, we have seen that our dataframe has object data types that are not supported. For backward compatibility reasons, Pandas DataFrame infers columns with strings as object data type. With Pandas 1.0 onwards we can explicitly use string type for such columns.\nLet’s see what happens when we use unsupported data types for feature definition.\n\n##\n# load unsupported feature definitions. This will generate an error.\nfeature_group.load_feature_definitions(data_frame=df)\n\n\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\n&lt;ipython-input-24-f77a8abe3ce7&gt; in &lt;module&gt;\n      1 ##\n      2 # load unsupported feature definitions. This will generate an error.\n----&gt; 3 feature_group.load_feature_definitions(data_frame=df)\n\n/opt/conda/lib/python3.7/site-packages/sagemaker/feature_store/feature_group.py in load_feature_definitions(self, data_frame)\n    570             else:\n    571                 raise ValueError(\n--&gt; 572                     f\"Failed to infer Feature type based on dtype {data_frame[column].dtype} \"\n    573                     f\"for column {column}.\"\n    574                 )\n\nValueError: Failed to infer Feature type based on dtype object for column job.\n\n\n\nIt throws an error, “ValueError: Failed to infer Feature type based on dtype object for column job.”\nOkay, let’s convet columns to proper data types.\n\n##\n# list of columns with `object` data type\ndf.select_dtypes(\"object\").columns.tolist()\n\n['job',\n 'marital',\n 'education',\n 'default',\n 'housing',\n 'loan',\n 'contact',\n 'month',\n 'day_of_week',\n 'poutcome',\n 'y',\n 'FS_event_time']\n\n\n\n##\n# covert `object` columns to `string` data type\nfor col in df.select_dtypes(\"object\").columns.tolist():\n    df[col] = df[col].astype(\"string\")\n\nLet’s verify the data types of all columns.\n\ndf.dtypes\n\nage                 int64\njob                string\nmarital            string\neducation          string\ndefault            string\nhousing            string\nloan               string\ncontact            string\nmonth              string\nday_of_week        string\nduration            int64\ncampaign            int64\npdays               int64\nprevious            int64\npoutcome           string\nemp.var.rate      float64\ncons.price.idx    float64\ncons.conf.idx     float64\neuribor3m         float64\nnr.employed       float64\ny                  string\nFS_id               int64\nFS_event_time      string\ndtype: object\n\n\n\ndf.head()\n\n\n\n\n\n\n\n\nage\njob\nmarital\neducation\ndefault\nhousing\nloan\ncontact\nmonth\nday_of_week\nduration\ncampaign\npdays\nprevious\npoutcome\nemp.var.rate\ncons.price.idx\ncons.conf.idx\neuribor3m\nnr.employed\ny\nFS_id\nFS_event_time\n\n\n\n\n0\n56\nhousemaid\nmarried\nbasic.4y\nno\nno\nno\ntelephone\nmay\nmon\n261\n1\n999\n0\nnonexistent\n1.1\n93.994\n-36.4\n4.857\n5191.0\nno\n0\n2022-08-08T06:06:07.524Z\n\n\n1\n57\nservices\nmarried\nhigh.school\nunknown\nno\nno\ntelephone\nmay\nmon\n149\n1\n999\n0\nnonexistent\n1.1\n93.994\n-36.4\n4.857\n5191.0\nno\n1\n2022-08-08T06:06:07.524Z\n\n\n2\n37\nservices\nmarried\nhigh.school\nno\nyes\nno\ntelephone\nmay\nmon\n226\n1\n999\n0\nnonexistent\n1.1\n93.994\n-36.4\n4.857\n5191.0\nno\n2\n2022-08-08T06:06:07.524Z\n\n\n3\n40\nadmin.\nmarried\nbasic.6y\nno\nno\nno\ntelephone\nmay\nmon\n151\n1\n999\n0\nnonexistent\n1.1\n93.994\n-36.4\n4.857\n5191.0\nno\n3\n2022-08-08T06:06:07.524Z\n\n\n4\n56\nservices\nmarried\nhigh.school\nno\nno\nyes\ntelephone\nmay\nmon\n307\n1\n999\n0\nnonexistent\n1.1\n93.994\n-36.4\n4.857\n5191.0\nno\n4\n2022-08-08T06:06:07.524Z\n\n\n\n\n\n\n\nLet’s load the feature definitions again.\n\nfeature_group.load_feature_definitions(data_frame=df)\n\n[FeatureDefinition(feature_name='age', feature_type=&lt;FeatureTypeEnum.INTEGRAL: 'Integral'&gt;),\n FeatureDefinition(feature_name='job', feature_type=&lt;FeatureTypeEnum.STRING: 'String'&gt;),\n FeatureDefinition(feature_name='marital', feature_type=&lt;FeatureTypeEnum.STRING: 'String'&gt;),\n FeatureDefinition(feature_name='education', feature_type=&lt;FeatureTypeEnum.STRING: 'String'&gt;),\n FeatureDefinition(feature_name='default', feature_type=&lt;FeatureTypeEnum.STRING: 'String'&gt;),\n FeatureDefinition(feature_name='housing', feature_type=&lt;FeatureTypeEnum.STRING: 'String'&gt;),\n FeatureDefinition(feature_name='loan', feature_type=&lt;FeatureTypeEnum.STRING: 'String'&gt;),\n FeatureDefinition(feature_name='contact', feature_type=&lt;FeatureTypeEnum.STRING: 'String'&gt;),\n FeatureDefinition(feature_name='month', feature_type=&lt;FeatureTypeEnum.STRING: 'String'&gt;),\n FeatureDefinition(feature_name='day_of_week', feature_type=&lt;FeatureTypeEnum.STRING: 'String'&gt;),\n FeatureDefinition(feature_name='duration', feature_type=&lt;FeatureTypeEnum.INTEGRAL: 'Integral'&gt;),\n FeatureDefinition(feature_name='campaign', feature_type=&lt;FeatureTypeEnum.INTEGRAL: 'Integral'&gt;),\n FeatureDefinition(feature_name='pdays', feature_type=&lt;FeatureTypeEnum.INTEGRAL: 'Integral'&gt;),\n FeatureDefinition(feature_name='previous', feature_type=&lt;FeatureTypeEnum.INTEGRAL: 'Integral'&gt;),\n FeatureDefinition(feature_name='poutcome', feature_type=&lt;FeatureTypeEnum.STRING: 'String'&gt;),\n FeatureDefinition(feature_name='emp.var.rate', feature_type=&lt;FeatureTypeEnum.FRACTIONAL: 'Fractional'&gt;),\n FeatureDefinition(feature_name='cons.price.idx', feature_type=&lt;FeatureTypeEnum.FRACTIONAL: 'Fractional'&gt;),\n FeatureDefinition(feature_name='cons.conf.idx', feature_type=&lt;FeatureTypeEnum.FRACTIONAL: 'Fractional'&gt;),\n FeatureDefinition(feature_name='euribor3m', feature_type=&lt;FeatureTypeEnum.FRACTIONAL: 'Fractional'&gt;),\n FeatureDefinition(feature_name='nr.employed', feature_type=&lt;FeatureTypeEnum.FRACTIONAL: 'Fractional'&gt;),\n FeatureDefinition(feature_name='y', feature_type=&lt;FeatureTypeEnum.STRING: 'String'&gt;),\n FeatureDefinition(feature_name='FS_id', feature_type=&lt;FeatureTypeEnum.INTEGRAL: 'Integral'&gt;),\n FeatureDefinition(feature_name='FS_event_time', feature_type=&lt;FeatureTypeEnum.STRING: 'String'&gt;)]\n\n\nWe have defined our feature group and its feature definitions, but it has not been created. To create it we need to call create method on the feature group. For this let’s define s3 URI for our feature store offline data storage.\n\nfs_offline_bucket_sdk = f\"s3://{bucket}/{bucket_prefix}/fs_offline/sdk\"\nfs_offline_bucket_sdk\n\n's3://sagemaker-us-east-1-801598032724/2022-08-05-sagemaker-feature-store/fs_offline/sdk'\n\n\n\n##\n# Now create feature group\nrecord_identifier_name = \"FS_id\"\nevent_time_feature_name = \"FS_event_time\"\ndescription = \"The data is related with direct marketing campaigns (phone calls) of a Portuguese banking institution\"\n\nfeature_group.create(\n    record_identifier_name=record_identifier_name,\n    event_time_feature_name=event_time_feature_name,\n    enable_online_store=True,\n    s3_uri=fs_offline_bucket_sdk,\n    role_arn=role,\n    description=description,\n)\n\n\n---------------------------------------------------------------------------\nClientError                               Traceback (most recent call last)\n&lt;ipython-input-31-5459ca8fa447&gt; in &lt;module&gt;\n     11     s3_uri=fs_offline_bucket_sdk,\n     12     role_arn=role,\n---&gt; 13     description=description,\n     14 )\n\n/opt/conda/lib/python3.7/site-packages/sagemaker/feature_store/feature_group.py in create(self, s3_uri, record_identifier_name, event_time_feature_name, role_arn, online_store_kms_key_id, enable_online_store, offline_store_kms_key_id, disable_glue_table_creation, data_catalog_config, description, tags)\n    519             )\n    520 \n--&gt; 521         return self.sagemaker_session.create_feature_group(**create_feature_store_args)\n    522 \n    523     def delete(self):\n\n/opt/conda/lib/python3.7/site-packages/sagemaker/session.py in create_feature_group(self, feature_group_name, record_identifier_name, event_time_feature_name, feature_definitions, role_arn, online_store_config, offline_store_config, description, tags)\n   4076             Tags=tags,\n   4077         )\n-&gt; 4078         return self.sagemaker_client.create_feature_group(**kwargs)\n   4079 \n   4080     def describe_feature_group(\n\n/opt/conda/lib/python3.7/site-packages/botocore/client.py in _api_call(self, *args, **kwargs)\n    506                 )\n    507             # The \"self\" in this scope is referring to the BaseClient.\n--&gt; 508             return self._make_api_call(operation_name, kwargs)\n    509 \n    510         _api_call.__name__ = str(py_operation_name)\n\n/opt/conda/lib/python3.7/site-packages/botocore/client.py in _make_api_call(self, operation_name, api_params)\n    913             error_code = parsed_response.get(\"Error\", {}).get(\"Code\")\n    914             error_class = self.exceptions.from_code(error_code)\n--&gt; 915             raise error_class(parsed_response, operation_name)\n    916         else:\n    917             return parsed_response\n\nClientError: An error occurred (ValidationException) when calling the CreateFeatureGroup operation: 4 validation errors detected: Value 'emp.var.rate' at 'featureDefinitions.16.member.featureName' failed to satisfy constraint: Member must satisfy regular expression pattern: ^[a-zA-Z0-9]([-_]*[a-zA-Z0-9]){0,63}; Value 'cons.price.idx' at 'featureDefinitions.17.member.featureName' failed to satisfy constraint: Member must satisfy regular expression pattern: ^[a-zA-Z0-9]([-_]*[a-zA-Z0-9]){0,63}; Value 'cons.conf.idx' at 'featureDefinitions.18.member.featureName' failed to satisfy constraint: Member must satisfy regular expression pattern: ^[a-zA-Z0-9]([-_]*[a-zA-Z0-9]){0,63}; Value 'nr.employed' at 'featureDefinitions.20.member.featureName' failed to satisfy constraint: Member must satisfy regular expression pattern: ^[a-zA-Z0-9]([-_]*[a-zA-Z0-9]){0,63}\n\n\n\nWe got an error as we have not fixed feature names. Error is saying that the feature name should satisfy the regular expression pattern: ^[a-zA-Z0-9]([-_]*[a-zA-Z0-9]){0,63}. Let’s fix our column names.\n\ncol_names = df.columns.tolist()\nfor idx in range(len(col_names)):\n    col_names[idx] = col_names[idx].replace(\".\", \"_\")\n\ndf.columns = col_names\ndf.head()\n\n\n\n\n\n\n\n\nage\njob\nmarital\neducation\ndefault\nhousing\nloan\ncontact\nmonth\nday_of_week\nduration\ncampaign\npdays\nprevious\npoutcome\nemp_var_rate\ncons_price_idx\ncons_conf_idx\neuribor3m\nnr_employed\ny\nFS_id\nFS_event_time\n\n\n\n\n0\n56\nhousemaid\nmarried\nbasic.4y\nno\nno\nno\ntelephone\nmay\nmon\n261\n1\n999\n0\nnonexistent\n1.1\n93.994\n-36.4\n4.857\n5191.0\nno\n0\n2022-08-08T06:06:07.524Z\n\n\n1\n57\nservices\nmarried\nhigh.school\nunknown\nno\nno\ntelephone\nmay\nmon\n149\n1\n999\n0\nnonexistent\n1.1\n93.994\n-36.4\n4.857\n5191.0\nno\n1\n2022-08-08T06:06:07.524Z\n\n\n2\n37\nservices\nmarried\nhigh.school\nno\nyes\nno\ntelephone\nmay\nmon\n226\n1\n999\n0\nnonexistent\n1.1\n93.994\n-36.4\n4.857\n5191.0\nno\n2\n2022-08-08T06:06:07.524Z\n\n\n3\n40\nadmin.\nmarried\nbasic.6y\nno\nno\nno\ntelephone\nmay\nmon\n151\n1\n999\n0\nnonexistent\n1.1\n93.994\n-36.4\n4.857\n5191.0\nno\n3\n2022-08-08T06:06:07.524Z\n\n\n4\n56\nservices\nmarried\nhigh.school\nno\nno\nyes\ntelephone\nmay\nmon\n307\n1\n999\n0\nnonexistent\n1.1\n93.994\n-36.4\n4.857\n5191.0\nno\n4\n2022-08-08T06:06:07.524Z\n\n\n\n\n\n\n\nAfter updating feature names, load the feature group definitions again.\n\nfeature_group.load_feature_definitions(data_frame=df)\n\n[FeatureDefinition(feature_name='age', feature_type=&lt;FeatureTypeEnum.INTEGRAL: 'Integral'&gt;),\n FeatureDefinition(feature_name='job', feature_type=&lt;FeatureTypeEnum.STRING: 'String'&gt;),\n FeatureDefinition(feature_name='marital', feature_type=&lt;FeatureTypeEnum.STRING: 'String'&gt;),\n FeatureDefinition(feature_name='education', feature_type=&lt;FeatureTypeEnum.STRING: 'String'&gt;),\n FeatureDefinition(feature_name='default', feature_type=&lt;FeatureTypeEnum.STRING: 'String'&gt;),\n FeatureDefinition(feature_name='housing', feature_type=&lt;FeatureTypeEnum.STRING: 'String'&gt;),\n FeatureDefinition(feature_name='loan', feature_type=&lt;FeatureTypeEnum.STRING: 'String'&gt;),\n FeatureDefinition(feature_name='contact', feature_type=&lt;FeatureTypeEnum.STRING: 'String'&gt;),\n FeatureDefinition(feature_name='month', feature_type=&lt;FeatureTypeEnum.STRING: 'String'&gt;),\n FeatureDefinition(feature_name='day_of_week', feature_type=&lt;FeatureTypeEnum.STRING: 'String'&gt;),\n FeatureDefinition(feature_name='duration', feature_type=&lt;FeatureTypeEnum.INTEGRAL: 'Integral'&gt;),\n FeatureDefinition(feature_name='campaign', feature_type=&lt;FeatureTypeEnum.INTEGRAL: 'Integral'&gt;),\n FeatureDefinition(feature_name='pdays', feature_type=&lt;FeatureTypeEnum.INTEGRAL: 'Integral'&gt;),\n FeatureDefinition(feature_name='previous', feature_type=&lt;FeatureTypeEnum.INTEGRAL: 'Integral'&gt;),\n FeatureDefinition(feature_name='poutcome', feature_type=&lt;FeatureTypeEnum.STRING: 'String'&gt;),\n FeatureDefinition(feature_name='emp_var_rate', feature_type=&lt;FeatureTypeEnum.FRACTIONAL: 'Fractional'&gt;),\n FeatureDefinition(feature_name='cons_price_idx', feature_type=&lt;FeatureTypeEnum.FRACTIONAL: 'Fractional'&gt;),\n FeatureDefinition(feature_name='cons_conf_idx', feature_type=&lt;FeatureTypeEnum.FRACTIONAL: 'Fractional'&gt;),\n FeatureDefinition(feature_name='euribor3m', feature_type=&lt;FeatureTypeEnum.FRACTIONAL: 'Fractional'&gt;),\n FeatureDefinition(feature_name='nr_employed', feature_type=&lt;FeatureTypeEnum.FRACTIONAL: 'Fractional'&gt;),\n FeatureDefinition(feature_name='y', feature_type=&lt;FeatureTypeEnum.STRING: 'String'&gt;),\n FeatureDefinition(feature_name='FS_id', feature_type=&lt;FeatureTypeEnum.INTEGRAL: 'Integral'&gt;),\n FeatureDefinition(feature_name='FS_event_time', feature_type=&lt;FeatureTypeEnum.STRING: 'String'&gt;)]\n\n\nNow create the feature group.\n\n##\n# create feature group\nfeature_group.create(\n    record_identifier_name=record_identifier_name,\n    event_time_feature_name=event_time_feature_name,\n    enable_online_store=True,\n    s3_uri=fs_offline_bucket_sdk,\n    role_arn=role,\n    description=description,\n)\n\n{'FeatureGroupArn': 'arn:aws:sagemaker:us-east-1:801598032724:feature-group/bank-marketing-sdk',\n 'ResponseMetadata': {'RequestId': '5c2afeb1-fa03-442b-a3ee-80b1b0ae1069',\n  'HTTPStatusCode': 200,\n  'HTTPHeaders': {'x-amzn-requestid': '5c2afeb1-fa03-442b-a3ee-80b1b0ae1069',\n   'content-type': 'application/x-amz-json-1.1',\n   'content-length': '95',\n   'date': 'Mon, 08 Aug 2022 06:06:32 GMT'},\n  'RetryAttempts': 0}}\n\n\nFeature group creation is an async method, and you need to wait for its creation before ingesting any data into it. For this you can use feature_group.describe method to get feature store creation status.\n\nfeature_group.describe().get('FeatureGroupStatus')\n\n'Creating'\n\n\nWe can create a wrapper function around this method to wait till the feature group is ready.\n\nimport time\n\ndef wait_for_feature_group_creation_complete(feature_group):\n    status = feature_group.describe().get(\"FeatureGroupStatus\")\n    print(f\"Initial status: {status}\")\n    while status == \"Creating\":\n        print(f\"Waiting for feature group: {feature_group.name} to be created ...\")\n        time.sleep(5)\n        status = feature_group.describe().get(\"FeatureGroupStatus\")\n\n    print(f\"FeatureGroup {feature_group.name} was successfully created.\")\n\n\nwait_for_feature_group_creation_complete(feature_group)\n\nInitial status: Creating\nWaiting for feature group: bank-marketing-sdk to be created ...\nWaiting for feature group: bank-marketing-sdk to be created ...\nWaiting for feature group: bank-marketing-sdk to be created ...\nWaiting for feature group: bank-marketing-sdk to be created ...\nFeatureGroup bank-marketing-sdk was successfully created."
  },
  {
    "objectID": "posts/2022-08-05-sagemaker-feature-store.html#accessing-online-feature-store-from-sdk",
    "href": "posts/2022-08-05-sagemaker-feature-store.html#accessing-online-feature-store-from-sdk",
    "title": "Building a Feature Repository with SageMaker Feature Store",
    "section": "Accessing online feature store from SDK",
    "text": "Accessing online feature store from SDK\nBoto3 SDK sagemaker-featurestore-runtime allows us to interact with the online feature store. These are the available methods:\n\nbatch_get_record()\ncan_paginate()\nclose()\ndelete_record()\nget_paginator()\nget_record()\nget_waiter()\nput_record()\n\nTo read more about them use Boto3 SageMakerFeatureStoreRuntime documentation.\n\nfeaturestore_runtime_client = session.boto_session.client(\n    \"sagemaker-featurestore-runtime\", region_name=region\n)\n\n\n##\n# select any random id to query online store\nsample_feature_id = str(df.sample().index.values[0])\nsample_feature_id\n\n'37156'\n\n\nNow query the online store.\n\n%%timeit\n\nfeaturestore_runtime_client.get_record(FeatureGroupName=feature_group_name, \n                                                        RecordIdentifierValueAsString=sample_feature_id)\n\n8.37 ms ± 238 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n\n\n\nfeature_record = featurestore_runtime_client.get_record(\n    FeatureGroupName=feature_group_name, RecordIdentifierValueAsString=sample_feature_id\n)\n\nfeature_record\n\n{'ResponseMetadata': {'RequestId': '946376b7-7745-4b25-9885-e93ba7a284a5',\n  'HTTPStatusCode': 200,\n  'HTTPHeaders': {'x-amzn-requestid': '946376b7-7745-4b25-9885-e93ba7a284a5',\n   'content-type': 'application/json',\n   'content-length': '1189',\n   'date': 'Mon, 08 Aug 2022 06:08:49 GMT'},\n  'RetryAttempts': 0},\n 'Record': [{'FeatureName': 'age', 'ValueAsString': '39'},\n  {'FeatureName': 'job', 'ValueAsString': 'blue-collar'},\n  {'FeatureName': 'marital', 'ValueAsString': 'married'},\n  {'FeatureName': 'education', 'ValueAsString': 'basic.9y'},\n  {'FeatureName': 'default', 'ValueAsString': 'no'},\n  {'FeatureName': 'housing', 'ValueAsString': 'no'},\n  {'FeatureName': 'loan', 'ValueAsString': 'no'},\n  {'FeatureName': 'contact', 'ValueAsString': 'cellular'},\n  {'FeatureName': 'month', 'ValueAsString': 'aug'},\n  {'FeatureName': 'day_of_week', 'ValueAsString': 'wed'},\n  {'FeatureName': 'duration', 'ValueAsString': '394'},\n  {'FeatureName': 'campaign', 'ValueAsString': '1'},\n  {'FeatureName': 'pdays', 'ValueAsString': '999'},\n  {'FeatureName': 'previous', 'ValueAsString': '0'},\n  {'FeatureName': 'poutcome', 'ValueAsString': 'nonexistent'},\n  {'FeatureName': 'emp_var_rate', 'ValueAsString': '-2.9'},\n  {'FeatureName': 'cons_price_idx', 'ValueAsString': '92.201'},\n  {'FeatureName': 'cons_conf_idx', 'ValueAsString': '-31.4'},\n  {'FeatureName': 'euribor3m', 'ValueAsString': '0.884'},\n  {'FeatureName': 'nr_employed', 'ValueAsString': '5076.2'},\n  {'FeatureName': 'y', 'ValueAsString': 'yes'},\n  {'FeatureName': 'FS_id', 'ValueAsString': '37156'},\n  {'FeatureName': 'FS_event_time',\n   'ValueAsString': '2022-08-08T06:06:07.834Z'}]}"
  },
  {
    "objectID": "posts/2022-08-05-sagemaker-feature-store.html#accessing-offline-feature-store-from-sdk",
    "href": "posts/2022-08-05-sagemaker-feature-store.html#accessing-offline-feature-store-from-sdk",
    "title": "Building a Feature Repository with SageMaker Feature Store",
    "section": "Accessing offline feature store from SDK",
    "text": "Accessing offline feature store from SDK\nLet’s query the offline store to get the same data. For offline feature storage, SageMaker stages the data in S3 bucket and creates AWS Data Catalog on it. This catalog is registered in AWS Athena and we can use Athena APIs to query offline store.\n\nquery = feature_group.athena_query()\nquery\n\nAthenaQuery(catalog='AwsDataCatalog', database='sagemaker_featurestore', table_name='bank-marketing-sdk-1659938792', sagemaker_session=&lt;sagemaker.session.Session object at 0x7fb40934c890&gt;, _current_query_execution_id=None, _result_bucket=None, _result_file_prefix=None)\n\n\n\ntable_name = query.table_name\ntable_name\n\n'bank-marketing-sdk-1659938792'\n\n\n\nquery_string = f'SELECT * FROM \"{table_name}\" WHERE FS_id = {sample_feature_id}'\nquery_string\n\n'SELECT * FROM \"bank-marketing-sdk-1659938792\" WHERE FS_id = 37156'\n\n\n\n%%timeit\nquery.run(query_string=query_string,output_location=f's3://{bucket}/{bucket_prefix}/query_results/')\nquery.wait()\n\n5.21 s ± 29.4 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n\n\nNotice that the offline store has taken a much longer time to return the results compared to the online store.\n\ndataset = query.as_dataframe()\ndataset.head()\n\n\n\n\n\n\n\n\nage\njob\nmarital\neducation\ndefault\nhousing\nloan\ncontact\nmonth\nday_of_week\nduration\ncampaign\npdays\nprevious\npoutcome\nemp_var_rate\ncons_price_idx\ncons_conf_idx\neuribor3m\nnr_employed\ny\nfs_id\nfs_event_time\nwrite_time\napi_invocation_time\nis_deleted\n\n\n\n\n\n\n\n\n\nData in an online store becomes available immediately but it can take a few minutes to become available in an offline store. That is why we have not received any data in the last cell. Let’s run the same query again after a few minutes.\n\n##\n# run query again\nquery.run(query_string=query_string,output_location=f's3://{bucket}/{bucket_prefix}/query_results/')\nquery.wait()\n\n# get query response\ndataset = query.as_dataframe()\ndataset.head()\n\n\n\n\n\n\n\n\nage\njob\nmarital\neducation\ndefault\nhousing\nloan\ncontact\nmonth\nday_of_week\nduration\ncampaign\npdays\nprevious\npoutcome\nemp_var_rate\ncons_price_idx\ncons_conf_idx\neuribor3m\nnr_employed\ny\nfs_id\nfs_event_time\nwrite_time\napi_invocation_time\nis_deleted\n\n\n\n\n0\n39\nblue-collar\nmarried\nbasic.9y\nno\nno\nno\ncellular\naug\nwed\n394\n1\n999\n0\nnonexistent\n-2.9\n92.201\n-31.4\n0.884\n5076.2\nyes\n37156\n2022-08-08T06:06:07.834Z\n2022-08-08 06:13:03.665\n2022-08-08 06:07:43.000\nFalse"
  },
  {
    "objectID": "posts/2022-08-05-sagemaker-feature-store.html#accessing-offline-store-from-athena",
    "href": "posts/2022-08-05-sagemaker-feature-store.html#accessing-offline-store-from-athena",
    "title": "Building a Feature Repository with SageMaker Feature Store",
    "section": "Accessing offline store from Athena",
    "text": "Accessing offline store from Athena\nThis time lets query the offline feature store directly from AWS Athena service.\n\n\n\nfeature_store_athena.PNG"
  },
  {
    "objectID": "posts/2022-07-05-aws-linear-learner-apache-mxnet-python.html",
    "href": "posts/2022-07-05-aws-linear-learner-apache-mxnet-python.html",
    "title": "Loading SageMaker Linear Learner Model with Apache MXNet in Python",
    "section": "",
    "text": "About\nYou have trained a model with Amazon SageMaker’s built-in algorithm Linear Learner. You can test this model by deploying it on a SageMaker endpoint. But you want to test this model in your local environment. In this post, we will learn to use Apache MXNet and Gluon API to load the model in a local environment, extract its parameters, and perform predictions.\n\n\nIntroduction\nApache MXNet is a fully featured, flexibly programmable, and ultra-scalable deep learning framework supporting state of the art in deep learning models, including convolutional neural networks (CNNs) and long short-term memory networks (LSTMs). Amazon has selected MXNet as their deep learning framework of choice (see Amazon CTO, Werner Vogels blog post on this). When you train a deep learning model using Amazon SageMaker builtin algorithm then there are high chances that the model has been trained and saved using MXNet framework. If a model has been saved with MXNet then we can use the same library to load that model in a local environment.\nIn my last post Demystifying Amazon SageMaker Training for scikit-learn Lovers, I used SageMaker builtin Linear Learner algorithm to train a model on Boston housing dataset. Once the training was complete the model artifacts were stored on the S3 bucket at the following location\ns3://sagemaker-us-east-1-801598032724/2022-06-08-sagemaker-training-overview/output/linear-learner-2022-06-16-09-04-57-576/output/model.tar.gz\nNote that Amazon Linear Learner is built using a Neural Network and is different from scikit-learn linear regression algorithm. Linear Learner documentation does not provide details on the architecture of this neural network but it does mention that it trains using a distributed implementation of stochastic gradient descent (SGD). We can also specify the hyperparameters such as momentum, learning rate, and the learning rate schedule. Also, note that not all SageMaker built-in models are using deep learning e.g. XGBoost which is based on regression trees. If you have trained xgboost model then to load this model in a local environment you will have to use xgboost library, and the MXNet library will not work for it.\nSince Linear Learner is based on deep learning, We can use MXNet Gluon API to load this model in our local environment and make some predictions.\nThis post assumes that you have already trained a Linear Learner model and its artifacts are available on the S3 bucket. If you have not done so then you may use my another post to train a Linear Learner on the Boston housing dataset.\n\n\nEnvironment\nThis notebook is prepared with Amazon SageMaker Studio using Python 3 (MXNet 1.9 Python 3.8 CPU Optimized) Kernel and ml.t3.medium instance.\n\n\n\nsagemaker-mxnet-container.png\n\n\n\n##\n# AWS CLI version\n!aws --version\n\naws-cli/1.22.42 Python/3.8.10 Linux/4.14.281-212.502.amzn2.x86_64 botocore/1.23.42\n\n\n\n##\n# OS version\n!cat /etc/os-release\n\nNAME=\"Ubuntu\"\nVERSION=\"20.04.3 LTS (Focal Fossa)\"\nID=ubuntu\nID_LIKE=debian\nPRETTY_NAME=\"Ubuntu 20.04.3 LTS\"\nVERSION_ID=\"20.04\"\nHOME_URL=\"https://www.ubuntu.com/\"\nSUPPORT_URL=\"https://help.ubuntu.com/\"\nBUG_REPORT_URL=\"https://bugs.launchpad.net/ubuntu/\"\nPRIVACY_POLICY_URL=\"https://www.ubuntu.com/legal/terms-and-policies/privacy-policy\"\nVERSION_CODENAME=focal\nUBUNTU_CODENAME=focal\n\n\n\n\nLoading a SageMaker Linear Learner model with Apache MXNet in Python\nLet’s initialize SageMaker API session.\n\nimport sagemaker\n\nsession = sagemaker.Session()\nrole = sagemaker.get_execution_role()\nbucket = session.default_bucket()\nregion = session.boto_region_name\n\nprint(f\"sagemaker.__version__: {sagemaker.__version__}\")\nprint(f\"Session: {session}\")\nprint(f\"Role: {role}\")\nprint(f\"Bucket: {bucket}\")\nprint(f\"Region: {region}\")\n\nsagemaker.__version__: 2.73.0\nSession: &lt;sagemaker.session.Session object at 0x7f6b0500a760&gt;\nRole: arn:aws:iam::801598032724:role/service-role/AmazonSageMaker-ExecutionRole-20220516T161743\nBucket: sagemaker-us-east-1-801598032724\nRegion: us-east-1\n\n\nWe have our trained model artifacts available on S3 bucket. Let’s define that bucket path.\n\nmodel_data = \"s3://sagemaker-us-east-1-801598032724/2022-06-08-sagemaker-training-overview/output/linear-learner-2022-06-16-09-04-57-576/output/model.tar.gz\"\n\nWe will use SageMaker SDK to download model artifacts from the S3 bucket to a local directory. Let’s define the local path.\n\nlocal_path = \"datasets/2022-07-05-aws-linear-learner-apache-mxnet-python/\"\n\nDownload the model artifacts.\n\nfrom sagemaker.s3 import S3Downloader\n\nS3Downloader.download(\n    s3_uri=model_data, \n    local_path=local_path, \n    sagemaker_session=session\n)\n\nOnce downloaded, you will find an archive “model.tar.gz” in the local directory. Let’s extract this file.\n\n!tar -xzvf $local_path/model.tar.gz -C $local_path\n\nmodel_algo-1\n\n\nExtracting once will give us a zip file. Let’s unzip it to get the model contents\n\n!unzip $local_path/model_algo-1 -d $local_path\n\nArchive:  datasets/2022-07-05-aws-linear-learner-apache-mxnet-python//model_algo-1\n extracting: datasets/2022-07-05-aws-linear-learner-apache-mxnet-python/additional-params.json  \n extracting: datasets/2022-07-05-aws-linear-learner-apache-mxnet-python/mx-mod-symbol.json  \n extracting: datasets/2022-07-05-aws-linear-learner-apache-mxnet-python/manifest.json  \n extracting: datasets/2022-07-05-aws-linear-learner-apache-mxnet-python/mx-mod-0000.params  \n\n\nExtracted model has two important files. * mx-mod-symbol.json is the JSON file that defines the computational graph for the model * mx-mod-0000.params is a binary file that contains the parameters for the trained model\nSerializing models as JSON files has the benefit that these models can be loaded from other language bindings like C++ or Scala for faster inference or inference in different environments. You can read more about it here: Saving and Loading Gluon Models.\n\nimport mxnet\nimport pprint\n\nfrom mxnet import gluon\nfrom json import load as json_load\nfrom json import dumps as json_dumps\n\nGluon API is a wrapper around low level MXNet API to provide a simple interface for deep learning. You may read more about this API here: mxnet.gluon\nLet’s read model computational graph.\n\nsym_json = json_load(open(f\"{local_path}mx-mod-symbol.json\"))\nsym_json_string = json_dumps(sym_json)\n\n\nfrom pprint import pprint\npprint(sym_json)\n\n{'arg_nodes': [0, 1, 3, 5],\n 'attrs': {'mxnet_version': ['int', 10301]},\n 'heads': [[6, 0, 0]],\n 'node_row_ptr': [0, 1, 2, 3, 4, 5, 6, 7],\n 'nodes': [{'inputs': [], 'name': 'data', 'op': 'null'},\n           {'attrs': {'__shape__': '(12, 1)'},\n            'inputs': [],\n            'name': 'fc0_weight',\n            'op': 'null'},\n           {'inputs': [[0, 0, 0], [1, 0, 0]], 'name': 'dot46', 'op': 'dot'},\n           {'attrs': {'__lr_mult__': '10.0', '__shape__': '(1, 1)'},\n            'inputs': [],\n            'name': 'fc0_bias',\n            'op': 'null'},\n           {'inputs': [[2, 0, 0], [3, 0, 0]],\n            'name': 'broadcast_plus46',\n            'op': 'broadcast_add'},\n           {'inputs': [], 'name': 'out_label', 'op': 'null'},\n           {'inputs': [[4, 0, 0], [5, 0, 0]],\n            'name': 'linearregressionoutput46',\n            'op': 'LinearRegressionOutput'}]}\n\n\n\n##\n# initialize the model graph\nmodel = gluon.nn.SymbolBlock(\n    outputs=mxnet.sym.load_json(sym_json_string), \n    inputs=mxnet.sym.var(\"data\")\n)\n\n/usr/local/lib/python3.8/dist-packages/mxnet/gluon/block.py:1849: UserWarning: Cannot decide type for the following arguments. Consider providing them as input:\n    data: None\n  input_sym_arg_type = in_param.infer_type()[0]\n\n\n\n##\n# load the model parameters\nmodel.load_parameters(f\"{local_path}mx-mod-0000.params\", allow_missing=True)\n\n\n##\n# finally initialize our model\nmodel.initialize()\n\n/usr/local/lib/python3.8/dist-packages/mxnet/gluon/parameter.py:896: UserWarning: Parameter 'fc0_weight' is already initialized, ignoring. Set force_reinit=True to re-initialize.\n  v.initialize(None, ctx, init, force_reinit=force_reinit)\n/usr/local/lib/python3.8/dist-packages/mxnet/gluon/parameter.py:896: UserWarning: Parameter 'fc0_bias' is already initialized, ignoring. Set force_reinit=True to re-initialize.\n  v.initialize(None, ctx, init, force_reinit=force_reinit)\n\n\nAt this point our model is ready in our local environment, and we can use it to make some predictions.\nLet’s prepare an input request. This request is same as used in the model training blog post.\n\ninput_request = [\n    0.00632,\n    18.00,\n    2.310,\n    0,\n    0.5380,\n    6.5750,\n    65.20,\n    4.0900,\n    1,\n    296.0,\n    15.30,\n    4.98,\n]\n\nWe need to convert our request Python list to MXNet array to be used for inference.\n\ninput_request_nd = mxnet.nd.array(input_request)\n\n\nprint(f\"type(input_request): {type(input_request)}\")\nprint(f\"type(input_request_nd): {type(input_request_nd)}\")\n\ntype(input_request): &lt;class 'list'&gt;\ntype(input_request_nd): &lt;class 'mxnet.ndarray.ndarray.NDArray'&gt;\n\n\nLet’s pass our converted request to model for inference.\n\nmodel(input_request_nd)[0].asscalar()\n\nExtension horovod.torch has not been built: /usr/local/lib/python3.8/dist-packages/horovod/torch/mpi_lib/_mpi_lib.cpython-38-x86_64-linux-gnu.so not found\nIf this is not expected, reinstall Horovod with HOROVOD_WITH_PYTORCH=1 to debug the build error.\nWarning! MPI libs are missing, but python applications are still avaiable.\n[2022-07-05 10:53:31.777 mxnet-1-9-cpu-py38-ub-ml-t3-medium-3179f602905714e1b45dfa06b970:222 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\n[2022-07-05 10:53:31.944 mxnet-1-9-cpu-py38-ub-ml-t3-medium-3179f602905714e1b45dfa06b970:222 INFO profiler_config_parser.py:111] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\n\n\n29.986717\n\n\nThat’s it. We have loaded SageMaker built-in model in our local envionment and have done prediction from it. But we can go one step further and explore this model’s trained parameters.\n\nparams = model.collect_params()\nparams\n\n(\n  Parameter fc0_weight (shape=(12, 1), dtype=&lt;class 'numpy.float32'&gt;)\n  Parameter fc0_bias (shape=(1, 1), dtype=&lt;class 'numpy.float32'&gt;)\n  Parameter out_label (shape=(1,), dtype=&lt;class 'numpy.float32'&gt;)\n)\n\n\nLet’s define a function to extract model’s weights and biases\n\ndef extract_weight_and_bias(model):\n    params = model.collect_params()\n    weight = params[\"fc0_weight\"].data().asnumpy()\n    bias = params[\"fc0_bias\"].data()[0].asscalar()\n    return {\"weight\": weight, \"bias\": bias}\n\n\nweight_and_bias = extract_weight_and_bias(model)\nweight_and_bias\n\n{'weight': array([[-1.6160294e-01],\n        [ 5.2438524e-02],\n        [ 1.5013154e-02],\n        [-4.4300285e-01],\n        [-2.0226759e+01],\n        [ 3.2423832e+00],\n        [ 7.3540364e-03],\n        [-1.4330027e+00],\n        [ 2.0710023e-01],\n        [-8.0383439e-03],\n        [-1.0465978e+00],\n        [-5.0012934e-01]], dtype=float32),\n 'bias': 44.62983}\n\n\nThis shows that model has 12 weights, one for each input parameter, and a bias. For linear learner there is no activation function so we can use summation formula to create a prediction using the provided weights and bais.\n\n\n\nnn-summation-formula.jpeg\n\n\n\n##\n# convert the input request to np.array\nimport numpy as np\n\ninput_request = np.array(input_request)\n\n\n##\n# extract weights and biases\nweight = weight_and_bias[\"weight\"]\nbias = weight_and_bias[\"bias\"]\n\nWe have all the ingredients ready. Let’s use them to calcualte the prediction ourselves.\n\n##\n# calculate the final prediction\nnp.sum(input_request.reshape((-1, 1)) * weight) + bias\n\n29.98671686516441"
  },
  {
    "objectID": "posts/2022-06-08-sagemaker-training-overview.html#reading-and-checking-the-data",
    "href": "posts/2022-06-08-sagemaker-training-overview.html#reading-and-checking-the-data",
    "title": "Demystifying Amazon SageMaker Training for scikit-learn Lovers",
    "section": "Reading and Checking the Data",
    "text": "Reading and Checking the Data\nIn this post, we will be using Boston Housing Dataset. This is a small dataset with 506 rows and 14 columns. medv is the target variable which means median value of owner-occupied homes in $1000s. This dataset is also available with this notebook. Let’s read it and see how it looks.\n\nimport pandas as pd\nimport numpy as np\n\ndata_location = \"./datasets/2022-06-08-sagemaker-training-overview/\"\n\ndf = pd.read_csv(data_location + 'housing.csv')\ndf.head()\n\n\n\n\n\n\n\n\ncrim\nzn\nindus\nchas\nnox\nage\nrm\ndis\nrad\ntax\nptratio\nlstat\nmedv\n\n\n\n\n0\n0.00632\n18.0\n2.31\n0\n0.538\n6.575\n65.2\n4.0900\n1\n296.0\n15.3\n4.98\n24.0\n\n\n1\n0.02731\n0.0\n7.07\n0\n0.469\n6.421\n78.9\n4.9671\n2\n242.0\n17.8\n9.14\n21.6\n\n\n2\n0.02729\n0.0\n7.07\n0\n0.469\n7.185\n61.1\n4.9671\n2\n242.0\n17.8\n4.03\n34.7\n\n\n3\n0.03237\n0.0\n2.18\n0\n0.458\n6.998\n45.8\n6.0622\n3\n222.0\n18.7\n2.94\n33.4\n\n\n4\n0.06905\n0.0\n2.18\n0\n0.458\n7.147\n54.2\n6.0622\n3\n222.0\n18.7\n5.33\n36.2\n\n\n\n\n\n\n\nlet’s quickly check the dimensions of our loaded dataset.\n\ndf.shape\n\n(506, 13)\n\n\nThe good thing about this dataset is that it does not require any preprocessing as all the features are already in numerical format (no categorical features), and also there are no missing values. We can quickly verify these assumptions.\nCheck the feature data types.\n\ndf.dtypes\n\ncrim       float64\nzn         float64\nindus      float64\nchas         int64\nnox        float64\nage        float64\nrm         float64\ndis        float64\nrad          int64\ntax        float64\nptratio    float64\nlstat      float64\nmedv       float64\ndtype: object\n\n\nCheck if there are any missing values in our dataset.\n\ndf.isnull().values.any()\n\nFalse"
  },
  {
    "objectID": "posts/2022-06-08-sagemaker-training-overview.html#preparing-the-data",
    "href": "posts/2022-06-08-sagemaker-training-overview.html#preparing-the-data",
    "title": "Demystifying Amazon SageMaker Training for scikit-learn Lovers",
    "section": "Preparing the Data",
    "text": "Preparing the Data\nAt this point, our data is ready to be used for training but we also need to check the algorithm we want to use for any specific requirements. We have selected Linear Learner so let’s check its documentation: Linear Learner Algorithm\nIn the documentation it says &gt;For training, the linear learner algorithm supports both recordIO-wrapped protobuf and CSV formats. For the application/x-recordio-protobuf input type, only Float32 tensors are supported. For the text/csv input type, the first column is assumed to be the label, which is the target variable for prediction. You can use either File mode or Pipe mode to train linear learner models on data that is formatted as recordIO-wrapped-protobuf or as CSV.\nIt means that we can use CSV format for our training data. It also mentions that the first column in the training dataset should be the target label. So let’s move our target label medv to the first column.\n\ndf = pd.concat([df['medv'], df.drop('medv', axis='columns')], axis='columns')\n\ndf.head()\n\n\n\n\n\n\n\n\nmedv\ncrim\nzn\nindus\nchas\nnox\nage\nrm\ndis\nrad\ntax\nptratio\nlstat\n\n\n\n\n0\n24.0\n0.00632\n18.0\n2.31\n0\n0.538\n6.575\n65.2\n4.0900\n1\n296.0\n15.3\n4.98\n\n\n1\n21.6\n0.02731\n0.0\n7.07\n0\n0.469\n6.421\n78.9\n4.9671\n2\n242.0\n17.8\n9.14\n\n\n2\n34.7\n0.02729\n0.0\n7.07\n0\n0.469\n7.185\n61.1\n4.9671\n2\n242.0\n17.8\n4.03\n\n\n3\n33.4\n0.03237\n0.0\n2.18\n0\n0.458\n6.998\n45.8\n6.0622\n3\n222.0\n18.7\n2.94\n\n\n4\n36.2\n0.06905\n0.0\n2.18\n0\n0.458\n7.147\n54.2\n6.0622\n3\n222.0\n18.7\n5.33\n\n\n\n\n\n\n\nWe can now proceed with splitting our data into train and validation sets. After splitting we will export it as CSV files so it could be uploaded to S3 in the next section. Before exporting our dataframe into any format we should also check SageMaker general instructions for that format. For this use the link Common Data Formats for Training and check section Using CSV Format. Here it mentions\n\nTo use data in CSV format for training, in the input data channel specification, specify text/csv as the ContentType. Amazon SageMaker requires that a CSV file does not have a header record and that the target variable is in the first column.\n\nWe have already moved our target label to the first column, and while exporting our dataframe to CSV format we should omit column headers as well.\n\nfrom sklearn.model_selection import train_test_split\n\n# out data size is very small so we will use a small test set\ntraining_data, validation_data = train_test_split(df, test_size=0.1, random_state=42)\n\ntraining_data.to_csv(data_location + \"training_data.csv\", index=False, header=False)\nvalidation_data.to_csv(data_location + \"validation_data.csv\", index=False, header=False)\n\nThe next step is to upload this data to S3 bucket, and for this we will take the help of SageMaker Python SDK. There are two Python SDKs (Software Development Kit) available for SageMaker.\n\nAWS SDK for Python (Boto3). It provides low-level access to SageMaker APIs.\nSageMaker Python SDK. It provides a high-level API interface, and you can do more with fewer lines of code. Internally it is calling Boto3 APIs.\n\nWe will be using SageMaker Python SDK for this post, and you will see that it has an interface similar to scikit-learn, and is a more natural choice for Data Scientists. SageMaker Python SDK documentation is super helpful, and it provides many examples to understand the working of its interface. Make sure that you check it out as well sagemaker.readthedocs.io. If you don’t have much time then I would suggest at least read the following sections from the documentation as we will be using them in the coming sections.\n\nInitialize a SageMaker Session\nUpload local file or directory to S3\ndefault_bucket\nCreate an Amazon SageMaker training job\nA generic Estimator to train using any supplied algorithm\n\nSince we are already running this notebook from SageMaker environment, we don’t need to care about credentials and permissions. We can simply start our new session with SageMaker environment using its SDK.\n\nimport sagemaker\n\nsession = sagemaker.Session()\nrole = sagemaker.get_execution_role()\nbucket = session.default_bucket()\nregion = session.boto_region_name\n\nprint(f\"sagemaker.__version__: {sagemaker.__version__}\")\nprint(f\"Session: {session}\")\nprint(f\"Role: {role}\")\nprint(f\"Bucket: {bucket}\")\nprint(f\"Region: {region}\")\n\nsagemaker.__version__: 2.88.1\nSession: &lt;sagemaker.session.Session object at 0x7ff91637cd50&gt;\nRole: arn:aws:iam::801598032724:role/service-role/AmazonSageMaker-ExecutionRole-20220516T161743\nBucket: sagemaker-us-east-1-801598032724\nRegion: us-east-1\n\n\nWhat we have done is * imported the SageMaker Python SDK into our runtime * get a session to work with SageMaker API and other AWS services * get the execution role associated with the user profile. It is the same profile that is available to the user to work from console UI and has AmazonSageMakerFullAccess policy attached to it. * create or get a default bucket to use and return its name. Default bucket name has the format sagemaker-{region}-{account_id}. If it doesn’t exist then our session will automatically create it. You may also use any other bucket in its place given that you have enough permission for reading and writing. * get the region name attached to our session\nNext, we will use this session to upload data to our default bucket.\n\n##\n# You may choose any other prefix for your bucket. All the data related to this post will be under this prefix.\nbucket_prefix = '2022-06-08-sagemaker-training-overview'\n\nLet’s upload our training data first. In the output, we will get the complete path (S3 URI) for our uploaded data.\n\ns3_train_data_path = session.upload_data(\n    path=data_location + \"training_data.csv\",\n    bucket=bucket,\n    key_prefix=bucket_prefix + '/input/training'\n)\n\nprint(s3_train_data_path)\n\ns3://sagemaker-us-east-1-801598032724/2022-06-08-sagemaker-training-overview/input/training/training_data.csv\n\n\nLet’s do the same for our validation data.\n\ns3_validation_data_path =  session.upload_data(\n    path=data_location + \"validation_data.csv\",\n    bucket=bucket,\n    key_prefix=bucket_prefix + '/input/validation_data'\n)\n\nprint(s3_validation_data_path)\n\ns3://sagemaker-us-east-1-801598032724/2022-06-08-sagemaker-training-overview/input/validation_data/validation_data.csv\n\n\nAt this point we have our data available on S3 bucket. We can now proceed to the next step and configure our training job."
  },
  {
    "objectID": "posts/2022-06-08-sagemaker-training-overview.html#finding-the-right-docker-container",
    "href": "posts/2022-06-08-sagemaker-training-overview.html#finding-the-right-docker-container",
    "title": "Demystifying Amazon SageMaker Training for scikit-learn Lovers",
    "section": "Finding the Right Docker Container",
    "text": "Finding the Right Docker Container\nAWS SageMaker built-in algorithms are fully managed containers that can be accessed with one call. Each algorithm has a separate container and is also dependent on the region in which you want to run it. Getting the container URI is not a problem as long as we know about the region and the algorithm framework name. We already have the region name from our SageMaker session. To get the algorithm framework name (for linear learner) visit the AWS Docker Registry Paths page. From this page select your region. In my case it is us-east-1. On the regional docker registry page find the algorithm you want to use; Linear Learner in our case. This will give you the example code and algorithm framework name as shown below.\n\n\n\nlinear-learner-framework-name\n\n\nSo let’s use the provided sample code to get the container URI for our linear learner algorithm.\n\nfrom sagemaker import image_uris\n\nimage_uri = image_uris.retrieve(framework='linear-learner',region=region)\n\nprint(image_uri)\n\n382416733822.dkr.ecr.us-east-1.amazonaws.com/linear-learner:1"
  },
  {
    "objectID": "posts/2022-06-08-sagemaker-training-overview.html#configure-the-estimator-object",
    "href": "posts/2022-06-08-sagemaker-training-overview.html#configure-the-estimator-object",
    "title": "Demystifying Amazon SageMaker Training for scikit-learn Lovers",
    "section": "Configure the Estimator Object",
    "text": "Configure the Estimator Object\nTo configure our estimator we need to fulfill the following requirements. * output path Define the output path where we want to store the trained model artifacts * instance type Since we have a small dataset and not so complex model so a small machine should suffice. ‘ml.m5.large’ will do. It is a general purpose instance with 2vCPU and 8GiB RAM. * hyperparameters For the hyperparameters check the Linear Learner model documentaion. From the documentation, we find that the most important parameters for our problem are * predictor_type: which should be ‘regressor’ in our case * mini_batch_size: default is 1000 which is too large for our small dataset. Let’s use 30 instead.\nIt is also important to note that the Estimator class will automatically provision a separate ml.m5.large machine to start the training run. This machine will be different from the one on which we are running this Jupyter notebook. Once training is complete this new machine will be terminated and we will be billed for only the time we have used it. This approach makes SageMaker very cost effective. We can keep using small less powerful machines for running Jupyter notebooks, and for training and other heavy workloads, we can provision separate machines for short durations and avoid any unnecessary bills.\n\n##\n# define the output path to store trained model artifacts\ns3_output_path = f\"s3://{bucket}/{bucket_prefix}/output/\"\n\nprint(s3_output_path)\n\ns3://sagemaker-us-east-1-801598032724/2022-06-08-sagemaker-training-overview/output/\n\n\n\nfrom sagemaker.estimator import Estimator\n\nll_estimator = Estimator(\n    image_uri = image_uri, # algorithm container\n    role = role, # execution role with necessary permissions\n    instance_count = 1,\n    instance_type = 'ml.m5.large',\n    sagemaker_session = session, # SageMaker API session\n    output_path = s3_output_path, # training artifacts output path\n    hyperparameters = {\n        'predictor_type': 'regressor',\n        'mini_batch_size': 30\n    }\n)\n\nIn the above cell, we have defined the hyperparameters within the Estimator object constructor. There is a second way to pass the hyperparameters to the Estimator object using the ‘set_hyperparameters’ function call. This method can be useful when we have a large number of parameters to set, or when we want to change them in multiple training runs.\nll_estimator.set_hyperparameters(\n    predictor_type='regressor', \n    mini_batch_size=30)\nYou might ask that for our problem even a small ml.t3.medium or ml.c5.large machine would have been sufficient. Why have not we used them? The answer to this is that AWS SageMaker at this time supports a limited number of machine types for training jobs and both of them are not supported. If you configure the Estimator object for these instance types you will get an error shown below\nAn error occurred (ValidationException) when calling the CreateTrainingJob operation: 1 validation error detected: Value 'ml.t3.medium' at 'resourceConfig.instanceType' failed to satisfy constraint: Member must satisfy enum value set: \n\n[ml.p2.xlarge, ml.m5.4xlarge, ml.m4.16xlarge, ml.p4d.24xlarge, ml.g5.2xlarge, ml.c5n.xlarge, ml.p3.16xlarge, ml.m5.large, ml.p2.16xlarge, ml.g5.4xlarge, ml.c4.2xlarge, ml.c5.2xlarge, ml.c4.4xlarge, ml.g5.8xlarge, ml.c5.4xlarge, ml.c5n.18xlarge, ml.g4dn.xlarge, ml.g4dn.12xlarge, ml.c4.8xlarge, ml.g4dn.2xlarge, ml.c5.9xlarge, ml.g4dn.4xlarge, ml.c5.xlarge, ml.g4dn.16xlarge, ml.c4.xlarge, ml.g4dn.8xlarge, ml.g5.xlarge, ml.c5n.2xlarge, ml.g5.12xlarge, ml.g5.24xlarge, ml.c5n.4xlarge, ml.c5.18xlarge, ml.p3dn.24xlarge, ml.g5.48xlarge, ml.g5.16xlarge, ml.p3.2xlarge, ml.m5.xlarge, ml.m4.10xlarge, ml.c5n.9xlarge, ml.m5.12xlarge, ml.m4.xlarge, ml.m5.24xlarge, ml.m4.2xlarge, ml.p2.8xlarge, ml.m5.2xlarge, ml.p3.8xlarge, ml.m4.4xlarge]"
  },
  {
    "objectID": "posts/2022-05-26-aws-sagemaker-wrangler-p5.html",
    "href": "posts/2022-05-26-aws-sagemaker-wrangler-p5.html",
    "title": "Data Preparation with SageMaker Data Wrangler (Part 5)",
    "section": "",
    "text": "Enviornment\nThis notebook is prepared with Amazon SageMaker Studio using Python 3 (Data Science) Kernel and ml.t3.medium instance.\n\n\nAbout\nThis is a detailed guide on using AWS SageMaker Data Wrangler service to prepare data for machine learning models. SageMaker Data Wrangler is a multipurpose tool with which you can * import data from multiple sources * explore data with visualizations * apply transformations * export data for ml training\nThis guide is divided into five parts * Part 1: Prepare synthetic data and place it on multiple sources * Part 2: Import data from multiple sources using Data Wrangler * Part 3: Explore data with Data Wrangler visualizations * Part 4: Preprocess data using Data Wrangler * Part 5: Export data for ML training (You are here)\n\n\nPart 5: Export data for ML training\nIt is important to note that the transformations we have used are not applied to the data yet. These transformations need to be executed to get the final transformed data. When we export Data Wrangler flow it generates the code that when executed will perform the data transformations. Data Wrangler supports four export methods: Save to S3, Pipeline, Python Code, and Feature Store. In this post, we will see how to export data to S3 as this is the most common use case.\nOpen the customer-churn-p4.flow file from part-4. On the last step click the plus sign and select Export to &gt; Amazon S3 (via Jupyter Notebook)\n\n\n\nexport_s3.png\n\n\nSagemaker Data Wrangler will auto-generate a Jupyter notebook that will contain all the required code to transform and put data on the S3 bucket.\n\n\n\nexport_notebook.png\n\n\nYou may review the code and make any changes otherwise run it as it is till point (Optional)Next Steps. This is the first time SageMaker will process the data and place the output on S3 bucket. SageMaker may take a couple of minutes to execute all the cells. It is important to note that this notebook will initiate a container running on a separate machine to do all the processing. The machine specs are defined in the notebook as\n# Processing Job Instance count and instance type.\ninstance_count = 2\ninstance_type = \"ml.m5.4xlarge\"\nOnce execution is complete you see the output message containing the S3 bucket location where the final output is stored.\n\n\n\nexport_s3_output.png\n\n\nThe optional part of this notebook also contains code to generate xgboost model on the transformed data. To execute these steps make the following changes in the notebook.\nChange the flag to run the optional steps.\nrun_optional_steps = True\nNext, update the xgboost hyperparameters to train a binary classification model (customer churn or not?).\nhyperparameters = {\n    \"max_depth\":\"5\",\n    \"objective\": \"binary:logistic\",\n    \"num_round\": \"10\",\n}\nExecute the optional steps. Again note that these steps will initiate a container running on a separate machine (“ml.m5.2xlarge”) to do the training work. The training job will take a few minutes to complete and once it is done trained model will be available on the S3 bucket for inference use. This autogenerated notebook customer-churn-p4.ipynb is available on GitHub here.\n\n\nSummary\nIn this last post of the series, we used SageMaker Data Wrangler to auto-generate code to preprocess the data and store the final output on S3 bucket. We also used the same notebook to train an xgboost model on the processed data."
  },
  {
    "objectID": "posts/2022-05-24-aws-sagemaker-wrangler-p3.html#preview-ml-model-performance-using-quick-model",
    "href": "posts/2022-05-24-aws-sagemaker-wrangler-p3.html#preview-ml-model-performance-using-quick-model",
    "title": "Data Preparation with SageMaker Data Wrangler (Part 3)",
    "section": "Preview ML model performance using Quick Model",
    "text": "Preview ML model performance using Quick Model\nQuick Model is another great feature of SageMaker wrangler with which we can quickly train a Random Forrest Classification model and analyze the importance of features. For this again click on the plus sign against the 2nd Join, and select Add Analysis. Then from the Analysis UI select\n\nAnalysis Type = Quick Model\nAnalysis Name = Quick model\nLabel = Churn?\n\nLabel is our target identifier. Click preview. Data Wrangler will take around a minute to train the model, and will provide a chart with feature importances.\n\n\n\nquick_model.png\n\n\nFrom this feature importance chart, we can see that the day_mins and night_charge features have the highest importance. It also shows that the model has achieved F1 score of 0.841 on the test data. We can take this model as a baseline and work on the important features and model tuning to improve its performance. Click Save to return to the main Data Flow UI."
  },
  {
    "objectID": "posts/2022-05-21-cloudwatch-agent-onprem.html#create-iam-roles-and-users-for-use-with-cloudwatch-agent",
    "href": "posts/2022-05-21-cloudwatch-agent-onprem.html#create-iam-roles-and-users-for-use-with-cloudwatch-agent",
    "title": "Collecting metrics and logs from on-premises servers with the CloudWatch agent",
    "section": "Create IAM roles and users for use with CloudWatch agent",
    "text": "Create IAM roles and users for use with CloudWatch agent\nAccess to AWS resources requires permissions. You create an IAM role, an IAM user, or both to grant permissions that the CloudWatch agent needs to write metrics to CloudWatch. * If you’re going to use the agent on Amazon EC2 instances, you should create an IAM role. * f you’re going to use the agent on on-premises servers, you should create an IAM user.\nSince we want to use EC2 machine as an on-premises machine so we will create an IAM user.\nTo create the IAM user necessary for the CloudWatch agent to run on on-premises servers follow these steps 1. Sign in to the AWS Management Console and open the IAM console at https://console.aws.amazon.com/iam/. 2. In the navigation pane on the left, choose Users and then Add users. 3. Enter the user name for the new user. 4. Select Access key - Programmatic access and choose Next: Permissions. 5. Choose Attach existing policies directly. 6. In the list of policies, select the check box next to CloudWatchAgentServerPolicy. If necessary, use the search box to find the policy. 7. Choose Next: Tags. 8. Optionally create tags for the new IAM user, and then choose Next:Review. 9. Confirm that the correct policy is listed, and choose Create user. 10. Next to the name of the new user, choose Show. Copy the access key and secret key to a file so that you can use them when installing the agent. Choose Close."
  },
  {
    "objectID": "posts/2022-05-21-cloudwatch-agent-onprem.html#install-and-configure-aws-cli-on-ubuntu-server",
    "href": "posts/2022-05-21-cloudwatch-agent-onprem.html#install-and-configure-aws-cli-on-ubuntu-server",
    "title": "Collecting metrics and logs from on-premises servers with the CloudWatch agent",
    "section": "Install and configure AWS CLI on Ubuntu server",
    "text": "Install and configure AWS CLI on Ubuntu server\nConnect to the Ubuntu server using any SSH client. We need to first download and install AWS CLI. Follow the below commands to download and install it. For installing AWS CLI on macOS and Windows take help from this post awscli-getting-started-install\n\n1. Download AWS CLI package\ncurl \"https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip\" -o \"awscliv2.zip\"\n\n\n2. Install UNZIP package\nsudo apt install unzip\n\n\n3. Unzip AWSCLI Package\nunzip awscliv2.zip\n\n\n4. Install AWS CLI\nsudo ./aws/install\n5. Verify AWS CLI Installation\naws --version\n\n\n\naws-cli-installation-complete\n\n\n\n\n6. Configure AWS CLI\nMake sure that you use AmazonCloudWatchAgent profile name as this is used by the OnPremise case by default. For more details, you may take help from this post install-CloudWatch-Agent-commandline-fleet\naws configure --profile AmazonCloudWatchAgent\n\n\n\nconfigure-aws-cli\n\n\n\n\n7. Verify credentials in User home directory\ncat /home/ubuntu/.aws/credentials"
  },
  {
    "objectID": "posts/2022-05-21-cloudwatch-agent-onprem.html#install-and-run-the-cloudwatch-agent-on-ubuntu-server",
    "href": "posts/2022-05-21-cloudwatch-agent-onprem.html#install-and-run-the-cloudwatch-agent-on-ubuntu-server",
    "title": "Collecting metrics and logs from on-premises servers with the CloudWatch agent",
    "section": "Install and run the CloudWatch agent on Ubuntu server",
    "text": "Install and run the CloudWatch agent on Ubuntu server\n\n1. Download the agent\nThe following download link is for Ubuntu. For any other OS you can take help from this post for downloaded agent download-cloudwatch-agent-commandline\nwget https://s3.amazonaws.com/amazoncloudwatch-agent/ubuntu/amd64/latest/amazon-cloudwatch-agent.deb\n\n\n2. Install the agent\nsudo dpkg -i -E ./amazon-cloudwatch-agent.deb\n\n\n3. Prepare agent configuration file\nPrepare agent configuration file. This config file will be provided to the agent in the run command. One such sample is provided below. For more details on this config file you may take help from this link create-cloudwatch-agent-configuration-file. Note the path of this config file (agent config) as we will need it in later commands.\n\n// config-cloudwatchagent.json\n{\n    \"agent\": {\n      \"metrics_collection_interval\": 10,\n      \"logfile\": \"/opt/aws/amazon-cloudwatch-agent/logs/amazon-cloudwatch-agent.log\",\n      \"run_as_user\": \"ubuntu\",\n      \"debug\": false\n    },\n    \"metrics\": {\n      \"namespace\": \"myblog/cloudwatchagent/demo\",\n      \"metrics_collected\": {\n        \"cpu\": {\n          \"resources\": [\n            \"*\"\n          ],\n          \"measurement\": [\n            {\"name\": \"cpu_usage_idle\", \"rename\": \"CPU_USAGE_IDLE\", \"unit\": \"Percent\"},\n            {\"name\": \"cpu_usage_nice\", \"unit\": \"Percent\"},\n            \"cpu_usage_guest\",\n            \"cpu_usage_active\"\n            \n          ],\n          \"totalcpu\": true,\n          \"metrics_collection_interval\": 10\n        },\n        \"disk\": {\n          \"resources\": [\n            \"/\",\n            \"/tmp\"\n          ],\n          \"measurement\": [\n            {\"name\": \"free\", \"rename\": \"DISK_FREE\", \"unit\": \"Gigabytes\"},\n            \"total\",\n            \"used\"\n          ],\n           \"ignore_file_system_types\": [\n            \"sysfs\", \"devtmpfs\"\n          ],\n          \"metrics_collection_interval\": 60\n        },\n        \"diskio\": {\n          \"resources\": [\n            \"*\"\n          ],\n          \"measurement\": [\n            \"reads\",\n            \"writes\",\n            \"read_time\",\n            \"write_time\",\n            \"io_time\"\n          ],\n          \"metrics_collection_interval\": 60\n        },\n        \"swap\": {\n          \"measurement\": [\n            \"swap_used\",\n            \"swap_free\",\n            \"swap_used_percent\"\n          ]\n        },\n        \"mem\": {\n          \"measurement\": [\n            \"mem_used\",\n            \"mem_cached\",\n            \"mem_total\"\n          ],\n          \"metrics_collection_interval\": 1\n        },\n        \"net\": {\n          \"resources\": [\n            \"eth0\"\n          ],\n          \"measurement\": [\n            \"bytes_sent\",\n            \"bytes_recv\",\n            \"drop_in\",\n            \"drop_out\"\n          ]\n        },\n        \"netstat\": {\n          \"measurement\": [\n            \"tcp_established\",\n            \"tcp_syn_sent\",\n            \"tcp_close\"\n          ],\n          \"metrics_collection_interval\": 60\n        },\n        \"processes\": {\n          \"measurement\": [\n            \"running\",\n            \"sleeping\",\n            \"dead\"\n          ]\n        }\n      },\n      \"force_flush_interval\" : 30\n    },\n    \"logs\": {\n      \"logs_collected\": {\n        \"files\": {\n          \"collect_list\": [\n            {\n              \"file_path\": \"/opt/aws/amazon-cloudwatch-agent/logs/amazon-cloudwatch-agent.log\",\n              \"log_group_name\": \"myblog/onprem/ubuntu/amazon-cloudwatch-agent\",\n              \"log_stream_name\": \"myblog-cloudwatchagent-demo.log\",\n              \"timezone\": \"UTC\"\n            }\n          ]\n        }\n      },\n      \"log_stream_name\": \"my_log_stream_name\",\n      \"force_flush_interval\" : 15\n    }\n  }\n\nSome important parts of this config file\nlogfile\n\"logfile\": \"/opt/aws/amazon-cloudwatch-agent/logs/amazon-cloudwatch-agent.log\"\nCloudWatch agent log file location on on-premise server is specified by this tag. After running the agent you can check this log file for any exception messages.\nlog_group_name\n\"log_group_name\": \"myblog/onprem/ubuntu/amazon-cloudwatch-agent\"\nAn on-premise logfile is also uploaded to CloudWatch under log-group-name specified by this tag.\nlog_stream_name\n\"log_stream_name\": \"myblog-cloudwatchagent-demo.log\"\nLog stream name of the CloudWatch where logfile log steam will be uploaded.\nnamespace\n\"namespace\": \"myblog/cloudwatchagent/demo\"\nOn CloudWatch console you find the uploaded metrics under the custom namespace specified by this tag. In our case, it is “myblog/cloudwatchagent/demo”\n\n\n4. Update shared configuration file\nFrom the config file 1. Uncomment the [credentails] tag 2. Update shared_credentails_profile name. This is the profile name with which we have configured our AWS CLI ‘AmazonCloudWatchAgent’. If you have used any other name then use that name here. 3. Update shared_credentials_file path. This is the path for AWS user credentails file created by AWS CLI. ‘/home/username/.aws/credentials’ and in our case it is /home/ubuntu/.aws/credentials\nConfiguration file is located at /opt/aws/amazon-cloudwatch-agent/etc/common-config.toml. For more details on this shared configuration file follow this link CloudWatch-Agent-profile-instance-first\nsudo vim /opt/aws/amazon-cloudwatch-agent/etc/common-config.toml\n\n\n\nagent-shared-config\n\n\n\n\n5. Start the agent\nsudo /opt/aws/amazon-cloudwatch-agent/bin/amazon-cloudwatch-agent-ctl -a fetch-config -m onPremise -s -c file:/home/ubuntu/config-cloudwatchagent.json\nMake sure that you provide the correct path to the JSON config file. In our case, it is file:/home/ubuntu/config-cloudwatchagent.json. For more details check this link start-CloudWatch-Agent-on-premise-SSM-onprem\n\n\n\nrun-agent-success\n\n\n\n\n6. Check agent status\nsudo /opt/aws/amazon-cloudwatch-agent/bin/amazon-cloudwatch-agent-ctl -m ec2 -a status\nIf the agent is running you will get status : running otherwise you will get status : stopped\n\n\n\ncheck-agent-status\n\n\n\n\n7. Check agent logs\nThe agent generates a log while it runs. This log includes troubleshooting information. This log is the amazon-cloudwatch-agent.log file. This file is located in /opt/aws/amazon-cloudwatch-agent/logs/amazon-cloudwatch-agent.log on Linux servers. This is the same logfile path we also defined in the JSON config file. If you are using multiple agents on the machine then you can give them separate log file paths using their JSON configurations.\nsudo tail -f /var/log/amazon/amazon-cloudwatch-agent/amazon-cloudwatch-agent.log\nCheck the logs if there is an exception message or not.\n\n\n\nagent-logs\n\n\nPlease note that both the log files are the same. It could be that agent is keeping multiple copies for internal processing.\n/opt/aws/amazon-cloudwatch-agent/logs/amazon-cloudwatch-agent.log\nor\n/var/log/amazon/amazon-cloudwatch-agent/amazon-cloudwatch-agent.log"
  },
  {
    "objectID": "posts/2022-05-21-cloudwatch-agent-onprem.html#check-the-agent-logs-on-aws-cloudwatch-console",
    "href": "posts/2022-05-21-cloudwatch-agent-onprem.html#check-the-agent-logs-on-aws-cloudwatch-console",
    "title": "Collecting metrics and logs from on-premises servers with the CloudWatch agent",
    "section": "Check the agent logs on AWS CloudWatch console",
    "text": "Check the agent logs on AWS CloudWatch console\nAgent logs are also uploaded to CloudWatch console under log group and stream that we mentioned in JSON config file. In our case it is\n\"log_group_name\": \"myblog/onprem/ubuntu/amazon-cloudwatch-agent\"\n\"log_stream_name\": \"myblog-cloudwatchagent-demo.log\"\n\n\n\nagent-log-group"
  },
  {
    "objectID": "posts/2022-05-21-cloudwatch-agent-onprem.html#check-the-machine-metrics-on-cloudwatch-console",
    "href": "posts/2022-05-21-cloudwatch-agent-onprem.html#check-the-machine-metrics-on-cloudwatch-console",
    "title": "Collecting metrics and logs from on-premises servers with the CloudWatch agent",
    "section": "Check the machine metrics on CloudWatch console",
    "text": "Check the machine metrics on CloudWatch console\nNow finally we can check the metrics uploaded by the agent on CloudWatch console under CloudWatch &gt; Metrics &gt; ALL metrics &gt; Custom namespaces\nThe name of the metrics namespace is the same as what we defined in our JSON config file\n\"metrics\": {\n      \"namespace\": \"myblog/cloudwatchagent/demo\"\n\n\n\nagent-metrics"
  },
  {
    "objectID": "posts/2022-05-21-cloudwatch-agent-onprem.html#to-stop-the-cloudwatch-agent-locally-using-the-command-line",
    "href": "posts/2022-05-21-cloudwatch-agent-onprem.html#to-stop-the-cloudwatch-agent-locally-using-the-command-line",
    "title": "Collecting metrics and logs from on-premises servers with the CloudWatch agent",
    "section": "To stop the CloudWatch agent locally using the command line",
    "text": "To stop the CloudWatch agent locally using the command line\nOn a Linux server, enter the following\nsudo /opt/aws/amazon-cloudwatch-agent/bin/amazon-cloudwatch-agent-ctl -m ec2 -a stop"
  },
  {
    "objectID": "posts/2022-05-21-cloudwatch-agent-onprem.html#i-updated-my-agent-configuration-but-dont-see-the-new-metrics-or-logs-in-the-cloudwatch-console",
    "href": "posts/2022-05-21-cloudwatch-agent-onprem.html#i-updated-my-agent-configuration-but-dont-see-the-new-metrics-or-logs-in-the-cloudwatch-console",
    "title": "Collecting metrics and logs from on-premises servers with the CloudWatch agent",
    "section": "I updated my agent configuration but don’t see the new metrics or logs in the CloudWatch console",
    "text": "I updated my agent configuration but don’t see the new metrics or logs in the CloudWatch console\nIf you update your CloudWatch agent configuration file, the next time that you start the agent, you need to use the fetch-config option. For example, if you stored the updated file on the local computer, enter the following command. Replace &lt;configuration-file-path&gt; with the actual config file path.\nsudo /opt/aws/amazon-cloudwatch-agent/bin/amazon-cloudwatch-agent-ctl -a fetch-config -s -m ec2 -c file:&lt;configuration-file-path&gt;"
  },
  {
    "objectID": "posts/2022-05-14-aws-ml-cert-notes.html",
    "href": "posts/2022-05-14-aws-ml-cert-notes.html",
    "title": "AWS Machine Learning Certification Notes (MLS-C01)",
    "section": "",
    "text": "AWS ML Speciality Badge"
  },
  {
    "objectID": "posts/2022-05-14-aws-ml-cert-notes.html#amazon-comprehend",
    "href": "posts/2022-05-14-aws-ml-cert-notes.html#amazon-comprehend",
    "title": "AWS Machine Learning Certification Notes (MLS-C01)",
    "section": "Amazon Comprehend",
    "text": "Amazon Comprehend\nAmazon Comprehend is a natural-language processing (NLP) service that uses machine learning to uncover valuable insights and connections in text.\nReferences\nhttps://aws.amazon.com/comprehend/"
  },
  {
    "objectID": "posts/2022-05-14-aws-ml-cert-notes.html#amazon-rekognition",
    "href": "posts/2022-05-14-aws-ml-cert-notes.html#amazon-rekognition",
    "title": "AWS Machine Learning Certification Notes (MLS-C01)",
    "section": "Amazon Rekognition",
    "text": "Amazon Rekognition\nAmazon Rekognition offers pre-trained and customizable computer vision (CV) capabilities to extract information and insights from your images and videos.\nReferences\nhttps://aws.amazon.com/rekognition/"
  },
  {
    "objectID": "posts/2022-05-14-aws-ml-cert-notes.html#amazon-polly",
    "href": "posts/2022-05-14-aws-ml-cert-notes.html#amazon-polly",
    "title": "AWS Machine Learning Certification Notes (MLS-C01)",
    "section": "Amazon Polly",
    "text": "Amazon Polly\nAmazon Polly is a service that turns text into lifelike speech, allowing you to create applications that talk, and build entirely new categories of speech-enabled products.\nReferences\nhttps://aws.amazon.com/polly/"
  },
  {
    "objectID": "posts/2022-05-14-aws-ml-cert-notes.html#amazon-lex",
    "href": "posts/2022-05-14-aws-ml-cert-notes.html#amazon-lex",
    "title": "AWS Machine Learning Certification Notes (MLS-C01)",
    "section": "Amazon Lex",
    "text": "Amazon Lex\nAmazon Lex is a fully managed artificial intelligence (AI) service with advanced natural language models to design, build, test, and deploy conversational interfaces in applications (chat bots).\nReferences\nhttps://aws.amazon.com/lex/"
  },
  {
    "objectID": "posts/2022-05-14-aws-ml-cert-notes.html#amazon-transcribe",
    "href": "posts/2022-05-14-aws-ml-cert-notes.html#amazon-transcribe",
    "title": "AWS Machine Learning Certification Notes (MLS-C01)",
    "section": "Amazon Transcribe",
    "text": "Amazon Transcribe\nAmazon Transcribe is an automatic speech recognition service that makes it easy to add speech to text capabilities to any application. Transcribe’s features enable you to ingest audio input, produce easy to read and review transcripts, improve accuracy with customization, and filter content to ensure customer privacy.\nReferences\nhttps://aws.amazon.com/transcribe/"
  },
  {
    "objectID": "posts/2022-05-14-aws-ml-cert-notes.html#latent-dirichlet-allocation-lda-algorithm",
    "href": "posts/2022-05-14-aws-ml-cert-notes.html#latent-dirichlet-allocation-lda-algorithm",
    "title": "AWS Machine Learning Certification Notes (MLS-C01)",
    "section": "Latent Dirichlet Allocation (LDA) Algorithm",
    "text": "Latent Dirichlet Allocation (LDA) Algorithm\n\nIt is a topic modeling technique to generate abstract topics based on word frequency from a set of documents\nIt is similar to unsupervised classification of documents\nIt is useful for automatically organizing, summerizing, understanding and searching large electronic archives. It can help in\n\ndiscovering hidden themes in the collection\nclassifying document into dicoverable themes\norganize/summerize/search the documents\n\n\nReferences\nhttps://towardsdatascience.com/latent-dirichlet-allocation-lda-9d1cd064ffa2"
  },
  {
    "objectID": "posts/2022-05-14-aws-ml-cert-notes.html#multinomial-logistic-regression-algorithm",
    "href": "posts/2022-05-14-aws-ml-cert-notes.html#multinomial-logistic-regression-algorithm",
    "title": "AWS Machine Learning Certification Notes (MLS-C01)",
    "section": "Multinomial Logistic Regression Algorithm",
    "text": "Multinomial Logistic Regression Algorithm\nMultinomial Logistic Regression is an extension of logistic regression (supervised) that allows more than two discrete outcomes (multiclass).\nReferences\nhttps://en.wikipedia.org/wiki/Multinomial_logistic_regression"
  },
  {
    "objectID": "posts/2022-05-14-aws-ml-cert-notes.html#factorization-machines-algorithm",
    "href": "posts/2022-05-14-aws-ml-cert-notes.html#factorization-machines-algorithm",
    "title": "AWS Machine Learning Certification Notes (MLS-C01)",
    "section": "Factorization Machines Algorithm",
    "text": "Factorization Machines Algorithm\nThe Factorization Machines algorithm is a general-purpose supervised learning algorithm that you can use for both classification and regression tasks. It is an extension of a linear model that is designed to capture interactions between features within high dimensional sparse datasets economically. For example, in a click prediction system, the Factorization Machines model can capture click rate patterns observed when ads from a certain ad-category are placed on pages from a certain page-category. Factorization machines are a good choice for tasks dealing with high dimensional sparse datasets, such as click prediction and item recommendation.\nReferences\nhttps://docs.aws.amazon.com/sagemaker/latest/dg/fact-machines.html"
  },
  {
    "objectID": "posts/2022-05-14-aws-ml-cert-notes.html#sequence-to-sequence-seq2seq-algorithm",
    "href": "posts/2022-05-14-aws-ml-cert-notes.html#sequence-to-sequence-seq2seq-algorithm",
    "title": "AWS Machine Learning Certification Notes (MLS-C01)",
    "section": "Sequence to Sequence (seq2seq) Algorithm",
    "text": "Sequence to Sequence (seq2seq) Algorithm\nAmazon SageMaker Sequence to Sequence is a supervised learning algorithm where the input is a sequence of tokens (for example, text, audio) and the output generated is another sequence of tokens. Example applications include * machine translation (input a sentence from one language and predict what that sentence would be in another language) * text summarization (input a longer string of words and predict a shorter string of words that is a summary) * speech-to-text (audio clips converted into output sentences in tokens)\nProblems in this domain have been successfully modeled with deep neural networks that show a significant performance boost over previous methodologies. Amazon SageMaker seq2seq uses Recurrent Neural Networks (RNNs) and Convolutional Neural Network (CNN) models with attention as encoder-decoder architectures.\nReferences\nhttps://docs.aws.amazon.com/sagemaker/latest/dg/seq-2-seq.html"
  },
  {
    "objectID": "posts/2022-05-14-aws-ml-cert-notes.html#term-frequency-inverse-document-frequency-algorithm",
    "href": "posts/2022-05-14-aws-ml-cert-notes.html#term-frequency-inverse-document-frequency-algorithm",
    "title": "AWS Machine Learning Certification Notes (MLS-C01)",
    "section": "Term frequency-inverse document frequency Algorithm",
    "text": "Term frequency-inverse document frequency Algorithm\nTF-IDF (term frequency-inverse document frequency) is a statistical measure that evaluates how relevant a word is to a document in a collection of documents.\nThis is done by multiplying two metrics: how many times a word appears in a document (frequency), and the inverse document frequency of the word across a set of documents.\n\nThe term frequency of a word in a document. There are several ways of calculating this frequency, with the simplest being a raw count of instances a word appears in a document.\nThe inverse document frequency of the word across a set of documents. This means, how common or rare a word is in the entire document set. The closer it is to 0, the more common a word is. This metric can be calculated by taking the total number of documents, dividing it by the number of documents that contain a word, and calculating the logarithm. So, if the word is very common and appears in many documents, this number will approach 0. Otherwise, it will approach 1\n\nIt has many uses, most importantly in automated text analysis, and is very useful for scoring words in machine learning algorithms for Natural Language Processing (NLP).\nReferences\nhttps://monkeylearn.com/blog/what-is-tf-idf"
  },
  {
    "objectID": "posts/2022-05-14-aws-ml-cert-notes.html#blazingtext-algorithm",
    "href": "posts/2022-05-14-aws-ml-cert-notes.html#blazingtext-algorithm",
    "title": "AWS Machine Learning Certification Notes (MLS-C01)",
    "section": "BlazingText Algorithm",
    "text": "BlazingText Algorithm\nThe Amazon SageMaker BlazingText algorithm provides highly optimized implementations of the Word2vec and text classification algorithms. The Word2vec algorithm is useful for many downstream natural language processing (NLP) tasks, such as sentiment analysis, named entity recognition, machine translation, etc. Text classification is an important task for applications that perform web searches, information retrieval, ranking, and document classification.\nThe Word2vec algorithm maps words to high-quality distributed vectors. The resulting vector representation of a word is called a word embedding. Words that are semantically similar correspond to vectors that are close together. That way, word embeddings capture the semantic relationships between words.\nReferences\nhttps://docs.aws.amazon.com/sagemaker/latest/dg/blazingtext.html"
  },
  {
    "objectID": "posts/2022-05-14-aws-ml-cert-notes.html#amazon-sagemaker-batch-transform",
    "href": "posts/2022-05-14-aws-ml-cert-notes.html#amazon-sagemaker-batch-transform",
    "title": "AWS Machine Learning Certification Notes (MLS-C01)",
    "section": "Amazon SageMaker Batch Transform",
    "text": "Amazon SageMaker Batch Transform\nUse batch transform when you need to do the following:\n\nPreprocess datasets to remove noise or bias that interferes with training or inference from your dataset.\nGet inferences from large datasets.\nRun inference when you don’t need a persistent endpoint.\nAssociate input records with inferences to assist the interpretation of results.\n\nReferences\nhttps://docs.aws.amazon.com/sagemaker/latest/dg/batch-transform.html"
  },
  {
    "objectID": "posts/2022-05-14-aws-ml-cert-notes.html#amazon-sagemaker-real-time-inference-hosting-services",
    "href": "posts/2022-05-14-aws-ml-cert-notes.html#amazon-sagemaker-real-time-inference-hosting-services",
    "title": "AWS Machine Learning Certification Notes (MLS-C01)",
    "section": "Amazon SageMaker Real-time inference / Hosting Services",
    "text": "Amazon SageMaker Real-time inference / Hosting Services\nReal-time inference is ideal for inference workloads where you have real-time, interactive, low latency requirements. You can deploy your model to SageMaker hosting services and get an endpoint that can be used for inference. These endpoints are fully managed and support autoscaling.\nReferences\nhttps://docs.aws.amazon.com/sagemaker/latest/dg/realtime-endpoints.html"
  },
  {
    "objectID": "posts/2022-05-14-aws-ml-cert-notes.html#amazon-sagemaker-inference-pipeline",
    "href": "posts/2022-05-14-aws-ml-cert-notes.html#amazon-sagemaker-inference-pipeline",
    "title": "AWS Machine Learning Certification Notes (MLS-C01)",
    "section": "Amazon SageMaker Inference Pipeline",
    "text": "Amazon SageMaker Inference Pipeline\nAn inference pipeline is a Amazon SageMaker model that is composed of a linear sequence of two to fifteen containers that process requests for inferences on data. You use an inference pipeline to define and deploy any combination of pretrained SageMaker built-in algorithms and your own custom algorithms packaged in Docker containers. You can use an inference pipeline to combine preprocessing, predictions, and post-processing data science tasks. Inference pipelines are fully managed.\nWithin an inference pipeline model, SageMaker handles invocations as a sequence of HTTP requests. The first container in the pipeline handles the initial request, then the intermediate response is sent as a request to the second container, and so on, for each container in the pipeline. SageMaker returns the final response to the client.\nWhen you deploy the pipeline model, SageMaker installs and runs all of the containers on each Amazon Elastic Compute Cloud (Amazon EC2) instance in the endpoint or transform job. Feature processing and inferences run with low latency because the containers are co-located on the same EC2 instances.\nReferences\nhttps://docs.aws.amazon.com/sagemaker/latest/dg/inference-pipelines.html"
  },
  {
    "objectID": "posts/2022-05-14-aws-ml-cert-notes.html#amazon-sagemaker-neo",
    "href": "posts/2022-05-14-aws-ml-cert-notes.html#amazon-sagemaker-neo",
    "title": "AWS Machine Learning Certification Notes (MLS-C01)",
    "section": "Amazon SageMaker Neo",
    "text": "Amazon SageMaker Neo\nAmazon SageMaker Neo automatically optimizes machine learning models for inference on cloud instances and edge devices to run faster with no loss in accuracy. You start with a machine learning model already built with DarkNet, Keras, MXNet, PyTorch, TensorFlow, TensorFlow-Lite, ONNX, or XGBoost and trained in Amazon SageMaker or anywhere else. Then you choose your target hardware platform, which can be a SageMaker hosting instance or an edge device based on processors from Ambarella, Apple, ARM, Intel, MediaTek, Nvidia, NXP, Qualcomm, RockChip, Texas Instruments, or Xilinx. With a single click, SageMaker Neo optimizes the trained model and compiles it into an executable. The compiler uses a machine learning model to apply the performance optimizations that extract the best available performance for your model on the cloud instance or edge device. You then deploy the model as a SageMaker endpoint or on supported edge devices and start making predictions.\nReferences\nhttps://aws.amazon.com/sagemaker/neo/"
  },
  {
    "objectID": "posts/2022-05-14-aws-ml-cert-notes.html#lstm-long-short-term-memory",
    "href": "posts/2022-05-14-aws-ml-cert-notes.html#lstm-long-short-term-memory",
    "title": "AWS Machine Learning Certification Notes (MLS-C01)",
    "section": "LSTM / Long Short-Term Memory",
    "text": "LSTM / Long Short-Term Memory\nLSTM is an artificial recurrent neural network (RNN) architecture used in the field of deep learning. Unlike standard feedforward neural networks, LSTM has feedback connections. It can process not only single data points (such as images), but also entire sequences of data (such as speech or video).\nLSTM networks are well-suited to classifying, processing and making predictions based on time series data, since there can be lags of unknown duration between important events in a time series. LSTM is applicable to tasks such as anomaly detection in network traffic or IDSs (intrusion detection systems)\nReferences\nhttps://en.wikipedia.org/wiki/Long_short-term_memory"
  },
  {
    "objectID": "posts/2022-05-14-aws-ml-cert-notes.html#semantic-segmentation-algorithm",
    "href": "posts/2022-05-14-aws-ml-cert-notes.html#semantic-segmentation-algorithm",
    "title": "AWS Machine Learning Certification Notes (MLS-C01)",
    "section": "Semantic Segmentation Algorithm",
    "text": "Semantic Segmentation Algorithm\nThe SageMaker semantic segmentation algorithm provides a fine-grained, pixel-level approach to developing computer vision applications. It tags every pixel in an image with a class label from a predefined set of classes. Tagging is fundamental for understanding scenes, which is critical to an increasing number of computer vision applications, such as self-driving vehicles, medical imaging diagnostics, and robot sensing.\nFor comparison, the SageMaker Image Classification Algorithm is a supervised learning algorithm that analyzes only whole images, classifying them into one of multiple output categories. The Object Detection Algorithm is a supervised learning algorithm that detects and classifies all instances of an object in an image. It indicates the location and scale of each object in the image with a rectangular bounding box.\nBecause the semantic segmentation algorithm classifies every pixel in an image, it also provides information about the shapes of the objects contained in the image. The segmentation output is represented as a grayscale image, called a segmentation mask. A segmentation mask is a grayscale image with the same shape as the input image.\nThe SageMaker semantic segmentation algorithm is built using the MXNet Gluon framework and the Gluon CV toolkit.\nReferences * https://docs.aws.amazon.com/sagemaker/latest/dg/semantic-segmentation.html"
  },
  {
    "objectID": "posts/2022-05-14-aws-ml-cert-notes.html#accuracy",
    "href": "posts/2022-05-14-aws-ml-cert-notes.html#accuracy",
    "title": "AWS Machine Learning Certification Notes (MLS-C01)",
    "section": "Accuracy",
    "text": "Accuracy\nAccuracy measures the fraction of correct predictions. The range is 0 to 1.\nAccuracy = (TP + TN) / (TP + FP + TN + FN)"
  },
  {
    "objectID": "posts/2022-05-14-aws-ml-cert-notes.html#precision",
    "href": "posts/2022-05-14-aws-ml-cert-notes.html#precision",
    "title": "AWS Machine Learning Certification Notes (MLS-C01)",
    "section": "Precision",
    "text": "Precision\nPrecision measures the fraction of actual positives among those examples that are predicted as positive. The range is 0 to 1. This formula tells us that the larger value of FP (False Positives), the lower the Precision.\nPrecision = TP / (TP + FP)\nFor maximun precision there should be no FP. FP are also called Type 1 error."
  },
  {
    "objectID": "posts/2022-05-14-aws-ml-cert-notes.html#recall",
    "href": "posts/2022-05-14-aws-ml-cert-notes.html#recall",
    "title": "AWS Machine Learning Certification Notes (MLS-C01)",
    "section": "Recall",
    "text": "Recall\nThe Recall measures the fraction of actual positives that are predicted as positive. The range is 0 to 1. This formula tells us that the larger value of FN (False Negatives), the lower the Recall.\nRecall = TP / (TP + FN)\nFor maximun recall there should be no FN. FN are also called Type 2 error.\nNote: Precision and Recall are inversely proportional to eachother.\nReferences\nhttps://towardsdatascience.com/model-evaluation-i-precision-and-recall-166ddb257c7b"
  },
  {
    "objectID": "posts/2022-05-14-aws-ml-cert-notes.html#l1-regularization",
    "href": "posts/2022-05-14-aws-ml-cert-notes.html#l1-regularization",
    "title": "AWS Machine Learning Certification Notes (MLS-C01)",
    "section": "L1 regularization",
    "text": "L1 regularization\nL1 regularization, also known as L1 norm or Lasso (in regression problems), combats overfitting by shrinking the parameters towards 0. This makes some features obsolete.\nIt’s a form of feature selection, because when we assign a feature with a 0 weight, we’re multiplying the feature values by 0 which returns 0, eradicating the significance of that feature. If the input features of our model have weights closer to 0, our L1 norm would be sparse. A selection of the input features would have weights equal to zero, and the rest would be non-zero."
  },
  {
    "objectID": "posts/2022-05-14-aws-ml-cert-notes.html#l2-regularization",
    "href": "posts/2022-05-14-aws-ml-cert-notes.html#l2-regularization",
    "title": "AWS Machine Learning Certification Notes (MLS-C01)",
    "section": "L2 regularization",
    "text": "L2 regularization\nL2 regularization, or the L2 norm, or Ridge (in regression problems), combats overfitting by forcing weights to be small, but not making them exactly 0. This regularization returns a non-sparse solution since the weights will be non-zero (although some may be close to 0). A major snag to consider when using L2 regularization is that it’s not robust to outliers.\nReferences\nhttps://neptune.ai/blog/fighting-overfitting-with-l1-or-l2-regularization"
  },
  {
    "objectID": "posts/2022-05-14-aws-ml-cert-notes.html#k-means",
    "href": "posts/2022-05-14-aws-ml-cert-notes.html#k-means",
    "title": "AWS Machine Learning Certification Notes (MLS-C01)",
    "section": "K-means",
    "text": "K-means\nK-means is a clustering algorithm that tries to partition a set of points into K sets (clusters) such that the points in each cluster tend to be near each other. It is unsupervised because the points have no external classification."
  },
  {
    "objectID": "posts/2022-05-14-aws-ml-cert-notes.html#k-nearest-neighbors",
    "href": "posts/2022-05-14-aws-ml-cert-notes.html#k-nearest-neighbors",
    "title": "AWS Machine Learning Certification Notes (MLS-C01)",
    "section": "K-nearest neighbors",
    "text": "K-nearest neighbors\nK-nearest neighbors is a classification (or regression) algorithm that in order to determine the classification of a point, combines the classification of the K nearest points. It is supervised because you are trying to classify a point based on the known classification of other points."
  },
  {
    "objectID": "posts/2022-05-14-aws-ml-cert-notes.html#exam-readiness-aws-certified-machine-learning---specialty",
    "href": "posts/2022-05-14-aws-ml-cert-notes.html#exam-readiness-aws-certified-machine-learning---specialty",
    "title": "AWS Machine Learning Certification Notes (MLS-C01)",
    "section": "Exam Readiness: AWS Certified Machine Learning - Specialty",
    "text": "Exam Readiness: AWS Certified Machine Learning - Specialty\nhttps://explore.skillbuilder.aws/learn/course/27/play/54/exam-readiness-aws-certified-machine-learning-specialty\nThis is overall a very good short course that can help you identify your strengths and weaknesses in each exam domain so you know where to focus when studying for the exam."
  },
  {
    "objectID": "posts/2022-05-14-aws-ml-cert-notes.html#acloudguru-aws-certified-machine-learning---specialty-2020",
    "href": "posts/2022-05-14-aws-ml-cert-notes.html#acloudguru-aws-certified-machine-learning---specialty-2020",
    "title": "AWS Machine Learning Certification Notes (MLS-C01)",
    "section": "ACloudGuru AWS Certified Machine Learning - Specialty 2020",
    "text": "ACloudGuru AWS Certified Machine Learning - Specialty 2020\nhttps://acloudguru.com/course/aws-certified-machine-learning-specialty\nThis is a detailed course on the topics covered in the exam. But this course lacks on “Modeling” domain and hands-on labs. Besides taking this course you should have a good knowledge and working experience in data science and machine learning domain. I already have AI/ML background so it was not an issue for me. Some people have recommended taking Machine Learning, Data Science and Deep Learning with Python from Frank Kane on Udemy if you don’t have an ML background but I am not sure about it’s worth."
  },
  {
    "objectID": "posts/2022-04-11-docker-logs-cloudwatch.html#sample-application",
    "href": "posts/2022-04-11-docker-logs-cloudwatch.html#sample-application",
    "title": "Docker - Send Container Logs to AWS CloudWatch",
    "section": "Sample Application",
    "text": "Sample Application\nLet us create a simple hello world application that will print “hello world” message to stdout. After each message the application sleeps for 5 seconds, and keeps on doing this for 5 mins (300 sec). After this the program exists.\nProject structure of this application is\napp/\n└── src/\n    └── hello.py\nWhere * app/ is the project root folder * src/ folder contain the python application code * src/hello.py is the main application\nCode files are provided below\n\n##\n# app/src/hello.py\n\nfrom datetime import datetime\nimport time\n\n\ndef main():\n    # run for about 5 min: 300 sec\n    for i in range(60):\n        now = datetime.now()\n        dt_string = now.strftime(\"%d/%m/%Y %H:%M:%S\")\n\n        # prepare message\n        msg = f\"hello world at {dt_string}\"\n\n        # put message to stdout and logs\n        print(msg)\n\n        # sleep for some seconds\n        time.sleep(5)\n\n\nif __name__ == \"__main__\":\n    main()\n\nWhen I run the hello.py file I get the output on the termial with hello world messages like this.\n\n\n\nhelloworld_output"
  },
  {
    "objectID": "posts/2022-04-11-docker-logs-cloudwatch.html#get-aws-credentials",
    "href": "posts/2022-04-11-docker-logs-cloudwatch.html#get-aws-credentials",
    "title": "Docker - Send Container Logs to AWS CloudWatch",
    "section": "Get AWS Credentials",
    "text": "Get AWS Credentials\nNow that we have our sample application and it’s docker container ready, we can work on pushing the docker logs to AWS CloudWatch. For this we need access credentials to AWS account where we want our logs to be available. We will create a separate account in AWS with CloudWatch access and use it’s credentials with docker daemon. Our steps will be * Create IAM policy with CloudWatch access * Create IAM group with that policy * Create IAM user and add that to this group\n\nCreate IAM Policy\n\nFrom AWS Console go to IAM Console\nSelect Policies, and click ‘Create Policy’\nFrom Create Policy window, select\n\nService = CloudWatch Logs\nActions = CreateLogStream, GetLogRecord, DescribeLogGroups, DescribeLogStreams, GetLogEvents, CreateLogGroup, PutLogEvents\nResources = All\n\n\nAfter giving required permissions, policy summary will be like\n{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Sid\": \"DockerContainerLogs\",\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"logs:CreateLogStream\",\n                \"logs:GetLogRecord\",\n                \"logs:DescribeLogGroups\",\n                \"logs:DescribeLogStreams\",\n                \"logs:GetLogEvents\",\n                \"logs:CreateLogGroup\",\n                \"logs:PutLogEvents\"\n            ],\n            \"Resource\": \"*\"\n        }\n    ]\n}\n\n\nCreate IAM Group and User\n\nFrom IAM console create a new IAM group and give it some appropriate name ‘docker-logs-group’\nAttach the above created policy to that group\nFrom the console create a new IAM user with “Access key - Programmatic access”. Give it some appropriate name ‘docker-logs-user’\nStore access key ID and secret access key\nAdd the user to the group created in last step"
  },
  {
    "objectID": "posts/2022-04-11-docker-logs-cloudwatch.html#configure-aws-credentials-for-docker-daemon",
    "href": "posts/2022-04-11-docker-logs-cloudwatch.html#configure-aws-credentials-for-docker-daemon",
    "title": "Docker - Send Container Logs to AWS CloudWatch",
    "section": "Configure AWS credentials for docker daemon",
    "text": "Configure AWS credentials for docker daemon\nTo configure docker daemon to use AWS access credentials, execute command from the terminal sudo systemctl edit docker. A new window will open for text to edit, and add the following lines to it. Replace my-aws-access-key and my-secret-access-key with your access keys.\n[Service]\nEnvironment=\"AWS_ACCESS_KEY_ID=my-aws-access-key\"\nEnvironment=\"AWS_SECRET_ACCESS_KEY=my-secret-access-key\"\nThis command will update the credentials in file /etc/systemd/system/docker.service.d/override.conf. Verify it using command\n$ cat /etc/systemd/system/docker.service.d/override.conf\n[Service]\nEnvironment=\"AWS_ACCESS_KEY_ID=AKIA3VIXXJNKPUSIOR3Y\"\nEnvironment=\"AWS_SECRET_ACCESS_KEY=XhjlKVkZm1XdXedjgBcfLVM3FBU6zkGU\"\nAfter making changes to Docker daemon we need to restart it. For this * Flush the change with command sudo systemctl daemon-reload * Restart the docker daemon with command sudo systemctl restart docker"
  },
  {
    "objectID": "posts/2022-04-11-docker-logs-cloudwatch.html#run-docker-container-with-awslogs-driver",
    "href": "posts/2022-04-11-docker-logs-cloudwatch.html#run-docker-container-with-awslogs-driver",
    "title": "Docker - Send Container Logs to AWS CloudWatch",
    "section": "Run docker container with awslogs driver",
    "text": "Run docker container with awslogs driver\nWe can now run the docker image with awslogs driver using command\ndocker run \\\n--log-driver=awslogs \\\n--log-opt awslogs-region=us-east-1 \\\n--log-opt awslogs-group=myLogGroup \\\n--log-opt awslogs-create-group=true \\\npython-docker\n\nlog-driver configures the driver to be used for logs. Default driver is ‘json-file’ and awslogs is for CloudWatch\nawslogs-region specifies the region for AWS CloudWatch logs\nawslogs-group specifies the log group for CloudWatch\nawslogs-create-group specifes that if provided log group does not exists on CloudWatch then create one\n\n\n\n\ndocker-images-cmd"
  },
  {
    "objectID": "posts/2022-04-11-docker-logs-cloudwatch.html#verify-logs-from-cloudwatch",
    "href": "posts/2022-04-11-docker-logs-cloudwatch.html#verify-logs-from-cloudwatch",
    "title": "Docker - Send Container Logs to AWS CloudWatch",
    "section": "Verify Logs from CloudWatch",
    "text": "Verify Logs from CloudWatch\nGo to CloudWatch console and select Log Groups and then myLogGroup. You will find the logs generated by docker container.\n\n\n\ndocker-images-cmd\n\n\nAll the code used for this post can be obtained from the GitHub repository hassaanbinaslam/2022-04-11-docker-logs-cloudwatch\n\nProject code files\nProject zip file"
  },
  {
    "objectID": "posts/2022-04-11-docker-logs-cloudwatch.html#error-messages",
    "href": "posts/2022-04-11-docker-logs-cloudwatch.html#error-messages",
    "title": "Docker - Send Container Logs to AWS CloudWatch",
    "section": "Error Messages",
    "text": "Error Messages\nIf docker daemon is not able to find AWS credentails then it will generate an error message similar to pasted below\ndocker: Error response from daemon: failed to initialize logging driver: failed to create Cloudwatch log stream: NoCredentialProviders: no valid providers in chain. Deprecated.\n        For verbose messaging see aws.Config.CredentialsChainVerboseErrors.\nIf you get this message then you need to recheck the credentails passed to docker daemon.\nOne thing I noticed is that on Windows there is no way to pass AWS credentials to docker daemon. People have reported similar issues with docker running on MAC OS. Refer to below link for this discussion\n\nhttps://github.com/docker/for-win/issues/9684\n\n\nOther method to provide AWS credentials to docker daemon\nDocker documentation mentions that AWS credentails can also be set * By configuring the environment variables AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY. I have tried this approach but docker daemon is not able to pick AWS credentials from environment variables * By using AWS credentials file ~/.aws/credentials. I have also tried this approach and it does not work either"
  },
  {
    "objectID": "posts/2022-03-29-efs-s3-datasync.html#create-an-s3-bucket",
    "href": "posts/2022-03-29-efs-s3-datasync.html#create-an-s3-bucket",
    "title": "AWS EFS Sync to S3 Using DataSync",
    "section": "Create an S3 bucket",
    "text": "Create an S3 bucket\nLet’s first create an S3 bucket that will contain our data, and this is the bucket we would like to be in sync with EFS. I am naming the bucket as mydata-202203. You may name it as you please. Choose a region of your choice and leave the rest of the settings as defaults."
  },
  {
    "objectID": "posts/2022-03-29-efs-s3-datasync.html#create-an-efs",
    "href": "posts/2022-03-29-efs-s3-datasync.html#create-an-efs",
    "title": "AWS EFS Sync to S3 Using DataSync",
    "section": "Create an EFS",
    "text": "Create an EFS\nFrom EFS console give name as mydata-efs. I am using default VPC for this post. Use Regional availability settings. Click Create. Once file system is created, click on Access points and create an access point for this efs to be mounted in other service. For access point use following settings * name = mydata-ap * root dir path = / * POSIX User * POSIX UID = 1000 * Group ID = 1000 * Root directory creation permissions * Owner user id = 1000 * Owner group id = 1000 * POSIX permissions = 777\nClick Create."
  },
  {
    "objectID": "posts/2022-03-29-efs-s3-datasync.html#note-on-efs-security-group-settings",
    "href": "posts/2022-03-29-efs-s3-datasync.html#note-on-efs-security-group-settings",
    "title": "AWS EFS Sync to S3 Using DataSync",
    "section": "Note on EFS security group settings",
    "text": "Note on EFS security group settings\nIn the last section, I have used a default VPC security group (sg) while creating EFS. Default sg allows traffic for all protocols and all ports, both inbound and outbound. But if you are using a custom security group then make sure that you have an inbound rule for * Type = NFS * Protocol = TCP * Port range = 2049\nOtherwise, you will not be able to access EFS using NFS clients, and if you find an error similar to the below then it means you need to check the security group settings.\n\n\n\nsecurity-group-error"
  },
  {
    "objectID": "posts/2022-03-29-efs-s3-datasync.html#create-datasync-service-task",
    "href": "posts/2022-03-29-efs-s3-datasync.html#create-datasync-service-task",
    "title": "AWS EFS Sync to S3 Using DataSync",
    "section": "Create DataSync service task",
    "text": "Create DataSync service task\n\nconfigure source location = create a new location\n\nlocation type = Amazon S3\nregion = us-east-1\ns3 bucket = mydata-202203\ns3 storage class = standard\nfolder = [leave empty]\nIAM role = click on auto generate\n\nconfigure destination location = create a new location\n\nlocation type = EFS\nregion = us-east-1\nefs file system = mydata-efs\nmount path = /efs\nsubnet = us-east-1a\nsecurity group = default\n\nconfigure settings\n\ntask name = mydata-datasync\ntask execution configuration\n\nverify data = verify only the data transferred\nset bandwidth limit = use available\n\ndata transfer configuration\n\ndata to scan = entire source location\ntransfer mode = transfer only the data that has changed\nuncheck “keep deleted files”\ncheck “overwrite files”\n\nschedule\n\nfrequency = not scheduled\n\ntask logging\n\ncloudwatch log group = autogenerate\n\n\n\nClick “next”. Review and Launch."
  },
  {
    "objectID": "posts/2022-03-29-efs-s3-datasync.html#test-datasync-service",
    "href": "posts/2022-03-29-efs-s3-datasync.html#test-datasync-service",
    "title": "AWS EFS Sync to S3 Using DataSync",
    "section": "Test DataSync Service",
    "text": "Test DataSync Service\nLet’s test datasync service by manually starting it. If S3 bucket is empty then datasync will throw an exception as below\n\n\n\ns3-empty\n\n\nThis is not an issue. Just place some files (test1.txt in my case) in the bucket and start the datasync service again. If it executes successfully then you will get a message as Execution Status = Success"
  },
  {
    "objectID": "posts/2022-03-29-efs-s3-datasync.html#datasync-can-work-without-internet-gateway-or-vpc-endpoint",
    "href": "posts/2022-03-29-efs-s3-datasync.html#datasync-can-work-without-internet-gateway-or-vpc-endpoint",
    "title": "AWS EFS Sync to S3 Using DataSync",
    "section": "DataSync can work without Internet Gateway or VPC Endpoint",
    "text": "DataSync can work without Internet Gateway or VPC Endpoint\nOne thing I noticed is that DataSync service can work even without the presence of an internet gateway or S3 service endpoint. EFS is VPC bound and S3 is global but DataSync can still communicate with both of them. This was different for Lambda. Once Lambda is configured for a VPC then it is not able to access S3 without an internet gateway or VPC endpoint."
  },
  {
    "objectID": "posts/2022-03-29-efs-s3-datasync.html#verify-efs-by-mounting-it-to-the-ec2-machine",
    "href": "posts/2022-03-29-efs-s3-datasync.html#verify-efs-by-mounting-it-to-the-ec2-machine",
    "title": "AWS EFS Sync to S3 Using DataSync",
    "section": "Verify EFS by mounting it to the EC2 machine",
    "text": "Verify EFS by mounting it to the EC2 machine\nIn the last section, we ran DataSync and it successfully copied files from S3 to EFS. So let’s verify our files from EFS by mounting it to an EC2 instance.\nCreate an EC2 machine * AMI = Amazon Linux 2 AMI (HVM) - Kernel 5.10, SSD Volume Type * Intance type = t2.micro (free tier) * Instance details * Network = default VPC * Auto-assign Public IP = Enable * Review and Lanunch &gt; Launch &gt; Proceed without key pair.\nOnce the instance is up and running, click on it and connect using EC2 instance connect option. Create a dir ‘efs’ using the command\nmkdir efs\nIn a separate tab open EFS, and click on the file system we have created. Click Attach. From “Mount via DNS” copy command for NFS client. paste that in EC2 bash terminal\nsudo mount -t nfs4 -o nfsvers=4.1,rsize=1048576,wsize=1048576,hard,timeo=600,retrans=2,noresvport fs-01acd308743098251.efs.us-east-1.amazonaws.com:/ efs\nOnce successfully mounted, verify that the file ‘test1.txt’ exists in EFS.\n\n\n\nefs-verify"
  },
  {
    "objectID": "posts/2022-03-29-efs-s3-datasync.html#create-lambda-function-to-trigger-datasync-task",
    "href": "posts/2022-03-29-efs-s3-datasync.html#create-lambda-function-to-trigger-datasync-task",
    "title": "AWS EFS Sync to S3 Using DataSync",
    "section": "Create Lambda function to trigger DataSync task",
    "text": "Create Lambda function to trigger DataSync task\nNow let’s create a lambda function that will trigger the datasync task. This function will itself be triggered by an S3 event notification whenever a file is uploaded or deleted.\n\nCreate a lambda function as\nname = datasync-trigger-s3\nruntime = Python 3.9\n\nLeave the rest of the settings as default, update the code as below, and deploy.\nIn the code, we are first filtering the object key for which the event is generated. Then we trigger the datasync task and pass the object key as a filter string. With the filter key provided datasync job will only sync provided object from S3 to EFS.\n\nimport json  \nimport boto3  \nimport os  \n  \nDataSync_task_arn = 'arn:aws:datasync:us-east-1:801598032724:task/task-0c04a4a15668b6b8a'  \nDataSync = boto3.client('datasync')\n      \ndef lambda_handler(event, context):  \n    objectKey = ''  \n    try:  \n        objectKey = event[\"Records\"][0][\"s3\"][\"object\"][\"key\"]  \n    except KeyError:  \n        raise KeyError(\"Received invalid event - unable to locate Object key to upload.\", event)  \n          \n    response = DataSync.start_task_execution(  \n        TaskArn=DataSync_task_arn,  \n        OverrideOptions={  \n            'OverwriteMode' : 'ALWAYS',\n            'PreserveDeletedFiles' : 'REMOVE',\n        },  \n        Includes=[  \n            {  \n                'FilterType': 'SIMPLE_PATTERN',  \n                'Value': '/' + os.path.basename(objectKey)  \n            }  \n        ]  \n    )  \n      \n    print(f\"response= {response}\")\n    return {  \n        'response' : response  \n    } \n\nAdd policy AWSDataSyncFullAccess to this lambda function role otherwise it will not be able to trigger datasync task."
  },
  {
    "objectID": "posts/2022-03-29-efs-s3-datasync.html#configure-s3-bucket-event-notifications",
    "href": "posts/2022-03-29-efs-s3-datasync.html#configure-s3-bucket-event-notifications",
    "title": "AWS EFS Sync to S3 Using DataSync",
    "section": "Configure S3 bucket event notifications",
    "text": "Configure S3 bucket event notifications\nOur lambda function is ready. Now we can enable S3 bucket event notifications as put the lambda function as a target. For this from S3 bucket Properties &gt; Event notifications &gt; Create event notifications\n\nevent name = object-put-delete\nevent type = s3:ObjectCreated:Put, and s3:ObjectRemoved:Delete\ndestination = lambda function (datasync-trigger-s3)\n\nClick Save changes"
  },
  {
    "objectID": "posts/2022-03-29-efs-s3-datasync.html#test-datasync-task-through-s3-events-trigger",
    "href": "posts/2022-03-29-efs-s3-datasync.html#test-datasync-task-through-s3-events-trigger",
    "title": "AWS EFS Sync to S3 Using DataSync",
    "section": "Test DataSync task through S3 events trigger",
    "text": "Test DataSync task through S3 events trigger\nNow let’s test our trigger by placing a new file in S3 bucket. In my case it is ‘test2.txt’. Once file is successfully uploaded we can check the EC2 instance to verify the file presence.\n\n\n\nec2-verify-files\n\n\nWe can also verify that the datasync job was triggered from lambda CloudWatch logs.\nresponse= {'TaskExecutionArn': 'arn:aws:datasync:us-east-1:801598032724:task/task-0c04a4a15668b6b8a/execution/exec-020e456f670ca2419', 'ResponseMetadata': {'RequestId': 'c8166ce4-ef14-415c-beff-09cc7720f4a3', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Wed, 30 Mar 2022 13:27:45 GMT', 'content-type': 'application/x-amz-json-1.1', 'content-length': '123', 'connection': 'keep-alive', 'x-amzn-requestid': 'c8166ce4-ef14-415c-beff-09cc7720f4a3'}, 'RetryAttempts': 0}}\nIn the logs we have task execution id exec-020e456f670ca2419 , and we can use that to verify task’s status from datasync console.\n\n\n\ndatasync-task-status"
  },
  {
    "objectID": "posts/2022-03-29-efs-s3-datasync.html#create-a-lambda-function",
    "href": "posts/2022-03-29-efs-s3-datasync.html#create-a-lambda-function",
    "title": "AWS EFS Sync to S3 Using DataSync",
    "section": "Create a lambda function",
    "text": "Create a lambda function\nLet’s create a new lambda function with the following code. This lambda will invoke the datasync task. Add permissions to this lambda AWSDataSyncFullAccess\n\nfunction name = datasync-trigger-scheduled\nruntime = Python 3.9\n\n\nimport json  \nimport boto3  \nimport os  \n  \nDataSync_task_arn = 'arn:aws:datasync:us-east-1:801598032724:task/task-0c04a4a15668b6b8a'  \nDataSync = boto3.client('datasync')\n      \ndef lambda_handler(event, context):\n    response = DataSync.start_task_execution(  \n        TaskArn=DataSync_task_arn,  \n        OverrideOptions={  \n            'OverwriteMode' : 'ALWAYS',\n            'PreserveDeletedFiles' : 'REMOVE',\n        }\n    )  \n    \n    print(f\"response= {response}\")\n    return {  \n        'response' : response  \n    }"
  },
  {
    "objectID": "posts/2022-03-29-efs-s3-datasync.html#create-eventbridge-event",
    "href": "posts/2022-03-29-efs-s3-datasync.html#create-eventbridge-event",
    "title": "AWS EFS Sync to S3 Using DataSync",
    "section": "Create EventBridge event",
    "text": "Create EventBridge event\nGo to EventBridge Events &gt; Rules &gt; select Create Rule - Define rule details - name = datasync-trigger - event bus = default - rule type = scheduled - Define schedule - Sample event = {} - Schedule Pattern - Rate expression = 5 min - Select Targets - target = Lambda function - function = datasync-trigger-scheduled\nClick Next and Create Rule\nEventBridge will automatically add a policy statement to lambda function (datasync-trigger-scheduled) allowing it to trigger lambda. You can verify the policy from lambda Configurations &gt; Permissions &gt; Resource based policy. If no resource policy exists then you need to manually add a policy to allow EventBridge to invoke it. For this click on Resource based policy &gt; Policy statements &gt; Add permissions. * policy statement = AWS service * service = EventBridge * statement id = eventbridge-1000 (or any unique id) * principal = events.amazonaws.com * source ARN = arn:aws:events:us-east-1:801598032724:rule/datasync-trigger (arn for eventbridge event)"
  },
  {
    "objectID": "posts/2022-03-29-efs-s3-datasync.html#verify-event-and-datasync-task-execution",
    "href": "posts/2022-03-29-efs-s3-datasync.html#verify-event-and-datasync-task-execution",
    "title": "AWS EFS Sync to S3 Using DataSync",
    "section": "Verify event and datasync task execution",
    "text": "Verify event and datasync task execution\n\nWe have configured eventbridge to fire an event after every 5 min we can verify it from eventbrige monitoring tab and its cloudwatch logs.\nLambda function invocations can be verified from its cloudwatch logs\nDatasync task execution status can be verified from its history tab and cloudwatch logs."
  },
  {
    "objectID": "posts/2022-03-16-lambda-debug.html#sample-application",
    "href": "posts/2022-03-16-lambda-debug.html#sample-application",
    "title": "AWS Lambda - Test and Debug Locally in Visual Studio Code",
    "section": "Sample Application",
    "text": "Sample Application\nFor this post we will use a simple hello world application as our focus is on debugging. We will use AWS SAM CLI to create our application. You can follow the steps provided in tutorial AWS SAM Developer Guide&gt;Getting started with AWS SAM to create this application.\nFrom the provided link (SAM Developer Guide): This application implements a basic API backend. It consists of an Amazon API Gateway endpoint and an AWS Lambda function. When you send a GET request to the API Gateway endpoint, the Lambda function is invoked. This function returns a hello world message.\nThe following diagram shows the components of this application:\n\n\n\nsam-getting-started-hello-world\n\n\nTo initialize a serverless app use command\nsam init\nComplete the SAM initialization setup steps\nPS C:\\MyWorkspace\\gitrepos\\2022-03-16-lambda-debug&gt; sam init\n\nYou can preselect a particular runtime or package type when using the `sam init` experience.\nCall `sam init --help` to learn more.\n\nWhich template source would you like to use?\n        1 - AWS Quick Start Templates\n        2 - Custom Template Location\nChoice: 1\n\nChoose an AWS Quick Start application template\n        1 - Hello World Example\n        2 - Multi-step workflow\n        3 - Serverless API\n        4 - Scheduled task\n        5 - Standalone function\n        6 - Data processing\n        7 - Infrastructure event management\n        8 - Machine Learning\nTemplate: 1\n\n Use the most popular runtime and package type? (Python and zip) [y/N]: y\n\nProject name [sam-app]:\n\nCloning from https://github.com/aws/aws-sam-cli-app-templates (process may take a moment)\n\n    -----------------------\n    Generating application:\n    -----------------------\n    Name: sam-app\n    Runtime: python3.9\n    Architectures: x86_64\n    Dependency Manager: pip\n    Application Template: hello-world\n    Output Directory: .\n\n    Next steps can be found in the README file at ./sam-app/README.md\n\n\n    Commands you can use next\n    =========================\n    [*] Create pipeline: cd sam-app && sam pipeline init --bootstrap\n    [*] Test Function in the Cloud: sam sync --stack-name {stack-name} --watch\nOnce the application is initialized the project structure will look like this\nsam-app/\n   ├── README.md\n   ├── events/\n   │   └── event.json\n   ├── hello_world/\n   │   ├── __init__.py\n   │   ├── app.py            #Contains your AWS Lambda handler logic.\n   │   └── requirements.txt  #Contains any Python dependencies the application requires, used for sam build\n   ├── template.yaml         #Contains the AWS SAM template defining your application's AWS resources.\n   └── tests/\n       └── unit/\n           ├── __init__.py\n           └── test_handler.py\nThere are three especially important files:\n\ntemplate.yaml: Contains the AWS SAM template that defines your application’s AWS resources.\nhello_world/app.py: Contains your actual Lambda handler logic.\nhello_world/requirements.txt: Contains any Python dependencies that the application requires, and is used for sam build.\n\nFollow the instructions from the tutorial to build, test, and deploy the application.\nAll the code used for this post can be obtained from the GitHub repository hassaanbinaslam/2022-03-16-lambda-debug\n\nProject code files\nProject zip file"
  },
  {
    "objectID": "posts/2022-03-16-lambda-debug.html#option-1-debug-through-sam-template",
    "href": "posts/2022-03-16-lambda-debug.html#option-1-debug-through-sam-template",
    "title": "AWS Lambda - Test and Debug Locally in Visual Studio Code",
    "section": "Option 1: Debug through SAM template",
    "text": "Option 1: Debug through SAM template\nFrom VSCode open template.yaml, and go to the resources section of the template that defines serverless resources. Click on the lambda function resource, which in our case is HelloWorldFunction. A tooltip will appear over it saying AWS: Add Debug Configuration. Click it as shown below.\n\n\n\nsam-template-add-debug-config\n\n\nThis will create a new folder in the project with debug launch configuration launch.json.\n\n\n\ndebug-launch-config\n\n\nLet’s add a breakpoint in our lambda handler code hello_world/app.py, and start debugging by clicking the green “play” button in the RUN view. When the debugging sessions starts, the DEBUG CONSOLE panel shows debugging output and displays any values returned by the Lambda function.\n\n\n\nlambda-debug-template"
  },
  {
    "objectID": "posts/2022-03-16-lambda-debug.html#option-2-debug-lambda-directly-from-code",
    "href": "posts/2022-03-16-lambda-debug.html#option-2-debug-lambda-directly-from-code",
    "title": "AWS Lambda - Test and Debug Locally in Visual Studio Code",
    "section": "Option 2: Debug Lambda Directly from Code",
    "text": "Option 2: Debug Lambda Directly from Code\nFrom VSCode open lambda handler code sam-app/hello_world/app.py. A tooltip will appear above the lambda_handler function with options * AWS: Add Debug Configuration * AWS: Edit Debug Configuration\n\n\n\nlambda-debug-tooltip\n\n\nClick on AWS: Add Debug Configuration and it will show two further options * template.yaml:HelloWorldFunction (to debug only the lambda function) * template.yaml:HelloWorldFunction (API Event: HelloWorld) (to debug lambda function along with API gateway)\n\n\n\nlambda-debug-options\n\n\nLet’s select API option this time. It will again create a launch configuration, and now we can debug our code. Click on the green “play” button again to start the debug session with request request coming from API Gateway to Lambda function.\nYou can also edit the debug config visually by selecting the AWS: Edit Debug Configuration, and a side pane will appear from where we can easily edit and update our debug configuration.\n\n\n\nedit-debug-config\n\n\nAll the code used for this post can be obtained from the GitHub repository hassaanbinaslam/2022-03-16-lambda-debug\n\nProject code files\nProject zip file"
  },
  {
    "objectID": "posts/2022-03-11-docker-app-logs.html#sample-application",
    "href": "posts/2022-03-11-docker-app-logs.html#sample-application",
    "title": "Docker - Accessing Python Application Logs",
    "section": "Sample Application",
    "text": "Sample Application\nLet us create a simple hello world application that will print “hello world” message to stdout, and also logs them in a logfile. After each message the application sleeps for 5 seconds, and keeps on doing this for 5 mins (300 sec). After this the program exists.\nProject structure of this application is\napp/\n└── src/\n    ├── commons/\n    │   └── logger.py\n    └── hello.py\nWhere * app/ is the project root folder * src/ folder contain the python application code * src/commons/logger.py is the logging module * src/hello.py is the main application\nCode files are provided below\n\n##\n# app/src/commons/logger.py\n\nimport logging\nimport os\n\nlogformat = \"%(levelname)s %(asctime)s - %(message)s\"\nfilename = \"logfile.log\"\n\n# Setting the config of the log object\nlogging.basicConfig(\n    format=logformat,\n    filename=filename,\n    level=logging.INFO,\n)\n\n\n##\n# app/src/hello.py\n\nfrom datetime import datetime\nimport time\nimport commons.logger as logger\n\n\ndef main():\n    # run for about 5 min: 300 sec\n    for i in range(60):\n        now = datetime.now()\n        dt_string = now.strftime(\"%d/%m/%Y %H:%M:%S\")\n\n        # prepare message\n        msg = f\"hello world at {dt_string}\"\n\n        # put message to stdout and logs\n        print(msg)\n        logger.logging.info(msg)\n\n        # sleep for some seconds\n        time.sleep(5)\n\n\nif __name__ == \"__main__\":\n    main()\n\nWhen I run the hello.py file I get the output on the termial with hello world messages like this.\n\n\n\nhelloworld_output\n\n\nWhen we run the application a “logfile.log” will also appear in the project directory containing the logged messages.\n.\n├── app/\n│   └── src/\n│       ├── commons/\n│       │   └── logger.py\n│       └── hello.py\n└── **logfile.log**\nContents of “logfile.log” file will look like this\nINFO 2022-03-11 13:01:56,451 - hello world at 11/03/2022 13:01:56\nINFO 2022-03-11 13:02:01,464 - hello world at 11/03/2022 13:02:01\nINFO 2022-03-11 13:02:06,466 - hello world at 11/03/2022 13:02:06\nINFO 2022-03-11 13:02:11,480 - hello world at 11/03/2022 13:02:11\nINFO 2022-03-11 13:02:16,496 - hello world at 11/03/2022 13:02:16\nAll the code till this point can be found at GitHub repository https://github.com/hassaanbinaslam/snapshots-docker-post-11032022 * Project code files * Project zip file"
  },
  {
    "objectID": "posts/2022-02-28-aws-cloudformation-template.html#awstemplateformatversion-optional",
    "href": "posts/2022-02-28-aws-cloudformation-template.html#awstemplateformatversion-optional",
    "title": "AWS CloudFormation Template, Functions, and Commands",
    "section": "AWSTemplateFormatVersion (optional)",
    "text": "AWSTemplateFormatVersion (optional)\nThe AWS CloudFormation template version that the template conforms to. ### Syntax\nAWSTemplateFormatVersion: \"2010-09-09\""
  },
  {
    "objectID": "posts/2022-02-28-aws-cloudformation-template.html#description-optional",
    "href": "posts/2022-02-28-aws-cloudformation-template.html#description-optional",
    "title": "AWS CloudFormation Template, Functions, and Commands",
    "section": "Description (optional)",
    "text": "Description (optional)\nA text string that describes the template. This section must always follow the template format version section. ### Syntax\nDescription: &gt;\n  Here are some\n  details about\n  the template."
  },
  {
    "objectID": "posts/2022-02-28-aws-cloudformation-template.html#metadata-optional",
    "href": "posts/2022-02-28-aws-cloudformation-template.html#metadata-optional",
    "title": "AWS CloudFormation Template, Functions, and Commands",
    "section": "Metadata (optional)",
    "text": "Metadata (optional)\nObjects that provide additional information about the template.\n\nDifference between Metadata and Description is that some cloudformation features can refer to the objects that are defined in Metadata section. For example, you can use a metadata key AWS::CloudFormation::Interface to define how parameters are grouped and sorted on AWS cloudformation console. By default, cloudformation console alphbetically sorts the parameters by their logical ID.\nAWS strongly recommends not to use this section for storing sensitive information such as passwords or secrets.\n\n\nSyntax\nMetadata:\n  Instances:\n    Description: \"Information about the instances\"\n  Databases: \n    Description: \"Information about the databases\""
  },
  {
    "objectID": "posts/2022-02-28-aws-cloudformation-template.html#parameters-optional",
    "href": "posts/2022-02-28-aws-cloudformation-template.html#parameters-optional",
    "title": "AWS CloudFormation Template, Functions, and Commands",
    "section": "Parameters (optional)",
    "text": "Parameters (optional)\nParameters enable you to input custom values to your template each time you create or update a stack. You can refer to parameters from the Resources and Outputs sections of the template using Ref intrinsic function.\nCloudFormation currently supports the following parameter types\n\nString – A literal string\nNumber – An integer or float\nList&lt;Number&gt; – An array of integers or floats\nCommaDelimitedList – An array of literal strings that are separated by commas\nAWS::EC2::KeyPair::KeyName – An Amazon EC2 key pair name\nAWS::EC2::SecurityGroup::Id – A security group ID\nAWS::EC2::Subnet::Id – A subnet ID\nAWS::EC2::VPC::Id – A VPC ID\nList&lt;AWS::EC2::VPC::Id&gt; – An array of VPC IDs\nList&lt;AWS::EC2::SecurityGroup::Id&gt; – An array of security group IDs\nList&lt;AWS::EC2::Subnet::Id&gt; – An array of subnet IDs\n\n\nSyntax\nThe following example declares a parameter named InstanceTypeParameter. This parameter lets you specify the Amazon EC2 instance type for the stack to use when you create or update the stack.\nNote that InstanceTypeParameter has a default value of t2.micro. This is the value that AWS CloudFormation will use to provision the stack unless another value is provided.\nParameters:\n  InstanceTypeParameter:\n    Type: String\n    Default: t2.micro\n    AllowedValues:\n      - t2.micro\n      - m1.small\n      - m1.large\n    Description: Enter t2.micro, m1.small, or m1.large. Default is t2.micro.\n\nReferencing a parameter in template (Ref function)\nIn the following example, the InstanceType property of the EC2 instance resource references the InstanceTypeParameter parameter value.\nEc2Instance:\n  Type: AWS::EC2::Instance\n  Properties:\n    InstanceType:\n      Ref: InstanceTypeParameter\n    ImageId: ami-0ff8a91507f77f867"
  },
  {
    "objectID": "posts/2022-02-28-aws-cloudformation-template.html#rules-optional",
    "href": "posts/2022-02-28-aws-cloudformation-template.html#rules-optional",
    "title": "AWS CloudFormation Template, Functions, and Commands",
    "section": "Rules (optional)",
    "text": "Rules (optional)\nValidates a parameter or a combination of parameters that are passed to a template during a stack creation or stack update.\nYou can use the following rule-specific intrinsic functions to define rule conditions and assertions: * Fn::And * Fn::Contains * Fn::EachMemberEquals * Fn::EachMemberIn * Fn::Equals * Fn::If * Fn::Not * Fn::Or * Fn::RefAll * Fn::ValueOf * Fn::ValueOfAll\n\nSyntax\nIn the following example, the rule checks the value of the InstanceType parameter. The user must specify a1.medium, if the value of the environment parameter is test.\nRules:\n  testInstanceType:\n    RuleCondition: !Equals \n      - !Ref Environment\n      - test\n    Assertions:\n      - Assert:\n          'Fn::Contains':\n            - - a1.medium\n            - !Ref InstanceType\n        AssertDescription: 'For a test environment, the instance type must be a1.medium'"
  },
  {
    "objectID": "posts/2022-02-28-aws-cloudformation-template.html#mappings-optional",
    "href": "posts/2022-02-28-aws-cloudformation-template.html#mappings-optional",
    "title": "AWS CloudFormation Template, Functions, and Commands",
    "section": "Mappings (optional)",
    "text": "Mappings (optional)\nThe optional Mappings section matches a key to a corresponding set of named values similar to a lookup table. For example, if you want to set values based on a region, you can create a mapping that uses the region name as a key and contains the values you want to specify for each specific region. You use the Fn::FindInMap intrinsic function in the Resources and Outputs to retrieve values in a map. Note that you can’t include parameters, pseudo parameters, or intrinsic functions in the Mappings section.\n\nFn::FindInMap\nThe intrinsic function Fn::FindInMap returns the value corresponding to keys in a two-level map that’s declared in the Mappings section.\nSyntax for the short form:\n!FindInMap [ MapName, TopLevelKey, SecondLevelKey ] \nParameters * MapName * The logical name of a mapping declared in the Mappings section that contains the keys and values. * TopLevelKey * The top-level key name. Its value is a list of key-value pairs. * SecondLevelKey * The second-level key name, which is set to one of the keys from the list assigned to TopLevelKey.\nA more concrete example\nMappings: \n  RegionMap: \n    us-east-1: \n      HVM64: \"ami-0ff8a91507f77f867\"\n      HVMG2: \"ami-0a584ac55a7631c0c\"\nResources: \n  myEC2Instance: \n    Type: \"AWS::EC2::Instance\"\n    Properties: \n      ImageId: !FindInMap\n        - RegionMap\n        - !Ref 'AWS::Region' # us-east-1\n        - HVM64\n      InstanceType: m1.small"
  },
  {
    "objectID": "posts/2022-02-28-aws-cloudformation-template.html#conditions-optional",
    "href": "posts/2022-02-28-aws-cloudformation-template.html#conditions-optional",
    "title": "AWS CloudFormation Template, Functions, and Commands",
    "section": "Conditions (optional)",
    "text": "Conditions (optional)\nConditions that control whether certain resources are created or whether certain resource properties are assigned a value during stack creation or update. For example, you could conditionally create a resource that depends on whether the stack is for a production or test environment.\nConditions are defined in Conditions section, and are then applied in following sections. * Parameters * Resources * Outputs\nYou can use following intrinsic functions to define your conditions * Fn::And * Fn::Equals * Fn::If * Fn::Not * Fn::Or\n\nSyntax\nConditions:\n  Logical ID:\n    Intrinsic function\nA more concrete example\nAWSTemplateFormatVersion: 2010-09-09\nParameters:\n  EnvType:\n    Description: Environment type.\n    Default: test\n    Type: String\n    AllowedValues:\n      - prod\n      - test\n    ConstraintDescription: must specify prod or test.\nConditions:\n  CreateProdResources: !Equals \n    - !Ref EnvType\n    - prod\nResources:\n  EC2Instance:\n    Type: 'AWS::EC2::Instance'\n    Properties:\n      ImageId: ami-0ff8a91507f77f867\n  MountPoint:\n    Type: 'AWS::EC2::VolumeAttachment'\n    Condition: CreateProdResources\n    Properties:\n      InstanceId: !Ref EC2Instance\n      VolumeId: !Ref NewVolume\n      Device: /dev/sdh\n  NewVolume:\n    Type: 'AWS::EC2::Volume'\n    Condition: CreateProdResources\n    Properties:\n      Size: 100\n      AvailabilityZone: !GetAtt \n        - EC2Instance\n        - AvailabilityZone\n\n\nDifference between Rules and Conditions usage?\n\nRules are used to evaluate the input given by the user in Parameters\nConditions turn come after all rules have been evaluated\nConditions are not limited to Parameters and can also work with Resources and Outputs"
  },
  {
    "objectID": "posts/2022-02-28-aws-cloudformation-template.html#transform-optional",
    "href": "posts/2022-02-28-aws-cloudformation-template.html#transform-optional",
    "title": "AWS CloudFormation Template, Functions, and Commands",
    "section": "Transform (optional)",
    "text": "Transform (optional)\nFor serverless applications (also referred to as Lambda-based applications), specifies the version of the AWS Serverless Application Model (AWS SAM) to use. When you specify a transform, you can use AWS SAM syntax to declare resources in your template. The model defines the syntax that you can use and how it's processed.\nYou can also use AWS::Include transforms to work with template snippets that are stored separately from the main AWS CloudFormation template. You can store your snippet files in an Amazon S3 bucket and then reuse the functions across multiple templates.\n\nSyntax\nTransform:\n  - MyMacro\n  - 'AWS::Serverless'\n\nAWS::Include transform\nUse the AWS::Include transform, which is a macro hosted by AWS CloudFormation, to insert boilerplate content into your templates. The AWS::Include transform lets you create a reference to a template snippet in an Amazon S3 bucket. The AWS::Include function behaves similarly to an include, copy, or import directive in programming languages.\n\nExample\nTransform:\n  Name: 'AWS::Include'\n  Parameters:\n    Location: 's3://MyAmazonS3BucketName/MyFileName.yaml'"
  },
  {
    "objectID": "posts/2022-02-28-aws-cloudformation-template.html#resources-required",
    "href": "posts/2022-02-28-aws-cloudformation-template.html#resources-required",
    "title": "AWS CloudFormation Template, Functions, and Commands",
    "section": "Resources (required)",
    "text": "Resources (required)\nSpecifies the stack resources and their properties, such as an Amazon Elastic Compute Cloud instance or an Amazon Simple Storage Service bucket. You can refer to resources in the Resources and Outputs sections of the template.\n\nSyntax\nResources:\n  Logical ID:\n    Type: Resource type\n    Properties:\n      Set of properties\nA more concrete example\nResources:\n  MyEC2Instance:\n    Type: \"AWS::EC2::Instance\"\n    Properties:\n      ImageId: \"ami-0ff8a91507f77f867\""
  },
  {
    "objectID": "posts/2022-02-28-aws-cloudformation-template.html#outputs-optional",
    "href": "posts/2022-02-28-aws-cloudformation-template.html#outputs-optional",
    "title": "AWS CloudFormation Template, Functions, and Commands",
    "section": "Outputs (optional)",
    "text": "Outputs (optional)\nThe optional Outputs section declares output values that you can import into other stacks (to create cross-stack references), return in response (to describe stack calls), or view on the AWS CloudFormation console. For example, you can output the S3 bucket name from a stack to make the bucket easier to find.\nNotes * You can declare a maximum of 200 outputs in a template. * AWS strongly recommend you don’t use this section to output sensitive information, such as passwords or secrets * Output values are available after the stack operation is complete. Stack output values aren’t available when a stack status is in any of the IN_PROGRESS status. * AWS also does not recommend establishing dependencies between a service runtime and the stack output value because output values might not be available at all times.\n\nSyntax\nOutputs:\n  Logical ID:\n    Description: Information about the value\n    Value: Value to return\n    Export:\n      Name: Name of resource to export\nA more concrete example where certain values are shown as output at the end of stack creation.\nOutputs:\n  BackupLoadBalancerDNSName:\n    Description: The DNSName of the backup load balancer\n    Value: !GetAtt BackupLoadBalancer.DNSName\n    Condition: CreateProdResources\n  InstanceID:\n    Description: The Instance ID\n    Value: !Ref EC2Instance\nFor Cross-Stack output use Export tag. Values outputed with “Export” tag can be imported in other stacks “in the same region”. Then, use the Fn::ImportValue intrinsic function to import the value in another stack “in the same region”.\nOutputs:\n  StackVPC:\n    Description: The ID of the VPC\n    Value: !Ref MyVPC\n    Export:\n      Name: !Sub \"${AWS::StackName}-VPCID\""
  },
  {
    "objectID": "posts/2022-02-28-aws-cloudformation-template.html#fngetatt",
    "href": "posts/2022-02-28-aws-cloudformation-template.html#fngetatt",
    "title": "AWS CloudFormation Template, Functions, and Commands",
    "section": "Fn::GetAtt",
    "text": "Fn::GetAtt\nThe Fn::GetAtt intrinsic function returns the value of an attribute from a resource in the template.\n\nSyntax\n!GetAtt logicalNameOfResource.attributeName\n\nlogicalNameOfResource\n\nThe logical name (also called logical ID) of the resource that contains the attribute that you want.\n\nattributeName\n\nThe name of the resource-specific attribute whose value you want. See the resource’s reference page for details about the attributes available for that resource type.\n\nReturn value\n\nThe attribute value.\n\n\nA more concrete example\n!GetAtt myELB.DNSName\nNotes: * For the Fn::GetAtt logical resource name, you can’t use functions. You must specify a string that’s a resource’s logical ID. * For the Fn::GetAtt attribute name, you can use the Ref function."
  },
  {
    "objectID": "posts/2022-02-28-aws-cloudformation-template.html#fnimportvalue",
    "href": "posts/2022-02-28-aws-cloudformation-template.html#fnimportvalue",
    "title": "AWS CloudFormation Template, Functions, and Commands",
    "section": "Fn::ImportValue",
    "text": "Fn::ImportValue\nThe intrinsic function Fn::ImportValue returns the value of an output exported by another stack. You typically use this function to create cross-stack references.\nNotes: * For each AWS account, Export names must be unique within a region. * You can’t create cross-stack references across regions. You can use the intrinsic function Fn::ImportValue to import only values that have been exported within the same region. * You can’t delete a stack if another stack references one of its outputs. * You can’t modify or remove an output value that is referenced by another stack.\n\nSyntax\n!ImportValue sharedValueToImport\nA more concrete example.\nFn::ImportValue:\n  !Sub \"${NetworkStackName}-SecurityGroupID\""
  },
  {
    "objectID": "posts/2022-02-28-aws-cloudformation-template.html#fnsub",
    "href": "posts/2022-02-28-aws-cloudformation-template.html#fnsub",
    "title": "AWS CloudFormation Template, Functions, and Commands",
    "section": "Fn::Sub",
    "text": "Fn::Sub\nThe intrinsic function Fn::Sub substitutes variables in an input string with values that you specify. In your templates, you can use this function to construct commands or outputs that include values that aren’t available until you create or update a stack.\n\nSyntax\n!Sub\n  - String\n  - VarName: VarValue\n\nParameters\n\nString\n\nA string with variables that AWS CloudFormation substitutes with their associated values at runtime. Write variables as ${MyVarName}. Variables can be template parameter names, resource logical IDs, resource attributes, or a variable in a key-value map.\n\nVarName\n\nThe name of a variable that you included in the String parameter.\n\nVarValue\n\nThe value that CloudFormation substitutes for the associated variable name at runtime.\n\n\nA more concrete example. The following example uses a mapping to substitute the ${Domain} variable with the resulting value from the Ref function.\nName: !Sub \n  - 'www.${Domain}'\n  - Domain: !Ref RootDomainName"
  },
  {
    "objectID": "posts/2022-02-22-aws-dns-records.html#domain-name",
    "href": "posts/2022-02-22-aws-dns-records.html#domain-name",
    "title": "AWS DNS Records - A, CNAME, ALIAS, & MX",
    "section": "Domain Name",
    "text": "Domain Name\n\n\nDomain + TLD = Domain Name\nWhen you buy a ‘domain’ from a a registrar or reseller, you buy the rights to a specific domain name (example.com), and any subdomains you want to create (my-site.example.com, mail.example.com, etc).\nThe domain name (example.com) is also called the apex, root or naked domain name.\nExamples of protocol are http, ftp, TCP, UDP, FTP, SMTP etc.\nExamples of top level domains are .org, .net, .com, .ai etc."
  },
  {
    "objectID": "posts/2022-02-22-aws-dns-records.html#example-use-cases",
    "href": "posts/2022-02-22-aws-dns-records.html#example-use-cases",
    "title": "AWS DNS Records - A, CNAME, ALIAS, & MX",
    "section": "Example use cases",
    "text": "Example use cases\n\n\nYou can point your root domain name example.com to an Elastic IP Address 192.0.2.23\n\n\nWe can also map EC2 instances IPv4 Public IP Address to an A record. But this is not recommended as EC2 instances public IP addresses change when you stop/start your server. We should always use Elastic IP addresses instead."
  },
  {
    "objectID": "posts/2022-02-22-aws-dns-records.html#example-use-cases-1",
    "href": "posts/2022-02-22-aws-dns-records.html#example-use-cases-1",
    "title": "AWS DNS Records - A, CNAME, ALIAS, & MX",
    "section": "Example use cases",
    "text": "Example use cases\n\n\n\nNAME\nTYPE\nVALUE\n\n\n\n\nwww.example.com\nCNAME\nexample.com\n\n\nexample.com\nA\n192.0.2.23\n\n\n\n\n\nAn A record for example.com (root domain) points to server IP address\n\n\nA CNAME record points www.example.com to example.com\n\n\nNow if the IP address of your server has changed you will have to update it only at one place A record. www.example.com and example.com will automatically inherit the changes.\nIMPORTANT\n\n\nCNAME entry for the root domain is not allowed.\n\n\n\n\n\n\n\n\n\n\nNAME\nTYPE\nVALUE\n\n\n\n\n example.com\nCNAME\napp.example.com\n\n\napp.example.com\nA\n192.0.2.23"
  },
  {
    "objectID": "posts/2022-02-11-python-stack-traceback-more-info.html#about",
    "href": "posts/2022-02-11-python-stack-traceback-more-info.html#about",
    "title": "Python - Getting more information from Tracebacks",
    "section": "About",
    "text": "About\nThis notebook demonstrates what the Python Traceback object is, and how can we get more information out of it to better diagnose exception messages.\n\nCredit\nThis blog post is based on an article originally written in Python Cookbook published by O'Reilly Media, Inc. and released July 2002. In book’s chapter 15, there is a section with the title Getting More Information from Tracebacks written by Bryn Keller. An online version of this article is available at https://www.oreilly.com/library/view/python-cookbook/0596001673/ch14s05.html.\nThe original article uses Python 2.2, but I have adapted it for Python 3.8. Also, I have added some commentary to give more insights on Python Traceback object.\n\n\nEnvironment Details\n\n\nCode\nfrom platform import python_version\n\nprint(\"python==\" + python_version())\n\n\npython==3.8.5"
  },
  {
    "objectID": "posts/2022-02-11-python-stack-traceback-more-info.html#discussion",
    "href": "posts/2022-02-11-python-stack-traceback-more-info.html#discussion",
    "title": "Python - Getting more information from Tracebacks",
    "section": "Discussion",
    "text": "Discussion\nConsider the following toy example where we are getting some data from an external source (an API call, a DB call, etc.), and we need to find the length of individual items provided in the list. We know that items in the list will be of type str so we have used a len() function on it.\nWe got an exception when we ran our function on received data, and now we are trying to investigate what caused the error.\n\n\nCode\n# this is intentionally hidden as we don't know about the data received from an external source. \ndata = [\"1\", \"22\", 333, \"4444\"]\n\n\n\n##\n# our toy example function.\nimport sys, traceback\n\ndef get_items_len(items: list) -&gt; list:\n    \"\"\"\n    this function returns the length of items received in a list.\n    \"\"\"\n    items_len = []\n    for i in items:\n        items_len.append(len(i))\n    \n    return items_len\n\n\n##\n# let's run our function on \"data\" received from an external source\ntry:\n    get_items_len(data)\nexcept Exception as e:\n    print(traceback.print_exc())\n\nNone\n\n\nTraceback (most recent call last):\n  File \"&lt;ipython-input-4-42cd486e1858&gt;\", line 4, in &lt;module&gt;\n    get_items_len(data)\n  File \"&lt;ipython-input-3-8421f841ba77&gt;\", line 11, in get_items_len\n    items_len.append(len(i))\nTypeError: object of type 'int' has no len()\n\n\nWe got an exception while data processing and the Traceback message gives us some details. It tells us that we have received some data of type integer instead of string, and we are trying to call len() function on it. But we don’t know the actual data value that caused the exception, and we don’t know the index of the item in the list that caused this error. Depending on the use case, information about the local variables, or input data that caused the error can be crucial in diagnosing the root cause of an error.\nFortunately, all this information is already available to us in the Traceback object, but there are no built-in methods that give this information directly. Let us try some of the built-in methods on the Traceback object to see the kind of information we could get from them.\n\n#collapse-output\n# calling traceback module built-in methods\ntry:\n    get_items_len(data)\nexcept Exception as e:\n    print(\"***** Exception *****\")\n    print(e)\n\n    exc_type, exc_value, exc_traceback = sys.exc_info()\n    print(\"\\n***** print_tb *****\")\n    traceback.print_tb(exc_traceback, limit=1, file=sys.stdout)\n\n    print(\"\\n***** print_exception *****\")\n    # exc_type below is ignored on 3.5 and later\n    traceback.print_exception(exc_type, exc_value, exc_traceback,\n                                limit=2, file=sys.stdout)\n    \n    print(\"\\n***** print_exc *****\")\n    traceback.print_exc(limit=2, file=sys.stdout)\n\n    print(\"\\n***** format_exc, first and last line *****\")\n    formatted_lines = traceback.format_exc().splitlines()\n    print(formatted_lines[0])\n    print(formatted_lines[-1])\n\n    print(\"\\n***** format_exception *****\")\n    # exc_type below is ignored on 3.5 and later\n    print(repr(traceback.format_exception(exc_type, exc_value,\n                                            exc_traceback)))\n                                            \n    print(\"\\n***** extract_tb *****\")\n    print(repr(traceback.extract_tb(exc_traceback)))\n\n    print(\"\\n***** format_tb *****\")\n    print(repr(traceback.format_tb(exc_traceback)))\n\n    print(\"\\n***** tb_lineno *****\", exc_traceback.tb_lineno)\n\n***** Exception *****\nobject of type 'int' has no len()\n\n***** print_tb *****\n  File \"&lt;ipython-input-5-73d5b316a567&gt;\", line 4, in &lt;module&gt;\n    get_items_len(data)\n\n***** print_exception *****\nTraceback (most recent call last):\n  File \"&lt;ipython-input-5-73d5b316a567&gt;\", line 4, in &lt;module&gt;\n    get_items_len(data)\n  File \"&lt;ipython-input-3-8421f841ba77&gt;\", line 11, in get_items_len\n    items_len.append(len(i))\nTypeError: object of type 'int' has no len()\n\n***** print_exc *****\nTraceback (most recent call last):\n  File \"&lt;ipython-input-5-73d5b316a567&gt;\", line 4, in &lt;module&gt;\n    get_items_len(data)\n  File \"&lt;ipython-input-3-8421f841ba77&gt;\", line 11, in get_items_len\n    items_len.append(len(i))\nTypeError: object of type 'int' has no len()\n\n***** format_exc, first and last line *****\nTraceback (most recent call last):\nTypeError: object of type 'int' has no len()\n\n***** format_exception *****\n['Traceback (most recent call last):\\n', '  File \"&lt;ipython-input-5-73d5b316a567&gt;\", line 4, in &lt;module&gt;\\n    get_items_len(data)\\n', '  File \"&lt;ipython-input-3-8421f841ba77&gt;\", line 11, in get_items_len\\n    items_len.append(len(i))\\n', \"TypeError: object of type 'int' has no len()\\n\"]\n\n***** extract_tb *****\n[&lt;FrameSummary file &lt;ipython-input-5-73d5b316a567&gt;, line 4 in &lt;module&gt;&gt;, &lt;FrameSummary file &lt;ipython-input-3-8421f841ba77&gt;, line 11 in get_items_len&gt;]\n\n***** format_tb *****\n['  File \"&lt;ipython-input-5-73d5b316a567&gt;\", line 4, in &lt;module&gt;\\n    get_items_len(data)\\n', '  File \"&lt;ipython-input-3-8421f841ba77&gt;\", line 11, in get_items_len\\n    items_len.append(len(i))\\n']\n\n***** tb_lineno ***** 4\n\n\nAll these methods are useful but we are still short on information about the state of local variables when the system crashed. Before writing our custom function to get the variables state at the time of exception, let us spend some time to understand the working of Traceback object.\n\nTraceback Module\n\nhttps://docs.python.org/3/library/traceback.html \n\nThis module provides an easy-to-use interface to work with traceback objects. It provides multiple functions that we can use to extract the required information from traceback. So far, we have used methods from this module in the above examples.\n\n\nTraceback Objects\n\nhttps://docs.python.org/3/reference/datamodel.html  On this page search for term “Traceback objects”\n\nTraceback objects represent a stack trace of an exception. A traceback object is implicitly created when an exception occurs and may also be explicitly created by initializing an instance of class types.TracebackType. traceback object is also an instance of types.TracebackType class. When an exception occurs, a traceback object is initialized for us, and we can obtain it from any of the following two methods. 1. It is available as a third item of the tuple returned by sys.exc_info() “(type, value, traceback)” 2. It is available as the __traceback__ object of the caught exception. “Exception.__traceback__”\nA traceback object is a linked list of nodes, where each node is a Frame object. Frame objects form their own linked list but in the opposite direction of traceback objects. Together they work like a doubly-linked list, and we can use them to move back and forth in the stack trace history. It is the frame objects that hold all the stack’s important information. traceback object has some special attributes * tb_next point to the next level in the stack trace (towards the frame where the exception occurred), or None if there is no next level * tb_frame points to the execution frame of the current level * tb_lineno gives the line number where the exception occurred\n\n##\n# method 1: get traceback object using sys.exc_info()\ntry:\n    get_items_len(data)\nexcept Exception as e:\n    print(sys.exc_info()[2])\n\n&lt;traceback object at 0x7f5c6c60e9c0&gt;\n\n\n\n##\n# method 2: get traceback object using Exception.__traceback__\ntry:\n    get_items_len(data)\nexcept Exception as e:\n    print(e.__traceback__ )\n\n&lt;traceback object at 0x7f5c6c5c0180&gt;\n\n\nIf there is no exception in the system, then calling sys.exc_info() will only return None values.\n\n##\n# no exception is generated so sys.exc_info() will return None values.\ntry:\n    get_items_len(['1','2','3','4'])\nexcept Exception as e:\n    print(sys.exc_info()[2])\n\n\n\nFrame Objects\n\nhttps://docs.python.org/3/reference/datamodel.html  On this page search for term “Frame objects”\n\nFrame objects represent execution frames. It has some special attributes * f_back is a reference to the previous stack frame (towards the caller), or None if this is the bottom stack frame * f_code is the code object being executed in this frame. We will discuss Code Objects in next the section * f_lineno is the current line number of the frame — writing to this from within a trace function jumps to the given line (only for the bottom-most frame). A debugger can implement a Jump command (aka Set Next Statement) by writing to f_lineno. This attribute will give you the line number in the code on which exception occurred * f_locals is a dictionary used to lookup local variables. From this dictionary we can get all the local variables and their state at the time of exception * f_globals is a dictionary for global varaibles\n\n\nCode Objects\n\nhttps://docs.python.org/3/reference/datamodel.html  On this page search for term “Code Objects”\n\nCode objects represent byte-compiled executable Python code or bytecode. Some of its attributes include * co_name gives the function name being executed * co_filename gives the filename from which the code was compiled\nThere are many other helpful attributes in this object, and you may read about them from the docs.\n\n\nVisual representation of Traceback, Frame and Code Objects\n\n*\n\nfigure 1: Visual representation of Traceback, Frame and Code Objects\n\n\n\n\n\n\nCustom fuction for additional exception info\nNow with this additional information on stack trace objects, let us create a function to get variables state at the time of exception.\n\n\nCode\ndef exc_info_plus():\n    \"\"\"\n    Provides the usual traceback information, followed by a listing of all the\n    local variables in each frame.\n    \"\"\"\n    tb = sys.exc_info()[2]\n\n    # iterate forward to the last (most recent) traceback object.\n    while 1:\n        if not tb.tb_next:\n            break\n        tb = tb.tb_next\n    stack = []\n\n    # get the most recent traceback frame\n    f = tb.tb_frame\n\n    # iterate backwards from recent to oldest traceback frame \n    while f:\n        stack.append(f)\n        f = f.f_back\n    \n    # stack.reverse() # uncomment to get innermost (most recent) frame at the last\n\n    # get exception information and stack trace entries from most recent traceback object\n    exc_msg = traceback.format_exc()\n\n    exc_msg += \"\\n*** Locals by frame, innermost first ***\"\n    for frame in stack:\n        exc_msg += f\"\\nFrame {frame.f_code.co_name} in {frame.f_code.co_filename} at line {frame.f_lineno}\"\n        for key, value in frame.f_locals.items():\n            exc_msg += f\"\\n\\t {key:20} = \"\n            try:\n                data = str(value)\n                # limit variable's output to a certain number. You can adjust it as per your requirement.\n                # But not to remove it as output from large objects (e.g. Pandas DataFrame) can be troublesome. \n                output_limit = 50\n                exc_msg += (data[:output_limit] + \"...\") if len(data) &gt; output_limit else data\n            except:\n                exc_msg += \"&lt;ERROR WHILE PRINTING VALUE&gt;\"\n\n    return exc_msg\n\n\n\n#collapse-output\n#now let us try our custom exception function and see the ouput\ntry:\n    get_items_len(data)\nexcept Exception as e:\n    print(exc_info_plus())\n\nTraceback (most recent call last):\n  File \"&lt;ipython-input-10-01264d9e470a&gt;\", line 4, in &lt;module&gt;\n    get_items_len(data)\n  File \"&lt;ipython-input-3-8421f841ba77&gt;\", line 11, in get_items_len\n    items_len.append(len(i))\nTypeError: object of type 'int' has no len()\n\n*** Locals by frame, innermost first ***\nFrame get_items_len in &lt;ipython-input-3-8421f841ba77&gt; at line 11\n     items                = ['1', '22', 333, '4444']\n     items_len            = [1, 2]\n     i                    = 333\nFrame &lt;module&gt; in &lt;ipython-input-10-01264d9e470a&gt; at line 6\n     __name__             = __main__\n     __doc__              = Automatically created module for IPython interacti...\n     __package__          = None\n     __loader__           = None\n     __spec__             = None\n     __builtin__          = &lt;module 'builtins' (built-in)&gt;\n     __builtins__         = &lt;module 'builtins' (built-in)&gt;\n     _ih                  = ['', '#collapse-hide\\nfrom platform import python_...\n     _oh                  = {}\n     _dh                  = ['/data/_notebooks']\n     In                   = ['', '#collapse-hide\\nfrom platform import python_...\n     Out                  = {}\n     get_ipython          = &lt;bound method InteractiveShell.get_ipython of &lt;ipy...\n     exit                 = &lt;IPython.core.autocall.ZMQExitAutocall object at 0...\n     quit                 = &lt;IPython.core.autocall.ZMQExitAutocall object at 0...\n     _                    = \n     __                   = \n     ___                  = \n     _i                   = #collapse-show\ndef exc_info_plus():\n    \"\"\"\n    Pr...\n     _ii                  = ##\n# no exception is generated so sys.exc_info() w...\n     _iii                 = ##\n# method 2: get traceback object using Exceptio...\n     _i1                  = #collapse-hide\nfrom platform import python_version...\n     python_version       = &lt;function python_version at 0x7f5c72dbc430&gt;\n     _i2                  = #collapse-hide\n# this is intentionally hidden as w...\n     data                 = ['1', '22', 333, '4444']\n     _i3                  = ##\n# our toy example function.\nimport sys, traceba...\n     sys                  = &lt;module 'sys' (built-in)&gt;\n     traceback            = &lt;module 'traceback' from '/usr/lib/python3.8/trace...\n     get_items_len        = &lt;function get_items_len at 0x7f5c6c62c790&gt;\n     _i4                  = ##\n# let's run our function on \"data\" received fro...\n     _i5                  = #collapse-output\n# calling traceback module built-...\n     exc_type             = &lt;class 'TypeError'&gt;\n     exc_value            = object of type 'int' has no len()\n     exc_traceback        = &lt;traceback object at 0x7f5c6c5cf700&gt;\n     formatted_lines      = ['Traceback (most recent call last):', '  File \"&lt;i...\n     _i6                  = ##\n# method 1: get traceback object using sys.exc_...\n     _i7                  = ##\n# method 2: get traceback object using Exceptio...\n     _i8                  = ##\n# no exception is generated so sys.exc_info() w...\n     _i9                  = #collapse-show\ndef exc_info_plus():\n    \"\"\"\n    Pr...\n     exc_info_plus        = &lt;function exc_info_plus at 0x7f5c6c62cc10&gt;\n     _i10                 = #collapse-output\n#now let us try our custom except...\n     e                    = object of type 'int' has no len()\nFrame run_code in /usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py at line 3418\n     self                 = &lt;ipykernel.zmqshell.ZMQInteractiveShell object at ...\n     code_obj             = &lt;code object &lt;module&gt; at 0x7f5c6c62eea0, file \"&lt;ip...\n     result               = &lt;ExecutionResult object at 7f5c6c5c88e0, execution...\n     async_               = False\n     __tracebackhide__    = __ipython_bottom__\n     old_excepthook       = &lt;bound method IPKernelApp.excepthook of &lt;ipykernel...\n     outflag              = True\nFrame run_ast_nodes in /usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py at line 3338\n     self                 = &lt;ipykernel.zmqshell.ZMQInteractiveShell object at ...\n     nodelist             = [&lt;_ast.Try object at 0x7f5c6c5c8850&gt;]\n     cell_name            = &lt;ipython-input-10-01264d9e470a&gt;\n     interactivity        = none\n     compiler             = &lt;IPython.core.compilerop.CachingCompiler object at...\n     result               = &lt;ExecutionResult object at 7f5c6c5c88e0, execution...\n     to_run_exec          = [&lt;_ast.Try object at 0x7f5c6c5c8850&gt;]\n     to_run_interactive   = []\n     mod                  = &lt;_ast.Module object at 0x7f5c6c5c8430&gt;\n     compare              = &lt;function InteractiveShell.run_ast_nodes.&lt;locals&gt;....\n     to_run               = [(&lt;_ast.Try object at 0x7f5c6c5c8850&gt;, 'exec')]\n     node                 = &lt;_ast.Try object at 0x7f5c6c5c8850&gt;\n     mode                 = exec\n     code                 = &lt;code object &lt;module&gt; at 0x7f5c6c62eea0, file \"&lt;ip...\n     asy                  = False\n     _async               = False\nFrame run_cell_async in /usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py at line 3146\n     raw_cell             = #collapse-output\n#now let us try our custom except...\n     silent               = False\n     shell_futures        = True\n     transformed_cell     = #collapse-output\n#now let us try our custom except...\n     preprocessing_exc_tuple = None\n     info                 = &lt;ExecutionInfo object at 7f5c6c5c8be0, raw_cell=\"#...\n     error_before_exec    = &lt;function InteractiveShell.run_cell_async.&lt;locals&gt;...\n     cell                 = #collapse-output\n#now let us try our custom except...\n     compiler             = &lt;IPython.core.compilerop.CachingCompiler object at...\n     _run_async           = False\n     cell_name            = &lt;ipython-input-10-01264d9e470a&gt;\n     code_ast             = &lt;_ast.Module object at 0x7f5c6c5c85e0&gt;\n     interactivity        = last_expr\n     result               = &lt;ExecutionResult object at 7f5c6c5c88e0, execution...\n     self                 = &lt;ipykernel.zmqshell.ZMQInteractiveShell object at ...\n     store_history        = True\nFrame _pseudo_sync_runner in /usr/local/lib/python3.8/dist-packages/IPython/core/async_helpers.py at line 68\n     coro                 = &lt;coroutine object InteractiveShell.run_cell_async ...\nFrame _run_cell in /usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py at line 2923\n     self                 = &lt;ipykernel.zmqshell.ZMQInteractiveShell object at ...\n     raw_cell             = #collapse-output\n#now let us try our custom except...\n     store_history        = True\n     silent               = False\n     shell_futures        = True\n     preprocessing_exc_tuple = None\n     transformed_cell     = #collapse-output\n#now let us try our custom except...\n     coro                 = &lt;coroutine object InteractiveShell.run_cell_async ...\n     runner               = &lt;function _pseudo_sync_runner at 0x7f5c724ba040&gt;\nFrame run_cell in /usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py at line 2877\n     self                 = &lt;ipykernel.zmqshell.ZMQInteractiveShell object at ...\n     raw_cell             = #collapse-output\n#now let us try our custom except...\n     store_history        = True\n     silent               = False\n     shell_futures        = True\n     result               = None\nFrame run_cell in /usr/local/lib/python3.8/dist-packages/ipykernel/zmqshell.py at line 539\n     self                 = &lt;ipykernel.zmqshell.ZMQInteractiveShell object at ...\n     args                 = ('#collapse-output\\n#now let us try our custom exc...\n     kwargs               = {'store_history': True, 'silent': False}\n     __class__            = &lt;class 'ipykernel.zmqshell.ZMQInteractiveShell'&gt;\nFrame do_execute in /usr/local/lib/python3.8/dist-packages/ipykernel/ipkernel.py at line 302\n     self                 = &lt;ipykernel.ipkernel.IPythonKernel object at 0x7f5c...\n     code                 = #collapse-output\n#now let us try our custom except...\n     silent               = False\n     store_history        = True\n     user_expressions     = {}\n     allow_stdin          = True\n     reply_content        = {}\n     run_cell             = &lt;bound method InteractiveShell.run_cell_async of &lt;...\n     should_run_async     = &lt;bound method InteractiveShell.should_run_async of...\n     shell                = &lt;ipykernel.zmqshell.ZMQInteractiveShell object at ...\nFrame wrapper in /usr/local/lib/python3.8/dist-packages/tornado/gen.py at line 234\n     args                 = (&lt;ipykernel.ipkernel.IPythonKernel object at 0x7f5...\n     kwargs               = {}\n     future               = &lt;Future pending&gt;\n     ctx_run              = &lt;built-in method run of Context object at 0x7f5c6c...\n     result               = &lt;generator object IPythonKernel.do_execute at 0x7f...\n     func                 = &lt;function IPythonKernel.do_execute at 0x7f5c6f6978...\nFrame execute_request in /usr/local/lib/python3.8/dist-packages/ipykernel/kernelbase.py at line 540\n     self                 = &lt;ipykernel.ipkernel.IPythonKernel object at 0x7f5c...\n     stream               = &lt;zmq.eventloop.zmqstream.ZMQStream object at 0x7f5...\n     ident                = [b'e2e3826d25fb4c63876268cdc5a787ad']\n     parent               = {'header': {'msg_id': '218114cb9837444cbd29466d87b...\n     content              = {'code': '#collapse-output\\n#now let us try our cu...\n     code                 = #collapse-output\n#now let us try our custom except...\n     silent               = False\n     store_history        = True\n     user_expressions     = {}\n     allow_stdin          = True\n     stop_on_error        = True\n     metadata             = {'started': datetime.datetime(2022, 2, 14, 9, 30, ...\nFrame wrapper in /usr/local/lib/python3.8/dist-packages/tornado/gen.py at line 234\n     args                 = (&lt;ipykernel.ipkernel.IPythonKernel object at 0x7f5...\n     kwargs               = {}\n     future               = &lt;Future pending&gt;\n     ctx_run              = &lt;built-in method run of Context object at 0x7f5c6c...\n     result               = &lt;generator object Kernel.execute_request at 0x7f5c...\n     func                 = &lt;function Kernel.execute_request at 0x7f5c6f747f70...\nFrame dispatch_shell in /usr/local/lib/python3.8/dist-packages/ipykernel/kernelbase.py at line 265\n     self                 = &lt;ipykernel.ipkernel.IPythonKernel object at 0x7f5c...\n     stream               = &lt;zmq.eventloop.zmqstream.ZMQStream object at 0x7f5...\n     msg                  = {'header': {'msg_id': '218114cb9837444cbd29466d87b...\n     idents               = [b'e2e3826d25fb4c63876268cdc5a787ad']\n     msg_type             = execute_request\n     handler              = &lt;bound method Kernel.execute_request of &lt;ipykernel...\nFrame wrapper in /usr/local/lib/python3.8/dist-packages/tornado/gen.py at line 234\n     args                 = (&lt;ipykernel.ipkernel.IPythonKernel object at 0x7f5...\n     kwargs               = {}\n     future               = &lt;Future pending&gt;\n     ctx_run              = &lt;built-in method run of Context object at 0x7f5c6f...\n     result               = &lt;generator object Kernel.dispatch_shell at 0x7f5c6...\n     func                 = &lt;function Kernel.dispatch_shell at 0x7f5c6f7473a0&gt;\nFrame process_one in /usr/local/lib/python3.8/dist-packages/ipykernel/kernelbase.py at line 362\n     self                 = &lt;ipykernel.ipkernel.IPythonKernel object at 0x7f5c...\n     wait                 = True\n     priority             = 10\n     t                    = 13\n     dispatch             = &lt;bound method Kernel.dispatch_shell of &lt;ipykernel....\n     args                 = (&lt;zmq.eventloop.zmqstream.ZMQStream object at 0x7f...\nFrame run in /usr/local/lib/python3.8/dist-packages/tornado/gen.py at line 775\n     self                 = &lt;tornado.gen.Runner object at 0x7f5c6c60f8e0&gt;\n     future               = None\n     exc_info             = None\n     value                = (10, 13, &lt;bound method Kernel.dispatch_shell of &lt;i...\nFrame inner in /usr/local/lib/python3.8/dist-packages/tornado/gen.py at line 814\n     f                    = None\n     self                 = &lt;tornado.gen.Runner object at 0x7f5c6c60f8e0&gt;\nFrame _run_callback in /usr/local/lib/python3.8/dist-packages/tornado/ioloop.py at line 741\n     self                 = &lt;tornado.platform.asyncio.AsyncIOMainLoop object a...\n     callback             = functools.partial(&lt;function Runner.handle_yield.&lt;l...\nFrame &lt;lambda&gt; in /usr/local/lib/python3.8/dist-packages/tornado/ioloop.py at line 688\n     f                    = &lt;Future finished result=(10, 13, &lt;bound method...7...\n     callback             = &lt;function Runner.handle_yield.&lt;locals&gt;.inner at 0x...\n     future               = &lt;Future finished result=(10, 13, &lt;bound method...7...\n     self                 = &lt;tornado.platform.asyncio.AsyncIOMainLoop object a...\nFrame _run in /usr/lib/python3.8/asyncio/events.py at line 81\n     self                 = &lt;Handle IOLoop.add_future.&lt;locals&gt;.&lt;lambda&gt;(&lt;Futur...\nFrame _run_once in /usr/lib/python3.8/asyncio/base_events.py at line 1859\n     self                 = &lt;_UnixSelectorEventLoop running=True closed=False ...\n     sched_count          = 0\n     handle               = &lt;Handle IOLoop.add_future.&lt;locals&gt;.&lt;lambda&gt;(&lt;Futur...\n     timeout              = 0\n     event_list           = []\n     end_time             = 113697.83311910101\n     ntodo                = 2\n     i                    = 0\nFrame run_forever in /usr/lib/python3.8/asyncio/base_events.py at line 570\n     self                 = &lt;_UnixSelectorEventLoop running=True closed=False ...\n     old_agen_hooks       = asyncgen_hooks(firstiter=None, finalizer=None)\nFrame start in /usr/local/lib/python3.8/dist-packages/tornado/platform/asyncio.py at line 199\n     self                 = &lt;tornado.platform.asyncio.AsyncIOMainLoop object a...\n     old_loop             = &lt;_UnixSelectorEventLoop running=True closed=False ...\nFrame start in /usr/local/lib/python3.8/dist-packages/ipykernel/kernelapp.py at line 612\n     self                 = &lt;ipykernel.kernelapp.IPKernelApp object at 0x7f5c7...\nFrame launch_instance in /usr/local/lib/python3.8/dist-packages/traitlets/config/application.py at line 845\n     cls                  = &lt;class 'ipykernel.kernelapp.IPKernelApp'&gt;\n     argv                 = None\n     kwargs               = {}\n     app                  = &lt;ipykernel.kernelapp.IPKernelApp object at 0x7f5c7...\nFrame &lt;module&gt; in /usr/local/lib/python3.8/dist-packages/ipykernel_launcher.py at line 16\n     __name__             = __main__\n     __doc__              = Entry point for launching an IPython kernel.\n\nThis...\n     __package__          = \n     __loader__           = &lt;_frozen_importlib_external.SourceFileLoader objec...\n     __spec__             = ModuleSpec(name='ipykernel_launcher', loader=&lt;_fro...\n     __annotations__      = {}\n     __builtins__         = &lt;module 'builtins' (built-in)&gt;\n     __file__             = /usr/local/lib/python3.8/dist-packages/ipykernel_l...\n     __cached__           = /usr/local/lib/python3.8/dist-packages/__pycache__...\n     sys                  = &lt;module 'sys' (built-in)&gt;\n     app                  = &lt;module 'ipykernel.kernelapp' from '/usr/local/lib...\nFrame _run_code in /usr/lib/python3.8/runpy.py at line 87\n     code                 = &lt;code object &lt;module&gt; at 0x7f5c7317e030, file \"/us...\n     run_globals          = {'__name__': '__main__', '__doc__': 'Entry point f...\n     init_globals         = None\n     mod_name             = __main__\n     mod_spec             = ModuleSpec(name='ipykernel_launcher', loader=&lt;_fro...\n     pkg_name             = \n     script_name          = None\n     loader               = &lt;_frozen_importlib_external.SourceFileLoader objec...\n     fname                = /usr/local/lib/python3.8/dist-packages/ipykernel_l...\n     cached               = /usr/local/lib/python3.8/dist-packages/__pycache__...\nFrame _run_module_as_main in /usr/lib/python3.8/runpy.py at line 194\n     mod_name             = ipykernel_launcher\n     alter_argv           = 1\n     mod_spec             = ModuleSpec(name='ipykernel_launcher', loader=&lt;_fro...\n     code                 = &lt;code object &lt;module&gt; at 0x7f5c7317e030, file \"/us...\n     main_globals         = {'__name__': '__main__', '__doc__': 'Entry point f...\n\n\nNote the output from the first stack frame in the above stack trace. It is easy now to see (items) that we received in our function. The item at index i is also available (333) on which our function crashed. Using our custom function unexpected errors are logged in a format that makes it a lot easier to find and fix the errors. Let’s fix our function to handle unexpected integer values.\n\n##\n# let's fix our function to handle unexpected 'int' items by converting them to 'str'\ndef get_items_len(items: list) -&gt; list:\n    \"\"\"\n    this function returns the length of items received in a list.\n    \"\"\"\n    items_len = []\n    for i in map(str, items):\n        items_len.append(len(i))\n    \n    return items_len\n\n# test it again\nget_items_len(data)\n\n[1, 2, 3, 4]"
  },
  {
    "objectID": "posts/2022-02-09-hello-world.html",
    "href": "posts/2022-02-09-hello-world.html",
    "title": "My First Blog Post from Jupyter Notebook",
    "section": "",
    "text": "Well, this is my first post using Jupyter notebook as a publishing medium. Besides this notebook, I am also using ‘nbdev’ library from FastAI as tooling to convert notebooks into static HTML pages. Once pushed to GitHub they will become new posts on my blog. I need to learn more about this setup, but it is looking very interesting.\n\n\nCode\n# I can also include some code directly into the blog post. No need for GitHub snippets.\nprint(\"nbdev and fastpages from Fast.AI are so cool! \")\n\n\nnbdev and fastpages from Fast.AI are so cool!"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hi there!\nI am Hassaan Bin Aslam, and welcome to my blog.\nI started this blog to document and share my learning. I make a living by working as a Machine Learning Solutions Architect. AI/ML, Cloud Architecture, DevOps are fascinating topics and also close to my heart. Every day I face exciting problems, and I like to share my understanding of them here. I am passionate about AWS as a strategic cloud platform and using AI/ML to solve business problems. I’m currently focusing a lot of my time on applying DevOps practices to Machine Learning workloads (MLOps) to enable customers to adopt Machine Learning at scale.\nYou can find and connect with me on - LinkedIn: linkedin.com/in/hassaanbinaslam - Twitter: twitter.com/hassaanbinaslam - GitHub: github.com/hassaanbinaslam"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Random Thoughts",
    "section": "",
    "text": "MobileNetV1 Explained: A Deep Dive into Lightweight Neural Networks\n\n\n\npapers\n\n\n\nA detailed, page-by-page breakdown of the MobileNetV1 paper, explaining how to build efficient neural networks for mobile and edge devices.\n\n\n\n\n\nSep 2, 2025\n\n48 min\n\n\n\n\n\n\n\n\n\n\n\nXception Explained: A Deep Dive into Extreme Inception and Smarter Convolutions\n\n\n\npapers\n\n\n\nA detailed, page-by-page breakdown of the Xception paper, explaining how to build smarter, more efficient neural networks by rethinking the classic convolution.\n\n\n\n\n\nAug 30, 2025\n\n50 min\n\n\n\n\n\n\n\n\n\n\n\nBuilding SqueezeNet from Scratch with PyTorch: A Hands-On Implementation\n\n\n\npapers\n\npytorch\n\n\n\nIn this step-by-step tutorial, we translate the groundbreaking SqueezeNet paper into a working PyTorch model, train it on CIFAR-10, and verify its incredible efficiency.\n\n\n\n\n\nAug 27, 2025\n\n16 min\n\n\n\n\n\n\n\n\n\n\n\nSqueezeNet Paper Explained: A Deep Dive into 50x Smaller, AlexNet-Level AI\n\n\n\npapers\n\n\n\nA detailed, page-by-page breakdown of the landmark paper that proved neural networks could be both powerful and incredibly efficient.\n\n\n\n\n\nAug 21, 2025\n\n35 min\n\n\n\n\n\n\n\n\n\n\n\nThe Horse in Motion and FFmpeg Gotchas - Part 3 Building FFmpeg From Scratch\n\n\n\npython\n\naws\n\n\n\nA deep-dive tutorial on setting up a build environment, compiling FFmpeg with its dependencies like libx264 and libfreetype, and deploying your custom binary to AWS Lambda.\n\n\n\n\n\nAug 3, 2025\n\n13 min\n\n\n\n\n\n\n\n\n\n\n\nThe Horse in Motion and FFmpeg Gotchas - Part 2 Serverless on AWS Lambda\n\n\n\npython\n\naws\n\n\n\nA hands-on guide to deploying a Python-based FFmpeg workflow on AWS Lambda using SAM, covering layer creation, binary selection, and executing subprocesses in a serverless environment.\n\n\n\n\n\nJul 30, 2025\n\n9 min\n\n\n\n\n\n\n\n\n\n\n\nThe Horse in Motion and FFmpeg Gotchas - Part 1\n\n\n\npython\n\naws\n\n\n\nA practical, hands-on guide to getting started with FFmpeg, covering image sequences, codecs, filters, and common gotchas in a real-world example.\n\n\n\n\n\nJul 28, 2025\n\n9 min\n\n\n\n\n\n\n\n\n\n\n\nFrom Noise to Numbers: A Hands-On Guide to Generating MNIST Digits with Diffusion Models (Part 3)\n\n\n\npython\n\ndl\n\n\n\nPart 3 dives deep into the core of diffusion models: noise prediction and scheduled denoising. Discover how these crucial techniques enable iterative image generation, starting from random noise. Get hands-on experience and see the impressive results as we finally achieve high-quality MNIST digit synthesis.\n\n\n\n\n\nFeb 18, 2025\n\n16 min\n\n\n\n\n\n\n\n\n\n\n\nFrom Noise to Numbers: A Hands-On Guide to Generating MNIST Digits with Diffusion Models (Part 2)\n\n\n\npython\n\ndl\n\n\n\nIn Part 2 of our diffusion model journey, we supercharge our MNIST digit generator! Building on our basic UNet from Part 1, we introduce the powerful diffusers library and its UNet2DModel. See how this enhanced architecture significantly improves denoising quality through direct image prediction, bringing us closer to clearer, sharper digit generation.\n\n\n\n\n\nFeb 15, 2025\n\n15 min\n\n\n\n\n\n\n\n\n\n\n\nFrom Noise to Numbers: A Hands-On Guide to Generating MNIST Digits with Diffusion Models (Part 1)\n\n\n\npython\n\ndl\n\n\n\nThis post walks you through building MNIST digit generators with diffusion models, starting from the basics. We introduce diffusion models, prepare the MNIST dataset, and train a simple convolutional UNet for direct image prediction. Finally, we evaluate its denoising performance as a foundation for more advanced models.\n\n\n\n\n\nFeb 10, 2025\n\n29 min\n\n\n\n\n\n\n\n\n\n\n\nBuilding a Web Server From Scratch in Pure Python\n\n\n\npython\n\n\n\nBuilding a web server from scratch in Python? Join me as I journey from simple http.server to raw sockets, threading, selectors, and asyncio, exploring different concurrency models and benchmarking performance along the way.\n\n\n\n\n\nJan 29, 2025\n\n27 min\n\n\n\n\n\n\n\n\n\n\n\nUnleash Your Creativity: A Rapid Prototyping App with Amazon Nova Canvas\n\n\n\npython\n\naws\n\n\n\nExplore the power of Amazon Nova Canvas with a hands-on Streamlit app. This rapid prototyping tool offers five powerful image processing features, including text-to-image, inpainting, and outpainting. Unleash your creativity and transform your ideas into stunning visuals.\n\n\n\n\n\nJan 26, 2025\n\n20 min\n\n\n\n\n\n\n\n\n\n\n\nMy Streamlit Deployment Journey on AWS : What Worked, What Didn’t\n\n\n\npython\n\naws\n\n\n\nDeploying Streamlit on AWS? I faced WebSocket issues with Lambda and App Runner, found a solution with Lightsail, and built a custom Fargate solution with AWS CDK. This post shares my lessons and cost insights.\n\n\n\n\n\nJan 21, 2025\n\n17 min\n\n\n\n\n\n\n\n\n\n\n\nStreaming Responses in FastAPI\n\n\n\npython\n\n\n\nIn this blog post, I explore how to stream responses in FastAPI using Server-Sent Events, StreamingResponse, and WebSockets. Through simple examples that simulate LLM outputs, I demonstrate how you can efficiently stream real-time data in your applications.\n\n\n\n\n\nJan 19, 2025\n\n19 min\n\n\n\n\n\n\n\n\n\n\n\nTips to Register for an AWS Certification Exam with Pearson VUE from Pakistan\n\n\n\naws\n\n\n\nThis blog post provides essential tips for individuals in Pakistan facing challenges with AWS certification exam registration through Pearson VUE. It outlines workarounds for the payment limitations, guides readers through the registration process, and offers insights to successfully navigate the system, ensuring a smooth experience.\n\n\n\n\n\nMay 29, 2023\n\n6 min\n\n\n\n\n\n\n\n\n\n\n\nAWS Certified Data Analytics Specialty DAS-C01 Notes\n\n\n\naws\n\n\n\nI am sharing the notes I used to prepare for the exam.\n\n\n\n\n\nMay 28, 2023\n\n26 min\n\n\n\n\n\n\n\n\n\n\n\nBuild your own Generative AI Art Studio with Amazon SageMaker JumpStart\n\n\n\naws\n\nml\n\n\n\nIn this notebook, I demonstrate how to use the SageMaker Jumpstart to generate images from text using state-of-the-art Stable Diffusion models.\n\n\n\n\n\nMay 10, 2023\n\n21 min\n\n\n\n\n\n\n\n\n\n\n\nCreating an Object Detection Model using Amazon Rekognition Custom Labels\n\n\n\naws\n\nml\n\n\n\nAmazon Rekognition Custom Labels is a feature of Amazon Rekognition that enables customers to build specialized image analysis capabilities to detect unique objects and scenes integral to their specific use case. In this post, we will use this service to train a custom model with a small set of labeled images and use it to analyze new images via an API. This service uses AutoML to train models to find objects, scenes, concepts, object locations, and brand locations in images.\n\n\n\n\n\nApr 26, 2023\n\n9 min\n\n\n\n\n\n\n\n\n\n\n\nWorking with partition indexes in AWS Glue\n\n\n\naws\n\n\n\nGlue table partition indexes can significantly improve query execution. This post signifies the importance of partition indexes and shows a query performance comparison between tables with partition indexes and without indexes.\n\n\n\n\n\nMar 1, 2023\n\n12 min\n\n\n\n\n\n\n\n\n\n\n\nImplementing AutoEncoder with PyTorch\n\n\n\npytorch\n\n\n\nThis is a practice notebook to implement AutoEncoder in PyTorch. An autoencoder takes an image as input, stores it in a lower dimension (term encoder), and tries to reproduce the same image as output, hence the term auto. Autoencoders come in handy to identify and group similar images.\n\n\n\n\n\nDec 14, 2022\n\n12 min\n\n\n\n\n\n\n\n\n\n\n\nImplementing Word2Vec with PyTorch\n\n\n\npytorch\n\nnlp\n\n\n\nThis is a practice notebook to implement word2vec in PyTorch.\n\n\n\n\n\nDec 2, 2022\n\n15 min\n\n\n\n\n\n\n\n\n\n\n\nGenerating Text with Recurrent Neural Networks in PyTorch\n\n\n\npytorch\n\nlstm\n\n\n\nThis is a practice notebook to build a character-level language model with LSTM using PyTorch. We will train a model on an input text, and our goal will be to generate some new text.\n\n\n\n\n\nNov 19, 2022\n\n8 min\n\n\n\n\n\n\n\n\n\n\n\nPredicting the Sentiment of IMDB Movie Reviews using LSTM in PyTorch\n\n\n\npytorch\n\nlstm\n\n\n\nThis is a practice notebook to work with a dataset of 50,000 movie reviews from the Internet Movie Database (IMDB) and build an LSTM predictor to distinguish between positive and negative reviews.\n\n\n\n\n\nNov 9, 2022\n\n9 min\n\n\n\n\n\n\n\n\n\n\n\nBuild Temporal Models for Univariate Time series Data with RNN, GRU, LSTM, CNN using PyTorch\n\n\n\npytorch\n\ndl\n\n\n\nThis is a practice notebook to understand and build models for time series data. We will explore some popular neural network architectures including RNN, GRU, LSTM, and 1D CNN.\n\n\n\n\n\nNov 7, 2022\n\n15 min\n\n\n\n\n\n\n\n\n\n\n\nDetect Vanishing Gradients in Deep Neural Networks by Plotting Gradient Distributions\n\n\n\npytorch\n\ndl\n\n\n\nIn this notebook, we will explore how vanishing gradients can affect the training of a deep neural network. We will visualize the gradient flow from the deeper to starting layers during the backpropagation for two popular activation functions, Sigmoid and ReLU.\n\n\n\n\n\nOct 23, 2022\n\n11 min\n\n\n\n\n\n\n\n\n\n\n\nConvolutional Neural Networks Filters and Feature Maps with PyTorch\n\n\n\npytorch\n\n\n\nThis is a practice notebook for implementing a convolutional neural network (CNN) on the MNIST dataset with PyTorch. We will implement the now famous LeNet-5 from Yann LeCun, a 7-layer CNN from 1989. Then we will explore and visualize the layers learned by our network including filters, feature maps, and output layers.\n\n\n\n\n\nOct 18, 2022\n\n10 min\n\n\n\n\n\n\n\n\n\n\n\nTwo Class (Binary) Logistic Regression in Pytorch\n\n\n\npytorch\n\n\n\nThis is a practice notebook for implementing a two class logistic regression model in PyTorch. We will start by generating some synthetic data and then build an end-to-end pipeline to train a model. We will also see two ways to implement logistic regression models.\n\n\n\n\n\nOct 11, 2022\n\n11 min\n\n\n\n\n\n\n\n\n\n\n\nLinear Regression with PyTorch\n\n\n\npytorch\n\n\n\nThis is a practice notebook for implementing a linear regression model in PyTorch. We will start by generating some synthetic linear data and then load it into DataLoader class for creating mini-batches. Then build the complete pipeline to train the model and visualize its loss progress in TensorBoard.\n\n\n\n\n\nOct 10, 2022\n\n5 min\n\n\n\n\n\n\n\n\n\n\n\nTrain an Image Classifier using Fastai (Deep Dive Analysis)\n\n\n\naws\n\nml\n\nsagemaker\n\nfastai\n\n\n\nThis is a practice notebook using the fastai library to build a simple image classifier for cricket, tennis, basketball, and soccer.\n\n\n\n\n\nAug 10, 2022\n\n21 min\n\n\n\n\n\n\n\n\n\n\n\nBuilding a Feature Repository with SageMaker Feature Store\n\n\n\naws\n\nml\n\nsagemaker\n\n\n\nThis notebook demonstrates how to build a central feature repository using Amazon SageMaker Feature Store. Feature Store is used to store, retrieve, and share machine learning features.\n\n\n\n\n\nAug 5, 2022\n\n11 min\n\n\n\n\n\n\n\n\n\n\n\nDeploy Scikit-learn Models to Amazon SageMaker with the SageMaker Python SDK using Script mode\n\n\n\naws\n\nml\n\nsagemaker\n\n\n\nThe aim of this notebook is to demonstrate how to train and deploy a scikit-learn model in Amazon SageMaker using script mode.\n\n\n\n\n\nJul 7, 2022\n\n20 min\n\n\n\n\n\n\n\n\n\n\n\nLoading SageMaker Linear Learner Model with Apache MXNet in Python\n\n\n\naws\n\nml\n\nsagemaker\n\n\n\nHow to load SageMaker builtin Linear Learner model with Apache MXNet in Python.\n\n\n\n\n\nJul 5, 2022\n\n4 min\n\n\n\n\n\n\n\n\n\n\n\nServerless Inference with SageMaker Serverless Endpoints\n\n\n\naws\n\nml\n\nsagemaker\n\n\n\nHow to call an ML model endpoint hosted by SageMaker using serverless technology.\n\n\n\n\n\nJun 17, 2022\n\n9 min\n\n\n\n\n\n\n\n\n\n\n\nDemystifying Amazon SageMaker Training for scikit-learn Lovers\n\n\n\naws\n\nml\n\nsagemaker\n\n\n\nHow to build, train, and deploy a machine learning model with Amazon SageMaker.\n\n\n\n\n\nJun 8, 2022\n\n15 min\n\n\n\n\n\n\n\n\n\n\n\nstoremagic - Don’t lose your variables in Jupyter Notebook\n\n\n\njupyter\n\npython\n\n\n\nA post on IPython extension ‘storemagic’ to keep your important variables in persistent storage. Use this trick to keep them safe and stop pulling your hair (if there are any left)\n\n\n\n\n\nMay 30, 2022\n\n2 min\n\n\n\n\n\n\n\n\n\n\n\nData Preparation with SageMaker Data Wrangler (Part 5)\n\n\n\naws\n\nml\n\nsagemaker\n\n\n\nA detailed guide on AWS SageMaker Data Wrangler to prepare data for machine learning models. This is a five parts series where we will prepare, import, explore, process, and export data using AWS Data Wrangler. You are reading Part 5:Export data for ML training.\n\n\n\n\n\nMay 26, 2022\n\n3 min\n\n\n\n\n\n\n\n\n\n\n\nData Preparation with SageMaker Data Wrangler (Part 4)\n\n\n\naws\n\nml\n\nsagemaker\n\n\n\nA detailed guide on AWS SageMaker Data Wrangler to prepare data for machine learning models. This is a five parts series where we will prepare, import, explore, process, and export data using AWS Data Wrangler. You are reading Part 4:Preprocess data using Data Wrangler.\n\n\n\n\n\nMay 25, 2022\n\n4 min\n\n\n\n\n\n\n\n\n\n\n\nData Preparation with SageMaker Data Wrangler (Part 3)\n\n\n\naws\n\nml\n\nsagemaker\n\n\n\nA detailed guide on AWS SageMaker Data Wrangler to prepare data for machine learning models. This is a five parts series where we will prepare, import, explore, process, and export data using AWS Data Wrangler. You are reading Part 3:Explore data with Data Wrangler visualizations.\n\n\n\n\n\nMay 24, 2022\n\n3 min\n\n\n\n\n\n\n\n\n\n\n\nData Preparation with SageMaker Data Wrangler (Part 2)\n\n\n\naws\n\nml\n\nsagemaker\n\n\n\nA detailed guide on AWS SageMaker Data Wrangler to prepare data for machine learning models. This is a five parts series where we will prepare, import, explore, process, and export data using AWS Data Wrangler. You are reading Part 2:Import data from multiple sources using Data Wrangler.\n\n\n\n\n\nMay 23, 2022\n\n4 min\n\n\n\n\n\n\n\n\n\n\n\nCollecting metrics and logs from on-premises servers with the CloudWatch agent\n\n\n\naws\n\ncloudwatch\n\n\n\nA detailed guide on using cloudwatch agent to collect logs and metrics from an on-premises Ubuntu server.\n\n\n\n\n\nMay 21, 2022\n\n6 min\n\n\n\n\n\n\n\n\n\n\n\nData Preparation with SageMaker Data Wrangler (Part 1)\n\n\n\naws\n\nml\n\nsagemaker\n\n\n\nA detailed guide on AWS SageMaker Data Wrangler to prepare data for machine learning models. This is a five parts series where we will prepare, import, explore, process, and export data using AWS Data Wrangler. You are reading Part 1:Prepare synthetic data and place it on multiple sources.\n\n\n\n\n\nMay 17, 2022\n\n5 min\n\n\n\n\n\n\n\n\n\n\n\nAWS Machine Learning Certification Notes (MLS-C01)\n\n\n\naws\n\nml\n\n\n\nMy notes for AWS ML speciality exam passed on May 14, 2022.\n\n\n\n\n\nMay 14, 2022\n\n11 min\n\n\n\n\n\n\n\n\n\n\n\nKaggle - Blue Book for Bulldozers\n\n\n\nml\n\n\n\nblue book\n\n\n\n\n\nApr 25, 2022\n\n18 min\n\n\n\n\n\n\n\n\n\n\n\nDocker - Send Container Logs to AWS CloudWatch\n\n\n\ndocker\n\npython\n\naws\n\ncloudwatch\n\n\n\nA tutorial on sending docker application logs to aws cloudwatch.\n\n\n\n\n\nApr 11, 2022\n\n7 min\n\n\n\n\n\n\n\n\n\n\n\nMachine Learning Nomenclature\n\n\n\nml\n\n\n\nA collection of machine learning terminologies\n\n\n\n\n\nMar 31, 2022\n\n4 min\n\n\n\n\n\n\n\n\n\n\n\nAWS EFS Sync to S3 Using DataSync\n\n\n\naws\n\nlambda\n\nefs\n\ns3\n\nsynchonization\n\ndatasync\n\n\n\nA tutorial to synchronize EFS with S3 bucket using DataSync service.\n\n\n\n\n\nMar 29, 2022\n\n8 min\n\n\n\n\n\n\n\n\n\n\n\nAWS EFS Sync to S3 Using Lambda\n\n\n\naws\n\nlambda\n\nefs\n\ns3\n\nsynchonization\n\n\n\nA tutorial on synchronizing EFS with S3 bucket using a Lambda function.\n\n\n\n\n\nMar 28, 2022\n\n10 min\n\n\n\n\n\n\n\n\n\n\n\nAWS Lambda - Test and Debug Locally in Visual Studio Code\n\n\n\naws\n\nlambda\n\ndocker\n\n\n\nA tutorial on testing and debugging AWS Lambda function from Visual Studio Code.\n\n\n\n\n\nMar 16, 2022\n\n8 min\n\n\n\n\n\n\n\n\n\n\n\nDocker - Debugging Python Application\n\n\n\ndocker\n\npython\n\n\n\nA tutorial on debugging Python application running on Docker inside WSL2.\n\n\n\n\n\nMar 14, 2022\n\n6 min\n\n\n\n\n\n\n\n\n\n\n\nDocker - Accessing Python Application Logs\n\n\n\ndocker\n\npython\n\n\n\nA tutorial on getting Python application logs running on Docker inside WSL2.\n\n\n\n\n\nMar 11, 2022\n\n8 min\n\n\n\n\n\n\n\n\n\n\n\nSklearn Pipeline and Transformers Deep Dive\n\n\n\nml\n\nsklearn\n\n\n\nA detailed tutorial on Sklearn Pipeline, ColumnTransformer, FunctionTransformer, and a custom transformer.\n\n\n\n\n\nMar 4, 2022\n\n12 min\n\n\n\n\n\n\n\n\n\n\n\nAWS CloudFormation Template, Functions, and Commands\n\n\n\naws\n\n\n\nSome useful notes on AWS CloudFormation template sections, intrinsic functions, and other tips.\n\n\n\n\n\nFeb 28, 2022\n\n11 min\n\n\n\n\n\n\n\n\n\n\n\nAWS IAM Policy Types\n\n\n\naws\n\n\n\nA summary of different AWS IAM policies.\n\n\n\n\n\nFeb 23, 2022\n\n3 min\n\n\n\n\n\n\n\n\n\n\n\nAWS DNS Records - A, CNAME, ALIAS, & MX\n\n\n\naws\n\n\n\nA summary of differences between AWS Route53 DNS Records\n\n\n\n\n\nFeb 22, 2022\n\n4 min\n\n\n\n\n\n\n\n\n\n\n\nPython - A collection of output formatting tips\n\n\n\npython\n\n\n\nSome handy Python output tips\n\n\n\n\n\nFeb 18, 2022\n\n1 min\n\n\n\n\n\n\n\n\n\n\n\nPython - Getting more information from Tracebacks\n\n\n\npython\n\n\n\nA tutorial to get more information from Python exception stack traceback.\n\n\n\n\n\nFeb 11, 2022\n\n5 min\n\n\n\n\n\n\n\n\n\n\n\nPython Dictionary - Multiple ways to get items\n\n\n\npython\n\n\n\nA tutorial on multiple ways to get items from a Python dictionary.\n\n\n\n\n\nFeb 10, 2022\n\n2 min\n\n\n\n\n\n\n\n\n\n\n\nMy First Blog Post from Jupyter Notebook\n\n\n\njupyter\n\nfastpages\n\n\n\nTrying to check if everything is working as intended\n\n\n\n\n\nFeb 9, 2022\n\n1 min\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2022-02-10-python-dictionary.html#about",
    "href": "posts/2022-02-10-python-dictionary.html#about",
    "title": "Python Dictionary - Multiple ways to get items",
    "section": "About",
    "text": "About\nThis notebook demonstrates multiple ways to get items from a Python dictionary.\n\nEnvironment Details\n\n\nCode\nfrom platform import python_version\n\nprint(\"python==\" + python_version())\n\n\npython==3.8.5\n\n\n\n\nExample Dictionaries\n\n##\n# simple dictionary\ncar = {\n    \"brand\": \"ford\",\n    \"model\": \"mustang\"\n}\n\ncar\n\n{'brand': 'ford', 'model': 'mustang'}\n\n\n\n##\n# nested dictionary\nfamily = {\n    'gfather' : {\n        'father': {\n            'son': {'love':'python'}\n        }\n    }\n}\n\nfamily\n\n{'gfather': {'father': {'son': {'love': 'python'}}}}"
  },
  {
    "objectID": "posts/2022-02-10-python-dictionary.html#method-1-square-brackets",
    "href": "posts/2022-02-10-python-dictionary.html#method-1-square-brackets",
    "title": "Python Dictionary - Multiple ways to get items",
    "section": "Method 1: Square brackets",
    "text": "Method 1: Square brackets\nA square bracket is the simplest approach to getting any item from a dictionary. You can get a value from a dictionary by providing it a key in [] brackets. For example, to get a value of model from a car\n\ncar['model']\n\n'mustang'\n\n\nProblem with this approach is that if the provided key is not available in the dictionary then it will throw a KeyError exception.\n\ncar['year']\n\n\nKeyErrorTraceback (most recent call last)\n&lt;ipython-input-5-ca220af55913&gt; in &lt;module&gt;\n----&gt; 1 car['year']\n\nKeyError: 'year'\n\n\n\nTo avoid KeyError, you can first check if the key is available in dictionary.\n\nif 'year' in car: # check if given key is available in dictionary\n    year = car['year'] # now get the value\nelse:\n    year = '1964' # (Optional) otherwise give this car a default value\n\nyear\n\n'1964'\n\n\nAn alternate approach could be to use a Try-Except block to handle the KeyError exception.\n\ntry:\n    year = car['year']\nexcept KeyError:\n    year = '1964' # give this car a default value\n\nyear\n\n'1964'\n\n\nFor nested dictionaries, you can use chained [] brackets. But beware that if any of the Keys is missing in the chain, you will get a KeyError exception.\n\n##\n# this will work. All keys are present.\nfamily['gfather']['father']['son']\n\n{'love': 'python'}\n\n\n\n##\n# this will not work. 'mother' key is not in dictionary\nfamily['gfather']['mother']['son']\n\n\nKeyErrorTraceback (most recent call last)\n&lt;ipython-input-9-3d615db6bfdf&gt; in &lt;module&gt;\n      1 # this will not work. 'mother' key is not in dictionary\n----&gt; 2 family['gfather']['mother']['son']\n\nKeyError: 'mother'"
  },
  {
    "objectID": "posts/2022-02-10-python-dictionary.html#method-2-get-function",
    "href": "posts/2022-02-10-python-dictionary.html#method-2-get-function",
    "title": "Python Dictionary - Multiple ways to get items",
    "section": "Method 2: Get function",
    "text": "Method 2: Get function\n\nhttps://docs.python.org/3/library/stdtypes.html#dict.get  get(key[, default])\n\nGet function will return the value for key if key is in the dictionary. Otherwise, it will return a default value which is None. You can provide your default value as well.\n\nyear = car.get('year', '1964')\nyear\n# year key is not present so get function will return a default value '1964'\n\n'1964'\n\n\nDepending on your use case there can be confusion with this approach when your item can also have None value. In that case, you will not know whether the None value was returned from the dictionary or it was the Get function.\n\nowner = car.get('owner')\nowner\n# owner has a None value. But is this value coming from dic or from Get function? \n# This can be confusing for large nested dictionaries.\n\nFor nested dictionaries you can use chained Get functions. But beware that missing Key items needs to be properly handled otherwise you will still get an exception.\n\n##\n# this will work. All keys are present.\nfamily.get('gfather').get('father').get('son')\n\n{'love': 'python'}\n\n\n\n##\n# this will still work. 'daughter' key is missing\n# but since it is at the end of chain it will return a default None value\nfamily.get('gfather').get('father').get('daughter')\n\n\n##\n# this will NOT work. 'mother' key is missing and it returned a default None value.\n# but since it is not at the end, and we called Get function on returned value 'None'\nfamily.get('gfather').get('mother').get('son')\n\n\nAttributeErrorTraceback (most recent call last)\n&lt;ipython-input-14-a35a8f091991&gt; in &lt;module&gt;\n      1 # this will NOT work. 'mother' key is missing and it returned a default None value.\n      2 # but since it is not at the end, and we called Get function on returned value 'None'\n----&gt; 3 family.get('gfather').get('mother').get('son')\n\nAttributeError: 'NoneType' object has no attribute 'get'\n\n\n\n\n##\n# this will work. 'mother' key is missing and it returned a default value.\n# but we have properly handled all the default values with empty dictionaries.\nfamily.get('gfather', {}).get('mother', {}).get('son', {})\n\n{}"
  },
  {
    "objectID": "posts/2022-02-18-python-tips-output-formatting.html#about",
    "href": "posts/2022-02-18-python-tips-output-formatting.html#about",
    "title": "Python - A collection of output formatting tips",
    "section": "About",
    "text": "About\nThis notebook is a collection of useful tips to format Python string literals and output."
  },
  {
    "objectID": "posts/2022-02-18-python-tips-output-formatting.html#environment",
    "href": "posts/2022-02-18-python-tips-output-formatting.html#environment",
    "title": "Python - A collection of output formatting tips",
    "section": "Environment",
    "text": "Environment\n\n\nCode\nfrom platform import python_version\n\nprint(\"python==\" + python_version())\n\n\npython==3.8.5"
  },
  {
    "objectID": "posts/2022-02-18-python-tips-output-formatting.html#f-string-expressions-inside-a-string",
    "href": "posts/2022-02-18-python-tips-output-formatting.html#f-string-expressions-inside-a-string",
    "title": "Python - A collection of output formatting tips",
    "section": "f-string: Expressions inside a string",
    "text": "f-string: Expressions inside a string\n\nr = 'red'\ng = 'green'\nb = 1001\n\n# f-string has a simple syntax. Put 'f' at the start of string, and put expressions in {}\nf\"Stop = {r}, Go = {g}\"\n\n'Stop = red, Go = green'\n\n\n\n##\n# 'F' can also be used to start an f-string\nF\"binary = {b}. If you need value in brackets {{{b}}}\"\n\n'binary = 1001. If you need value in brackets {1001}'\n\n\n\n##\n# f-string can also be started with \"\"\" quotes\nf\"\"\"{r} or {g}\"\"\"\n\n'red or green'\n\n\n\n##\n# f-string on multiple lines. \n# 1. Use \"\"\" with backslash \\\nf\"\"\"{r}\\\n or \\\n{g}\"\"\"\n\n'red or green'\n\n\n\n##\n# f-string on multiple lines. \n# 2. Use only backslash \\\nf\"{r}\" \\\n\" or \" \\\nf\"{g}\"\n\n'red or green'\n\n\n\n##\n# f-string on multiple lines. \n# 3. Use  brackets ()\n(f\"{r}\"\n\" or \"\nf\"{g}\")\n\n'red or green'\n\n\n\n##\n# you can also compute an expression in an f-string\nf\"{ 40 + 2}\"\n\n'42'\n\n\n\n##\n# functions can also be called from inside an f-string\nf\"This is in CAPS: { str.upper(r) }\"\n\n# same as above\nf\"This is in CAPS: { r.upper() }\"\n\n'This is in CAPS: RED'\n\n\n\nf-string: Padding the output\n\n##\n# Inside f-string, passing an integer after ':' will cause that field to be a minimum number of characters wide. \n# This is useful for making columns line up.\n\ngroups = {\n    'small': 100,\n    'medium': 100100,\n    'large': 100100100\n}\n\nfor group, value in groups.items():\n    print(f\"{value:10} ==&gt; {group:20}\")\n\nprint(f\"{'****'*10}\") # another nice trick\n\nfor group, value in groups.items():\n    print(f\"{group:10} ==&gt; {value:20}\")\n\n       100 ==&gt; small               \n    100100 ==&gt; medium              \n 100100100 ==&gt; large               \n****************************************\nsmall      ==&gt;                  100\nmedium     ==&gt;               100100\nlarge      ==&gt;            100100100\n\n\n\n\nf-string: Binary and hexadecimal format\n\n##\n# you can convert integers to binary and hexadecimal format\nprint( f\"5 in binary {5:b}\" )\n\nprint( f\"5 in hexadecimal {5:#b}\" )\n\n5 in binary 101\n5 in hexadecimal 0b101\n\n\n\n\nf-string: Controlling the decimal places\n\nimport math\nprint(f'The value of pi is approximately (no formatting) {math.pi}')\nprint(f'The value of pi is approximately {math.pi :.3f}')\n\nThe value of pi is approximately (no formatting) 3.141592653589793\nThe value of pi is approximately 3.142\n\n\n\n\nf-string: Putting commas in numerical output\n\nnum = 3214298342.234\nf\"{num:,}\"\n\n'3,214,298,342.234'"
  },
  {
    "objectID": "posts/2022-02-23-aws-policy-types.html#identity-based-policies",
    "href": "posts/2022-02-23-aws-policy-types.html#identity-based-policies",
    "title": "AWS IAM Policy Types",
    "section": "Identity-based policies",
    "text": "Identity-based policies\nIdentity-based policies – Attach managed and inline policies to IAM identities (users, groups to which users belong, or roles). Identity-based policies grant permissions to an identity."
  },
  {
    "objectID": "posts/2022-02-23-aws-policy-types.html#resource-based-policies",
    "href": "posts/2022-02-23-aws-policy-types.html#resource-based-policies",
    "title": "AWS IAM Policy Types",
    "section": "Resource-based policies",
    "text": "Resource-based policies\nResource-based policies – Attach inline policies to resources. The most common examples of resource-based policies are Amazon S3 bucket policies and IAM role trust policies. Resource-based policies grant permissions to the principal that is specified in the policy. Principals can be in the same account as the resource or in other accounts.\n\nIdentity vs Resource based policy\n  * Identity-based policies are applied to IAM identities, and grant them access to AWS resources. * Resource-based policies are applied to AWS resources, and they grant access to Principals (IAM identities, and applications)"
  },
  {
    "objectID": "posts/2022-02-23-aws-policy-types.html#permissions-boundaries",
    "href": "posts/2022-02-23-aws-policy-types.html#permissions-boundaries",
    "title": "AWS IAM Policy Types",
    "section": "Permissions boundaries",
    "text": "Permissions boundaries\nPermissions boundaries – Use a customer or AWS managed policy as the permissions boundary for an IAM entity (user or role). That policy defines the maximum permissions that the identity-based policies can grant to an entity, but does not grant permissions. Permissions boundaries do not define the maximum permissions that a resource-based policy can grant to an entity."
  },
  {
    "objectID": "posts/2022-02-23-aws-policy-types.html#organizations-scps",
    "href": "posts/2022-02-23-aws-policy-types.html#organizations-scps",
    "title": "AWS IAM Policy Types",
    "section": "Organizations SCPs",
    "text": "Organizations SCPs\nOrganizations SCPs – Use an AWS Organizations service control policy (SCP) to define the maximum permissions for account members of an organization or organizational unit (OU). SCPs limit permissions that identity-based policies or resource-based policies grant to entities (users or roles) within the account, but do not grant permissions.\n\nPermission boundries vs Organization SCP\n\nBoth permission boundries and SCP only limit permissions. They don’t give any permissions.\nPermission boundries limits permissions of identity-based policies only.\nSCP limits permissions on both identity and resource based policies."
  },
  {
    "objectID": "posts/2022-02-23-aws-policy-types.html#access-control-lists-acls",
    "href": "posts/2022-02-23-aws-policy-types.html#access-control-lists-acls",
    "title": "AWS IAM Policy Types",
    "section": "Access control lists (ACLs)",
    "text": "Access control lists (ACLs)\nAccess control lists (ACLs) – Use ACLs to control which principals in other accounts can access the resource to which the ACL is attached. ACLs are similar to resource-based policies, although they are the only policy type that does not use the JSON policy document structure. ACLs are cross-account permissions policies that grant permissions to the specified principal. ACLs cannot grant permissions to entities within the same account.\n\nACL vs Resource based policy\n\nResource based policies can grant permission to entities in same or different account\nACL can only grant permissions to entities in different account\nOnly a few resources support ACL including AWS Amazon S3, AWS WAF, and Amazon VPC. It is a legacy IAM policy type and AWS recommends not to use it."
  },
  {
    "objectID": "posts/2022-02-23-aws-policy-types.html#session-policies",
    "href": "posts/2022-02-23-aws-policy-types.html#session-policies",
    "title": "AWS IAM Policy Types",
    "section": "Session policies",
    "text": "Session policies\nSession policies – Pass advanced session policies when you use the AWS CLI or AWS API to assume a role or a federated user. Session policies limit the permissions that the role or user’s identity-based policies grant to the session. Session policies limit permissions for a created session, but do not grant permissions. For more information, see Session Policies."
  },
  {
    "objectID": "posts/2022-03-04-sklearn-pipeline-deep-dive.html#about",
    "href": "posts/2022-03-04-sklearn-pipeline-deep-dive.html#about",
    "title": "Sklearn Pipeline and Transformers Deep Dive",
    "section": "About",
    "text": "About\nThis notebook shows various ways to work with Skearn Pipelines. * We will start with some of the limitations of pipelines and how to overcome them * We will discuss getting a dataframe from a pipeline instead of a NumPy array, and the benefits of this approach * We will learn how to use CustomTransformer and a FunctionTransformer * We will also build a custom transformer to do some feature engineering * Along the way, we will also see how to avoid common mistakes while creating pipelines"
  },
  {
    "objectID": "posts/2022-03-04-sklearn-pipeline-deep-dive.html#setup",
    "href": "posts/2022-03-04-sklearn-pipeline-deep-dive.html#setup",
    "title": "Sklearn Pipeline and Transformers Deep Dive",
    "section": "Setup",
    "text": "Setup\n\nEnvironment Details\n\n\nCode\nfrom platform import python_version\nimport sklearn, numpy, matplotlib, pandas\n\nprint(\"python==\" + python_version())\nprint(\"sklearn==\" + sklearn.__version__)\nprint(\"numpy==\" + numpy.__version__)\nprint(\"pandas==\" + pandas.__version__)\nprint(\"matplotlib==\" + matplotlib.__version__)\n\n\npython==3.8.8\nsklearn==1.0.2\nnumpy==1.20.1\npandas==1.2.3\nmatplotlib==3.5.1\n\n\n\n\nLoading Data\nFor this example we will use the original Titanic dataset, describing the survival status of individual passengers on the Titanic ship.\nSome notes from original source: * The variables on our extracted dataset are ‘pclass’, ‘name’, ‘sex’, ‘age’, ‘sibsp’, ‘parch’, ‘ticket’, ‘fare’,‘cabin’, ‘embarked’, ‘boat’, ‘body’, and ‘home.dest’. * pclass refers to passenger class (1st, 2nd, 3rd), and is a proxy for socio-economic class. * Age is in years, and some infants had fractional values. * sibsp = Number of Siblings/Spouses aboard * parch = Number of Parents/Children aboard * The target is either a person survived or not (1 or 0)\nImportant note: The purpose of this notebook is not to train a best model on titanic data, but to understand the working of Sklearn pipeline and transformers. So please be mindful of that.\n\nfrom sklearn.datasets import fetch_openml\nfrom sklearn.model_selection import train_test_split\nimport numpy as np\n\nnp.random.seed(42) # for consistency\n\n# Load data from https://www.openml.org/d/40945\nX, y = fetch_openml(\"titanic\", version=1, as_frame=True, return_X_y=True)\n\nX.head()\n\n\n\n\n\n\n\n\npclass\nname\nsex\nage\nsibsp\nparch\nticket\nfare\ncabin\nembarked\nboat\nbody\nhome.dest\n\n\n\n\n0\n1.0\nAllen, Miss. Elisabeth Walton\nfemale\n29.0000\n0.0\n0.0\n24160\n211.3375\nB5\nS\n2\nNaN\nSt Louis, MO\n\n\n1\n1.0\nAllison, Master. Hudson Trevor\nmale\n0.9167\n1.0\n2.0\n113781\n151.5500\nC22 C26\nS\n11\nNaN\nMontreal, PQ / Chesterville, ON\n\n\n2\n1.0\nAllison, Miss. Helen Loraine\nfemale\n2.0000\n1.0\n2.0\n113781\n151.5500\nC22 C26\nS\nNone\nNaN\nMontreal, PQ / Chesterville, ON\n\n\n3\n1.0\nAllison, Mr. Hudson Joshua Creighton\nmale\n30.0000\n1.0\n2.0\n113781\n151.5500\nC22 C26\nS\nNone\n135.0\nMontreal, PQ / Chesterville, ON\n\n\n4\n1.0\nAllison, Mrs. Hudson J C (Bessie Waldo Daniels)\nfemale\n25.0000\n1.0\n2.0\n113781\n151.5500\nC22 C26\nS\nNone\nNaN\nMontreal, PQ / Chesterville, ON\n\n\n\n\n\n\n\n\n##\n# let's check the frequency of missing values in each feature\nX.isnull().sum().sort_values(ascending=False)\n\nbody         1188\ncabin        1014\nboat          823\nhome.dest     564\nage           263\nembarked        2\nfare            1\npclass          0\nname            0\nsex             0\nsibsp           0\nparch           0\nticket          0\ndtype: int64\n\n\n\n##\n# let's drop top 4 features with highest percentage of missing data\n# This step is done to make our working with pipeline simpler and easier to understand\n\nX.drop(['body', 'cabin', 'boat', 'home.dest'], axis=1, inplace=True)\nX_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2)\nX_train.head()\n\n\n\n\n\n\n\n\npclass\nname\nsex\nage\nsibsp\nparch\nticket\nfare\nembarked\n\n\n\n\n999\n3.0\nMcCarthy, Miss. Catherine 'Katie'\nfemale\nNaN\n0.0\n0.0\n383123\n7.7500\nQ\n\n\n392\n2.0\ndel Carlo, Mrs. Sebastiano (Argenia Genovesi)\nfemale\n24.0\n1.0\n0.0\nSC/PARIS 2167\n27.7208\nC\n\n\n628\n3.0\nAndersson, Miss. Sigrid Elisabeth\nfemale\n11.0\n4.0\n2.0\n347082\n31.2750\nS\n\n\n1165\n3.0\nSaad, Mr. Khalil\nmale\n25.0\n0.0\n0.0\n2672\n7.2250\nC\n\n\n604\n3.0\nAbelseth, Miss. Karen Marie\nfemale\n16.0\n0.0\n0.0\n348125\n7.6500\nS"
  },
  {
    "objectID": "posts/2022-03-04-sklearn-pipeline-deep-dive.html#some-terminology-first",
    "href": "posts/2022-03-04-sklearn-pipeline-deep-dive.html#some-terminology-first",
    "title": "Sklearn Pipeline and Transformers Deep Dive",
    "section": "Some Terminology First",
    "text": "Some Terminology First\n\nDatasets\nScikit-learn deals with learning information from one or more datasets that are represented as 2D arrays. They can be understood as a list of multi-dimensional observations. We say that the first axis of these arrays is the samples axis, while the second is the features axis. &gt; (n_samples, n_features)\n\n##\n# for our titanic dataset:\n# n_samples = 1309\n# n_features = 9\nX.shape\n\n(1309, 9)\n\n\n\n\nEstimator\nAn estimator is any object that learns from data; it may be a classification, regression or clustering algorithm or a transformer that extracts/filters useful features from raw data.\nAll estimator objects expose a fit method that takes a dataset (usually a 2-d array) &gt; estimator.fit(data)\n\n\nTransformer\nAn estimator supporting transform and/or fit_transform methods.\nA transformer, transforms the input, usually only X, into some transformed space. Output is an array or sparse matrix of length n_samples and with the number of columns fixed after fitting.\n\n\nFit\nThe fit method is provided on every estimator. It usually takes some samples X, targets y if the model is supervised, and potentially other sample properties such as sample_weight.\nIt should: * clear any prior attributes stored on the estimator, unless warm_start is used * validate and interpret any parameters, ideally raising an error if invalid * validate the input data * estimate and store model attributes from the estimated parameters and provided data; and * return the now fitted estimator to facilitate method chaining\nNote: * Fitting = Calling fit (or fit_transform, fit_predict) method on an estimator. * Fitted = The state of an estimator after fitting."
  },
  {
    "objectID": "posts/2022-03-04-sklearn-pipeline-deep-dive.html#sklearn-pipeline",
    "href": "posts/2022-03-04-sklearn-pipeline-deep-dive.html#sklearn-pipeline",
    "title": "Sklearn Pipeline and Transformers Deep Dive",
    "section": "Sklearn Pipeline",
    "text": "Sklearn Pipeline\n\nclass sklearn.pipeline.Pipeline(steps, *, memory=None, verbose=False)\n\nIt is a pipeline of transformers with a final estimator.\nIt sequentially applies a list of transforms and a final estimator. Intermediate steps of the pipeline must be transforms, that is, they must implement fit and transform methods. The final estimator only needs to implement fit.\nLets create a simple pipeline to better understand its componets. Steps in our pipeline will be * replace missing values using the mean along each numerical feature column; and * then scale them\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler\n\npipe = Pipeline(steps=[\n    ('imputer', SimpleImputer()),\n    ('scaler', StandardScaler())\n])\n\n# our first pipeline has been initialized\npipe\n\nPipeline(steps=[('imputer', SimpleImputer()), ('scaler', StandardScaler())])\n\n\nWe can also visualize the pipeline as a diagram. It has two steps: imputer and scaler in sequence.\n\nfrom sklearn import set_config\nset_config(display=\"diagram\")\n\npipe\n\nPipeline(steps=[('imputer', SimpleImputer()), ('scaler', StandardScaler())])Please rerun this cell to show the HTML repr or trust the notebook.PipelinePipeline(steps=[('imputer', SimpleImputer()), ('scaler', StandardScaler())])SimpleImputerSimpleImputer()StandardScalerStandardScaler()\n\n\nnow lets call fit_transform method to run this pipeline, and preprocess our loaded data\n\n#collapse-output\npipe.fit_transform(X_train, y_train)\n\n\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\nInput In [8], in &lt;module&gt;\n      1 #collapse-output\n----&gt; 2 pipe.fit_transform(X_train, y_train)\n\nFile ~\\anaconda3\\envs\\sc_mlflow\\lib\\site-packages\\sklearn\\pipeline.py:426, in Pipeline.fit_transform(self, X, y, **fit_params)\n    399 \"\"\"Fit the model and transform with the final estimator.\n    400 \n    401 Fits all the transformers one after the other and transform the\n   (...)\n    423     Transformed samples.\n    424 \"\"\"\n    425 fit_params_steps = self._check_fit_params(**fit_params)\n--&gt; 426 Xt = self._fit(X, y, **fit_params_steps)\n    428 last_step = self._final_estimator\n    429 with _print_elapsed_time(\"Pipeline\", self._log_message(len(self.steps) - 1)):\n\nFile ~\\anaconda3\\envs\\sc_mlflow\\lib\\site-packages\\sklearn\\pipeline.py:348, in Pipeline._fit(self, X, y, **fit_params_steps)\n    346     cloned_transformer = clone(transformer)\n    347 # Fit or load from cache the current transformer\n--&gt; 348 X, fitted_transformer = fit_transform_one_cached(\n    349     cloned_transformer,\n    350     X,\n    351     y,\n    352     None,\n    353     message_clsname=\"Pipeline\",\n    354     message=self._log_message(step_idx),\n    355     **fit_params_steps[name],\n    356 )\n    357 # Replace the transformer of the step with the fitted\n    358 # transformer. This is necessary when loading the transformer\n    359 # from the cache.\n    360 self.steps[step_idx] = (name, fitted_transformer)\n\nFile ~\\anaconda3\\envs\\sc_mlflow\\lib\\site-packages\\joblib\\memory.py:352, in NotMemorizedFunc.__call__(self, *args, **kwargs)\n    351 def __call__(self, *args, **kwargs):\n--&gt; 352     return self.func(*args, **kwargs)\n\nFile ~\\anaconda3\\envs\\sc_mlflow\\lib\\site-packages\\sklearn\\pipeline.py:893, in _fit_transform_one(transformer, X, y, weight, message_clsname, message, **fit_params)\n    891 with _print_elapsed_time(message_clsname, message):\n    892     if hasattr(transformer, \"fit_transform\"):\n--&gt; 893         res = transformer.fit_transform(X, y, **fit_params)\n    894     else:\n    895         res = transformer.fit(X, y, **fit_params).transform(X)\n\nFile ~\\anaconda3\\envs\\sc_mlflow\\lib\\site-packages\\sklearn\\base.py:855, in TransformerMixin.fit_transform(self, X, y, **fit_params)\n    852     return self.fit(X, **fit_params).transform(X)\n    853 else:\n    854     # fit method of arity 2 (supervised transformation)\n--&gt; 855     return self.fit(X, y, **fit_params).transform(X)\n\nFile ~\\anaconda3\\envs\\sc_mlflow\\lib\\site-packages\\sklearn\\impute\\_base.py:319, in SimpleImputer.fit(self, X, y)\n    302 def fit(self, X, y=None):\n    303     \"\"\"Fit the imputer on `X`.\n    304 \n    305     Parameters\n   (...)\n    317         Fitted estimator.\n    318     \"\"\"\n--&gt; 319     X = self._validate_input(X, in_fit=True)\n    321     # default fill_value is 0 for numerical input and \"missing_value\"\n    322     # otherwise\n    323     if self.fill_value is None:\n\nFile ~\\anaconda3\\envs\\sc_mlflow\\lib\\site-packages\\sklearn\\impute\\_base.py:285, in SimpleImputer._validate_input(self, X, in_fit)\n    279 if \"could not convert\" in str(ve):\n    280     new_ve = ValueError(\n    281         \"Cannot use {} strategy with non-numeric data:\\n{}\".format(\n    282             self.strategy, ve\n    283         )\n    284     )\n--&gt; 285     raise new_ve from None\n    286 else:\n    287     raise ve\n\nValueError: Cannot use mean strategy with non-numeric data:\ncould not convert string to float: \"McCarthy, Miss. Catherine 'Katie'\"\n\n\n\nAaargh! this is not what we intended. Let us try to understand why our pipeline did not work and then fix it. The exception message says:\nValueError: Cannot use mean strategy with non-numeric data: could not convert string to float: \"McCarthy, Miss. Catherine 'Katie'\nFrom the error message we can deduce that Pipeline is trying to apply its transformers on all columns in the dataset. This was not our intention, as we wanted to apply the transformers to numeric data only. Let’s limit our simple pipeline to numerical columns and run again.\n\nnum_cols = ['age', 'fare']\npipe.fit_transform(X_train[num_cols], y_train)\n\narray([[ 0.        , -0.49963779],\n       [-0.43641134, -0.09097855],\n       [-1.44872891, -0.01824953],\n       ...,\n       [-0.98150542, -0.49349894],\n       [-0.82576425, -0.44336498],\n       [-0.59215251, -0.49349894]])\n\n\nAlright, our pipeline has run now and we can also observe a few outcomes. * When we apply a pipeline to a dataset it will run transformers to all features in the dataset. * Output from one transformer will be passed on to the next one until we reach the end of the pipeline * If we want to apply different transformers for numerical and categorical features (heterogeneous data) then the pipeline will not work for us. We would have to create separate pipelines for the different feature sets and then join the output.\nTo overcome the limitation of a pipeline for heterogeneous data, Sklearn recommends using ColumnTransformer. With ColumnTransformer we can provide column names against the transformers on which we want to apply them."
  },
  {
    "objectID": "posts/2022-03-04-sklearn-pipeline-deep-dive.html#columntransformer",
    "href": "posts/2022-03-04-sklearn-pipeline-deep-dive.html#columntransformer",
    "title": "Sklearn Pipeline and Transformers Deep Dive",
    "section": "ColumnTransformer",
    "text": "ColumnTransformer\nLet’s see our first ColumnTransformer in action.\n\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder\n\n# Note the sequence when creating a ColumnTransformer\n# 1. a name for the transformer\n# 2. the transformer\n# 3. the column names\n\npipe = ColumnTransformer([\n    ('standardscaler', StandardScaler(), ['age', 'fare'] ),\n    ('onehotencoder', OneHotEncoder(), ['sex'])\n    ])\n\npipe\n\nColumnTransformer(transformers=[('standardscaler', StandardScaler(),\n                                 ['age', 'fare']),\n                                ('onehotencoder', OneHotEncoder(), ['sex'])])Please rerun this cell to show the HTML repr or trust the notebook.ColumnTransformerColumnTransformer(transformers=[('standardscaler', StandardScaler(),\n                                 ['age', 'fare']),\n                                ('onehotencoder', OneHotEncoder(), ['sex'])])standardscaler['age', 'fare']StandardScalerStandardScaler()onehotencoder['sex']OneHotEncoderOneHotEncoder()\n\n\n\n##\n# Let's execute our pipeline to see the output.\npipe.fit_transform(X_train, y_train)\n\narray([[        nan, -0.49939913,  1.        ,  0.        ],\n       [-0.39043136, -0.09093509,  1.        ,  0.        ],\n       [-1.2960919 , -0.01824081,  1.        ,  0.        ],\n       ...,\n       [-0.87809473, -0.49326321,  0.        ,  1.        ],\n       [-0.73876234, -0.4431532 ,  0.        ,  1.        ],\n       [-0.52976375, -0.49326321,  0.        ,  1.        ]])\n\n\n\nget_feature_names_out(input_features=None)\nAt this point I will also introduce a very useful function get_feature_names_out(input_features=None). Using this method we can get output feature names as well.\n\npipe.get_feature_names_out()\n\narray(['standardscaler__age', 'standardscaler__fare',\n       'onehotencoder__sex_female', 'onehotencoder__sex_male'],\n      dtype=object)\n\n\nNotice the output * Output feature names appear as &lt;transformer_name&gt;__&lt;feature_name&gt; * For OneHotEncoded feature “sex”, output feature names have the label attached to them\n\n\nmake_column_transformer\nSklean also provides a wrapper function for ColumnTransformer where we don’t have to provide names for the transformers.\n\nfrom sklearn.compose import make_column_transformer\n\n# Note the sequence when using make_column_transformer\n# 1. the transformer\n# 2. the column names\n\npipe = make_column_transformer(\n    (StandardScaler(), ['age', 'fare']),\n    (OneHotEncoder(), ['sex'] ),\n    verbose_feature_names_out=False # to keep output feature names simple\n)\n\npipe\n\nColumnTransformer(transformers=[('standardscaler', StandardScaler(),\n                                 ['age', 'fare']),\n                                ('onehotencoder', OneHotEncoder(), ['sex'])],\n                  verbose_feature_names_out=False)Please rerun this cell to show the HTML repr or trust the notebook.ColumnTransformerColumnTransformer(transformers=[('standardscaler', StandardScaler(),\n                                 ['age', 'fare']),\n                                ('onehotencoder', OneHotEncoder(), ['sex'])],\n                  verbose_feature_names_out=False)standardscaler['age', 'fare']StandardScalerStandardScaler()onehotencoder['sex']OneHotEncoderOneHotEncoder()\n\n\n\n##\n# Let's execute our pipeline.\npipe.fit_transform(X_train, y_train)\n\narray([[        nan, -0.49939913,  1.        ,  0.        ],\n       [-0.39043136, -0.09093509,  1.        ,  0.        ],\n       [-1.2960919 , -0.01824081,  1.        ,  0.        ],\n       ...,\n       [-0.87809473, -0.49326321,  0.        ,  1.        ],\n       [-0.73876234, -0.4431532 ,  0.        ,  1.        ],\n       [-0.52976375, -0.49326321,  0.        ,  1.        ]])\n\n\n\n##\n# notice the feature names this time. they are shorter.\n# we have used attribute \"verbose_feature_names_out=False\" in our pipeline above.\npipe.get_feature_names_out()\n\narray(['age', 'fare', 'sex_female', 'sex_male'], dtype=object)\n\n\n\n\nImportant difference between Pipeline and ColumnTransformer\n\nPipeline applies transformer in sequence on all columns\nColumnTranformer applies transformers in parallel to specified columns and then concats the output\n\n\n\nOpen questions?\nSo our ColumnTransformer is working. But we have a few more questions to address. * Why is the output from our pipeline or ColumnTransformer not shown as a dataframe with output features nicely separated in different columns? * Our input dataset had more features besides age, fare, and sex. Why are they not present in the output? * What happens if I change the sequence of transformers, and feature names in my ColumnTransformer?\nIn the coming sections, we will try to address these questions.\n\nWhy is the output not a dataframe?\nThe output from a pipeline or a ColumnTransformer is an nd-array where the first index is the number of samples, and second index are the output features (n_samples, n_output_features). Since we are only getting numpy array as an output, we are losing information about the column names.\n\ntemp = pipe.fit_transform(X_train, y_train)\n\nprint(type(temp))\nprint(temp.shape)\n\n&lt;class 'numpy.ndarray'&gt;\n(1047, 4)\n\n\n\n\nCan we get the feature names back?\nWe have already seen that we can get the output feature names using method get_feature_names_out. But this time let’s try to analyze our ColumnsTransformer more closely. The transformer attributes discussed here also applies to Pipeline object.\n\n##\n# print the internals of ColumnTransformer\n\nset_config(display='text')\npipe\n\nColumnTransformer(transformers=[('standardscaler', StandardScaler(),\n                                 ['age', 'fare']),\n                                ('onehotencoder', OneHotEncoder(), ['sex'])],\n                  verbose_feature_names_out=False)\n\n\nColumnTransformer has an attribute ‘transformers’ that is keeping a list of all the provided transformers. Let’s print it.\n\npipe.transformers\n\n[('standardscaler', StandardScaler(), ['age', 'fare']),\n ('onehotencoder', OneHotEncoder(), ['sex'])]\n\n\nThese are the transformers list at the initialization time. If we want to check the transformers after fit function has been called, then we need to print a different attribute transformers_.\n\npipe.transformers_\n\n[('standardscaler', StandardScaler(), ['age', 'fare']),\n ('onehotencoder', OneHotEncoder(), ['sex']),\n ('remainder', 'drop', [0, 1, 4, 5, 6, 8])]\n\n\nYou can see the difference. There is an extra transformer with the name remainder at the end. It was not present at the initialization time. What it does is that it drops all remaining columns from the dataset that have not been explicitly used in the ColumnTransformer. Since, at the initialization time, ColumnTransformer does not know about the other columns that it needs to drop this transformer is missing. During fit it sees the dataset and knows about the other columns, it then keeps a list of them to drop (0, 1, 4, 5, 6, 8).\nWe can also index through the transformers as well to fetch anyone from the list.\n\n##\n# second transformer from the list\npipe.transformers_[1]\n\n('onehotencoder', OneHotEncoder(), ['sex'])\n\n\nNotice the tuple sequence. * First is the name * Second is the transformer * Third are the column names\nWe can also call get_feature_names_out method on a separate transformer from the list.\n\n##\n# output features from second tranformer\npipe.transformers_[1][1].get_feature_names_out()\n\narray(['sex_female', 'sex_male'], dtype=object)\n\n\n\n##\n# output features from last tranformer\npipe.transformers_[-1][1].get_feature_names_out()\n\n# No. We cannot do this on last transformer (remainder).\n\n\n---------------------------------------------------------------------------\nAttributeError                            Traceback (most recent call last)\nInput In [22], in &lt;module&gt;\n      1 #collapse-output\n      2 # output features from last tranformer\n----&gt; 3 pipe.transformers_[-1][1].get_feature_names_out()\n\nAttributeError: 'str' object has no attribute 'get_feature_names_out'\n\n\n\nWe now have output feature names, and the output (nd-array). Can we convert them to a DataFrame?\n\nimport pandas as pd\n\ntemp = pipe.fit_transform(X_train, y_train)\ncol_names = pipe.get_feature_names_out()\n\noutput = pd.DataFrame(temp.T, col_names).T\noutput.head()\n\n\n\n\n\n\n\n\nage\nfare\nsex_female\nsex_male\n\n\n\n\n0\nNaN\n-0.499399\n1.0\n0.0\n\n\n1\n-0.390431\n-0.090935\n1.0\n0.0\n\n\n2\n-1.296092\n-0.018241\n1.0\n0.0\n\n\n3\n-0.320765\n-0.510137\n0.0\n1.0\n\n\n4\n-0.947761\n-0.501444\n1.0\n0.0"
  },
  {
    "objectID": "posts/2022-03-04-sklearn-pipeline-deep-dive.html#functiontransformer",
    "href": "posts/2022-03-04-sklearn-pipeline-deep-dive.html#functiontransformer",
    "title": "Sklearn Pipeline and Transformers Deep Dive",
    "section": "FunctionTransformer",
    "text": "FunctionTransformer\nWe know how to convert the transformer output to a DataFrame. It would be much simpler if we don’t have to do an extra step, and can directly get a Dataframe from our fitted ColumnTransformer.\nFor this we can take the help of FunctionTransformer &gt; A FunctionTransformer forwards its X (and optionally y) arguments to a user-defined function or function object and returns the result of this function.\nLet’s see a FunctionTransformer in action.\n\nfrom sklearn.preprocessing import FunctionTransformer\n\npreprocessor = make_column_transformer(\n    (StandardScaler(), ['age', 'fare']),\n    (OneHotEncoder(), ['sex'] ),\n    verbose_feature_names_out=False\n)\n\npipe = Pipeline([\n                    (\"preprocess\", preprocessor),\n                    (\"dataframer\",FunctionTransformer(lambda x: pd.DataFrame(x, columns = preprocessor.get_feature_names_out())))\n                    ])\n\nset_config(display=\"diagram\")\npipe\n\nPipeline(steps=[('preprocess',\n                 ColumnTransformer(transformers=[('standardscaler',\n                                                  StandardScaler(),\n                                                  ['age', 'fare']),\n                                                 ('onehotencoder',\n                                                  OneHotEncoder(), ['sex'])],\n                                   verbose_feature_names_out=False)),\n                ('dataframer',\n                 FunctionTransformer(func=&lt;function &lt;lambda&gt; at 0x0000016196A1B040&gt;))])Please rerun this cell to show the HTML repr or trust the notebook.PipelinePipeline(steps=[('preprocess',\n                 ColumnTransformer(transformers=[('standardscaler',\n                                                  StandardScaler(),\n                                                  ['age', 'fare']),\n                                                 ('onehotencoder',\n                                                  OneHotEncoder(), ['sex'])],\n                                   verbose_feature_names_out=False)),\n                ('dataframer',\n                 FunctionTransformer(func=&lt;function &lt;lambda&gt; at 0x0000016196A1B040&gt;))])preprocess: ColumnTransformerColumnTransformer(transformers=[('standardscaler', StandardScaler(),\n                                 ['age', 'fare']),\n                                ('onehotencoder', OneHotEncoder(), ['sex'])],\n                  verbose_feature_names_out=False)standardscaler['age', 'fare']StandardScalerStandardScaler()onehotencoder['sex']OneHotEncoderOneHotEncoder()FunctionTransformerFunctionTransformer(func=&lt;function &lt;lambda&gt; at 0x0000016196A1B040&gt;)\n\n\nNotice that we have applied FunctionTransformer after ColumnTransformer in a Pipeline. When we fit our pipeline on the dataset, ColumnTransformer will be fitted first and then the FunctionTransformer. Since the ColumnTransformer has been fitted first, we will be able to call get_feature_names_out on it while passing data to FunctionTransformer.\n\n##\n# let's run our pipeline again\ntemp = pipe.fit_transform(X_train, y_train)\ntemp.head()\n\n\n\n\n\n\n\n\nage\nfare\nsex_female\nsex_male\n\n\n\n\n0\nNaN\n-0.499399\n1.0\n0.0\n\n\n1\n-0.390431\n-0.090935\n1.0\n0.0\n\n\n2\n-1.296092\n-0.018241\n1.0\n0.0\n\n\n3\n-0.320765\n-0.510137\n0.0\n1.0\n\n\n4\n-0.947761\n-0.501444\n1.0\n0.0\n\n\n\n\n\n\n\nThis is looking good. We are now getting back a dataframe directly from the pipeline. With a dataframe it is a lot easier to view and verify the output from the preprocessor.\nBut we have to be very careful with FunctionTransformer. In Sklearn docs, it says\nNote: If a lambda is used as the function, then the resulting transformer will not be pickleable.\nHuh! that is a very concerning point. We have also used a lambda function, and we will not be able to pickle it. Let’s check it first.\n\nimport pickle\n\n# save our pipeline\ns1 = pickle.dumps(pipe)\n\n# reload it\ns2 = pickle.loads(s1)\ns2\n\n\n---------------------------------------------------------------------------\nPicklingError                             Traceback (most recent call last)\nInput In [26], in &lt;module&gt;\n      1 import pickle\n      3 # save our pipeline\n----&gt; 4 s1 = pickle.dumps(pipe)\n      6 # reload it\n      7 s2 = pickle.loads(s1)\n\nPicklingError: Can't pickle &lt;function &lt;lambda&gt; at 0x0000016196A1B040&gt;: attribute lookup &lt;lambda&gt; on __main__ failed\n\n\n\nThe documentation was right about it. We have used a Lambda function in our FunctionTranformer and we got a pickle error. Since, the limitation is said for Lambda function, changing it with a normal function should work. Let’s do that.\n\ndef get_dataframe(X, transformer):\n    \"\"\" \n    x: an nd-array\n    transformer: fitted transformer\n    \"\"\"\n    col_names = transformer.get_feature_names_out()\n    output = pd.DataFrame(X.T, col_names).T\n    return output\n\npreprocessor = make_column_transformer(\n    (StandardScaler(), ['age', 'fare']),\n    (OneHotEncoder(), ['sex'] ),\n    verbose_feature_names_out=False\n)\n\ndataframer = FunctionTransformer(func=get_dataframe, kw_args={\"transformer\": preprocessor})\n\npipe = Pipeline([\n                    (\"preprocess\", preprocessor),\n                    (\"dataframer\", dataframer)\n                ])\n\nset_config(display=\"diagram\")\npipe\n\nPipeline(steps=[('preprocess',\n                 ColumnTransformer(transformers=[('standardscaler',\n                                                  StandardScaler(),\n                                                  ['age', 'fare']),\n                                                 ('onehotencoder',\n                                                  OneHotEncoder(), ['sex'])],\n                                   verbose_feature_names_out=False)),\n                ('dataframer',\n                 FunctionTransformer(func=&lt;function get_dataframe at 0x00000161953C3670&gt;,\n                                     kw_args={'transformer': ColumnTransformer(transformers=[('standardscaler',\n                                                                                              StandardScaler(),\n                                                                                              ['age',\n                                                                                               'fare']),\n                                                                                             ('onehotencoder',\n                                                                                              OneHotEncoder(),\n                                                                                              ['sex'])],\n                                                                               verbose_feature_names_out=False)}))])Please rerun this cell to show the HTML repr or trust the notebook.PipelinePipeline(steps=[('preprocess',\n                 ColumnTransformer(transformers=[('standardscaler',\n                                                  StandardScaler(),\n                                                  ['age', 'fare']),\n                                                 ('onehotencoder',\n                                                  OneHotEncoder(), ['sex'])],\n                                   verbose_feature_names_out=False)),\n                ('dataframer',\n                 FunctionTransformer(func=&lt;function get_dataframe at 0x00000161953C3670&gt;,\n                                     kw_args={'transformer': ColumnTransformer(transformers=[('standardscaler',\n                                                                                              StandardScaler(),\n                                                                                              ['age',\n                                                                                               'fare']),\n                                                                                             ('onehotencoder',\n                                                                                              OneHotEncoder(),\n                                                                                              ['sex'])],\n                                                                               verbose_feature_names_out=False)}))])preprocess: ColumnTransformerColumnTransformer(transformers=[('standardscaler', StandardScaler(),\n                                 ['age', 'fare']),\n                                ('onehotencoder', OneHotEncoder(), ['sex'])],\n                  verbose_feature_names_out=False)standardscaler['age', 'fare']StandardScalerStandardScaler()onehotencoder['sex']OneHotEncoderOneHotEncoder()FunctionTransformerFunctionTransformer(func=&lt;function get_dataframe at 0x00000161953C3670&gt;,\n                    kw_args={'transformer': ColumnTransformer(transformers=[('standardscaler',\n                                                                             StandardScaler(),\n                                                                             ['age',\n                                                                              'fare']),\n                                                                            ('onehotencoder',\n                                                                             OneHotEncoder(),\n                                                                             ['sex'])],\n                                                              verbose_feature_names_out=False)})\n\n\nNotice the arguments for FunctionTransformer in the above code. * first argument is the function to be called * second argument are the parameters to be passed to our function\nThe sequence of arguments for the callable function will be * first argument will be the output from any previous step in the pipeline (if there is any). In our case, it is nd-array coming from ColumnTransformer. It will be mapped to X. We don’t have to do anything about it. * second argument (if any) we want to pass to function. In our case we need it to be the fitted transformer from the previous step so we have explicitly passed it using kw_args as key-value pair. Where key name is the same as callable method argument name (‘transformer’ in our case).\nNow let’s do our pickle test one more time.\n\n##\n# save our pipeline\ns1 = pickle.dumps(pipe)\n\n# reload it\ns2 = pickle.loads(s1)\ns2\n\nPipeline(steps=[('preprocess',\n                 ColumnTransformer(transformers=[('standardscaler',\n                                                  StandardScaler(),\n                                                  ['age', 'fare']),\n                                                 ('onehotencoder',\n                                                  OneHotEncoder(), ['sex'])],\n                                   verbose_feature_names_out=False)),\n                ('dataframer',\n                 FunctionTransformer(func=&lt;function get_dataframe at 0x00000161953C3670&gt;,\n                                     kw_args={'transformer': ColumnTransformer(transformers=[('standardscaler',\n                                                                                              StandardScaler(),\n                                                                                              ['age',\n                                                                                               'fare']),\n                                                                                             ('onehotencoder',\n                                                                                              OneHotEncoder(),\n                                                                                              ['sex'])],\n                                                                               verbose_feature_names_out=False)}))])Please rerun this cell to show the HTML repr or trust the notebook.PipelinePipeline(steps=[('preprocess',\n                 ColumnTransformer(transformers=[('standardscaler',\n                                                  StandardScaler(),\n                                                  ['age', 'fare']),\n                                                 ('onehotencoder',\n                                                  OneHotEncoder(), ['sex'])],\n                                   verbose_feature_names_out=False)),\n                ('dataframer',\n                 FunctionTransformer(func=&lt;function get_dataframe at 0x00000161953C3670&gt;,\n                                     kw_args={'transformer': ColumnTransformer(transformers=[('standardscaler',\n                                                                                              StandardScaler(),\n                                                                                              ['age',\n                                                                                               'fare']),\n                                                                                             ('onehotencoder',\n                                                                                              OneHotEncoder(),\n                                                                                              ['sex'])],\n                                                                               verbose_feature_names_out=False)}))])preprocess: ColumnTransformerColumnTransformer(transformers=[('standardscaler', StandardScaler(),\n                                 ['age', 'fare']),\n                                ('onehotencoder', OneHotEncoder(), ['sex'])],\n                  verbose_feature_names_out=False)standardscaler['age', 'fare']StandardScalerStandardScaler()onehotencoder['sex']OneHotEncoderOneHotEncoder()FunctionTransformerFunctionTransformer(func=&lt;function get_dataframe at 0x00000161953C3670&gt;,\n                    kw_args={'transformer': ColumnTransformer(transformers=[('standardscaler',\n                                                                             StandardScaler(),\n                                                                             ['age',\n                                                                              'fare']),\n                                                                            ('onehotencoder',\n                                                                             OneHotEncoder(),\n                                                                             ['sex'])],\n                                                              verbose_feature_names_out=False)})\n\n\nAlright, no more issues so let’s proceed to our next question.\n\nWhere are the rest of the columns?\nBy default, only the specified columns in transformers are transformed and combined in the output, and the non-specified columns are dropped. (default of remainder='drop'). By specifying remainder='passthrough', all remaining columns that were not specified in transformers will be automatically passed through. This subset of columns is concatenated with the output of the transformers.\nLet’s see it in action.\n\npreprocessor = make_column_transformer(\n    (StandardScaler(), ['age', 'fare']),\n    (OneHotEncoder(), ['sex'] ),\n    verbose_feature_names_out=False,\n    remainder='passthrough'\n)\n\n# get_dataframe is already defined in last section. Intentionally omitted here. \ndataframer = FunctionTransformer(func=get_dataframe, kw_args={\"transformer\": preprocessor})\n\npipe = Pipeline([\n                    (\"preprocess\", preprocessor),\n                    (\"dataframer\", dataframer)\n                ])\n\ntemp = pipe.fit_transform(X_train, y_train)\ntemp.head()\n\n\n\n\n\n\n\n\nage\nfare\nsex_female\nsex_male\npclass\nname\nsibsp\nparch\nticket\nembarked\n\n\n\n\n0\nNaN\n-0.499399\n1.0\n0.0\n3.0\nMcCarthy, Miss. Catherine 'Katie'\n0.0\n0.0\n383123\nQ\n\n\n1\n-0.390431\n-0.090935\n1.0\n0.0\n2.0\ndel Carlo, Mrs. Sebastiano (Argenia Genovesi)\n1.0\n0.0\nSC/PARIS 2167\nC\n\n\n2\n-1.296092\n-0.018241\n1.0\n0.0\n3.0\nAndersson, Miss. Sigrid Elisabeth\n4.0\n2.0\n347082\nS\n\n\n3\n-0.320765\n-0.510137\n0.0\n1.0\n3.0\nSaad, Mr. Khalil\n0.0\n0.0\n2672\nC\n\n\n4\n-0.947761\n-0.501444\n1.0\n0.0\n3.0\nAbelseth, Miss. Karen Marie\n0.0\n0.0\n348125\nS\n\n\n\n\n\n\n\nWe have our remaining features back now, so let’s proceed to our next question.\n\n\nWhat happens if I change the sequence in ColumnTranformer?\nIt is better to make some changes and then see the results. I am making two changes in ColumnTransformer 1. Changed the order of transformers (OHE before scaling) 2. Changed the order of features inside the transformer (‘fare’ before ‘age’)\n\npreprocessor = make_column_transformer(\n    (OneHotEncoder(), ['sex'] ),\n    (StandardScaler(), ['fare', 'age']),\n    verbose_feature_names_out=False,\n    remainder='passthrough'\n)\n\n# get_dataframe is already defined in last section. Intentionally omitted here. \ndataframer = FunctionTransformer(func=get_dataframe, kw_args={\"transformer\": preprocessor})\n\npipe = Pipeline([\n                    (\"preprocess\", preprocessor),\n                    (\"dataframer\", dataframer)\n                ])\n\ntemp = pipe.fit_transform(X_train, y_train)\ntemp.head()\n\n\n\n\n\n\n\n\nsex_female\nsex_male\nfare\nage\npclass\nname\nsibsp\nparch\nticket\nembarked\n\n\n\n\n0\n1.0\n0.0\n-0.499399\nNaN\n3.0\nMcCarthy, Miss. Catherine 'Katie'\n0.0\n0.0\n383123\nQ\n\n\n1\n1.0\n0.0\n-0.090935\n-0.390431\n2.0\ndel Carlo, Mrs. Sebastiano (Argenia Genovesi)\n1.0\n0.0\nSC/PARIS 2167\nC\n\n\n2\n1.0\n0.0\n-0.018241\n-1.296092\n3.0\nAndersson, Miss. Sigrid Elisabeth\n4.0\n2.0\n347082\nS\n\n\n3\n0.0\n1.0\n-0.510137\n-0.320765\n3.0\nSaad, Mr. Khalil\n0.0\n0.0\n2672\nC\n\n\n4\n1.0\n0.0\n-0.501444\n-0.947761\n3.0\nAbelseth, Miss. Karen Marie\n0.0\n0.0\n348125\nS\n\n\n\n\n\n\n\nWe can see that changing the sequence in ColumnTransformer does change the output. Also note * Specified columns in transformers are transformed and combined in the output * Transformers sequence in ColumnTransformer also represents the columns sequence in the output * When remainder=passthrough is used then remaining columns will be appended at the end. Remainder columns sequence will be same as in the input."
  },
  {
    "objectID": "posts/2022-03-04-sklearn-pipeline-deep-dive.html#pipeline-inside-columntransformer",
    "href": "posts/2022-03-04-sklearn-pipeline-deep-dive.html#pipeline-inside-columntransformer",
    "title": "Sklearn Pipeline and Transformers Deep Dive",
    "section": "Pipeline inside ColumnTransformer",
    "text": "Pipeline inside ColumnTransformer\nLet’s assume we have more requirements this time. I want * for numerical features (age, fare): impute the missing values first, and then scale them * for categorical features (sex): one hot encode them\nOur pipeline will look like this.\n\n#collapse-output\nnumeric_features = [\"age\", \"fare\"]\nnumeric_transformer = Pipeline(\n    steps=[\n        (\"imputer\", SimpleImputer(strategy=\"median\")),\n        (\"scaler\", StandardScaler())\n        ])\n\ncategorical_features = [\"sex\"]\ncategorical_transformer = OneHotEncoder(handle_unknown=\"ignore\")\n\npreprocessor = make_column_transformer(\n        (numeric_transformer, numeric_features), # note \"numeric_transformer\" is a pipeline this time\n        (categorical_transformer, categorical_features),\n        remainder='passthrough',\n        verbose_feature_names_out=False\n)\n\ndataframer = FunctionTransformer(func=get_dataframe, kw_args={\"transformer\": preprocessor})\npipe = Pipeline([\n                    (\"preprocess\", preprocessor),\n                    (\"dataframer\", dataframer)\n                ])\n\ntemp = pipe.fit_transform(X_train, y_train)\ntemp.head()\n\n\n---------------------------------------------------------------------------\nAttributeError                            Traceback (most recent call last)\nInput In [31], in &lt;module&gt;\n     18 dataframer = FunctionTransformer(func=get_dataframe, kw_args={\"transformer\": preprocessor})\n     19 pipe = Pipeline([\n     20                     (\"preprocess\", preprocessor),\n     21                     (\"dataframer\", dataframer)\n     22                 ])\n---&gt; 24 temp = pipe.fit_transform(X_train, y_train)\n     25 temp.head()\n\nFile ~\\anaconda3\\envs\\sc_mlflow\\lib\\site-packages\\sklearn\\pipeline.py:434, in Pipeline.fit_transform(self, X, y, **fit_params)\n    432 fit_params_last_step = fit_params_steps[self.steps[-1][0]]\n    433 if hasattr(last_step, \"fit_transform\"):\n--&gt; 434     return last_step.fit_transform(Xt, y, **fit_params_last_step)\n    435 else:\n    436     return last_step.fit(Xt, y, **fit_params_last_step).transform(Xt)\n\nFile ~\\anaconda3\\envs\\sc_mlflow\\lib\\site-packages\\sklearn\\base.py:855, in TransformerMixin.fit_transform(self, X, y, **fit_params)\n    852     return self.fit(X, **fit_params).transform(X)\n    853 else:\n    854     # fit method of arity 2 (supervised transformation)\n--&gt; 855     return self.fit(X, y, **fit_params).transform(X)\n\nFile ~\\anaconda3\\envs\\sc_mlflow\\lib\\site-packages\\sklearn\\preprocessing\\_function_transformer.py:182, in FunctionTransformer.transform(self, X)\n    169 \"\"\"Transform X using the forward function.\n    170 \n    171 Parameters\n   (...)\n    179     Transformed input.\n    180 \"\"\"\n    181 X = self._check_input(X, reset=False)\n--&gt; 182 return self._transform(X, func=self.func, kw_args=self.kw_args)\n\nFile ~\\anaconda3\\envs\\sc_mlflow\\lib\\site-packages\\sklearn\\preprocessing\\_function_transformer.py:205, in FunctionTransformer._transform(self, X, func, kw_args)\n    202 if func is None:\n    203     func = _identity\n--&gt; 205 return func(X, **(kw_args if kw_args else {}))\n\nInput In [27], in get_dataframe(X, transformer)\n      1 def get_dataframe(X, transformer):\n      2     \"\"\" \n      3     x: an nd-array\n      4     transformer: fitted transformer\n      5     \"\"\"\n----&gt; 6     col_names = transformer.get_feature_names_out()\n      7     output = pd.DataFrame(X.T, col_names).T\n      8     return output\n\nFile ~\\anaconda3\\envs\\sc_mlflow\\lib\\site-packages\\sklearn\\compose\\_column_transformer.py:481, in ColumnTransformer.get_feature_names_out(self, input_features)\n    479 transformer_with_feature_names_out = []\n    480 for name, trans, column, _ in self._iter(fitted=True):\n--&gt; 481     feature_names_out = self._get_feature_name_out_for_transformer(\n    482         name, trans, column, input_features\n    483     )\n    484     if feature_names_out is None:\n    485         continue\n\nFile ~\\anaconda3\\envs\\sc_mlflow\\lib\\site-packages\\sklearn\\compose\\_column_transformer.py:454, in ColumnTransformer._get_feature_name_out_for_transformer(self, name, trans, column, feature_names_in)\n    450 if isinstance(column, Iterable) and not all(\n    451     isinstance(col, str) for col in column\n    452 ):\n    453     column = _safe_indexing(feature_names_in, column)\n--&gt; 454 return trans.get_feature_names_out(column)\n\nFile ~\\anaconda3\\envs\\sc_mlflow\\lib\\site-packages\\sklearn\\pipeline.py:751, in Pipeline.get_feature_names_out(self, input_features)\n    749 for _, name, transform in self._iter():\n    750     if not hasattr(transform, \"get_feature_names_out\"):\n--&gt; 751         raise AttributeError(\n    752             \"Estimator {} does not provide get_feature_names_out. \"\n    753             \"Did you mean to call pipeline[:-1].get_feature_names_out\"\n    754             \"()?\".format(name)\n    755         )\n    756     feature_names_out = transform.get_feature_names_out(feature_names_out)\n    757 return feature_names_out\n\nAttributeError: Estimator imputer does not provide get_feature_names_out. Did you mean to call pipeline[:-1].get_feature_names_out()?\n\n\n\nOh geez! What went wrong this time. The error message says\nAttributeError: Estimator imputer does not provide get_feature_names_out. Did you mean to call pipeline[:-1].get_feature_names_out()?\nFrom the error message I am getting that &gt; Estimator imputer does not provide get_feature_names_out\nHmmm, this is strange. Why is this estimator missing a very useful function? Let’s check the docs first on SimpleImputer. For the docs I indeed could not find this method get_feature_names_out() for this transformer. A little googling lead me to this Sklearn Github issue page Implement get_feature_names_out for all estimators. Developers are actively adding get_feature_names_out() to all estimators and transformers, and it looks like this feature has not been implemented for SimpleImputer till Sklearn version==1.0.2.\nBut no worries we can overcome this limitation, and implement this feature ourselves through a custom transformer."
  },
  {
    "objectID": "posts/2022-03-04-sklearn-pipeline-deep-dive.html#custom-transformer",
    "href": "posts/2022-03-04-sklearn-pipeline-deep-dive.html#custom-transformer",
    "title": "Sklearn Pipeline and Transformers Deep Dive",
    "section": "Custom Transformer",
    "text": "Custom Transformer\nWe can create a custom transformer or an estimator simply by inheriting a class from BaseEstimator and optionally the mixin classes in sklearn.base. Sklean provides a template that we can use to create our custom transformer. Template link is here: https://github.com/scikit-learn-contrib/project-template/blob/master/skltemplate/_template.py#L146\nLet us use the same pipeline as in last cell but replace SimpleImputer with a custom one.\n\nfrom sklearn.base import BaseEstimator, TransformerMixin\n\nclass SimpleImputerCustom(BaseEstimator, TransformerMixin):\n    def __init__(self, strategy='mean'):\n        self.strategy = strategy\n        self.imputer = SimpleImputer(strategy=self.strategy)\n       \n        \n    def fit(self, X, y):\n        self.imputer.fit(X, y)\n        return self\n    \n    def transform(self, X):\n        return self.imputer.transform(X)\n    \n    def get_feature_names_out(self, input_features=None):\n        # we have returned the input features name as out features will have the same name\n        return input_features\n\n\nnumeric_features = [\"age\", \"fare\"]\nnumeric_transformer = Pipeline(\n    steps=[\n            (\"imputer\", SimpleImputerCustom(strategy='mean')),\n            (\"scaler\", StandardScaler())\n             ]\n)\n\n\ncategorical_features = [\"sex\"]\ncategorical_transformer = OneHotEncoder(handle_unknown=\"ignore\")\n\npreprocessor = make_column_transformer(\n        (numeric_transformer, numeric_features), # note \"numeric_transformer\" is a pipeline\n        (categorical_transformer, categorical_features),\n        remainder='passthrough',\n    verbose_feature_names_out=False\n)\n\ndataframer = FunctionTransformer(func=get_dataframe, kw_args={\"transformer\": preprocessor})\npipe = Pipeline([\n                    (\"preprocess\", preprocessor),\n                    (\"dataframer\", dataframer)\n                ])\n\ntemp = pipe.fit_transform(X_train, y_train)\ntemp.head()\n\n\n\n\n\n\n\n\nage\nfare\nsex_female\nsex_male\npclass\nname\nsibsp\nparch\nticket\nembarked\n\n\n\n\n0\n0.0\n-0.499638\n1.0\n0.0\n3.0\nMcCarthy, Miss. Catherine 'Katie'\n0.0\n0.0\n383123\nQ\n\n\n1\n-0.436411\n-0.090979\n1.0\n0.0\n2.0\ndel Carlo, Mrs. Sebastiano (Argenia Genovesi)\n1.0\n0.0\nSC/PARIS 2167\nC\n\n\n2\n-1.448729\n-0.01825\n1.0\n0.0\n3.0\nAndersson, Miss. Sigrid Elisabeth\n4.0\n2.0\n347082\nS\n\n\n3\n-0.358541\n-0.510381\n0.0\n1.0\n3.0\nSaad, Mr. Khalil\n0.0\n0.0\n2672\nC\n\n\n4\n-1.059376\n-0.501684\n1.0\n0.0\n3.0\nAbelseth, Miss. Karen Marie\n0.0\n0.0\n348125\nS"
  },
  {
    "objectID": "posts/2022-03-04-sklearn-pipeline-deep-dive.html#feature-engineering-with-custom-transformer",
    "href": "posts/2022-03-04-sklearn-pipeline-deep-dive.html#feature-engineering-with-custom-transformer",
    "title": "Sklearn Pipeline and Transformers Deep Dive",
    "section": "Feature Engineering with Custom Transformer",
    "text": "Feature Engineering with Custom Transformer\nSo far, so good! Let’s assume that we have another requirement and it is about feature engineering. We have to combine ‘sibsp’ and ‘parch’ into two new features: family_size and is_alone.\nLet’s implement this now.\n\nclass FamilyFeatureTransformer(BaseEstimator, TransformerMixin):\n    def __init__(self):\n        pass\n       \n    def fit(self, X, y):\n        return self\n    \n    def transform(self, X):\n        X['family_size'] = X['parch'] + X['sibsp']\n        X.drop(['parch', 'sibsp'], axis=1, inplace=True) # we can drop this feature now\n        X['is_alone'] = 1\n        X.loc[X['family_size'] &gt; 1, 'is_alone'] = 0\n        return X\n    \n    def get_feature_names_out(self, input_features=None):\n        # this time we have created new features. Their names are different from input features.\n        # so we have explicitly mentioned them here.\n        return ['family_size', 'is_alone']\n\nnumeric_features = [\"age\", \"fare\"]\nnumeric_transformer = Pipeline(\n    steps=[\n            (\"imputer\", SimpleImputerCustom(strategy='mean')),\n            (\"scaler\", StandardScaler())\n             ]\n)\n\ncategorical_features = [\"sex\"]\ncategorical_transformer = OneHotEncoder(handle_unknown=\"ignore\")\n\nfamily_features = [\"parch\", \"sibsp\"]\nfamily_transformer = FamilyFeatureTransformer()\n\npreprocessor = make_column_transformer(\n        (numeric_transformer, numeric_features), \n        (categorical_transformer, categorical_features),\n        (family_transformer, family_features),\n        remainder='drop', # let's drop extra features this time\n    verbose_feature_names_out=False\n)\n\ndataframer = FunctionTransformer(func=get_dataframe, kw_args={\"transformer\": preprocessor})\npipe = Pipeline([\n                    (\"preprocess\", preprocessor),\n                    (\"dataframer\", dataframer)\n                ])\n\ntemp = pipe.fit_transform(X_train, y_train)\ntemp.head()\n\n\n\n\n\n\n\n\nage\nfare\nsex_female\nsex_male\nfamily_size\nis_alone\n\n\n\n\n0\n0.000000\n-0.499638\n1.0\n0.0\n0.0\n1.0\n\n\n1\n-0.436411\n-0.090979\n1.0\n0.0\n1.0\n1.0\n\n\n2\n-1.448729\n-0.018250\n1.0\n0.0\n6.0\n0.0\n\n\n3\n-0.358541\n-0.510381\n0.0\n1.0\n0.0\n1.0\n\n\n4\n-1.059376\n-0.501684\n1.0\n0.0\n0.0\n1.0"
  },
  {
    "objectID": "posts/2022-03-04-sklearn-pipeline-deep-dive.html#sklean-pipeline-with-feature-importance",
    "href": "posts/2022-03-04-sklearn-pipeline-deep-dive.html#sklean-pipeline-with-feature-importance",
    "title": "Sklearn Pipeline and Transformers Deep Dive",
    "section": "Sklean Pipeline with Feature Importance",
    "text": "Sklean Pipeline with Feature Importance\nAlright, we have our required features ready and we can now pass them to a classifier. Let’s use RandomForrest as our classifier and run our pipeline with it.\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\n\n# 'preprocessor' and 'dataframer' are already declared in last section, and intentionally omitted here.\npipe = Pipeline([\n                    (\"preprocess\", preprocessor),\n                    (\"dataframer\", dataframer),\n                    ('rf_estimator', RandomForestClassifier())\n\n                ])\n\ntemp = pipe.fit_transform(X_train, y_train)\ntemp.head()\n\n\n---------------------------------------------------------------------------\nAttributeError                            Traceback (most recent call last)\nInput In [34], in &lt;module&gt;\n      4 # 'preprocessor' and 'dataframer' are already declared in last section, and intentionally omitted here.\n      5 pipe = Pipeline([\n      6                     (\"preprocess\", preprocessor),\n      7                     (\"dataframer\", dataframer),\n      8                     ('rf_estimator', RandomForestClassifier())\n      9 \n     10                 ])\n---&gt; 12 temp = pipe.fit_transform(X_train, y_train)\n     13 temp.head()\n\nFile ~\\anaconda3\\envs\\sc_mlflow\\lib\\site-packages\\sklearn\\pipeline.py:436, in Pipeline.fit_transform(self, X, y, **fit_params)\n    434     return last_step.fit_transform(Xt, y, **fit_params_last_step)\n    435 else:\n--&gt; 436     return last_step.fit(Xt, y, **fit_params_last_step).transform(Xt)\n\nAttributeError: 'RandomForestClassifier' object has no attribute 'transform'\n\n\n\nOkay, looks like we have made a mistake here. Error message is saying\nAttributeError: 'RandomForestClassifier' object has no attribute 'transform'\nI get that. In our pipeline we have an estimator that does not have a transform method defined for it. We should use predict method instead.\nNote: * Estimators implement predict method (Template reference Estimator, Template reference Classifier) * Transformers implement transform method (Template reference Transformer) * fit_transform is same calling fit and then transform\nLet us fix the error and run our pipeline again.\n\n##\n# pipeline created in last section and intentionally omitted here.\npipe.fit(X_train, y_train)\ny_pred = pipe.predict(X_test)\naccuracy_score(y_test, y_pred)\n\n0.7862595419847328\n\n\nLet’s see how our final pipeline looks visually.\n\n##\n# set_config(display='text')\npipe\n\nPipeline(steps=[('preprocess',\n                 ColumnTransformer(transformers=[('pipeline',\n                                                  Pipeline(steps=[('imputer',\n                                                                   SimpleImputerCustom()),\n                                                                  ('scaler',\n                                                                   StandardScaler())]),\n                                                  ['age', 'fare']),\n                                                 ('onehotencoder',\n                                                  OneHotEncoder(handle_unknown='ignore'),\n                                                  ['sex']),\n                                                 ('familyfeaturetransformer',\n                                                  FamilyFeatureTransformer(),\n                                                  ['parch', 'sibsp'])],\n                                   verbose_feature_names_out=False)),\n                ('data...\n                                     kw_args={'transformer': ColumnTransformer(transformers=[('pipeline',\n                                                                                              Pipeline(steps=[('imputer',\n                                                                                                               SimpleImputerCustom()),\n                                                                                                              ('scaler',\n                                                                                                               StandardScaler())]),\n                                                                                              ['age',\n                                                                                               'fare']),\n                                                                                             ('onehotencoder',\n                                                                                              OneHotEncoder(handle_unknown='ignore'),\n                                                                                              ['sex']),\n                                                                                             ('familyfeaturetransformer',\n                                                                                              FamilyFeatureTransformer(),\n                                                                                              ['parch',\n                                                                                               'sibsp'])],\n                                                                               verbose_feature_names_out=False)})),\n                ('rf_estimator', RandomForestClassifier())])Please rerun this cell to show the HTML repr or trust the notebook.PipelinePipeline(steps=[('preprocess',\n                 ColumnTransformer(transformers=[('pipeline',\n                                                  Pipeline(steps=[('imputer',\n                                                                   SimpleImputerCustom()),\n                                                                  ('scaler',\n                                                                   StandardScaler())]),\n                                                  ['age', 'fare']),\n                                                 ('onehotencoder',\n                                                  OneHotEncoder(handle_unknown='ignore'),\n                                                  ['sex']),\n                                                 ('familyfeaturetransformer',\n                                                  FamilyFeatureTransformer(),\n                                                  ['parch', 'sibsp'])],\n                                   verbose_feature_names_out=False)),\n                ('data...\n                                     kw_args={'transformer': ColumnTransformer(transformers=[('pipeline',\n                                                                                              Pipeline(steps=[('imputer',\n                                                                                                               SimpleImputerCustom()),\n                                                                                                              ('scaler',\n                                                                                                               StandardScaler())]),\n                                                                                              ['age',\n                                                                                               'fare']),\n                                                                                             ('onehotencoder',\n                                                                                              OneHotEncoder(handle_unknown='ignore'),\n                                                                                              ['sex']),\n                                                                                             ('familyfeaturetransformer',\n                                                                                              FamilyFeatureTransformer(),\n                                                                                              ['parch',\n                                                                                               'sibsp'])],\n                                                                               verbose_feature_names_out=False)})),\n                ('rf_estimator', RandomForestClassifier())])preprocess: ColumnTransformerColumnTransformer(transformers=[('pipeline',\n                                 Pipeline(steps=[('imputer',\n                                                  SimpleImputerCustom()),\n                                                 ('scaler', StandardScaler())]),\n                                 ['age', 'fare']),\n                                ('onehotencoder',\n                                 OneHotEncoder(handle_unknown='ignore'),\n                                 ['sex']),\n                                ('familyfeaturetransformer',\n                                 FamilyFeatureTransformer(),\n                                 ['parch', 'sibsp'])],\n                  verbose_feature_names_out=False)pipeline['age', 'fare']SimpleImputerCustomSimpleImputerCustom()StandardScalerStandardScaler()onehotencoder['sex']OneHotEncoderOneHotEncoder(handle_unknown='ignore')familyfeaturetransformer['parch', 'sibsp']FamilyFeatureTransformerFamilyFeatureTransformer()FunctionTransformerFunctionTransformer(func=&lt;function get_dataframe at 0x00000161953C3670&gt;,\n                    kw_args={'transformer': ColumnTransformer(transformers=[('pipeline',\n                                                                             Pipeline(steps=[('imputer',\n                                                                                              SimpleImputerCustom()),\n                                                                                             ('scaler',\n                                                                                              StandardScaler())]),\n                                                                             ['age',\n                                                                              'fare']),\n                                                                            ('onehotencoder',\n                                                                             OneHotEncoder(handle_unknown='ignore'),\n                                                                             ['sex']),\n                                                                            ('familyfeaturetransformer',\n                                                                             FamilyFeatureTransformer(),\n                                                                             ['parch',\n                                                                              'sibsp'])],\n                                                              verbose_feature_names_out=False)})RandomForestClassifierRandomForestClassifier()\n\n\nWe can also get the importance of features in our dataset from RandomForrest classifier.\n\nimport matplotlib.pyplot as plt\n\nclf = pipe[-1] # last estimator is the RF classifier\nimportances = clf.feature_importances_\nfeatures = clf.feature_names_in_\n\nindices = np.argsort(importances)\n\nplt.title('Feature Importances')\nplt.barh(range(len(indices)), importances[indices], color='b', align='center')\nplt.yticks(range(len(indices)), [features[i] for i in indices])\nplt.xlabel('Relative Importance')\nplt.show()\n\n\n\n\n\n\n\n\nNote that all the feature names were passed to the RF Classifier and that is why we were able to get them back using its attribute feature_names_in_. This can be super useful when you have many model deployed in the environment, and you can just use the model object to get information about the features it was trained on.\nFor a moment let’s also remove the feature names from our pipeline and see how it will effect our feature importance plot.\n\npipe = Pipeline([\n                    (\"preprocess\", preprocessor),\n                    # (\"dataframer\", dataframer),\n                    ('rf_estimator', RandomForestClassifier())\n\n                ])\n\n# fit the pipeline\npipe.fit(X_train, y_train)\n\n# get the feature importance plot\nclf = pipe[-1]\nimportances = clf.feature_importances_\nfeatures = clf.feature_names_in_\n\nindices = np.argsort(importances)\n\nplt.title('Feature Importances')\nplt.barh(range(len(indices)), importances[indices], color='b', align='center')\nplt.yticks(range(len(indices)), [features[i] for i in indices])\nplt.xlabel('Relative Importance')\nplt.show()\n\n\n---------------------------------------------------------------------------\nAttributeError                            Traceback (most recent call last)\nInput In [38], in &lt;module&gt;\n     12 clf = pipe[-1]\n     13 importances = clf.feature_importances_\n---&gt; 14 features = clf.feature_names_in_\n     16 indices = np.argsort(importances)\n     18 plt.title('Feature Importances')\n\nAttributeError: 'RandomForestClassifier' object has no attribute 'feature_names_in_'\n\n\n\nNo feature names were passed to our classifier this time and it is missing feature_names_in_ attribute. We can circumvent this and still get feature importance plot.\n\npipe = Pipeline([\n                    (\"preprocess\", preprocessor),\n                    # (\"dataframer\", dataframer),\n                    ('rf_estimator', RandomForestClassifier())\n\n                ])\n\n# fit the pipeline\npipe.fit(X_train, y_train)\n\n# get the feature importance plot\nclf = pipe[-1]\nimportances = clf.feature_importances_\n# features = clf.feature_names_in_\n\nindices = np.argsort(importances)\n\nplt.title('Feature Importances')\nplt.barh(range(len(indices)), importances[indices], color='b', align='center')\nplt.yticks(range(len(indices)), [i for i in indices])\nplt.xlabel('Relative Importance')\nplt.show()\n\n\n\n\n\n\n\n\nThis time we get the same plot but not withOUT feature names, and it is not useful anymore. So definitely we need to keep the feature names with the final estimator. Feature names can help us a lot in interpreting the model."
  },
  {
    "objectID": "posts/2022-03-04-sklearn-pipeline-deep-dive.html#the-complete-pipeline",
    "href": "posts/2022-03-04-sklearn-pipeline-deep-dive.html#the-complete-pipeline",
    "title": "Sklearn Pipeline and Transformers Deep Dive",
    "section": "The complete Pipeline",
    "text": "The complete Pipeline\nFor an easy reference, let’s put the whole pipeline in one place. ### Load Data\n\nfrom sklearn.datasets import fetch_openml\nfrom sklearn.model_selection import train_test_split\nimport pandas as pd\nimport numpy as np\n\nnp.random.seed(42) # for consistency\n\nX, y = fetch_openml(\"titanic\", version=1, as_frame=True, return_X_y=True)\n\nX.drop(['body', 'cabin', 'boat', 'home.dest'], axis=1, inplace=True)\nX_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2)\nX_train.head()\n\n\n\n\n\n\n\n\npclass\nname\nsex\nage\nsibsp\nparch\nticket\nfare\nembarked\n\n\n\n\n999\n3.0\nMcCarthy, Miss. Catherine 'Katie'\nfemale\nNaN\n0.0\n0.0\n383123\n7.7500\nQ\n\n\n392\n2.0\ndel Carlo, Mrs. Sebastiano (Argenia Genovesi)\nfemale\n24.0\n1.0\n0.0\nSC/PARIS 2167\n27.7208\nC\n\n\n628\n3.0\nAndersson, Miss. Sigrid Elisabeth\nfemale\n11.0\n4.0\n2.0\n347082\n31.2750\nS\n\n\n1165\n3.0\nSaad, Mr. Khalil\nmale\n25.0\n0.0\n0.0\n2672\n7.2250\nC\n\n\n604\n3.0\nAbelseth, Miss. Karen Marie\nfemale\n16.0\n0.0\n0.0\n348125\n7.6500\nS\n\n\n\n\n\n\n\n\nTrain Model\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import OneHotEncoder, FunctionTransformer, StandardScaler\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.compose import make_column_transformer\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.impute import SimpleImputer\n\nclass FamilyFeatureTransformer(BaseEstimator, TransformerMixin):\n    def __init__(self):\n        pass\n       \n    def fit(self, X, y):\n        return self\n    \n    def transform(self, X):\n        X['family_size'] = X['parch'] + X['sibsp']\n        X.drop(['parch', 'sibsp'], axis=1, inplace=True) # we can drop this feature now\n        X['is_alone'] = 1\n        X.loc[X['family_size'] &gt; 1, 'is_alone'] = 0\n        return X\n    \n    def get_feature_names_out(self, input_features=None):\n        # this time we have created new features. Their names are different from input features.\n        # so we have explicitly mentioned them here.\n        return ['family_size', 'is_alone']\n\nclass SimpleImputerCustom(BaseEstimator, TransformerMixin):\n    def __init__(self, strategy='mean'):\n        self.strategy = strategy\n        self.imputer = SimpleImputer(strategy=self.strategy)\n       \n        \n    def fit(self, X, y):\n        self.imputer.fit(X, y)\n        return self\n    \n    def transform(self, X):\n        return self.imputer.transform(X)\n    \n    def get_feature_names_out(self, input_features=None):\n        # we have returned the input features name as out features will have the same name\n        return input_features\n\ndef get_dataframe(X, transformer):\n    \"\"\" \n    x: an nd-array\n    transformer: fitted transformer\n    \"\"\"\n    col_names = transformer.get_feature_names_out()\n    output = pd.DataFrame(X.T, col_names).T\n    return output\n    \nnumeric_features = [\"age\", \"fare\"]\nnumeric_transformer = Pipeline(\n    steps=[\n            (\"imputer\", SimpleImputerCustom(strategy='mean')),\n            (\"scaler\", StandardScaler())\n             ]\n)\n\ncategorical_features = [\"sex\"]\ncategorical_transformer = OneHotEncoder(handle_unknown=\"ignore\")\n\nfamily_features = [\"parch\", \"sibsp\"]\nfamily_transformer = FamilyFeatureTransformer()\n\npreprocessor = make_column_transformer(\n        (numeric_transformer, numeric_features), \n        (categorical_transformer, categorical_features),\n        (family_transformer, family_features),\n        remainder='drop', # let's drop extra features this time\n    verbose_feature_names_out=False\n)\n\ndataframer = FunctionTransformer(func=get_dataframe, kw_args={\"transformer\": preprocessor})\npipe = Pipeline([\n                    (\"preprocess\", preprocessor),\n                    (\"dataframer\", dataframer),\n                    ('rf_estimator', RandomForestClassifier())\n\n                ])\n\npipe.fit(X_train, y_train)\ny_pred = pipe.predict(X_test)\naccuracy_score(y_test, y_pred)\n\n0.8015267175572519\n\n\n\n\nPlot Feature Importance\n\nimport matplotlib.pyplot as plt\n\nclf = pipe[-1] # last estimator is the RF classifier\nimportances = clf.feature_importances_\nfeatures = clf.feature_names_in_\n\nindices = np.argsort(importances)\n\nplt.title('Feature Importances')\nplt.barh(range(len(indices)), importances[indices], color='b', align='center')\nplt.yticks(range(len(indices)), [features[i] for i in indices])\nplt.xlabel('Relative Importance')\nplt.show()\n\n\n\n\n\n\n\n\n\n\nPickle Test\n\nfrom sklearn import set_config\nset_config(display=\"diagram\")\n\nimport pickle\n\n# save our pipeline\ns1 = pickle.dumps(pipe)\n\n# reload it\ns2 = pickle.loads(s1)\ns2\n\nPipeline(steps=[('preprocess',\n                 ColumnTransformer(transformers=[('pipeline',\n                                                  Pipeline(steps=[('imputer',\n                                                                   SimpleImputerCustom()),\n                                                                  ('scaler',\n                                                                   StandardScaler())]),\n                                                  ['age', 'fare']),\n                                                 ('onehotencoder',\n                                                  OneHotEncoder(handle_unknown='ignore'),\n                                                  ['sex']),\n                                                 ('familyfeaturetransformer',\n                                                  FamilyFeatureTransformer(),\n                                                  ['parch', 'sibsp'])],\n                                   verbose_feature_names_out=False)),\n                ('data...\n                                     kw_args={'transformer': ColumnTransformer(transformers=[('pipeline',\n                                                                                              Pipeline(steps=[('imputer',\n                                                                                                               SimpleImputerCustom()),\n                                                                                                              ('scaler',\n                                                                                                               StandardScaler())]),\n                                                                                              ['age',\n                                                                                               'fare']),\n                                                                                             ('onehotencoder',\n                                                                                              OneHotEncoder(handle_unknown='ignore'),\n                                                                                              ['sex']),\n                                                                                             ('familyfeaturetransformer',\n                                                                                              FamilyFeatureTransformer(),\n                                                                                              ['parch',\n                                                                                               'sibsp'])],\n                                                                               verbose_feature_names_out=False)})),\n                ('rf_estimator', RandomForestClassifier())])Please rerun this cell to show the HTML repr or trust the notebook.PipelinePipeline(steps=[('preprocess',\n                 ColumnTransformer(transformers=[('pipeline',\n                                                  Pipeline(steps=[('imputer',\n                                                                   SimpleImputerCustom()),\n                                                                  ('scaler',\n                                                                   StandardScaler())]),\n                                                  ['age', 'fare']),\n                                                 ('onehotencoder',\n                                                  OneHotEncoder(handle_unknown='ignore'),\n                                                  ['sex']),\n                                                 ('familyfeaturetransformer',\n                                                  FamilyFeatureTransformer(),\n                                                  ['parch', 'sibsp'])],\n                                   verbose_feature_names_out=False)),\n                ('data...\n                                     kw_args={'transformer': ColumnTransformer(transformers=[('pipeline',\n                                                                                              Pipeline(steps=[('imputer',\n                                                                                                               SimpleImputerCustom()),\n                                                                                                              ('scaler',\n                                                                                                               StandardScaler())]),\n                                                                                              ['age',\n                                                                                               'fare']),\n                                                                                             ('onehotencoder',\n                                                                                              OneHotEncoder(handle_unknown='ignore'),\n                                                                                              ['sex']),\n                                                                                             ('familyfeaturetransformer',\n                                                                                              FamilyFeatureTransformer(),\n                                                                                              ['parch',\n                                                                                               'sibsp'])],\n                                                                               verbose_feature_names_out=False)})),\n                ('rf_estimator', RandomForestClassifier())])preprocess: ColumnTransformerColumnTransformer(transformers=[('pipeline',\n                                 Pipeline(steps=[('imputer',\n                                                  SimpleImputerCustom()),\n                                                 ('scaler', StandardScaler())]),\n                                 ['age', 'fare']),\n                                ('onehotencoder',\n                                 OneHotEncoder(handle_unknown='ignore'),\n                                 ['sex']),\n                                ('familyfeaturetransformer',\n                                 FamilyFeatureTransformer(),\n                                 ['parch', 'sibsp'])],\n                  verbose_feature_names_out=False)pipeline['age', 'fare']SimpleImputerCustomSimpleImputerCustom()StandardScalerStandardScaler()onehotencoder['sex']OneHotEncoderOneHotEncoder(handle_unknown='ignore')familyfeaturetransformer['parch', 'sibsp']FamilyFeatureTransformerFamilyFeatureTransformer()FunctionTransformerFunctionTransformer(func=&lt;function get_dataframe at 0x00000161997DF3A0&gt;,\n                    kw_args={'transformer': ColumnTransformer(transformers=[('pipeline',\n                                                                             Pipeline(steps=[('imputer',\n                                                                                              SimpleImputerCustom()),\n                                                                                             ('scaler',\n                                                                                              StandardScaler())]),\n                                                                             ['age',\n                                                                              'fare']),\n                                                                            ('onehotencoder',\n                                                                             OneHotEncoder(handle_unknown='ignore'),\n                                                                             ['sex']),\n                                                                            ('familyfeaturetransformer',\n                                                                             FamilyFeatureTransformer(),\n                                                                             ['parch',\n                                                                              'sibsp'])],\n                                                              verbose_feature_names_out=False)})RandomForestClassifierRandomForestClassifier()"
  },
  {
    "objectID": "posts/2022-03-14-docker-app-debug.html#sample-application",
    "href": "posts/2022-03-14-docker-app-debug.html#sample-application",
    "title": "Docker - Debugging Python Application",
    "section": "Sample Application",
    "text": "Sample Application\nFor this post I will use a a simple hello world application that will print “hello world” messages to stdout, and also logs them in a logfilelog. After each message the application sleeps for 5 seconds, and keeps on doing this for 5 mins (300 sec). After this the program exists.\nThis application was created as part of the previous blog post Docker - Accessing Python Application Logs. It is a very simple application, and you can find all the code in GitHub repository snapshots-docker-post-11032022 * Project code files * Project zip file\nProject structure of this application is\napp/\n├── src/\n│   ├── commons/\n│   │   └── logger.py\n│   └── hello.py\n└── Dockerfile\nWhere * app/ is the project root folder * src/ folder contain the python application code * src/commons/logger.py is the logging module * src/hello.py is the main application * app/Dockerfile is the Docker image build file\nWhen I run the src/hello.py file from my local machine (Windows 10) I get the output on the termial with hello world messages like this.\n\n\n\napp-run-local\n\n\nA “logfile.log” will also appear in the project directory containing the logged messages.\n.\n├── app/\n│   ├── src/\n│   │   ├── commons/\n│   │   │   └── logger.py\n│   │   └── hello.py\n│   └── Dockerfile\n└── **logfile.log**\nContents of “logfile.log” file will look like this\nINFO 2022-03-11 13:01:56,451 - hello world at 11/03/2022 13:01:56\nINFO 2022-03-11 13:02:01,464 - hello world at 11/03/2022 13:02:01\nINFO 2022-03-11 13:02:06,466 - hello world at 11/03/2022 13:02:06\nINFO 2022-03-11 13:02:11,480 - hello world at 11/03/2022 13:02:11\nINFO 2022-03-11 13:02:16,496 - hello world at 11/03/2022 13:02:16\nWe can build our docker image and run it using commands\ndocker build --tag python-docker .\ndocker run --name helloworld python-docker\nOutput on the terminal will be like this \nNotice the difference in the print message when the application was is locally, and from the docker container. * Local (Win10) message = hello world at 14/03/2022 18:04:02 from Windows * Docker container message = hello world at 14/03/2022 13:12:14 from Linux"
  },
  {
    "objectID": "posts/2022-03-28-efs-s3-sync-lambda.html#create-an-s3-bucket",
    "href": "posts/2022-03-28-efs-s3-sync-lambda.html#create-an-s3-bucket",
    "title": "AWS EFS Sync to S3 Using Lambda",
    "section": "Create an S3 bucket",
    "text": "Create an S3 bucket\nLet’s first create an S3 bucket that will contain our data, and this is the bucket we would like to be in sync with EFS. I am naming the bucket mydata-202203. You may name it as you please. Choose a region of your choice and leave the rest of the settings as defaults."
  },
  {
    "objectID": "posts/2022-03-28-efs-s3-sync-lambda.html#create-a-lambda-function",
    "href": "posts/2022-03-28-efs-s3-sync-lambda.html#create-a-lambda-function",
    "title": "AWS EFS Sync to S3 Using Lambda",
    "section": "Create a Lambda function",
    "text": "Create a Lambda function\nNow create a lambda function that will receive event notifications from the S3 bucket, and sync files on efs. I am naming it mydata-sync and our runtime will be Python 3.9. Keep the rest of the settings as default, and create the function."
  },
  {
    "objectID": "posts/2022-03-28-efs-s3-sync-lambda.html#create-s3-event-notifications",
    "href": "posts/2022-03-28-efs-s3-sync-lambda.html#create-s3-event-notifications",
    "title": "AWS EFS Sync to S3 Using Lambda",
    "section": "Create S3 event notifications",
    "text": "Create S3 event notifications\nFrom the bucket, mydata-sync go to Properties. Scroll down to Event notifications and click create. Give any name to the event. I am calling it object-sync. From the provided event types select * s3:ObjectCreated:Put * s3:ObjectRemoved:Delete\nFrom the section Destination select Lambda Function, and from the list choose the lambda function name we created in the last section mydata-sync\nClick Save Changes"
  },
  {
    "objectID": "posts/2022-03-28-efs-s3-sync-lambda.html#test-s3-notifications",
    "href": "posts/2022-03-28-efs-s3-sync-lambda.html#test-s3-notifications",
    "title": "AWS EFS Sync to S3 Using Lambda",
    "section": "Test S3 notifications",
    "text": "Test S3 notifications\nLet’s now test if S3 event notifications are being received by our lambda function. For this update lambda function code and simply print the event received. After updating the lambda function, make sure to deploy it.\n\nimport json\n\ndef lambda_handler(event, context):\n    print(event)\n    return {\n        'statusCode': 200,\n        'body': json.dumps('Hello from Lambda!')\n    }\n\nNow upload some files in our S3 bucket, and it should trigger our lambda function. For testing, I have uploaded an empty test1.txt file in our bucket. Once successfully uploaded I check the Lambda function logs to see if any event is received. For this go to lambda function mydata-sync &gt; Monitor &gt; Logs &gt; View logs in CloudWatch. For the CloudWatch console view the latest log stream. Below is the event I have received in the logs\n{'Records': [{'eventVersion': '2.1', 'eventSource': 'aws:s3', 'awsRegion': 'us-east-1', 'eventTime': '2022-03-28T16:08:00.896Z', 'eventName': 'ObjectCreated:Put', 'userIdentity': {'principalId': 'AWS:AIDA3VIXXJNKIVU6P5NY3'}, 'requestParameters': {'sourceIPAddress': '202.163.113.76'}, 'responseElements': {'x-amz-request-id': '39MD61ZS00SNK2RT', 'x-amz-id-2': 'U+zPUWOrfzTuVi7kbaBONLHoJXKqUICsVqyKBg4yPKYbUV7pQLGc4Z5A2fSIVvDFtSJHC6v29EDJoXhypWsj2wXanUu8YrLocr3z+yK1qoo='}, 's3': {'s3SchemaVersion': '1.0', 'configurationId': 'object-sync', 'bucket': {'name': 'mydata-202203', 'ownerIdentity': {'principalId': 'AYAQOSFZ1VPK'}, 'arn': 'arn:aws:s3:::mydata-202203'}, 'object': {'key': 'test1.txt', 'size': 0, 'eTag': 'd41d8cd98f00b204e9800998ecf8427e', 'sequencer': '006241DD60D67A4556'}}}]}\nlet’s load this event in a dictionary and find some important parameters\n\nevent = {'Records': [{'eventVersion': '2.1', 'eventSource': 'aws:s3', 'awsRegion': 'us-east-1', 'eventTime': '2022-03-28T16:08:00.896Z', 'eventName': 'ObjectCreated:Put', 'userIdentity': {'principalId': 'AWS:AIDA3VIXXJNKIVU6P5NY3'}, 'requestParameters': {'sourceIPAddress': '202.163.113.76'}, 'responseElements': {'x-amz-request-id': '39MD61ZS00SNK2RT', 'x-amz-id-2': 'U+zPUWOrfzTuVi7kbaBONLHoJXKqUICsVqyKBg4yPKYbUV7pQLGc4Z5A2fSIVvDFtSJHC6v29EDJoXhypWsj2wXanUu8YrLocr3z+yK1qoo='}, 's3': {'s3SchemaVersion': '1.0', 'configurationId': 'object-sync', 'bucket': {'name': 'mydata-202203', 'ownerIdentity': {'principalId': 'AYAQOSFZ1VPK'}, 'arn': 'arn:aws:s3:::mydata-202203'}, 'object': {'key': 'test1.txt', 'size': 0, 'eTag': 'd41d8cd98f00b204e9800998ecf8427e', 'sequencer': '006241DD60D67A4556'}}}]}\nevent\n\n{'Records': [{'eventVersion': '2.1',\n   'eventSource': 'aws:s3',\n   'awsRegion': 'us-east-1',\n   'eventTime': '2022-03-28T16:08:00.896Z',\n   'eventName': 'ObjectCreated:Put',\n   'userIdentity': {'principalId': 'AWS:AIDA3VIXXJNKIVU6P5NY3'},\n   'requestParameters': {'sourceIPAddress': '202.163.113.76'},\n   'responseElements': {'x-amz-request-id': '39MD61ZS00SNK2RT',\n    'x-amz-id-2': 'U+zPUWOrfzTuVi7kbaBONLHoJXKqUICsVqyKBg4yPKYbUV7pQLGc4Z5A2fSIVvDFtSJHC6v29EDJoXhypWsj2wXanUu8YrLocr3z+yK1qoo='},\n   's3': {'s3SchemaVersion': '1.0',\n    'configurationId': 'object-sync',\n    'bucket': {'name': 'mydata-202203',\n     'ownerIdentity': {'principalId': 'AYAQOSFZ1VPK'},\n     'arn': 'arn:aws:s3:::mydata-202203'},\n    'object': {'key': 'test1.txt',\n     'size': 0,\n     'eTag': 'd41d8cd98f00b204e9800998ecf8427e',\n     'sequencer': '006241DD60D67A4556'}}}]}\n\n\n\n##\n# event name\nevent['Records'][0]['eventName']\n\n'ObjectCreated:Put'\n\n\n\n##\n# bucket name\nevent['Records'][0]['s3']['bucket']['name']\n\n'mydata-202203'\n\n\n\n##\n# uploaded object key\nevent['Records'][0]['s3']['object']['key']\n\n'test1.txt'\n\n\nAlright, we have seen that we are receiving notifications from S3 bucket so let’s now move on to the next section."
  },
  {
    "objectID": "posts/2022-03-28-efs-s3-sync-lambda.html#create-an-efs",
    "href": "posts/2022-03-28-efs-s3-sync-lambda.html#create-an-efs",
    "title": "AWS EFS Sync to S3 Using Lambda",
    "section": "Create an EFS",
    "text": "Create an EFS\nFrom EFS console give name as mydata-efs. I am using default VPC for this post. Use Regional availability settings. Click Create. Once file system is created, click on Access points and create an access point for this efs to be mounted in other service. For access point use following settings * name = mydata-ap * root dir path = /efs * POSIX User * POSIX UID = 1000 * Group ID = 1000 * Root directory creation permissions * Owner user id = 1000 * Owner group id = 1000 * POSIX permissions = 777\nClick Create.\nHere I have used the root dir path as /efs this means that from this access point my access will be limited to folder /efs. If you want to provide full access to all folders then set to root path to /."
  },
  {
    "objectID": "posts/2022-03-28-efs-s3-sync-lambda.html#note-on-efs-security-group-settings",
    "href": "posts/2022-03-28-efs-s3-sync-lambda.html#note-on-efs-security-group-settings",
    "title": "AWS EFS Sync to S3 Using Lambda",
    "section": "Note on EFS security group settings",
    "text": "Note on EFS security group settings\nIn the last section, I have used a default VPC security group (sg) while creating EFS. default sg allows traffic for all protocols and all ports, both for inbound and outbound traffic. But if you are using a custom security group then make sure that you have an inbound rule for * Type = NFS * Protocol = TCP * Port range = 2049\nOtherwise, you will not be able to access EFS using NFS clients."
  },
  {
    "objectID": "posts/2022-03-28-efs-s3-sync-lambda.html#mount-efs-to-lambda-function",
    "href": "posts/2022-03-28-efs-s3-sync-lambda.html#mount-efs-to-lambda-function",
    "title": "AWS EFS Sync to S3 Using Lambda",
    "section": "Mount EFS to Lambda Function",
    "text": "Mount EFS to Lambda Function\nTo mount an EFS to the Lambda function requires some additional steps.\nFirst add permissions to Lambda function. From lambda function &gt; Configurations &gt; Permissions &gt; Execution role. Click on the execution role to open it in IAM concole. For the selected role attach an additional policy AmazonElasticFileSystemFullAccess.\nSecond, add the lambda to a VPC group in which efs was created. We have created efs in default VPC so let’s add lambda to it. For this from lambda Configurations &gt; VPC click edit. For the next pane select default VPC, all subnets, default VPC security group, and click save.\nNow we can add EFS to lambda. Go to lambda Configurations &gt; File Systems &gt; Add file system. Select the file system mydata-efs and associated access point mydata-ap and local mount point as /mnt/efs. The local mount point is the mounted directory from where we can access our EFS from inside the lambda environment. Click Save"
  },
  {
    "objectID": "posts/2022-03-28-efs-s3-sync-lambda.html#check-efs-mount-point-from-lambda",
    "href": "posts/2022-03-28-efs-s3-sync-lambda.html#check-efs-mount-point-from-lambda",
    "title": "AWS EFS Sync to S3 Using Lambda",
    "section": "Check EFS mount point from Lambda",
    "text": "Check EFS mount point from Lambda\nLet’s verify from lambda that EFS has been mounted and can we access it. So update the lambda code as below and deploy it.\n\nimport json\nimport os\n\ndef lambda_handler(event, context):\n    mount_path = '/mnt/efs'\n    if os.path.exists(mount_path):\n        print(f\"{mount_path} exists\")\n        print(os.listdir('/mnt/efs'))\n    \n    print(event)\n\n    return {\n        'statusCode': 200,\n        'body': json.dumps('Hello from Lambda!')\n    }\n\nNow test this code using a test event S3 Put. For this go to lambda Test &gt; Create new event &gt; Template (s3-put). ‘S3 Put’ test event is similar to the one we saw in the last section. We can use this request template to simulate the event received from S3 bucket. Once the test is successfully executed, check the log output.\nSTART RequestId: 2e307a14-f373-46d5-b763-594d5f406ae6 Version: $LATEST\n/mnt/efs exists\n[]\n{'Records': [{'eventVersion': '2.0', 'eventSource': 'aws:s3', 'awsRegion': 'us-east-1', 'eventTime': '1970-01-01T00:00:00.000Z', 'eventName': 'ObjectCreated:Put', 'userIdentity': {'principalId': 'EXAMPLE'}, 'requestParameters': {'sourceIPAddress': '127.0.0.1'}, 'responseElements': {'x-amz-request-id': 'EXAMPLE123456789', 'x-amz-id-2': 'EXAMPLE123/5678abcdefghijklambdaisawesome/mnopqrstuvwxyzABCDEFGH'}, 's3': {'s3SchemaVersion': '1.0', 'configurationId': 'testConfigRule', 'bucket': {'name': 'example-bucket', 'ownerIdentity': {'principalId': 'EXAMPLE'}, 'arn': 'arn:aws:s3:::example-bucket'}, 'object': {'key': 'test%2Fkey', 'size': 1024, 'eTag': '0123456789abcdef0123456789abcdef', 'sequencer': '0A1B2C3D4E5F678901'}}}]}\nEND RequestId: 2e307a14-f373-46d5-b763-594d5f406ae6\nREPORT RequestId: 2e307a14-f373-46d5-b763-594d5f406ae6  Duration: 7.02 ms   Billed Duration: 8 ms   Memory Size: 128 MB Max Memory Used: 37 MB  Init Duration: 93.81 ms \nFrom the logs we can see that the mounted EFS directory exists /mnt/efs but currently the folder is empty."
  },
  {
    "objectID": "posts/2022-03-28-efs-s3-sync-lambda.html#configure-vpc-endpoint-for-s3",
    "href": "posts/2022-03-28-efs-s3-sync-lambda.html#configure-vpc-endpoint-for-s3",
    "title": "AWS EFS Sync to S3 Using Lambda",
    "section": "Configure VPC endpoint for S3",
    "text": "Configure VPC endpoint for S3\nTill now we have configured S3 notifications to trigger a lambda function and also mounted EFS to it. Our next step is to process the event received in lambda, and download the file from S3 to EFS. But since our lambda function is configured for a VPC we cannot connect to S3 from it. Even though we can still receive S3 event notification, when we try to connect to S3 to download any file we will get a timeout error. To fix this we will create a VPC endpoint for S3 bucket.\nFor this go to VPC console &gt; Endpoints &gt; Create endpoint, and set the following * name = mydata-ep * service category = aws services * services = com.amazonaws.us-east-1.s3 (Gateway) * vpc = default * route table = default (main route table) * policy = full access\nClick Create endpoint"
  },
  {
    "objectID": "posts/2022-03-28-efs-s3-sync-lambda.html#configure-s3-permissions-for-lambda",
    "href": "posts/2022-03-28-efs-s3-sync-lambda.html#configure-s3-permissions-for-lambda",
    "title": "AWS EFS Sync to S3 Using Lambda",
    "section": "Configure S3 permissions for Lambda",
    "text": "Configure S3 permissions for Lambda\nFor lambda to be able to connect to S3 we also need to give it proper permissions. For this go to Lambda &gt; Configurations &gt; Permissions &gt; Execution Role &gt; click on role name. From the IAM Role console select add permissions, and then select AmazonS3FullAccess"
  },
  {
    "objectID": "posts/2022-03-28-efs-s3-sync-lambda.html#process-s3-event-notifications",
    "href": "posts/2022-03-28-efs-s3-sync-lambda.html#process-s3-event-notifications",
    "title": "AWS EFS Sync to S3 Using Lambda",
    "section": "Process S3 event notifications",
    "text": "Process S3 event notifications\nOur lambda and EFS are ready and we can now process S3 events. Update the lambda code as below to process S3 events. It will download and delete from EFS to keep it in sync with S3 bucket.\n\nimport json\nimport boto3\nimport os\n\ns3 = boto3.client(\"s3\")\n\ndef lambda_handler(event, context):\n\n    event_name = event[\"Records\"][0][\"eventName\"]\n    bucket_name = event[\"Records\"][0][\"s3\"][\"bucket\"][\"name\"]\n    object_key = event[\"Records\"][0][\"s3\"][\"object\"][\"key\"]\n\n    efs_file_name = \"/mnt/efs/\" + object_key\n\n    # S3 put\n    if event_name == \"ObjectCreated:Put\":\n        s3.download_file(bucket_name, object_key, efs_file_name)\n        print(f\"file downloaded: {efs_file_name}\")\n\n    # S3 delete\n    if event_name == \"ObjectRemoved:Delete\":\n        # check if file exists on efs\n        if os.path.exists(efs_file_name):\n            os.remove(efs_file_name)\n            print(f\"file deleted: {efs_file_name}\")\n\n    return {\"statusCode\": 200, \"body\": json.dumps(event)}\n\nWe can test this code using the S3-put test event we used last time. Modify the event for bucket name and object key as below.\n{\n  \"Records\": [\n    {\n      \"eventVersion\": \"2.0\",\n      \"eventSource\": \"aws:s3\",\n      \"awsRegion\": \"us-east-1\",\n      \"eventTime\": \"1970-01-01T00:00:00.000Z\",\n      \"eventName\": \"ObjectCreated:Put\",\n      \"userIdentity\": {\n        \"principalId\": \"EXAMPLE\"\n      },\n      \"requestParameters\": {\n        \"sourceIPAddress\": \"127.0.0.1\"\n      },\n      \"responseElements\": {\n        \"x-amz-request-id\": \"EXAMPLE123456789\",\n        \"x-amz-id-2\": \"EXAMPLE123/5678abcdefghijklambdaisawesome/mnopqrstuvwxyzABCDEFGH\"\n      },\n      \"s3\": {\n        \"s3SchemaVersion\": \"1.0\",\n        \"configurationId\": \"testConfigRule\",\n        \"bucket\": {\n          \"name\": \"mydata-202203\",\n          \"ownerIdentity\": {\n            \"principalId\": \"EXAMPLE\"\n          },\n          \"arn\": \"arn:aws:s3:::example-bucket\"\n        },\n        \"object\": {\n          \"key\": \"test1.txt\",\n          \"size\": 1024,\n          \"eTag\": \"0123456789abcdef0123456789abcdef\",\n          \"sequencer\": \"0A1B2C3D4E5F678901\"\n        }\n      }\n    }\n  ]\n}\nClick test. From the output logs, we can see that our code was able to download the file from S3 bucket and write it on EFS.\nSTART RequestId: 7e9c0dc2-f970-426e-8372-e59b07f5536c Version: $LATEST\nfile downloaded: /mnt/efs/test1.txt\nEND RequestId: 7e9c0dc2-f970-426e-8372-e59b07f5536c\nREPORT RequestId: 7e9c0dc2-f970-426e-8372-e59b07f5536c  Duration: 370.00 ms Billed Duration: 371 ms Memory Size: 128 MB Max Memory Used: 72 MB  Init Duration: 367.68 ms\nNote that if you get any permission errors then it could be due to the mounting path errors. Please do check the access point path and lambda mount path."
  },
  {
    "objectID": "posts/2022-03-28-efs-s3-sync-lambda.html#verify-file-on-efs",
    "href": "posts/2022-03-28-efs-s3-sync-lambda.html#verify-file-on-efs",
    "title": "AWS EFS Sync to S3 Using Lambda",
    "section": "Verify file on EFS",
    "text": "Verify file on EFS\nWe can verify files on EFS by directly mounting them to an EC2 machine and verifying from there. So let’s do that.\nCreate an EC2 machine * AMI = Amazon Linux 2 AMI (HVM) - Kernel 5.10, SSD Volume Type * Intance type = t2.micro (free tier) * Instance details * Network = default VPC * Auto-assign Public IP = Enable * Review and Lanunch &gt; Launch &gt; Proceed without key pair.\nOnce the instance is up and running, click on it and connect using EC2 instance connect option. Create a dir ‘efs’ using the command\nmkdir efs\nIn a separate tab open EFS, and click on the file system we have created. Click Attach. From “Mount via DNS” copy command for NFS client. paste that in EC2 bash terminal\nsudo mount -t nfs4 -o nfsvers=4.1,rsize=1048576,wsize=1048576,hard,timeo=600,retrans=2,noresvport fs-0c9526e2f48ece247.efs.us-east-1.amazonaws.com:/ efs\nOnce successfully mounted verify that the file ‘test1.txt’ exists in EFS. We can also delete the file from S3 and similarly verify from EFS that the file has been removed."
  },
  {
    "objectID": "posts/2022-03-31-ml-nomenclature.html#dataset",
    "href": "posts/2022-03-31-ml-nomenclature.html#dataset",
    "title": "Machine Learning Nomenclature",
    "section": "Dataset",
    "text": "Dataset\nThe data we use in ML is usually defined as dataset, and datasets are a collection of data. The dataset contains the features and target to predict.\nIt has other names * data * input data * train and test data"
  },
  {
    "objectID": "posts/2022-03-31-ml-nomenclature.html#instance",
    "href": "posts/2022-03-31-ml-nomenclature.html#instance",
    "title": "Machine Learning Nomenclature",
    "section": "Instance",
    "text": "Instance\nAn instance is a row in the dataset.\nIs has other names * row * observation * sample * (data) point"
  },
  {
    "objectID": "posts/2022-03-31-ml-nomenclature.html#feature",
    "href": "posts/2022-03-31-ml-nomenclature.html#feature",
    "title": "Machine Learning Nomenclature",
    "section": "Feature",
    "text": "Feature\nFeature is a column in the dataset. It is used as an input used for prediction or classification. Features are commonly represented by x variable.\nIt has other names * column * attribute * (input) variable\nFeatures are of two types * Categorical or qualitative * Numerical or quantitative"
  },
  {
    "objectID": "posts/2022-03-31-ml-nomenclature.html#target",
    "href": "posts/2022-03-31-ml-nomenclature.html#target",
    "title": "Machine Learning Nomenclature",
    "section": "Target",
    "text": "Target\nIt is the information a machine learning algorithm learns to predict. Target is commonly represented by y variable.\nIt has other names * label * output"
  },
  {
    "objectID": "posts/2022-03-31-ml-nomenclature.html#labeled-data",
    "href": "posts/2022-03-31-ml-nomenclature.html#labeled-data",
    "title": "Machine Learning Nomenclature",
    "section": "Labeled Data",
    "text": "Labeled Data\nA data that has both the feature and target attributes defined"
  },
  {
    "objectID": "posts/2022-03-31-ml-nomenclature.html#unlabeled-data",
    "href": "posts/2022-03-31-ml-nomenclature.html#unlabeled-data",
    "title": "Machine Learning Nomenclature",
    "section": "Unlabeled Data",
    "text": "Unlabeled Data\nA data that has the features defined but has no target attribute."
  },
  {
    "objectID": "posts/2022-03-31-ml-nomenclature.html#categorical-feature",
    "href": "posts/2022-03-31-ml-nomenclature.html#categorical-feature",
    "title": "Machine Learning Nomenclature",
    "section": "Categorical Feature",
    "text": "Categorical Feature\nA feature that is not measureable and has discrete set of values like gender, family retionships, movie categories etc. We commonly use bar charts and pie graphs for categorical features.\nIt has other names * qualitative feature\nCategorical features are of two types * Nominal * Ordinal"
  },
  {
    "objectID": "posts/2022-03-31-ml-nomenclature.html#nominal-feature",
    "href": "posts/2022-03-31-ml-nomenclature.html#nominal-feature",
    "title": "Machine Learning Nomenclature",
    "section": "Nominal feature",
    "text": "Nominal feature\nNominal (categorical) feature is one that can not be measured and has no order assgined to it e.g. eye colors, gender etc."
  },
  {
    "objectID": "posts/2022-03-31-ml-nomenclature.html#ordinal-feature",
    "href": "posts/2022-03-31-ml-nomenclature.html#ordinal-feature",
    "title": "Machine Learning Nomenclature",
    "section": "Ordinal feature",
    "text": "Ordinal feature\nOrdinal (categorical) feature is one that can not be measured but has some order assgined to it like movie ratings, military ranks etc."
  },
  {
    "objectID": "posts/2022-03-31-ml-nomenclature.html#numerical-feature",
    "href": "posts/2022-03-31-ml-nomenclature.html#numerical-feature",
    "title": "Machine Learning Nomenclature",
    "section": "Numerical feature",
    "text": "Numerical feature\nNumerical features are those that can be measured or counted and have some ascending or descending order assigned to them.\nIt has other names * Continious feature * Quantitative feature\nNumerical features can be of two types * Discrete * Continous"
  },
  {
    "objectID": "posts/2022-03-31-ml-nomenclature.html#discrete-feature",
    "href": "posts/2022-03-31-ml-nomenclature.html#discrete-feature",
    "title": "Machine Learning Nomenclature",
    "section": "Discrete feature",
    "text": "Discrete feature\nDiscrete (numerical) feature is one that has specified values and are usually counted e.g. number of facebook like, number of tickets sold etc."
  },
  {
    "objectID": "posts/2022-03-31-ml-nomenclature.html#continous-feature",
    "href": "posts/2022-03-31-ml-nomenclature.html#continous-feature",
    "title": "Machine Learning Nomenclature",
    "section": "Continous feature",
    "text": "Continous feature\nContinous (numerical) feature is one that can have any value assigned to it, and is usually measured e.g. temperature, wind speed etc.\nData\n├── Categorical / Qualitative\n│   ├── Nominal\n│   └── Ordinal\n└── Numerical / Quantitative\n    ├── Discrete\n    └── Continous"
  },
  {
    "objectID": "posts/2022-03-31-ml-nomenclature.html#classification",
    "href": "posts/2022-03-31-ml-nomenclature.html#classification",
    "title": "Machine Learning Nomenclature",
    "section": "Classification",
    "text": "Classification\nIf the target feature is categorical then the ML task is called classification."
  },
  {
    "objectID": "posts/2022-03-31-ml-nomenclature.html#regression",
    "href": "posts/2022-03-31-ml-nomenclature.html#regression",
    "title": "Machine Learning Nomenclature",
    "section": "Regression",
    "text": "Regression\nIf the target feature is numerical then the ML task is called regression."
  },
  {
    "objectID": "posts/2022-03-31-ml-nomenclature.html#positive-class",
    "href": "posts/2022-03-31-ml-nomenclature.html#positive-class",
    "title": "Machine Learning Nomenclature",
    "section": "Positive class",
    "text": "Positive class\nIn binary classification the output class is usually labelled as positive or negative. The positive class is the thing we are testing for. For example, positive class for an email classifier is ‘spam’, and positive class for a medical test can be ‘tumor’."
  },
  {
    "objectID": "posts/2022-03-31-ml-nomenclature.html#negative-class",
    "href": "posts/2022-03-31-ml-nomenclature.html#negative-class",
    "title": "Machine Learning Nomenclature",
    "section": "Negative class",
    "text": "Negative class\nNegative class is the opposite to positive class. For example, negative class for an email classifier is ‘not spam’, and a negative class for a medical test can be ‘not tumor’."
  },
  {
    "objectID": "posts/2022-03-31-ml-nomenclature.html#true-positive-tp",
    "href": "posts/2022-03-31-ml-nomenclature.html#true-positive-tp",
    "title": "Machine Learning Nomenclature",
    "section": "True positive (TP)",
    "text": "True positive (TP)\nModel correctly predicted the positve class"
  },
  {
    "objectID": "posts/2022-03-31-ml-nomenclature.html#true-negative-tn",
    "href": "posts/2022-03-31-ml-nomenclature.html#true-negative-tn",
    "title": "Machine Learning Nomenclature",
    "section": "True negative (TN)",
    "text": "True negative (TN)\nModel correctly predicted the negative class"
  },
  {
    "objectID": "posts/2022-03-31-ml-nomenclature.html#false-positive-fp",
    "href": "posts/2022-03-31-ml-nomenclature.html#false-positive-fp",
    "title": "Machine Learning Nomenclature",
    "section": "False positive (FP)",
    "text": "False positive (FP)\nModel incorrectly predicted the positive class. Actual class is negative. It has other names * Type I error"
  },
  {
    "objectID": "posts/2022-03-31-ml-nomenclature.html#false-negative-fn",
    "href": "posts/2022-03-31-ml-nomenclature.html#false-negative-fn",
    "title": "Machine Learning Nomenclature",
    "section": "False negative (FN)",
    "text": "False negative (FN)\nModel incorrectly predicted the negative class. Actual class is positive. It has other names * Type II error"
  },
  {
    "objectID": "posts/2022-03-31-ml-nomenclature.html#accuracy",
    "href": "posts/2022-03-31-ml-nomenclature.html#accuracy",
    "title": "Machine Learning Nomenclature",
    "section": "Accuracy",
    "text": "Accuracy\nAccuracy = (TP + TN) / (TP + TN + FP + FN)"
  },
  {
    "objectID": "posts/2022-03-31-ml-nomenclature.html#precision",
    "href": "posts/2022-03-31-ml-nomenclature.html#precision",
    "title": "Machine Learning Nomenclature",
    "section": "Precision",
    "text": "Precision\nIt tells how accurate the positive predictions are.\nPrecision = TP / (TP + FP)\nIt is a good metric when cost of false positives is high."
  },
  {
    "objectID": "posts/2022-03-31-ml-nomenclature.html#true-positive-rate-tpr",
    "href": "posts/2022-03-31-ml-nomenclature.html#true-positive-rate-tpr",
    "title": "Machine Learning Nomenclature",
    "section": "True Positive Rate (TPR)",
    "text": "True Positive Rate (TPR)\nTPR = TP / (TP + FN)\nIt is the probability that an actual positive class will test positive.\nIt has other names * Recall * Sensitivity\nTrue positive is the y-axis in an ROC curve. It is a good metric when cost of false negatives is high."
  },
  {
    "objectID": "posts/2022-03-31-ml-nomenclature.html#false-positive-rate-fpr",
    "href": "posts/2022-03-31-ml-nomenclature.html#false-positive-rate-fpr",
    "title": "Machine Learning Nomenclature",
    "section": "False Positive Rate (FPR)",
    "text": "False Positive Rate (FPR)\nFPR = FP / (FP + TN)\nIt has other names * 1 - specificity\nIt is x-axis on ROC curve."
  },
  {
    "objectID": "posts/2022-03-31-ml-nomenclature.html#roc-curve",
    "href": "posts/2022-03-31-ml-nomenclature.html#roc-curve",
    "title": "Machine Learning Nomenclature",
    "section": "ROC Curve",
    "text": "ROC Curve\nReceiver Operating Characteristic (ROC) is a curve of TPR vs FPR at different classification thresholds."
  },
  {
    "objectID": "posts/2022-03-31-ml-nomenclature.html#true-negative-rate-tnr",
    "href": "posts/2022-03-31-ml-nomenclature.html#true-negative-rate-tnr",
    "title": "Machine Learning Nomenclature",
    "section": "True Negative Rate (TNR)",
    "text": "True Negative Rate (TNR)\nTNR = TN / (TN + FP)\nIt is the probability of a negative class to test negative.\nIt has other names * Specificity"
  },
  {
    "objectID": "posts/2022-04-25-bluebook-for-bulldozers.html#environment-details",
    "href": "posts/2022-04-25-bluebook-for-bulldozers.html#environment-details",
    "title": "Kaggle - Blue Book for Bulldozers",
    "section": "Environment Details",
    "text": "Environment Details\n\n\nCode\nfrom platform import python_version\nimport sklearn, numpy, matplotlib, pandas\n\nprint(\"python==\" + python_version())\nprint(\"sklearn==\" + sklearn.__version__)\nprint(\"numpy==\" + numpy.__version__)\nprint(\"pandas==\" + pandas.__version__)\nprint(\"matplotlib==\" + matplotlib.__version__)\n\n\npython==3.8.8\nsklearn==1.0.2\nnumpy==1.20.1\npandas==1.2.3\nmatplotlib==3.5.1\n\n\n\n##\n# Notebook settings\nimport pandas as pd\n\n# display all dataframe columns \npd.set_option('display.max_columns', None)"
  },
  {
    "objectID": "posts/2022-04-25-bluebook-for-bulldozers.html#prepare-the-dataset",
    "href": "posts/2022-04-25-bluebook-for-bulldozers.html#prepare-the-dataset",
    "title": "Kaggle - Blue Book for Bulldozers",
    "section": "Prepare the dataset",
    "text": "Prepare the dataset\n\nDownload the dataset files\n\nTrain.zip and extract Train.csv. This is our training dataset.\nValid.csv. This is our validation dataset.\nTest.csv. This is our test dataset.\n\nThis dataset can be downloaded from the Kaggle competition site, and extracted files should be placed under folder ./datasets/2022-04-35-bluebook-for-bulldozers/. These files are made available with this notebook in the GitHub repository and can be downloaded from there too. If you are using Git then it will not download them from the remote server as they exceed 50MB limit (read more here). For working with large files Git needs an extra extension to work with them called git-lfs.\nFollow the steps from Git-LFS site to install it on the system. To install it directly from the notebook (running on Linux) use these commands\n\n## \n# download and install git-lfs. Uncomment them as execute.\n\n# !curl -s https://packagecloud.io/install/repositories/github/git-lfs/script.rpm.sh | sudo bash\n# !sudo yum install git-lfs -y\n# !git lfs install\n\nOnce git-lfs is installed use the below command to download large files from the remote server (GitHub).\n\n##\n# uncomment them and execute\n\n# git lfs fetch --all\n# git lfs checkout\n\n\n\nTake an initial look at the training data\nLoad the training data and look for the following information * column names * column data types * how much data is missing? * sample data elements\n\n##\n# load the training dataset\ndataset_path = 'datasets/2022-04-35-bluebook-for-bulldozers/'\n\ndf_raw = pd.read_csv(dataset_path+'Train.csv', low_memory=False)\ndf = df_raw.copy()\n\n\n##\n# print training dataset summary information\ndf_info = pd.DataFrame()\ndf_info['sample'] = df.iloc[0]\ndf_info['data_type'] = df.dtypes\ndf_info['percent_missing'] = 100*df.isnull().sum() / len(df)\nprint(f\"Total features: {len(df.columns)}\")\ndf_info.sort_values('percent_missing')\n\n\n\n\n\n\n\n\nsample\ndata_type\npercent_missing\n\n\n\n\nSalesID\n1139246\nint64\n0.000000\n\n\nstate\nAlabama\nobject\n0.000000\n\n\nfiProductClassDesc\nWheel Loader - 110.0 to 120.0 Horsepower\nobject\n0.000000\n\n\nfiBaseModel\n521\nobject\n0.000000\n\n\nfiModelDesc\n521D\nobject\n0.000000\n\n\nProductGroup\nWL\nobject\n0.000000\n\n\nsaledate\n11/16/2006 0:00\nobject\n0.000000\n\n\ndatasource\n121\nint64\n0.000000\n\n\nModelID\n3157\nint64\n0.000000\n\n\nMachineID\n999089\nint64\n0.000000\n\n\nSalePrice\n66000\nint64\n0.000000\n\n\nYearMade\n2004\nint64\n0.000000\n\n\nProductGroupDesc\nWheel Loader\nobject\n0.000000\n\n\nEnclosure\nEROPS w AC\nobject\n0.081022\n\n\nauctioneerID\n3.0\nfloat64\n5.019882\n\n\nHydraulics\n2 Valve\nobject\n20.082269\n\n\nfiSecondaryDesc\nD\nobject\n34.201558\n\n\nCoupler\nNone or Unspecified\nobject\n46.662013\n\n\nForks\nNone or Unspecified\nobject\n52.115425\n\n\nProductSize\nNaN\nobject\n52.545964\n\n\nTransmission\nNaN\nobject\n54.320972\n\n\nRide_Control\nNone or Unspecified\nobject\n62.952696\n\n\nMachineHoursCurrentMeter\n68.0\nfloat64\n64.408850\n\n\nDrive_System\nNaN\nobject\n73.982923\n\n\nRipper\nNaN\nobject\n74.038766\n\n\nUndercarriage_Pad_Width\nNaN\nobject\n75.102026\n\n\nThumb\nNaN\nobject\n75.247616\n\n\nStick_Length\nNaN\nobject\n75.265067\n\n\nPattern_Changer\nNaN\nobject\n75.265067\n\n\nGrouser_Type\nNaN\nobject\n75.281271\n\n\nTrack_Type\nNaN\nobject\n75.281271\n\n\nTire_Size\nNone or Unspecified\nobject\n76.386912\n\n\nTravel_Controls\nNaN\nobject\n80.097476\n\n\nBlade_Type\nNaN\nobject\n80.097725\n\n\nTurbocharged\nNaN\nobject\n80.271985\n\n\nStick\nNaN\nobject\n80.271985\n\n\nPad_Type\nNaN\nobject\n80.271985\n\n\nBackhoe_Mounting\nNaN\nobject\n80.387161\n\n\nfiModelDescriptor\nNaN\nobject\n82.070676\n\n\nUsageBand\nLow\nobject\n82.639078\n\n\nDifferential_Type\nStandard\nobject\n82.695918\n\n\nSteering_Controls\nConventional\nobject\n82.706388\n\n\nfiModelSeries\nNaN\nobject\n85.812901\n\n\nCoupler_System\nNaN\nobject\n89.165971\n\n\nGrouser_Tracks\nNaN\nobject\n89.189903\n\n\nHydraulics_Flow\nNaN\nobject\n89.189903\n\n\nScarifier\nNaN\nobject\n93.710190\n\n\nPushblock\nNaN\nobject\n93.712932\n\n\nEngine_Horsepower\nNaN\nobject\n93.712932\n\n\nEnclosure_Type\nNaN\nobject\n93.712932\n\n\nBlade_Width\nNaN\nobject\n93.712932\n\n\nBlade_Extension\nNaN\nobject\n93.712932\n\n\nTip_Control\nNaN\nobject\n93.712932\n\n\n\n\n\n\n\n\n##\n# print some unique values against each feature\ndef sniff(df, rows=7):\n    \"\"\"\n    For each column return a set of unique values\n    \"\"\"\n    data = {}\n    for col in df.columns:\n        data[col] = df[col].unique()[:rows]\n    \n    return pd.DataFrame.from_dict(data, orient='index').T\n\n\nsniff(df)\n\n\n\n\n\n\n\n\nSalesID\nSalePrice\nMachineID\nModelID\ndatasource\nauctioneerID\nYearMade\nMachineHoursCurrentMeter\nUsageBand\nsaledate\nfiModelDesc\nfiBaseModel\nfiSecondaryDesc\nfiModelSeries\nfiModelDescriptor\nProductSize\nfiProductClassDesc\nstate\nProductGroup\nProductGroupDesc\nDrive_System\nEnclosure\nForks\nPad_Type\nRide_Control\nStick\nTransmission\nTurbocharged\nBlade_Extension\nBlade_Width\nEnclosure_Type\nEngine_Horsepower\nHydraulics\nPushblock\nRipper\nScarifier\nTip_Control\nTire_Size\nCoupler\nCoupler_System\nGrouser_Tracks\nHydraulics_Flow\nTrack_Type\nUndercarriage_Pad_Width\nStick_Length\nThumb\nPattern_Changer\nGrouser_Type\nBackhoe_Mounting\nBlade_Type\nTravel_Controls\nDifferential_Type\nSteering_Controls\n\n\n\n\n0\n1139246\n66000\n999089\n3157\n121\n3.0\n2004\n68.0\nLow\n11/16/2006 0:00\n521D\n521\nD\nNaN\nNaN\nNaN\nWheel Loader - 110.0 to 120.0 Horsepower\nAlabama\nWL\nWheel Loader\nNaN\nEROPS w AC\nNone or Unspecified\nNaN\nNone or Unspecified\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n2 Valve\nNaN\nNaN\nNaN\nNaN\nNone or Unspecified\nNone or Unspecified\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nStandard\nConventional\n\n\n1\n1139248\n57000\n117657\n77\n132\n1.0\n1996\n4640.0\nHigh\n3/26/2004 0:00\n950FII\n950\nF\nII\nLC\nMedium\nWheel Loader - 150.0 to 175.0 Horsepower\nNorth Carolina\nSSL\nSkid Steer Loaders\nFour Wheel Drive\nOROPS\nNaN\nNone or Unspecified\nNaN\nExtended\nPowershuttle\nNone or Unspecified\nYes\nNone or Unspecified\nNone or Unspecified\nNo\nAuxiliary\nNone or Unspecified\nNone or Unspecified\nYes\nSideshift & Tip\n23.5\nNaN\nNone or Unspecified\nNone or Unspecified\nStandard\nSteel\nNone or Unspecified\nNone or Unspecified\nNone or Unspecified\nNone or Unspecified\nDouble\nNone or Unspecified\nPAT\nNone or Unspecified\nNaN\nNaN\n\n\n2\n1139249\n10000\n434808\n7009\n136\n2.0\n2001\n2838.0\nMedium\n2/26/2004 0:00\n226\n226\nNaN\n-6E\n6\nSmall\nSkid Steer Loader - 1351.0 to 1601.0 Lb Operat...\nNew York\nTEX\nTrack Excavators\nTwo Wheel Drive\nEROPS\nYes\nReversible\nNo\nStandard\nStandard\nYes\nNone or Unspecified\n12'\nLow Profile\nVariable\nNaN\nYes\nYes\nNone or Unspecified\nNone or Unspecified\nNaN\nManual\nYes\nYes\nHigh Flow\nRubber\n16 inch\n11' 0\"\nHydraulic\nYes\nTriple\nYes\nNone or Unspecified\nDifferential Steer\nLimited Slip\nCommand Control\n\n\n3\n1139251\n38500\n1026470\n332\n149\n11.0\n2007\n3486.0\nNaN\n5/19/2011 0:00\nPC120-6E\nPC120\nG\nLC\nL\nLarge / Medium\nHydraulic Excavator, Track - 12.0 to 14.0 Metr...\nTexas\nBL\nBackhoe Loaders\nNo\nNaN\nNone\nStreet\nYes\nNone\nPowershift\nNone\nNone\n14'\nHigh Profile\nNone\nStandard\nNone\nSingle Shank\nNone\nTip\n13\"\nHydraulic\nNone\nNone\nNone or Unspecified\nNone\n32 inch\n15' 9\"\nManual\nNo\nSingle\nNone\nSemi U\nLever\nNo Spin\nFour Wheel Standard\n\n\n4\n1139253\n11000\n1057373\n17311\n172\n4.0\n1993\n722.0\nNone\n7/23/2009 0:00\nS175\nS175\nE\n-5\nLT\nMini\nSkid Steer Loader - 1601.0 to 1751.0 Lb Operat...\nArizona\nTTT\nTrack Type Tractors\nAll Wheel Drive\nEROPS AC\nNone\nGrouser\nNone\nNone\nNone or Unspecified\nNone\nNone\n13'\nNone\nNone\nBase + 1 Function\nNone\nMulti Shank\nNone\nNone\n26.5\nNone\nNone\nNone\nNone\nNone\n28 inch\n10' 2\"\nNone\nNone\nNone\nNone\nVPAT\nFinger Tip\nLocking\nWheel\n\n\n5\n1139255\n26500\n1001274\n4605\nNone\n7.0\n2008\n508.0\nNone\n12/18/2008 0:00\n310G\n310\nHAG\nIII\nCR\nLarge\nBackhoe Loader - 14.0 to 15.0 Ft Standard Digg...\nFlorida\nMG\nMotor Graders\nNone\nNO ROPS\nNone\nNone\nNone\nNone\nHydrostatic\nNone\nNone\n16'\nNone\nNone\nBase + 3 Function\nNone\nNone\nNone\nNone\n29.5\nNone\nNone\nNone\nNone\nNone\n30 inch\n10' 6\"\nNone\nNone\nNone\nNone\nStraight\n2 Pedal\nNone\nNo\n\n\n6\n1139256\n21000\n772701\n1937\nNone\n99.0\n1000\n11540.0\nNone\n8/26/2004 0:00\n790ELC\n790\nB\n-1\nSB\nCompact\nHydraulic Excavator, Track - 21.0 to 24.0 Metr...\nIllinois\nNone\nNone\nNone\nNone or Unspecified\nNone\nNone\nNone\nNone\nAutoshift\nNone\nNone\n&lt;12'\nNone\nNone\n4 Valve\nNone\nNone\nNone\nNone\n14\"\nNone\nNone\nNone\nNone\nNone\n22 inch\n9' 10\"\nNone\nNone\nNone\nNone\nAngle\nPedal\nNone\nNone\n\n\n\n\n\n\n\nFrom this first look at the data, we can see that * data is of three types * numeric * string * datetime * some columns have missing data up to 94% e.g. Tip_Control * missing data is represented as * NaN * None or unspecified * some columns’ data types need to be corrected for example * SaleID, MachineID are represented as integers but they are categorical nominal features meaning each value is discrete and has no relation among them * UsageBand is of type string but is a categorical ordinal feature meaning their values cannot be measured but have some order between them * Tire_size, Stick_length are actual measurements and need to be converted to appropriate units"
  },
  {
    "objectID": "posts/2022-04-25-bluebook-for-bulldozers.html#baseline-model",
    "href": "posts/2022-04-25-bluebook-for-bulldozers.html#baseline-model",
    "title": "Kaggle - Blue Book for Bulldozers",
    "section": "Baseline Model",
    "text": "Baseline Model\nIt is a good idea to create a baseline model early in the data science project as it can help to establish a baseline for * time it takes to train a model * if the baseline model is taking too much time then we may use a smaller set of the training data for further steps * feature importances * it can help us establish a relationship between features and the target * help us remove features that have no relationship with the target sooner * model performance * we can take this model performance as a baseline, and compare it to see how much cleanup and feature engineering steps improve the model performance\nFor the baseline model, we would have to rely on numerical features as they don’t require any preprocessing and can be readily used. Some numerical features have too much missing data so we have to be selective here.\n\n##\n# filter columns that are not string along with their percentage of missing data\nnumerical_features = df_info.loc[df_info.data_type != 'object'].sort_values('percent_missing')\nnumerical_features\n\n\n\n\n\n\n\n\nsample\ndata_type\npercent_missing\n\n\n\n\nSalesID\n1139246\nint64\n0.000000\n\n\nSalePrice\n66000\nint64\n0.000000\n\n\nMachineID\n999089\nint64\n0.000000\n\n\nModelID\n3157\nint64\n0.000000\n\n\ndatasource\n121\nint64\n0.000000\n\n\nYearMade\n2004\nint64\n0.000000\n\n\nauctioneerID\n3.0\nfloat64\n5.019882\n\n\nMachineHoursCurrentMeter\n68.0\nfloat64\n64.408850\n\n\n\n\n\n\n\nFrom these numerical features MachineHoursCurrentMeter has around 64% missing data. Let’s keep this feature as well for our baseline model.\n\n##\n# establish target and baseline features\ntarget = 'SalePrice' # this is the feature we are trying to predict\nbaseline_features = list(numerical_features.index)\nbaseline_features.remove(target) # remove target feature form input variables\nbaseline_features\n\n['SalesID',\n 'MachineID',\n 'ModelID',\n 'datasource',\n 'YearMade',\n 'auctioneerID',\n 'MachineHoursCurrentMeter']\n\n\nWe have established our target and features, and can now train our baseline model. We will use only RandomForrest for this dataset. We have 7 features to learn from so let’s start with n_estimators=70\n\nfrom sklearn.ensemble import RandomForestRegressor\n\nX, y = df[baseline_features], df[target]\nX = X.fillna(0) # replace missing numerical values with 0\n\nrf = RandomForestRegressor(n_estimators=70, oob_score=True, n_jobs=-1, verbose=1)\nrf.fit(X, y)\noob_score = rf.oob_score_\noob_score\n\n[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 4 concurrent workers.\n[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:  1.2min\n[Parallel(n_jobs=-1)]: Done  70 out of  70 | elapsed:  1.9min finished\n\n\n0.7901663917842495\n\n\nBesides the OOB score, we can also interpret our model by looking into the trained model trees’ depth and leaves. If OOB score is same but our trees are smaller with fewer nodes then that model is better and more generalized. Larger trees make the model more complex and less generalized. For this let’s create two more functions.\n\nimport numpy as np\n\ndef rf_n_leaves(rf):\n    \"\"\"\n    return the total number of nodes in all the trees of the forrest.\n    \"\"\"\n    return sum(est.tree_.n_leaves for est in rf.estimators_)\n\ndef rf_m_depth(rf):\n    \"\"\"\n    return a median height of all the trees of the forrest.\n    \"\"\"\n    return np.median([est.tree_.max_depth for est in rf.estimators_])\n\n\n##\n# print model oob_score, number of forrest leaves and median height\nn_leaves = rf_n_leaves(rf)\nm_depth = rf_m_depth(rf)\n\nprint(f\"OOB scrore = {oob_score: .3f} \\nTree leaves = {n_leaves: ,d} \\nMedian depth = {m_depth}\")\n\nOOB scrore =  0.790 \nTree leaves =  16,209,726 \nMedian depth = 53.5\n\n\nOur baseline model has an OOB score of just around 79% which is not bad as a starter. Now let’s also plot the feature importance for this model.\n\ndef plot_feature_importance(feature_importance, feature_names, figsize=(7,7)):\n    \"\"\"\n    plot the feature importances in a bar graph along with feature names.\n    \"\"\"\n    fimp = pd.Series(feature_importance, feature_names)\n    fimp.nlargest(len(fimp)).plot(figsize=figsize, kind='barh').invert_yaxis()\n\n\nfeature_importance = rf.feature_importances_\nfeature_names = X.columns\nplot_feature_importance(feature_importance, feature_names)\n\n\n\n\n\n\n\n\nFrom this feature importance plot, we can see that * ModelID is the highest predictor of SalePrice. This could be because vehicles belonging to a certain ModelID category could have their SalePrice in the same range. * SalesID and MachineID are coming up next as important features. This is not a good signal as both these features are unique for sale transactions and machine identification. MachineID also has inconsistencies as noted in this kaggle discussion. A model using these features will not be generalized. It would be better if we remove these features altogether otherwise they can affect the model’s performance. * YearMade comes next which also makes sense as older vehicles will have less price compared to the new ones."
  },
  {
    "objectID": "posts/2022-04-25-bluebook-for-bulldozers.html#cleaning-up",
    "href": "posts/2022-04-25-bluebook-for-bulldozers.html#cleaning-up",
    "title": "Kaggle - Blue Book for Bulldozers",
    "section": "Cleaning up",
    "text": "Cleaning up\nIn this section we will remove unimportanct features and fix the data types of remaining features.\n\nRemove ID columns\nAs noted in last section we noted that following ID features can be removed from the dataset. * SalesID * MachineID\n\ndel df['SalesID']\ndel df['MachineID']"
  },
  {
    "objectID": "posts/2022-04-25-bluebook-for-bulldozers.html#fix-data-types-and-data-issues",
    "href": "posts/2022-04-25-bluebook-for-bulldozers.html#fix-data-types-and-data-issues",
    "title": "Kaggle - Blue Book for Bulldozers",
    "section": "Fix data types and data issues",
    "text": "Fix data types and data issues\nLet’s visit each feature from our dataset and check whether we need to fix the data type. Use df_info created in the last section to verify the data types of each feature.\n\nNumerical features\nLet’s first visit the numerical feature.\n\nauctioneerID\nIt has the datatype as float64 but this feature is actually categorical nominal as each ID is discrete and has no relation between them. It should be of type str. So let’s fix that.\n\ndf['auctioneerID'] = df['auctioneerID'].astype(str)\n\n\n\n\nDatetime feature\nLet’s visit DateTime features and correct their data type #### saledate ‘saledate’ is a DateTime feature. So let’s correct its data type.\n\ndf['saledate'] = pd.to_datetime(df['saledate'])\n\n\n\nCategorical features\nLet’s now visit the categorical features.\nFor categorical features, there is no better way than printing the unique values for each column and spend some time analyzing the values. Analyze * if the feature has some missing values * if there are any missing values but are represented by some other value like ‘Unspecified’, ‘None or Unspecified’ * keep a separate sheet with all the features and make notes for each feature like * there are no further actions required. The feature is good for use * need to replace missing values * any other observations * etc.\n\nTransform missing values\nAfter visiting all the features we have found that missing values are represented in multiple ways like * Unspecified * None or Unspecified * None * #NAME? * “”\nSo we would transform and replace all these values with np.nan so they all represent the same thing.\n\n##\n# before transformation. \n# let's use this feature to verify results.\ndf['Hydraulics'].unique()\n\narray(['2 Valve', 'Auxiliary', nan, 'Standard', 'Base + 1 Function',\n       'Base + 3 Function', '4 Valve', '3 Valve', 'Base + 2 Function',\n       'Base + 4 Function', 'None or Unspecified', 'Base + 5 Function',\n       'Base + 6 Function'], dtype=object)\n\n\n\n#collapse-output\ndef normalize_str_values(df):\n    \"\"\"\n    normalize dataframe str values\n    * transform case to lowercase\n    * replace missing values with np.nan\n    \"\"\"\n    for col in df.columns:\n        if df[col].dtype == object: \n            print(f\"normalize column: {col}\")\n            df[col] = df[col].str.lower()\n            df[col] = df[col].fillna(np.nan)\n            df[col] = df[col].replace('unspecified', np.nan)\n            df[col] = df[col].replace('none or unspecified', np.nan)\n            df[col] = df[col].replace('none', np.nan)\n            df[col] = df[col].replace('#name?', np.nan)\n            df[col] = df[col].replace('', np.nan)\n\nnormalize_str_values(df)\n\nnormalize column: auctioneerID\nnormalize column: UsageBand\nnormalize column: fiModelDesc\nnormalize column: fiBaseModel\nnormalize column: fiSecondaryDesc\nnormalize column: fiModelSeries\nnormalize column: fiModelDescriptor\nnormalize column: ProductSize\nnormalize column: fiProductClassDesc\nnormalize column: state\nnormalize column: ProductGroup\nnormalize column: ProductGroupDesc\nnormalize column: Drive_System\nnormalize column: Enclosure\nnormalize column: Forks\nnormalize column: Pad_Type\nnormalize column: Ride_Control\nnormalize column: Stick\nnormalize column: Transmission\nnormalize column: Turbocharged\nnormalize column: Blade_Extension\nnormalize column: Blade_Width\nnormalize column: Enclosure_Type\nnormalize column: Engine_Horsepower\nnormalize column: Hydraulics\nnormalize column: Pushblock\nnormalize column: Ripper\nnormalize column: Scarifier\nnormalize column: Tip_Control\nnormalize column: Tire_Size\nnormalize column: Coupler\nnormalize column: Coupler_System\nnormalize column: Grouser_Tracks\nnormalize column: Hydraulics_Flow\nnormalize column: Track_Type\nnormalize column: Undercarriage_Pad_Width\nnormalize column: Stick_Length\nnormalize column: Thumb\nnormalize column: Pattern_Changer\nnormalize column: Grouser_Type\nnormalize column: Backhoe_Mounting\nnormalize column: Blade_Type\nnormalize column: Travel_Controls\nnormalize column: Differential_Type\nnormalize column: Steering_Controls\n\n\n\n##\n# after transformation.\n# remember that transformation is applied to all string type columns. We are using just one column to verify the results.\ndf['Hydraulics'].unique()\n\narray(['2 valve', 'auxiliary', nan, 'standard', 'base + 1 function',\n       'base + 3 function', '4 valve', '3 valve', 'base + 2 function',\n       'base + 4 function', 'base + 5 function', 'base + 6 function'],\n      dtype=object)\n\n\n\n\nTransform measurements\nSome features are represented as a string but actually they are numerical measurement values. For example * Tire_Size has the size in inches with a symbol attached \" * Undercarriage_Pad_Width has the size in inches with the unit attached inch * Blade_Width has the size in cm with a symbol attached '. It also has values less the 12cm represented as &lt;12' * Stick_Length has values in both feet and inches. We can simply convert them from 19\\'8\" to 19.8 * After the above transformations, their data types should be converted to numeric\nlet’s apply these changes to our dataset.\n\n##\n# before transformation\nfor col in ['Tire_Size', 'Undercarriage_Pad_Width', 'Blade_Width', 'Stick_Length']:\n    print(f\"**{col}**: \", df[col].unique())\n\n**Tire_Size**:  [nan '23.5' '13\"' '26.5' '29.5' '14\"' '20.5' '17.5\"' '15.5\"' '20.5\"'\n '17.5' '7.0\"' '15.5' '23.5\"' '10\"' '23.1\"' '10 inch']\n**Undercarriage_Pad_Width**:  [nan '16 inch' '32 inch' '28 inch' '30 inch' '22 inch' '24 inch' '18 inch'\n '36 inch' '20 inch' '27 inch' '15 inch' '26 inch' '34 inch' '33 inch'\n '14 inch' '31 inch' '25 inch' '31.5 inch']\n**Blade_Width**:  [nan \"12'\" \"14'\" \"13'\" \"16'\" \"&lt;12'\"]\n**Stick_Length**:  [nan '11\\' 0\"' '15\\' 9\"' '10\\' 2\"' '10\\' 6\"' '9\\' 10\"' '10\\' 10\"' '9\\' 6\"'\n '9\\' 7\"' '12\\' 8\"' '8\\' 2\"' '8\\' 6\"' '9\\' 8\"' '12\\' 10\"' '11\\' 10\"'\n '8\\' 10\"' '8\\' 4\"' '12\\' 4\"' '9\\' 5\"' '6\\' 3\"' '14\\' 1\"' '13\\' 7\"'\n '13\\' 10\"' '13\\' 9\"' '7\\' 10\"' '15\\' 4\"' '9\\' 2\"' '24\\' 3\"' '19\\' 8\"']\n\n\n\ndf['Stick_Length'] = df['Stick_Length'].replace(r\"' \", \".\", regex=True)\nfor col in ['Tire_Size', 'Undercarriage_Pad_Width', 'Blade_Width', 'Stick_Length']:\n    df[col] = df[col].str.extract(r'([0-9.]*)', expand=True)\n    df[col] = df[col].replace('', np.nan)\n    df[col] = pd.to_numeric(df[col])\n\n\n##\n# after transformation\nfor col in ['Tire_Size', 'Undercarriage_Pad_Width', 'Blade_Width', 'Stick_Length']:\n    print(f\"**{col}**: \", df[col].unique())\n\n**Tire_Size**:  [ nan 23.5 13.  26.5 29.5 14.  20.5 17.5 15.5  7.  10.  23.1]\n**Undercarriage_Pad_Width**:  [ nan 16.  32.  28.  30.  22.  24.  18.  36.  20.  27.  15.  26.  34.\n 33.  14.  31.  25.  31.5]\n**Blade_Width**:  [nan 12. 14. 13. 16.]\n**Stick_Length**:  [ nan 11.  15.9 10.2 10.6  9.1 10.1  9.6  9.7 12.8  8.2  8.6  9.8 12.1\n 11.1  8.1  8.4 12.4  9.5  6.3 14.1 13.7 13.1 13.9  7.1 15.4  9.2 24.3\n 19.8]"
  },
  {
    "objectID": "posts/2022-04-25-bluebook-for-bulldozers.html#dealing-with-missing-data",
    "href": "posts/2022-04-25-bluebook-for-bulldozers.html#dealing-with-missing-data",
    "title": "Kaggle - Blue Book for Bulldozers",
    "section": "Dealing with missing data",
    "text": "Dealing with missing data\n\nReplace missing numeric values\nFor numerical features, we will follow the following approach to replace missing values * For a column x create a new column x_na where x_na[i] is marked as True if x[i] is missing * Replace the missing values in the x column with a median value\n\ndef fix_missing_num(df, colname):\n    \"\"\"\n    replace missing values with\n    * median value\n    * flag the missing value in a separate *_na column\n    \"\"\"\n    df[colname+'_na'] = pd.isnull(df[colname])\n    df[colname].fillna(df[colname].median(), inplace=True)\n\n\nYearMade\n“YearMade” doesn’t show any missing values but if we look closely at the data we will find that some instances have the value “1000”. The year 1000 is very unlikely for any vehicle to be made in and we can consider these instances as missing values. Let’s do that\n\n##\n# befor transformation\ndf.plot.scatter('YearMade', 'SalePrice')\n\n\n\n\n\n\n\n\n\ndf.loc[df.YearMade==1000, 'YearMade'] = np.nan\n\n\n##\n# after transformation\ndf.plot.scatter('YearMade', 'SalePrice')\n\n\n\n\n\n\n\n\nThe plot now shows a more clear relationship between ‘YearMade’ and ‘SalePrice’. But the spike in the year 1920 is still concerning. Most probably it is also a recording error when the manufacturing year was not known then it was assigned some lowest available value in the system (similar to the year 1000). Let’s take this assumption that manufacturing years before 1950 are unknown and should be assigned np.nan\n\ndf.loc[df.YearMade&lt;1950, 'YearMade'] = np.nan\n\n\n##\n# after transformation\ndf.plot.scatter('YearMade', 'SalePrice')\n\n\n\n\n\n\n\n\nLet’s also replace the missing values with the function created above.\n\nfix_missing_num(df, 'YearMade')\n\n\n\nMachineHoursCurrentMeter\nThe next numerical feature that comes is MachineHoursCurrentMeter. This feature tells us the number of hours a machine has been in use when it was brought to the auction. So older machines are much more likely to have more hours on them as compared to newer machines. There should be a correlation between machine hours and the vehicle in use period (a period between manufacturing and auction). To verify this relationship we first need to find the period in years between manufacturing and auction. We have the ‘YearMade’ that tells us when the vehicle was made. We have the ‘saledate’ which is a DateTime string object but we can use it to find the ‘YearSold’.\n\ndf['YearSold'] = df['saledate'].dt.year\n\n\n##\n# verify that we have correct data\ndf[['saledate', 'YearSold']].head()\n\n\n\n\n\n\n\n\nsaledate\nYearSold\n\n\n\n\n0\n2006-11-16\n2006\n\n\n1\n2004-03-26\n2004\n\n\n2\n2004-02-26\n2004\n\n\n3\n2011-05-19\n2011\n\n\n4\n2009-07-23\n2009\n\n\n\n\n\n\n\nNow we can use ‘YearMade’ and ‘YearSold’ to find the number of years the vehicle remained in use. Let’s call this new column ‘YearsInUse’\n\ndf['YearsInUse'] = df['YearSold'] - df['YearMade']\n\n\n##\n# verify the results\ndf[['YearsInUse', 'YearSold', 'YearMade']].head()\n\n\n\n\n\n\n\n\nYearsInUse\nYearSold\nYearMade\n\n\n\n\n0\n2.0\n2006\n2004.0\n\n\n1\n8.0\n2004\n1996.0\n\n\n2\n3.0\n2004\n2001.0\n\n\n3\n10.0\n2011\n2001.0\n\n\n4\n2.0\n2009\n2007.0\n\n\n\n\n\n\n\nA sold year cannot be less than a manufacturing year. So let’s verify data integrity as well.\n\ndf.loc[df.YearsInUse&lt;0, ['YearsInUse', 'saledate', 'YearSold', 'YearMade']].head()\n\n\n\n\n\n\n\n\nYearsInUse\nsaledate\nYearSold\nYearMade\n\n\n\n\n24007\n-2.0\n1994-02-11\n1994\n1996.0\n\n\n24009\n-1.0\n1995-04-18\n1995\n1996.0\n\n\n24015\n-2.0\n1994-09-20\n1994\n1996.0\n\n\n24029\n-1.0\n1995-04-28\n1995\n1996.0\n\n\n24064\n-1.0\n1995-04-28\n1995\n1996.0\n\n\n\n\n\n\n\nYearInUse cannot have a negative value and this shows that either ‘YearMade’ or ‘saledate’ is incorrect. We can assume that error can be with ‘YearMade’ as this is an auction dataset and ‘saledate’ will be more reliable. For entries where ‘YearMade’ is greater than ‘YearSold’ we can replace ‘YearMade’ with ‘YearSold’ (better to have ‘YearsInUse’ equal to zero than negative).\n\ndf.loc[df.YearMade&gt;df.YearSold, 'YearMade'] = df.YearSold\n\nLet’s recalculate the ‘YearsInUse’ with corrected data.\n\ndf['YearsInUse'] = df['YearSold'] - df['YearMade']\n\nLet’s verify that the data is consistent and all vehicles have ‘YearMade’ less than their ‘YearSold’\n\ndf.loc[df.YearsInUse&lt;0, ['YearsInUse', 'saledate', 'YearSold', 'YearMade']].head()\n\n\n\n\n\n\n\n\nYearsInUse\nsaledate\nYearSold\nYearMade\n\n\n\n\n\n\n\n\n\nWe can now plot the relationship between ‘YearsInUse’ and ‘MachineHoursCurrentMeter’\n\ndf.plot.scatter('YearsInUse', 'MachineHoursCurrentMeter')\n\n\n\n\n\n\n\n\nThis plot shows that there is some relation between a vehicle being in use and its meter hours. As the ‘YearsInUse’ value increases we also see an increase in meter hours, but after around 15 ‘YearsInUse’ the relationship does not hold on and meter hours start dropping to zero. It means that MachineHoursCurrentMeter data has inconsistencies as many vehicles remained in use for multiple years but they also have zero meter readings. This is very unrealistic and vehicles will not be sitting idle for many years till their auction. It could be that the meter reading for them was not known and 0 could have been used for the ‘Unspecified or Unknown’ value.\nLet’s take this assumption and transform ‘MachineHoursCurrentMeter’ to correctly represent that\n\ndf.loc[df.MachineHoursCurrentMeter==0, 'MachineHoursCurrentMeter'] = np.nan\n\nAlso apply our missing values fix on this feature\n\nfix_missing_num(df, 'MachineHoursCurrentMeter')\n\n\n\nTire_Size\nThe next numerical feature is ‘Tire_Size’. We can plot the distribution of tire sizes to find any outliers.\n\ndf['Tire_Size'].hist()\n\n\n\n\n\n\n\n\n\n##\n# print tire sizes\nnp.sort(df['Tire_Size'].unique())\n\narray([ 7. , 10. , 13. , 14. , 15.5, 17.5, 20.5, 23.1, 23.5, 26.5, 29.5,\n        nan])\n\n\nThe plot does not show any outliers and data seems consistant, so we can apply our missing values fix on this feature.\n\nfix_missing_num(df, 'Tire_Size')\n\n\n\nStick_Length\nThe Next numerical feature is ‘Stick_Lenght’. Let’s plot the distribution to check for any outliers.\n\nnp.sort(df['Stick_Length'].unique())\n\narray([ 6.3,  7.1,  8.1,  8.2,  8.4,  8.6,  9.1,  9.2,  9.5,  9.6,  9.7,\n        9.8, 10.1, 10.2, 10.6, 11. , 11.1, 12.1, 12.4, 12.8, 13.1, 13.7,\n       13.9, 14.1, 15.4, 15.9, 19.8, 24.3,  nan])\n\n\n\ndf['Stick_Length'].plot.hist()\n\n\n\n\n\n\n\n\nThe above plot shows a normal distribution and no outliers. So we can apply our missing values fix on this feature.\n\nfix_missing_num(df, 'Stick_Length')\n\n\n\nUndercarriage_Pad_Width\nNext numerical feature is ‘Undercarriage_Pad_Width’. Let’s follow the same steps for this feature.\n\nnp.sort(df['Undercarriage_Pad_Width'].unique())\n\narray([14. , 15. , 16. , 18. , 20. , 22. , 24. , 25. , 26. , 27. , 28. ,\n       30. , 31. , 31.5, 32. , 33. , 34. , 36. ,  nan])\n\n\n\ndf['Undercarriage_Pad_Width'].plot.hist()\n\n\n\n\n\n\n\n\nThe distribution for this feature looks fine, and we can apply missing values fix on it.\n\nfix_missing_num(df, 'Undercarriage_Pad_Width')\n\n\n\nBlade_Width\nNext numerical feature in ‘Blade_Width’. Following the same steps as before.\n\nnp.sort(df['Blade_Width'].unique())\n\narray([12., 13., 14., 16., nan])\n\n\n\ndf['Blade_Width'].plot.hist()\n\n\n\n\n\n\n\n\nApply the fix on this feature.\n\nfix_missing_num(df, 'Blade_Width')\n\n\n\n\nReplace missing categorical values\nencoding and checking the importance We will now replace missing values for categorical features in the following way. * We will label encode them. We will treat them as ordinal features and assign them a numeric value * Missing values will automatically be assigned a value, and that will be 0\nSome important discussion points on treating nominal categorical features as ordinal and then encoding them. A more prevalent approach is to one hot encode (OHE) them. The drawback of OHE approach is that it makes the decision trees very unbalanced if the dataset has multiple categorical features with high variance. So instead of applying OHE to all features, we will do it in a two-step approach. First, we will label encode them and train a model on them. After that, we will check their feature importance, and if a feature comes up as an important with a low variance then we will use OHE for it. Otherwise we will leave them with label encoding.\nMore can be read about categorical features encoding from these references * The Mechanics of Machine Learning by Terence Parr and Jeremy Howard section 6.2 * Getting Deeper into Categorical Encodings for Machine Learning * One-Hot Encoding is making your Tree-Based Ensembles worse, here’s why?\nLet’s create some functions to encode our categorical features.\n\nfrom pandas.api.types import is_categorical_dtype, is_string_dtype\n\ndef df_string_to_cat(df):\n    for col in df.columns:\n        if is_string_dtype(df[col]):\n            print(f\"label encoding applied on {col}\")\n            df[col] = df[col].astype('category').cat.as_ordered()\n\ndef df_cat_to_catcode(df):\n    for col in df.columns:\n        if is_categorical_dtype(df[col]):\n            df[col] = df[col].cat.codes + 1\n\nPlease note that Pandas represents np.nan with category code “-1”, and so adding “1” in function df_cat_to_catcode shifts np.nan to 0 and all category codes to be 1 and above.\n\n##\n# before transformation\ndf.head(5).T.head(10)\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n\n\n\n\nSalePrice\n66000\n57000\n10000\n38500\n11000\n\n\nModelID\n3157\n77\n7009\n332\n17311\n\n\ndatasource\n121\n121\n121\n121\n121\n\n\nauctioneerID\n3.0\n3.0\n3.0\n3.0\n3.0\n\n\nYearMade\n2004.0\n1996.0\n2001.0\n2001.0\n2007.0\n\n\nMachineHoursCurrentMeter\n68.0\n4640.0\n2838.0\n3486.0\n722.0\n\n\nUsageBand\nlow\nlow\nhigh\nhigh\nmedium\n\n\nsaledate\n2006-11-16 00:00:00\n2004-03-26 00:00:00\n2004-02-26 00:00:00\n2011-05-19 00:00:00\n2009-07-23 00:00:00\n\n\nfiModelDesc\n521d\n950fii\n226\npc120-6e\ns175\n\n\nfiBaseModel\n521\n950\n226\npc120\ns175\n\n\n\n\n\n\n\n\n#collapse-output\n# apply the cat transformation\ndf_string_to_cat(df)\ndf_cat_to_catcode(df)\n\nlabel encoding applied on auctioneerID\nlabel encoding applied on UsageBand\nlabel encoding applied on fiModelDesc\nlabel encoding applied on fiBaseModel\nlabel encoding applied on fiSecondaryDesc\nlabel encoding applied on fiModelSeries\nlabel encoding applied on fiModelDescriptor\nlabel encoding applied on ProductSize\nlabel encoding applied on fiProductClassDesc\nlabel encoding applied on state\nlabel encoding applied on ProductGroup\nlabel encoding applied on ProductGroupDesc\nlabel encoding applied on Drive_System\nlabel encoding applied on Enclosure\nlabel encoding applied on Forks\nlabel encoding applied on Pad_Type\nlabel encoding applied on Ride_Control\nlabel encoding applied on Stick\nlabel encoding applied on Transmission\nlabel encoding applied on Turbocharged\nlabel encoding applied on Blade_Extension\nlabel encoding applied on Enclosure_Type\nlabel encoding applied on Engine_Horsepower\nlabel encoding applied on Hydraulics\nlabel encoding applied on Pushblock\nlabel encoding applied on Ripper\nlabel encoding applied on Scarifier\nlabel encoding applied on Tip_Control\nlabel encoding applied on Coupler\nlabel encoding applied on Coupler_System\nlabel encoding applied on Grouser_Tracks\nlabel encoding applied on Hydraulics_Flow\nlabel encoding applied on Track_Type\nlabel encoding applied on Thumb\nlabel encoding applied on Pattern_Changer\nlabel encoding applied on Grouser_Type\nlabel encoding applied on Backhoe_Mounting\nlabel encoding applied on Blade_Type\nlabel encoding applied on Travel_Controls\nlabel encoding applied on Differential_Type\nlabel encoding applied on Steering_Controls\n\n\n\n##\n# after transformation\ndf.head(5).T.head(10)\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n\n\n\n\nSalePrice\n66000\n57000\n10000\n38500\n11000\n\n\nModelID\n3157\n77\n7009\n332\n17311\n\n\ndatasource\n121\n121\n121\n121\n121\n\n\nauctioneerID\n23\n23\n23\n23\n23\n\n\nYearMade\n2004.0\n1996.0\n2001.0\n2001.0\n2007.0\n\n\nMachineHoursCurrentMeter\n68.0\n4640.0\n2838.0\n3486.0\n722.0\n\n\nUsageBand\n2\n2\n1\n1\n3\n\n\nsaledate\n2006-11-16 00:00:00\n2004-03-26 00:00:00\n2004-02-26 00:00:00\n2011-05-19 00:00:00\n2009-07-23 00:00:00\n\n\nfiModelDesc\n950\n1725\n331\n3674\n4208\n\n\nfiBaseModel\n296\n527\n110\n1375\n1529"
  },
  {
    "objectID": "posts/2022-04-25-bluebook-for-bulldozers.html#preprocessed-dataset",
    "href": "posts/2022-04-25-bluebook-for-bulldozers.html#preprocessed-dataset",
    "title": "Kaggle - Blue Book for Bulldozers",
    "section": "Preprocessed dataset",
    "text": "Preprocessed dataset\nAt this point, all our numerical and categorical features have been preprocessed. There should be no missing values, and all categorical features should have been encoded. Only DateTime columns are remaining to be processed and we will do that in the next section.\nLet’s verify the data using summary information.\n\ndf_info = pd.DataFrame()\ndf_info['sample'] = df.iloc[0]\ndf_info['data_type'] = df.dtypes\ndf_info['percent_missing'] = 100*df.isnull().sum() / len(df)\nprint(f\"Total features: {len(df.columns)}\")\ndf_info.sort_values('percent_missing')\n\nTotal features: 59\n\n\n\n\n\n\n\n\n\nsample\ndata_type\npercent_missing\n\n\n\n\nSalePrice\n66000\nint64\n0.0\n\n\nPushblock\n0\nint8\n0.0\n\n\nRipper\n0\nint8\n0.0\n\n\nScarifier\n0\nint8\n0.0\n\n\nTip_Control\n0\nint8\n0.0\n\n\nTire_Size\n20.5\nfloat64\n0.0\n\n\nCoupler\n0\nint8\n0.0\n\n\nCoupler_System\n0\nint8\n0.0\n\n\nGrouser_Tracks\n0\nint8\n0.0\n\n\nHydraulics_Flow\n0\nint8\n0.0\n\n\nTrack_Type\n0\nint8\n0.0\n\n\nUndercarriage_Pad_Width\n28.0\nfloat64\n0.0\n\n\nStick_Length\n9.7\nfloat64\n0.0\n\n\nHydraulics\n1\nint8\n0.0\n\n\nThumb\n0\nint8\n0.0\n\n\nGrouser_Type\n0\nint8\n0.0\n\n\nBackhoe_Mounting\n0\nint8\n0.0\n\n\nBlade_Type\n0\nint8\n0.0\n\n\nTravel_Controls\n0\nint8\n0.0\n\n\nDifferential_Type\n4\nint8\n0.0\n\n\nSteering_Controls\n2\nint8\n0.0\n\n\nYearMade_na\nFalse\nbool\n0.0\n\n\nYearSold\n2006\nint64\n0.0\n\n\nYearsInUse\n2.0\nfloat64\n0.0\n\n\nMachineHoursCurrentMeter_na\nFalse\nbool\n0.0\n\n\nTire_Size_na\nTrue\nbool\n0.0\n\n\nStick_Length_na\nTrue\nbool\n0.0\n\n\nPattern_Changer\n0\nint8\n0.0\n\n\nUndercarriage_Pad_Width_na\nTrue\nbool\n0.0\n\n\nEngine_Horsepower\n0\nint8\n0.0\n\n\nBlade_Width\n14.0\nfloat64\n0.0\n\n\nModelID\n3157\nint64\n0.0\n\n\ndatasource\n121\nint64\n0.0\n\n\nauctioneerID\n23\nint8\n0.0\n\n\nYearMade\n2004.0\nfloat64\n0.0\n\n\nMachineHoursCurrentMeter\n68.0\nfloat64\n0.0\n\n\nUsageBand\n2\nint8\n0.0\n\n\nsaledate\n2006-11-16 00:00:00\ndatetime64[ns]\n0.0\n\n\nfiModelDesc\n950\nint16\n0.0\n\n\nfiBaseModel\n296\nint16\n0.0\n\n\nfiSecondaryDesc\n40\nint16\n0.0\n\n\nfiModelSeries\n0\nint8\n0.0\n\n\nfiModelDescriptor\n0\nint16\n0.0\n\n\nEnclosure_Type\n0\nint8\n0.0\n\n\nProductSize\n0\nint8\n0.0\n\n\nstate\n1\nint8\n0.0\n\n\nProductGroup\n6\nint8\n0.0\n\n\nProductGroupDesc\n6\nint8\n0.0\n\n\nDrive_System\n0\nint8\n0.0\n\n\nEnclosure\n3\nint8\n0.0\n\n\nForks\n0\nint8\n0.0\n\n\nPad_Type\n0\nint8\n0.0\n\n\nRide_Control\n0\nint8\n0.0\n\n\nStick\n0\nint8\n0.0\n\n\nTransmission\n0\nint8\n0.0\n\n\nTurbocharged\n0\nint8\n0.0\n\n\nBlade_Extension\n0\nint8\n0.0\n\n\nfiProductClassDesc\n59\nint8\n0.0\n\n\nBlade_Width_na\nTrue\nbool\n0.0\n\n\n\n\n\n\n\nLet’s retrain our base model one more time but this time with all the features except datetime columns to see where we stand in our OOB score. Below is a utility function created to quickly iterate over model training.\n\ndef train_and_plot_model(df, target='SalePrice', drop_features=[], n_estimators=70, plot=True, verbose=1):\n    \"\"\"\n    A utility function to train a RandomForrest model on the provided data, and plot the feature importances.\n    \n    Parameters\n    ----------\n    df: pandas.DataFrame\n        input dataset to be used for training\n    target: str\n        target feature. this is the feature we are trying to predict\n    drop_features: list\n        any features to be dropped before training. Default is empty list.\n    n_estimators: int\n        number of estimators to be used for model training. Default is 50.\n    \"\"\"\n\n    # target = 'SalePrice' # this is the feature we are trying to predict\n    features = list(df.columns)\n\n    # remove target feature and other specified features form the input variables\n    features.remove(target)\n    for f in drop_features:\n        features.remove(f)\n\n    X, y = df[features], df[target]\n\n    rf = RandomForestRegressor(n_estimators, oob_score=True, n_jobs=-1, verbose=verbose)\n    rf.fit(X, y)\n    oob_score = rf.oob_score_\n\n    # get trained model leaves and depth    \n    n_leaves = rf_n_leaves(rf)\n    m_depth = rf_m_depth(rf)\n\n    # print trained model info\n    print(f\"OOB scrore = {oob_score: .3f} \\nTree leaves = {n_leaves: ,d} \\nMedian depth = {m_depth}\")\n\n    # plot trained model feature importance\n    feature_importance = rf.feature_importances_\n    if plot:\n        plot_feature_importance(feature_importance, features, (10,15))\n    \n    # return trained model, feature names, and their importances\n    return (rf, features, feature_importance, oob_score)\n\n\n##\n# keeping n_estimators same as previous base model i.e. 70\n(rf, feature_names, feature_importance, oob_pre) = train_and_plot_model(df, drop_features=['saledate'], n_estimators=70)\n\n[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 4 concurrent workers.\n[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:  2.7min\n[Parallel(n_jobs=-1)]: Done  70 out of  70 | elapsed:  4.2min finished\n\n\nOOB scrore =  0.904 \nTree leaves =  14,660,873 \nMedian depth = 45.0\n\n\n\n\n\n\n\n\n\nThis is a big improvement in our model performance. Our base model had 0.790 OOB score and now we are at 0.904. Our features count has also increased from 7 to 59, so we can take one more shot at it by increasing the estomators count (n_estimators). Let’s use 150 trees this time (double that last time) to see how much effect it can have on model performance.\n\n(rf, feature_names, feature_importance, oob_pre) = train_and_plot_model(df, drop_features=['saledate'], n_estimators=150, plot=False)\n\n[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 4 concurrent workers.\n[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:  2.8min\n[Parallel(n_jobs=-1)]: Done 150 out of 150 | elapsed:  9.2min finished\n\n\nOOB scrore =  0.906 \nTree leaves =  31,408,663 \nMedian depth = 46.0\n\n\n\n\n\n\n\n\n\nThough there is only a slight increase in model performance but it took us significantly more time to train the model. So we will keep our estimators low and revisit them during the tuning phase. With “70” estimators our model performance is\nOOB scrore =  0.904\nTree leaves =  14,660,873 \nMedian depth = 45.0\nAt this point, our features have correct data types and their missing values are properly adjusted. We can now focus on some feature engineering aspects. Before moving further let’s also save our dataset till this point so if we make an error we can restart from this checkpoint.\n\n##\n# store preprocessed data as a check point for this state\ndf.to_pickle(dataset_path+'preprocessed.pkl')\n\nWe have used pickle format to preserve data types for saved data.\n\n##\n# load preprocessed data (optional step)\n# df = pd.read_pickle(dataset_path+'preprocessed.pkl')\n# (rf, feature_names, feature_importance, oob_pre) = train_and_plot_model(df, drop_features=['saledate'], n_estimators=70)"
  },
  {
    "objectID": "posts/2022-04-25-bluebook-for-bulldozers.html#feature-engineering",
    "href": "posts/2022-04-25-bluebook-for-bulldozers.html#feature-engineering",
    "title": "Kaggle - Blue Book for Bulldozers",
    "section": "Feature Engineering",
    "text": "Feature Engineering\nFor feature engineering, we will give priority to important features. For this let us again analyze the preprocessed dataset starting from important features to see what can be done against each feature.\n\n##\n# sort the dataframe with important features at the start\ntemp = pd.Series(feature_importance, feature_names)\ncols = temp.nlargest(len(temp)).index\n\nsniff(df[cols], 10)\n\n\n\n\n\n\n\n\nProductSize\nYearsInUse\nfiBaseModel\nfiSecondaryDesc\nYearMade\nfiProductClassDesc\nModelID\nYearMade_na\nfiModelDesc\nHydraulics_Flow\nYearSold\nstate\nEnclosure\nHydraulics\nauctioneerID\nfiModelSeries\nfiModelDescriptor\nMachineHoursCurrentMeter\nTransmission\nPushblock\nEngine_Horsepower\nRipper\nDrive_System\ndatasource\nBlade_Type\nStick_Length\nUsageBand\nCoupler\nTire_Size\nTire_Size_na\nUndercarriage_Pad_Width\nThumb\nGrouser_Type\nTravel_Controls\nTrack_Type\nStick_Length_na\nForks\nMachineHoursCurrentMeter_na\nRide_Control\nUndercarriage_Pad_Width_na\nTip_Control\nDifferential_Type\nPattern_Changer\nProductGroup\nProductGroupDesc\nScarifier\nEnclosure_Type\nStick\nSteering_Controls\nBlade_Width\nBlade_Width_na\nPad_Type\nBlade_Extension\nTurbocharged\nGrouser_Tracks\nCoupler_System\nBackhoe_Mounting\n\n\n\n\n0\n0\n2.0\n296\n40\n2004.0\n59\n3157\nFalse\n950\n0\n2006\n1\n3\n1\n23\n0\n0\n68.0\n0\n0\n0\n0\n0\n121\n0\n9.7\n2\n0\n20.5\nTrue\n28.0\n0\n0\n0\n0\nTrue\n0\nFalse\n0\nTrue\n0\n4\n0\n6\n6\n0\n0\n0\n2\n14.0\nTrue\n0\n0\n0\n0\n0\n0\n\n\n1\n4\n8.0\n527\n54\n1996.0\n62\n77\nTrue\n1725\n2\n2004\n33\n5\n4\n2\n97\n65\n4640.0\n5\n1\n1\n3\n2\n132\n5\n11.0\n1\n2\n23.5\nFalse\n16.0\n1\n1\n3\n2\nFalse\n1\nTrue\n1\nFalse\n1\n0\n2\n3\n3\n1\n2\n1\n0\n12.0\nFalse\n2\n1\n1\n1\n1\n1\n\n\n2\n6.0\n3.0\n110.0\n0.0\n2001.0\n39.0\n7009.0\nNaN\n331.0\n1.0\n2011.0\n32.0\n1.0\n0.0\n13.0\n44.0\n20.0\n2838.0\n6.0\nNaN\n2.0\n2.0\n4.0\n136.0\n6.0\n15.9\n3.0\n1.0\n13.0\nNaN\n32.0\n2.0\n3.0\n5.0\n1.0\nNaN\nNaN\nNaN\n2.0\nNaN\n2.0\n1.0\n1.0\n4.0\n4.0\nNaN\n1.0\n2.0\n1.0\n13.0\nNaN\n3.0\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n3\n3.0\n10.0\n1375.0\n56.0\n2007.0\n8.0\n332.0\nNaN\n3674.0\nNaN\n2009.0\n44.0\n0.0\n11.0\n4.0\n102.0\n64.0\n3486.0\n4.0\nNaN\nNaN\n1.0\n3.0\n149.0\n9.0\n10.2\n0.0\nNaN\n26.5\nNaN\n30.0\nNaN\n2.0\n4.0\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n3.0\nNaN\n1.0\n1.0\nNaN\nNaN\nNaN\n3.0\n16.0\nNaN\n1.0\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n4\n5.0\n4.0\n1529.0\n47.0\n1993.0\n40.0\n17311.0\nNaN\n4208.0\nNaN\n2008.0\n3.0\n2.0\n5.0\n24.0\n33.0\n83.0\n722.0\n3.0\nNaN\nNaN\nNaN\n1.0\n172.0\n7.0\n10.6\nNaN\nNaN\n29.5\nNaN\n22.0\nNaN\nNaN\n2.0\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n2.0\nNaN\n5.0\n5.0\nNaN\nNaN\nNaN\n5.0\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n5\n2.0\n11.0\n175.0\n61.0\n2008.0\n2.0\n4605.0\nNaN\n493.0\nNaN\n2005.0\n9.0\n4.0\n7.0\n27.0\n98.0\n33.0\n508.0\n1.0\nNaN\nNaN\nNaN\nNaN\nNaN\n1.0\n9.1\nNaN\nNaN\n14.0\nNaN\n24.0\nNaN\nNaN\n6.0\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n2.0\n2.0\nNaN\nNaN\nNaN\n4.0\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n6\n1.0\n1.0\n419.0\n20.0\n1998.0\n14.0\n1937.0\nNaN\n1453.0\nNaN\n2007.0\n13.0\nNaN\n3.0\n30.0\n2.0\n100.0\n11540.0\n2.0\nNaN\nNaN\nNaN\nNaN\nNaN\n4.0\n10.1\nNaN\nNaN\n17.5\nNaN\n18.0\nNaN\nNaN\n1.0\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n7\nNaN\n7.0\n243.0\n105.0\n1999.0\n17.0\n3539.0\nNaN\n740.0\nNaN\n2010.0\n37.0\nNaN\n2.0\n26.0\n73.0\n128.0\n4883.0\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n8.0\n9.6\nNaN\nNaN\n15.5\nNaN\n36.0\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n8\nNaN\n5.0\n250.0\n133.0\n2003.0\n68.0\n36003.0\nNaN\n779.0\nNaN\n2000.0\n35.0\nNaN\n6.0\n25.0\n13.0\n71.0\n302.0\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n3.0\n12.8\nNaN\nNaN\n7.0\nNaN\n20.0\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n9\nNaN\n14.0\n540.0\n129.0\n1991.0\n51.0\n3883.0\nNaN\n1771.0\nNaN\n2002.0\n4.0\nNaN\n8.0\n11.0\n54.0\n122.0\n20700.0\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n2.0\n8.2\nNaN\nNaN\n10.0\nNaN\n27.0\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n\n\n\n\n\nThe above table is sorted based on the importance of each feature. Features at the start have more importance. So let’s visit each feature to see if any feature engineering (FE) can be used to gain more insights from the data.\n\nProductSize, YearsInUse: These features have numbers. Not a candidate for FE.\nfiBaseModel: It is label encoded. Let’s visit this column’s original values to see if any more features can be generated from it.\n\n\ndf_raw['fiBaseModel'].unique()[:50]\n\narray(['521D', '950FII', '226', 'PC120-6E', 'S175', '310G', '790ELC',\n       '416D', '430HAG', '988B', 'D31E', 'PC200LC6', '420D', '214E',\n       '310E', '334', '45NX', '302.5', '580SUPER K', 'JS260', '120G',\n       '966FII', 'EX550STD', '685B', '345BL', '330BL', '873', 'WA250',\n       '750BLT', '303CR', '95ZII', '416', '303.5', 'CTL60', '140G',\n       '307CSB', 'EC210LC', 'MF650', 'RC30', 'EX120-5', '70XT', '772A',\n       '160HNA', '216', '304CR', 'D3CIIIXL', '236', '120C', 'PC228',\n       'SK160LC'], dtype=object)\n\n\n\nfiBaseModel: original values look very random and do not give much information. There are two other columns in the importance list ‘fiModelDesc’, and ‘fiSecondaryDesc’ and from their name they look related to ‘fiBaseModel’. So let’s analyze them together.\n\n\ndf_raw[['fiBaseModel', 'fiModelDesc', 'fiSecondaryDesc']].head(10)\n\n\n\n\n\n\n\n\nfiBaseModel\nfiModelDesc\nfiSecondaryDesc\n\n\n\n\n0\n521\n521D\nD\n\n\n1\n950\n950FII\nF\n\n\n2\n226\n226\nNaN\n\n\n3\nPC120\nPC120-6E\nNaN\n\n\n4\nS175\nS175\nNaN\n\n\n5\n310\n310G\nG\n\n\n6\n790\n790ELC\nE\n\n\n7\n416\n416D\nD\n\n\n8\n430\n430HAG\nHAG\n\n\n9\n988\n988B\nB\n\n\n\n\n\n\n\n\nfiBaseModel, fiModelDesc, fiSecondaryDesc: From the above table all these three features are very much related but their values are very random and do not give us much information. So let’s leave them as it is.\nYearMade: It has numbers. Not a candidate for FE.\nfiProductClassDesc. This feature is also encode so let’s visit it’s original values.\n\n\ndf_raw['fiProductClassDesc'].unique()[:15]\n\narray(['Wheel Loader - 110.0 to 120.0 Horsepower',\n       'Wheel Loader - 150.0 to 175.0 Horsepower',\n       'Skid Steer Loader - 1351.0 to 1601.0 Lb Operating Capacity',\n       'Hydraulic Excavator, Track - 12.0 to 14.0 Metric Tons',\n       'Skid Steer Loader - 1601.0 to 1751.0 Lb Operating Capacity',\n       'Backhoe Loader - 14.0 to 15.0 Ft Standard Digging Depth',\n       'Hydraulic Excavator, Track - 21.0 to 24.0 Metric Tons',\n       'Hydraulic Excavator, Track - 3.0 to 4.0 Metric Tons',\n       'Wheel Loader - 350.0 to 500.0 Horsepower',\n       'Track Type Tractor, Dozer - 20.0 to 75.0 Horsepower',\n       'Hydraulic Excavator, Track - 19.0 to 21.0 Metric Tons',\n       'Hydraulic Excavator, Track - 4.0 to 5.0 Metric Tons',\n       'Hydraulic Excavator, Track - 2.0 to 3.0 Metric Tons',\n       'Hydraulic Excavator, Track - 24.0 to 28.0 Metric Tons',\n       'Motorgrader - 45.0 to 130.0 Horsepower'], dtype=object)\n\n\n\nfiProductClassDesc: This feature has text strings and it is also showing that they are not randow but has some pattern in then. They seems to be a good candidate for FE. We will do that in next section.\nModelID, YearSold: These features have numbers. Not a candidate for FE.\nHydraulics_Flow: It is encoded so let’s visit original values first.\n\n\ndf_raw['Hydraulics_Flow'].unique()\n\narray([nan, 'Standard', 'High Flow', 'None or Unspecified'], dtype=object)\n\n\n\nHydraulics_Flow: We label encoded it and it came up as an important feature. Its values are showing low variance so it is a better candidate for one-hot encoding. We will do that in the next section.\nstate: It is encoded so let’s visit its original values.\n\n\nprint(f\"total unique values: {len(df_raw['state'].unique())}\")\ndf_raw['state'].unique()[:15]\n\ntotal unique values: 53\n\n\narray(['Alabama', 'North Carolina', 'New York', 'Texas', 'Arizona',\n       'Florida', 'Illinois', 'Oregon', 'Ohio', 'Arkansas', 'Wisconsin',\n       'Kansas', 'Nevada', 'Iowa', 'Maine'], dtype=object)\n\n\n\nstate: By looking at original values we can see that it is a categorical nominal feature. It can be one hot encoded but since it has high variance (53 unique values) it is better to keep it as label encoded. So leave this feature as it is.\nEnclosure: It is encoded. So let’s check original values\n\n\nprint(f\"total unique values: {len(df_raw['Enclosure'].unique())}\")\ndf_raw['Enclosure'].unique()\n\ntotal unique values: 7\n\n\narray(['EROPS w AC', 'OROPS', 'EROPS', nan, 'EROPS AC', 'NO ROPS',\n       'None or Unspecified'], dtype=object)\n\n\n\nEnclosure: Original values show that it is a categorical feature with good importance and low variance, so it is also suitable for OHE.\nHydraulics: It is also encoded. So let’s check original values\n\n\nprint(f\"total unique values: {len(df_raw['Hydraulics'].unique())}\")\ndf_raw['Hydraulics'].unique()\n\ntotal unique values: 13\n\n\narray(['2 Valve', 'Auxiliary', nan, 'Standard', 'Base + 1 Function',\n       'Base + 3 Function', '4 Valve', '3 Valve', 'Base + 2 Function',\n       'Base + 4 Function', 'None or Unspecified', 'Base + 5 Function',\n       'Base + 6 Function'], dtype=object)\n\n\n\nHydraulics: Now this feature is again categorical, does not have high variance but also has low importance. We can consider it for OHE but since it is coming at the lower end feature importance, it will not have much impact on model performace. So we can skip it for OHE.\nFor the remaining features, importance is not significant enough to be considered for any FE. We can keep them as it is.\nsaledate: We did not use this feature in our last model training. But from the feature importance we can see that features that contain any date information are showing significant importance. So we should also include this feature in our next model.\n\nTo summarize this section, the features that are suitable for any FE are * fiProductClassDesc * Hydraulics_Flow * Enclosure * saledate\n\nfiProductClassDesc\nLet’s check the original values for this feature one more time.\n\ndf_raw['fiProductClassDesc'].head()\n\n0             Wheel Loader - 110.0 to 120.0 Horsepower\n1             Wheel Loader - 150.0 to 175.0 Horsepower\n2    Skid Steer Loader - 1351.0 to 1601.0 Lb Operat...\n3    Hydraulic Excavator, Track - 12.0 to 14.0 Metr...\n4    Skid Steer Loader - 1601.0 to 1751.0 Lb Operat...\nName: fiProductClassDesc, dtype: object\n\n\nThough this feature is named ‘ProductClassDesc’ but by looking at its value we can see that besides class description there is also information on class specification. If we take the first value then * ‘Wheel Loader’ -&gt; this is the class description * ‘110.0 to 120.0 Horsepower’ -&gt; this is class specification\nand even in the class specification we have * 110 -&gt; spec lower limit * 120 -&gt; spec upper limit * ‘Horsepower’ -&gt; spec unit\nUse this information to create new columns\n\n## \n# split the class description\ndf_split = df_raw.fiProductClassDesc.str.split(' - ',expand=True).values\n\n\n##\n# on 0 index we have class description\ndf_split[:,0]\n\narray(['Wheel Loader', 'Wheel Loader', 'Skid Steer Loader', ...,\n       'Hydraulic Excavator, Track', 'Hydraulic Excavator, Track',\n       'Hydraulic Excavator, Track'], dtype=object)\n\n\n\n##\n# on 1 index we have class specification\ndf_split[:,1]\n\narray(['110.0 to 120.0 Horsepower', '150.0 to 175.0 Horsepower',\n       '1351.0 to 1601.0 Lb Operating Capacity', ...,\n       '3.0 to 4.0 Metric Tons', '2.0 to 3.0 Metric Tons',\n       '2.0 to 3.0 Metric Tons'], dtype=object)\n\n\n\n##\n# let's create two new columns for this\ndf['fiProductClassDesc'] = df_split[:,0] \ndf['fiProductClassSpec'] = df_split[:,1]\n\n\n##\n# split class spec further to get limits and units\npattern = r'([0-9.\\+]*)(?: to ([0-9.\\+]*)|\\+) ([a-zA-Z ]*)'\ndf_split = df['fiProductClassSpec'].str.extract(pattern, expand=True).values\ndf_split = pd.DataFrame(df_split, columns=['fiProductClassSpec_lower', 'fiProductClassSpec_upper', 'fiProductClassSpec_units'])\ndf_split.head()\n\n\n\n\n\n\n\n\nfiProductClassSpec_lower\nfiProductClassSpec_upper\nfiProductClassSpec_units\n\n\n\n\n0\n110.0\n120.0\nHorsepower\n\n\n1\n150.0\n175.0\nHorsepower\n\n\n2\n1351.0\n1601.0\nLb Operating Capacity\n\n\n3\n12.0\n14.0\nMetric Tons\n\n\n4\n1601.0\n1751.0\nLb Operating Capacity\n\n\n\n\n\n\n\n\n##\n# merge new columns to our dataset\ndf = pd.concat([df, df_split], axis=1)\ndel df['fiProductClassSpec'] # class spec is no more required. we have it's sub-features\ndf.head()\n\n\n\n\n\n\n\n\nSalePrice\nModelID\ndatasource\nauctioneerID\nYearMade\nMachineHoursCurrentMeter\nUsageBand\nsaledate\nfiModelDesc\nfiBaseModel\nfiSecondaryDesc\nfiModelSeries\nfiModelDescriptor\nProductSize\nfiProductClassDesc\nstate\nProductGroup\nProductGroupDesc\nDrive_System\nEnclosure\nForks\nPad_Type\nRide_Control\nStick\nTransmission\nTurbocharged\nBlade_Extension\nBlade_Width\nEnclosure_Type\nEngine_Horsepower\nHydraulics\nPushblock\nRipper\nScarifier\nTip_Control\nTire_Size\nCoupler\nCoupler_System\nGrouser_Tracks\nHydraulics_Flow\nTrack_Type\nUndercarriage_Pad_Width\nStick_Length\nThumb\nPattern_Changer\nGrouser_Type\nBackhoe_Mounting\nBlade_Type\nTravel_Controls\nDifferential_Type\nSteering_Controls\nYearMade_na\nYearSold\nYearsInUse\nMachineHoursCurrentMeter_na\nTire_Size_na\nStick_Length_na\nUndercarriage_Pad_Width_na\nBlade_Width_na\nfiProductClassSpec_lower\nfiProductClassSpec_upper\nfiProductClassSpec_units\n\n\n\n\n0\n66000\n3157\n121\n23\n2004.0\n68.0\n2\n2006-11-16\n950\n296\n40\n0\n0\n0\nWheel Loader\n1\n6\n6\n0\n3\n0\n0\n0\n0\n0\n0\n0\n14.0\n0\n0\n1\n0\n0\n0\n0\n20.5\n0\n0\n0\n0\n0\n28.0\n9.7\n0\n0\n0\n0\n0\n0\n4\n2\nFalse\n2006\n2.0\nFalse\nTrue\nTrue\nTrue\nTrue\n110.0\n120.0\nHorsepower\n\n\n1\n57000\n77\n121\n23\n1996.0\n4640.0\n2\n2004-03-26\n1725\n527\n54\n97\n0\n4\nWheel Loader\n33\n6\n6\n0\n3\n0\n0\n0\n0\n0\n0\n0\n14.0\n0\n0\n1\n0\n0\n0\n0\n23.5\n0\n0\n0\n0\n0\n28.0\n9.7\n0\n0\n0\n0\n0\n0\n4\n2\nFalse\n2004\n8.0\nFalse\nFalse\nTrue\nTrue\nTrue\n150.0\n175.0\nHorsepower\n\n\n2\n10000\n7009\n121\n23\n2001.0\n2838.0\n1\n2004-02-26\n331\n110\n0\n0\n0\n0\nSkid Steer Loader\n32\n3\n3\n0\n5\n0\n0\n0\n0\n0\n0\n0\n14.0\n0\n0\n4\n0\n0\n0\n0\n20.5\n0\n0\n0\n2\n0\n28.0\n9.7\n0\n0\n0\n0\n0\n0\n0\n0\nFalse\n2004\n3.0\nFalse\nTrue\nTrue\nTrue\nTrue\n1351.0\n1601.0\nLb Operating Capacity\n\n\n3\n38500\n332\n121\n23\n2001.0\n3486.0\n1\n2011-05-19\n3674\n1375\n0\n44\n0\n6\nHydraulic Excavator, Track\n44\n4\n4\n0\n3\n0\n0\n0\n0\n0\n0\n0\n14.0\n0\n0\n1\n0\n0\n0\n0\n20.5\n0\n0\n0\n0\n0\n28.0\n9.7\n0\n0\n0\n0\n0\n0\n0\n0\nFalse\n2011\n10.0\nFalse\nTrue\nTrue\nTrue\nTrue\n12.0\n14.0\nMetric Tons\n\n\n4\n11000\n17311\n121\n23\n2007.0\n722.0\n3\n2009-07-23\n4208\n1529\n0\n0\n0\n0\nSkid Steer Loader\n32\n3\n3\n0\n1\n0\n0\n0\n0\n0\n0\n0\n14.0\n0\n0\n4\n0\n0\n0\n0\n20.5\n0\n0\n0\n2\n0\n28.0\n9.7\n0\n0\n0\n0\n0\n0\n0\n0\nFalse\n2009\n2.0\nFalse\nTrue\nTrue\nTrue\nTrue\n1601.0\n1751.0\nLb Operating Capacity\n\n\n\n\n\n\n\n\n##\n# convert to numerical features\ndf['fiProductClassSpec_lower'] = pd.to_numeric(df['fiProductClassSpec_lower'])\ndf['fiProductClassSpec_upper'] = pd.to_numeric(df['fiProductClassSpec_upper'])\n\n# apply fix for numerical features\nfix_missing_num(df, 'fiProductClassSpec_lower')\nfix_missing_num(df, 'fiProductClassSpec_upper')\n\n# apply fix for categorical features\ndf_string_to_cat(df)\ndf_cat_to_catcode(df)\n\nlabel encoding applied on fiProductClassDesc\nlabel encoding applied on fiProductClassSpec_units\n\n\n\n(rf, feature_names, feature_importance, oob_hydralics) = train_and_plot_model(df, drop_features=['saledate'])\n\n[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 4 concurrent workers.\n[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:  1.6min\n[Parallel(n_jobs=-1)]: Done  70 out of  70 | elapsed:  2.6min finished\n\n\nOOB scrore =  0.905 \nTree leaves =  14,650,747 \nMedian depth = 47.0\n\n\n\n\n\n\n\n\n\nThere is only a slight increase in OOB score but if we check the feature importance plot both fiProductClassSpec_upper and fiProductClassSpec_lower are showing high importance. We can take this as a positive signal for good features.\n\n\nHydralics_Flow\nWe need to apply one hot encoding (OHE) to this feature. Let’s start by checking unique values for Hydraulics_Flow.\n\ndf['Hydraulics_Flow'].value_counts()\n\n0    357788\n2     42784\n1       553\nName: Hydraulics_Flow, dtype: int64\n\n\nWe have encoded this feature in the preprocessing section. Although we can use this encoded feature for one-hot encoding but we don’t have original labels at this point. It would be better if we use original labels for OHE so that the dummy columns created as a result of that also have proper names with labels. Using encoded dummy column names makes them difficult to understand and follow. let’s use the original dataframe to check the unique values.\n\ndf_raw['Hydraulics_Flow'].value_counts(dropna=False)\n\nNaN                    357763\nStandard                42784\nHigh Flow                 553\nNone or Unspecified        25\nName: Hydraulics_Flow, dtype: int64\n\n\nBefore applying OHE we need to preprocess ‘None or Unspecified’ as they repsent the same as np.nan. So let’s do that.\n\n## \n# get the original values\ndf['Hydraulics_Flow'] = df_raw['Hydraulics_Flow']\ndf['Hydraulics_Flow'] = df['Hydraulics_Flow'].replace('None or Unspecified', np.nan)\n\ndf['Hydraulics_Flow'].value_counts(dropna=False)\n\nNaN          357788\nStandard      42784\nHigh Flow       553\nName: Hydraulics_Flow, dtype: int64\n\n\nLet’s check the first few rows of this column. We will use them to verify our final result.\n\ndf['Hydraulics_Flow'].head()\n\n0         NaN\n1         NaN\n2    Standard\n3         NaN\n4    Standard\nName: Hydraulics_Flow, dtype: object\n\n\nNotice that in the first five rows there are ‘Standard’ values at row index 2 and 4, and the remaining are ‘NaN’ values. We will OHE them in the next step and compare the results to ensure encoding is properly working.\n\nfrom sklearn.preprocessing import OneHotEncoder\n\nonehot_encoder = OneHotEncoder()\nonehot_output = onehot_encoder.fit_transform(df[['Hydraulics_Flow']])\n\n# check the output\nprint(onehot_output[:5].toarray())\n\n[[0. 0. 1.]\n [0. 0. 1.]\n [0. 1. 0.]\n [0. 0. 1.]\n [0. 1. 0.]]\n\n\nThese are same five rows but this time encoded with one-hot values. From the position of ‘1’ appearing in different columns we can deduce that first column is for label ‘High Flow’ and second is for ‘Standard’ and third is for ‘NaN’. It would be easier for us to track these dummy columns if we have proper names on them. So let’s do that.\nWe can get the dummy column names by calling get_feature_names_out() on our encoder.\n\n# name of the columns\nonehot_encoder.get_feature_names_out()\n\narray(['Hydraulics_Flow_High Flow', 'Hydraulics_Flow_Standard',\n       'Hydraulics_Flow_nan'], dtype=object)\n\n\nTo create a dataframe of these dummy variables.\n\ndf_onehot = pd.DataFrame(onehot_output.toarray(), columns=onehot_encoder.get_feature_names_out())\ndf_onehot.head()\n\n\n\n\n\n\n\n\nHydraulics_Flow_High Flow\nHydraulics_Flow_Standard\nHydraulics_Flow_nan\n\n\n\n\n0\n0.0\n0.0\n1.0\n\n\n1\n0.0\n0.0\n1.0\n\n\n2\n0.0\n1.0\n0.0\n\n\n3\n0.0\n0.0\n1.0\n\n\n4\n0.0\n1.0\n0.0\n\n\n\n\n\n\n\nAt this point Hydraulics_Flow is OHE so we can drop the original column from the dataset and add these encoded columns.\n\ndel df['Hydraulics_Flow']\n\ndf = pd.concat([df, df_onehot], axis=1) # concat dataframes column wise\ndf.head()\n\n\n\n\n\n\n\n\nSalePrice\nModelID\ndatasource\nauctioneerID\nYearMade\nMachineHoursCurrentMeter\nUsageBand\nsaledate\nfiModelDesc\nfiBaseModel\nfiSecondaryDesc\nfiModelSeries\nfiModelDescriptor\nProductSize\nfiProductClassDesc\nstate\nProductGroup\nProductGroupDesc\nDrive_System\nEnclosure\nForks\nPad_Type\nRide_Control\nStick\nTransmission\nTurbocharged\nBlade_Extension\nBlade_Width\nEnclosure_Type\nEngine_Horsepower\nHydraulics\nPushblock\nRipper\nScarifier\nTip_Control\nTire_Size\nCoupler\nCoupler_System\nGrouser_Tracks\nTrack_Type\nUndercarriage_Pad_Width\nStick_Length\nThumb\nPattern_Changer\nGrouser_Type\nBackhoe_Mounting\nBlade_Type\nTravel_Controls\nDifferential_Type\nSteering_Controls\nYearMade_na\nYearSold\nYearsInUse\nMachineHoursCurrentMeter_na\nTire_Size_na\nStick_Length_na\nUndercarriage_Pad_Width_na\nBlade_Width_na\nfiProductClassSpec_lower\nfiProductClassSpec_upper\nfiProductClassSpec_units\nfiProductClassSpec_lower_na\nfiProductClassSpec_upper_na\nHydraulics_Flow_High Flow\nHydraulics_Flow_Standard\nHydraulics_Flow_nan\n\n\n\n\n0\n66000\n3157\n121\n23\n2004.0\n68.0\n2\n2006-11-16\n950\n296\n40\n0\n0\n0\n6\n1\n6\n6\n0\n3\n0\n0\n0\n0\n0\n0\n0\n14.0\n0\n0\n1\n0\n0\n0\n0\n20.5\n0\n0\n0\n0\n28.0\n9.7\n0\n0\n0\n0\n0\n0\n4\n2\nFalse\n2006\n2.0\nFalse\nTrue\nTrue\nTrue\nTrue\n110.0\n120.0\n2\nFalse\nFalse\n0.0\n0.0\n1.0\n\n\n1\n57000\n77\n121\n23\n1996.0\n4640.0\n2\n2004-03-26\n1725\n527\n54\n97\n0\n4\n6\n33\n6\n6\n0\n3\n0\n0\n0\n0\n0\n0\n0\n14.0\n0\n0\n1\n0\n0\n0\n0\n23.5\n0\n0\n0\n0\n28.0\n9.7\n0\n0\n0\n0\n0\n0\n4\n2\nFalse\n2004\n8.0\nFalse\nFalse\nTrue\nTrue\nTrue\n150.0\n175.0\n2\nFalse\nFalse\n0.0\n0.0\n1.0\n\n\n2\n10000\n7009\n121\n23\n2001.0\n2838.0\n1\n2004-02-26\n331\n110\n0\n0\n0\n0\n4\n32\n3\n3\n0\n5\n0\n0\n0\n0\n0\n0\n0\n14.0\n0\n0\n4\n0\n0\n0\n0\n20.5\n0\n0\n0\n0\n28.0\n9.7\n0\n0\n0\n0\n0\n0\n0\n0\nFalse\n2004\n3.0\nFalse\nTrue\nTrue\nTrue\nTrue\n1351.0\n1601.0\n3\nFalse\nFalse\n0.0\n1.0\n0.0\n\n\n3\n38500\n332\n121\n23\n2001.0\n3486.0\n1\n2011-05-19\n3674\n1375\n0\n44\n0\n6\n2\n44\n4\n4\n0\n3\n0\n0\n0\n0\n0\n0\n0\n14.0\n0\n0\n1\n0\n0\n0\n0\n20.5\n0\n0\n0\n0\n28.0\n9.7\n0\n0\n0\n0\n0\n0\n0\n0\nFalse\n2011\n10.0\nFalse\nTrue\nTrue\nTrue\nTrue\n12.0\n14.0\n4\nFalse\nFalse\n0.0\n0.0\n1.0\n\n\n4\n11000\n17311\n121\n23\n2007.0\n722.0\n3\n2009-07-23\n4208\n1529\n0\n0\n0\n0\n4\n32\n3\n3\n0\n1\n0\n0\n0\n0\n0\n0\n0\n14.0\n0\n0\n4\n0\n0\n0\n0\n20.5\n0\n0\n0\n0\n28.0\n9.7\n0\n0\n0\n0\n0\n0\n0\n0\nFalse\n2009\n2.0\nFalse\nTrue\nTrue\nTrue\nTrue\n1601.0\n1751.0\n3\nFalse\nFalse\n0.0\n1.0\n0.0\n\n\n\n\n\n\n\nLet’s retain our model to check if there is any affect on model performance.\n\n(rf, feature_names, feature_importance, oob_hydralics) = train_and_plot_model(df, drop_features=['saledate'])\n\n[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 4 concurrent workers.\n[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:  1.6min\n[Parallel(n_jobs=-1)]: Done  70 out of  70 | elapsed:  2.6min finished\n\n\nOOB scrore =  0.905 \nTree leaves =  14,650,376 \nMedian depth = 48.0\n\n\n\n\n\n\n\n\n\nThere is no effect on the model performance but one feature ‘Hydraulics_Flow_nan’ is showing some importance on the plot. The remaining features (‘Hydraulics_Flow_High Flow’ and ‘Hydraulics_Flow_Standard’) do not affect the model’s performance. If it was not of ‘Hydraulics_Flow_nan’ importance we could have skipped OHE for ‘Hydraulics_Flow’.\n\n\nEnclosure\nNext feature is Enclosure, and we will follow the same steps as for last feature to one-hot encode it.\n\n##\n# check value counts\ndf_raw['Enclosure'].value_counts(dropna=False)\n\nOROPS                  173932\nEROPS                  139026\nEROPS w AC              87820\nNaN                       325\nEROPS AC                   17\nNO ROPS                     3\nNone or Unspecified         2\nName: Enclosure, dtype: int64\n\n\nHere ROPS is an abbreviation for Roll Over Protection System and there are multiple variants of this standard * OROPS = Open ROPS * EROPS = Enclosed ROPS * EROPS AC = Enclosed ROPS with Air Conditioning * EROPS w AC = Enclosed ROPS with Air Conditioning. Same as ‘EROPS AC’ * NO ROPS = No ROPS. Same as ‘NaN’ or ‘None or Unspecified’\nYou can read more about ROPS standards here * http://www.miningrops.com.au/ropsintro.html * https://www.youtube.com/watch?v=LZ40O1My8E4&ab_channel=MissouriEarthMovers\nUsing this information we can also preprocess this feature to make its values more consistent.\n\n## \n# get the original values\ndf['Enclosure'] = df_raw['Enclosure']\n\n# change 'None or Unspecified' and 'NO ROPS' to np.nan\ndf['Enclosure'] = df['Enclosure'].replace('None or Unspecified', np.nan)\ndf['Enclosure'] = df['Enclosure'].replace('NO ROPS', np.nan)\n\n# change 'EROPS w AC' to 'EROPS AC'\ndf['Enclosure'] = df['Enclosure'].replace('EROPS w AC', 'EROPS AC')\n\ndf['Enclosure'].value_counts(dropna=False)\n\nOROPS       173932\nEROPS       139026\nEROPS AC     87837\nNaN            330\nName: Enclosure, dtype: int64\n\n\n\n##\n# before OHE\ndf.head()\n\n\n\n\n\n\n\n\nSalePrice\nModelID\ndatasource\nauctioneerID\nYearMade\nMachineHoursCurrentMeter\nUsageBand\nsaledate\nfiModelDesc\nfiBaseModel\nfiSecondaryDesc\nfiModelSeries\nfiModelDescriptor\nProductSize\nfiProductClassDesc\nstate\nProductGroup\nProductGroupDesc\nDrive_System\nEnclosure\nForks\nPad_Type\nRide_Control\nStick\nTransmission\nTurbocharged\nBlade_Extension\nBlade_Width\nEnclosure_Type\nEngine_Horsepower\nHydraulics\nPushblock\nRipper\nScarifier\nTip_Control\nTire_Size\nCoupler\nCoupler_System\nGrouser_Tracks\nTrack_Type\nUndercarriage_Pad_Width\nStick_Length\nThumb\nPattern_Changer\nGrouser_Type\nBackhoe_Mounting\nBlade_Type\nTravel_Controls\nDifferential_Type\nSteering_Controls\nYearMade_na\nYearSold\nYearsInUse\nMachineHoursCurrentMeter_na\nTire_Size_na\nStick_Length_na\nUndercarriage_Pad_Width_na\nBlade_Width_na\nfiProductClassSpec_lower\nfiProductClassSpec_upper\nfiProductClassSpec_units\nfiProductClassSpec_lower_na\nfiProductClassSpec_upper_na\nHydraulics_Flow_High Flow\nHydraulics_Flow_Standard\nHydraulics_Flow_nan\n\n\n\n\n0\n66000\n3157\n121\n23\n2004.0\n68.0\n2\n2006-11-16\n950\n296\n40\n0\n0\n0\n6\n1\n6\n6\n0\nEROPS AC\n0\n0\n0\n0\n0\n0\n0\n14.0\n0\n0\n1\n0\n0\n0\n0\n20.5\n0\n0\n0\n0\n28.0\n9.7\n0\n0\n0\n0\n0\n0\n4\n2\nFalse\n2006\n2.0\nFalse\nTrue\nTrue\nTrue\nTrue\n110.0\n120.0\n2\nFalse\nFalse\n0.0\n0.0\n1.0\n\n\n1\n57000\n77\n121\n23\n1996.0\n4640.0\n2\n2004-03-26\n1725\n527\n54\n97\n0\n4\n6\n33\n6\n6\n0\nEROPS AC\n0\n0\n0\n0\n0\n0\n0\n14.0\n0\n0\n1\n0\n0\n0\n0\n23.5\n0\n0\n0\n0\n28.0\n9.7\n0\n0\n0\n0\n0\n0\n4\n2\nFalse\n2004\n8.0\nFalse\nFalse\nTrue\nTrue\nTrue\n150.0\n175.0\n2\nFalse\nFalse\n0.0\n0.0\n1.0\n\n\n2\n10000\n7009\n121\n23\n2001.0\n2838.0\n1\n2004-02-26\n331\n110\n0\n0\n0\n0\n4\n32\n3\n3\n0\nOROPS\n0\n0\n0\n0\n0\n0\n0\n14.0\n0\n0\n4\n0\n0\n0\n0\n20.5\n0\n0\n0\n0\n28.0\n9.7\n0\n0\n0\n0\n0\n0\n0\n0\nFalse\n2004\n3.0\nFalse\nTrue\nTrue\nTrue\nTrue\n1351.0\n1601.0\n3\nFalse\nFalse\n0.0\n1.0\n0.0\n\n\n3\n38500\n332\n121\n23\n2001.0\n3486.0\n1\n2011-05-19\n3674\n1375\n0\n44\n0\n6\n2\n44\n4\n4\n0\nEROPS AC\n0\n0\n0\n0\n0\n0\n0\n14.0\n0\n0\n1\n0\n0\n0\n0\n20.5\n0\n0\n0\n0\n28.0\n9.7\n0\n0\n0\n0\n0\n0\n0\n0\nFalse\n2011\n10.0\nFalse\nTrue\nTrue\nTrue\nTrue\n12.0\n14.0\n4\nFalse\nFalse\n0.0\n0.0\n1.0\n\n\n4\n11000\n17311\n121\n23\n2007.0\n722.0\n3\n2009-07-23\n4208\n1529\n0\n0\n0\n0\n4\n32\n3\n3\n0\nEROPS\n0\n0\n0\n0\n0\n0\n0\n14.0\n0\n0\n4\n0\n0\n0\n0\n20.5\n0\n0\n0\n0\n28.0\n9.7\n0\n0\n0\n0\n0\n0\n0\n0\nFalse\n2009\n2.0\nFalse\nTrue\nTrue\nTrue\nTrue\n1601.0\n1751.0\n3\nFalse\nFalse\n0.0\n1.0\n0.0\n\n\n\n\n\n\n\n\n##\n# one hot encode 'Enclosure'\nonehot_encoder = OneHotEncoder()\nonehot_output = onehot_encoder.fit_transform(df[['Enclosure']])\n\ndf_onehot = pd.DataFrame(onehot_output.toarray(), columns=onehot_encoder.get_feature_names_out())\ndf_onehot.head()\n\n\n\n\n\n\n\n\nEnclosure_EROPS\nEnclosure_EROPS AC\nEnclosure_OROPS\nEnclosure_nan\n\n\n\n\n0\n0.0\n1.0\n0.0\n0.0\n\n\n1\n0.0\n1.0\n0.0\n0.0\n\n\n2\n0.0\n0.0\n1.0\n0.0\n\n\n3\n0.0\n1.0\n0.0\n0.0\n\n\n4\n1.0\n0.0\n0.0\n0.0\n\n\n\n\n\n\n\n\n## \n# drop original column\ndel df['Enclosure']\n\n# add dummy columns to the dataframe\ndf = pd.concat([df, df_onehot], axis=1) # concat dataframes column wise\n\n# after OHE\ndf.head() \n\n\n\n\n\n\n\n\nSalePrice\nModelID\ndatasource\nauctioneerID\nYearMade\nMachineHoursCurrentMeter\nUsageBand\nsaledate\nfiModelDesc\nfiBaseModel\nfiSecondaryDesc\nfiModelSeries\nfiModelDescriptor\nProductSize\nfiProductClassDesc\nstate\nProductGroup\nProductGroupDesc\nDrive_System\nForks\nPad_Type\nRide_Control\nStick\nTransmission\nTurbocharged\nBlade_Extension\nBlade_Width\nEnclosure_Type\nEngine_Horsepower\nHydraulics\nPushblock\nRipper\nScarifier\nTip_Control\nTire_Size\nCoupler\nCoupler_System\nGrouser_Tracks\nTrack_Type\nUndercarriage_Pad_Width\nStick_Length\nThumb\nPattern_Changer\nGrouser_Type\nBackhoe_Mounting\nBlade_Type\nTravel_Controls\nDifferential_Type\nSteering_Controls\nYearMade_na\nYearSold\nYearsInUse\nMachineHoursCurrentMeter_na\nTire_Size_na\nStick_Length_na\nUndercarriage_Pad_Width_na\nBlade_Width_na\nfiProductClassSpec_lower\nfiProductClassSpec_upper\nfiProductClassSpec_units\nfiProductClassSpec_lower_na\nfiProductClassSpec_upper_na\nHydraulics_Flow_High Flow\nHydraulics_Flow_Standard\nHydraulics_Flow_nan\nEnclosure_EROPS\nEnclosure_EROPS AC\nEnclosure_OROPS\nEnclosure_nan\n\n\n\n\n0\n66000\n3157\n121\n23\n2004.0\n68.0\n2\n2006-11-16\n950\n296\n40\n0\n0\n0\n6\n1\n6\n6\n0\n0\n0\n0\n0\n0\n0\n0\n14.0\n0\n0\n1\n0\n0\n0\n0\n20.5\n0\n0\n0\n0\n28.0\n9.7\n0\n0\n0\n0\n0\n0\n4\n2\nFalse\n2006\n2.0\nFalse\nTrue\nTrue\nTrue\nTrue\n110.0\n120.0\n2\nFalse\nFalse\n0.0\n0.0\n1.0\n0.0\n1.0\n0.0\n0.0\n\n\n1\n57000\n77\n121\n23\n1996.0\n4640.0\n2\n2004-03-26\n1725\n527\n54\n97\n0\n4\n6\n33\n6\n6\n0\n0\n0\n0\n0\n0\n0\n0\n14.0\n0\n0\n1\n0\n0\n0\n0\n23.5\n0\n0\n0\n0\n28.0\n9.7\n0\n0\n0\n0\n0\n0\n4\n2\nFalse\n2004\n8.0\nFalse\nFalse\nTrue\nTrue\nTrue\n150.0\n175.0\n2\nFalse\nFalse\n0.0\n0.0\n1.0\n0.0\n1.0\n0.0\n0.0\n\n\n2\n10000\n7009\n121\n23\n2001.0\n2838.0\n1\n2004-02-26\n331\n110\n0\n0\n0\n0\n4\n32\n3\n3\n0\n0\n0\n0\n0\n0\n0\n0\n14.0\n0\n0\n4\n0\n0\n0\n0\n20.5\n0\n0\n0\n0\n28.0\n9.7\n0\n0\n0\n0\n0\n0\n0\n0\nFalse\n2004\n3.0\nFalse\nTrue\nTrue\nTrue\nTrue\n1351.0\n1601.0\n3\nFalse\nFalse\n0.0\n1.0\n0.0\n0.0\n0.0\n1.0\n0.0\n\n\n3\n38500\n332\n121\n23\n2001.0\n3486.0\n1\n2011-05-19\n3674\n1375\n0\n44\n0\n6\n2\n44\n4\n4\n0\n0\n0\n0\n0\n0\n0\n0\n14.0\n0\n0\n1\n0\n0\n0\n0\n20.5\n0\n0\n0\n0\n28.0\n9.7\n0\n0\n0\n0\n0\n0\n0\n0\nFalse\n2011\n10.0\nFalse\nTrue\nTrue\nTrue\nTrue\n12.0\n14.0\n4\nFalse\nFalse\n0.0\n0.0\n1.0\n0.0\n1.0\n0.0\n0.0\n\n\n4\n11000\n17311\n121\n23\n2007.0\n722.0\n3\n2009-07-23\n4208\n1529\n0\n0\n0\n0\n4\n32\n3\n3\n0\n0\n0\n0\n0\n0\n0\n0\n14.0\n0\n0\n4\n0\n0\n0\n0\n20.5\n0\n0\n0\n0\n28.0\n9.7\n0\n0\n0\n0\n0\n0\n0\n0\nFalse\n2009\n2.0\nFalse\nTrue\nTrue\nTrue\nTrue\n1601.0\n1751.0\n3\nFalse\nFalse\n0.0\n1.0\n0.0\n1.0\n0.0\n0.0\n0.0\n\n\n\n\n\n\n\nLet’s retain our model to check if there is any affect on model performance.\n\n(rf, feature_names, feature_importance, oob_enclosure) = train_and_plot_model(df, drop_features=['saledate'])\n\n[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 4 concurrent workers.\n[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:  2.0min\n[Parallel(n_jobs=-1)]: Done  70 out of  70 | elapsed:  3.6min finished\n\n\nOOB scrore =  0.902 \nTree leaves =  14,668,686 \nMedian depth = 45.0\n\n\n\n\n\n\n\n\n\nThere is a slight decrease in model performance but one new feature ‘Enclosure_EROPS AC’ is showing very high on the importance plot.\n\n\nsaledate\nWe have already created ‘yearsold’ feature. We can more consequent features from ‘saledate’.\n\ndf[\"salemonth\"] = df['saledate'].dt.month\ndf[\"saleday\"] = df['saledate'].dt.day\ndf[\"saledayofweek\"] = df['saledate'].dt.dayofweek\ndf[\"saledayofyear\"] = df['saledate'].dt.dayofyear\n\n# we can drop the orignal\ndel df['saledate']\n\n\ndf.head()\n\n\n\n\n\n\n\n\nSalePrice\nModelID\ndatasource\nauctioneerID\nYearMade\nMachineHoursCurrentMeter\nUsageBand\nfiModelDesc\nfiBaseModel\nfiSecondaryDesc\nfiModelSeries\nfiModelDescriptor\nProductSize\nfiProductClassDesc\nstate\nProductGroup\nProductGroupDesc\nDrive_System\nForks\nPad_Type\nRide_Control\nStick\nTransmission\nTurbocharged\nBlade_Extension\nBlade_Width\nEnclosure_Type\nEngine_Horsepower\nHydraulics\nPushblock\nRipper\nScarifier\nTip_Control\nTire_Size\nCoupler\nCoupler_System\nGrouser_Tracks\nTrack_Type\nUndercarriage_Pad_Width\nStick_Length\nThumb\nPattern_Changer\nGrouser_Type\nBackhoe_Mounting\nBlade_Type\nTravel_Controls\nDifferential_Type\nSteering_Controls\nYearMade_na\nYearSold\nYearsInUse\nMachineHoursCurrentMeter_na\nTire_Size_na\nStick_Length_na\nUndercarriage_Pad_Width_na\nBlade_Width_na\nfiProductClassSpec_lower\nfiProductClassSpec_upper\nfiProductClassSpec_units\nfiProductClassSpec_lower_na\nfiProductClassSpec_upper_na\nHydraulics_Flow_High Flow\nHydraulics_Flow_Standard\nHydraulics_Flow_nan\nEnclosure_EROPS\nEnclosure_EROPS AC\nEnclosure_OROPS\nEnclosure_nan\nsalemonth\nsaleday\nsaledayofweek\nsaledayofyear\n\n\n\n\n0\n66000\n3157\n121\n23\n2004.0\n68.0\n2\n950\n296\n40\n0\n0\n0\n6\n1\n6\n6\n0\n0\n0\n0\n0\n0\n0\n0\n14.0\n0\n0\n1\n0\n0\n0\n0\n20.5\n0\n0\n0\n0\n28.0\n9.7\n0\n0\n0\n0\n0\n0\n4\n2\nFalse\n2006\n2.0\nFalse\nTrue\nTrue\nTrue\nTrue\n110.0\n120.0\n2\nFalse\nFalse\n0.0\n0.0\n1.0\n0.0\n1.0\n0.0\n0.0\n11\n16\n3\n320\n\n\n1\n57000\n77\n121\n23\n1996.0\n4640.0\n2\n1725\n527\n54\n97\n0\n4\n6\n33\n6\n6\n0\n0\n0\n0\n0\n0\n0\n0\n14.0\n0\n0\n1\n0\n0\n0\n0\n23.5\n0\n0\n0\n0\n28.0\n9.7\n0\n0\n0\n0\n0\n0\n4\n2\nFalse\n2004\n8.0\nFalse\nFalse\nTrue\nTrue\nTrue\n150.0\n175.0\n2\nFalse\nFalse\n0.0\n0.0\n1.0\n0.0\n1.0\n0.0\n0.0\n3\n26\n4\n86\n\n\n2\n10000\n7009\n121\n23\n2001.0\n2838.0\n1\n331\n110\n0\n0\n0\n0\n4\n32\n3\n3\n0\n0\n0\n0\n0\n0\n0\n0\n14.0\n0\n0\n4\n0\n0\n0\n0\n20.5\n0\n0\n0\n0\n28.0\n9.7\n0\n0\n0\n0\n0\n0\n0\n0\nFalse\n2004\n3.0\nFalse\nTrue\nTrue\nTrue\nTrue\n1351.0\n1601.0\n3\nFalse\nFalse\n0.0\n1.0\n0.0\n0.0\n0.0\n1.0\n0.0\n2\n26\n3\n57\n\n\n3\n38500\n332\n121\n23\n2001.0\n3486.0\n1\n3674\n1375\n0\n44\n0\n6\n2\n44\n4\n4\n0\n0\n0\n0\n0\n0\n0\n0\n14.0\n0\n0\n1\n0\n0\n0\n0\n20.5\n0\n0\n0\n0\n28.0\n9.7\n0\n0\n0\n0\n0\n0\n0\n0\nFalse\n2011\n10.0\nFalse\nTrue\nTrue\nTrue\nTrue\n12.0\n14.0\n4\nFalse\nFalse\n0.0\n0.0\n1.0\n0.0\n1.0\n0.0\n0.0\n5\n19\n3\n139\n\n\n4\n11000\n17311\n121\n23\n2007.0\n722.0\n3\n4208\n1529\n0\n0\n0\n0\n4\n32\n3\n3\n0\n0\n0\n0\n0\n0\n0\n0\n14.0\n0\n0\n4\n0\n0\n0\n0\n20.5\n0\n0\n0\n0\n28.0\n9.7\n0\n0\n0\n0\n0\n0\n0\n0\nFalse\n2009\n2.0\nFalse\nTrue\nTrue\nTrue\nTrue\n1601.0\n1751.0\n3\nFalse\nFalse\n0.0\n1.0\n0.0\n1.0\n0.0\n0.0\n0.0\n7\n23\n3\n204\n\n\n\n\n\n\n\n\n(rf, feature_names, feature_importance, oob_date) = train_and_plot_model(df)\n\n[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 4 concurrent workers.\n[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:  2.4min\n[Parallel(n_jobs=-1)]: Done  70 out of  70 | elapsed:  3.9min finished\n\n\nOOB scrore =  0.907 \nTree leaves =  14,493,456 \nMedian depth = 45.0\n\n\n\n\n\n\n\n\n\nThere is an increase in model performance and multiple newly created date features are showing good importance on the plot."
  },
  {
    "objectID": "posts/2022-05-17-aws-sagemaker-wrangler-p1.html#data",
    "href": "posts/2022-05-17-aws-sagemaker-wrangler-p1.html#data",
    "title": "Data Preparation with SageMaker Data Wrangler (Part 1)",
    "section": "Data",
    "text": "Data\nMobile operators have historical records on which customers ultimately ended up churning and which continued using the service. We can use this historical information to construct an ML model of one mobile operator’s churn using a process called training. After training the model, we can pass the profile information of an arbitrary customer (the same profile information that we used to train the model) to the model, and have the model predict whether this customer is going to churn. Of course, we expect the model to make mistakes. After all, predicting the future is a tricky business! But we’ll learn how to deal with prediction errors.\nThe dataset we use is publicly available and was mentioned in the book Discovering Knowledge in Data by Daniel T. Larose. It is attributed by the author to the University of California Irvine Repository of Machine Learning Datasets (Jafari-Marandi, R., Denton, J., Idris, A., Smith, B. K., & Keramati, A. (2020)."
  },
  {
    "objectID": "posts/2022-05-17-aws-sagemaker-wrangler-p1.html#preparation",
    "href": "posts/2022-05-17-aws-sagemaker-wrangler-p1.html#preparation",
    "title": "Data Preparation with SageMaker Data Wrangler (Part 1)",
    "section": "Preparation",
    "text": "Preparation\n\n##\n# install aws data wrangler package\n# restart kernel after installation\n# more on this package later in the notebook.\n!pip install -q awswrangler\n\n\nWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n\nWARNING: You are using pip version 22.0.4; however, version 22.1 is available.\n\nYou should consider upgrading via the '/opt/conda/bin/python -m pip install --upgrade pip' command.\n\n\n\n\n\n\n## import required libraries\nimport pandas as pd\nimport sagemaker\n\nsess = sagemaker.Session()\nprefix = 'myblog/demo-customer-churn'\n\n\n!aws s3 cp s3://sagemaker-sample-files/datasets/tabular/synthetic/churn.txt ./\n\ndownload: s3://sagemaker-sample-files/datasets/tabular/synthetic/churn.txt to ./churn.txt\n\n\n\ndf = pd.read_csv(\"./churn.txt\")\n\n# make 'CustomerID' column from the index\ndf['CustomerID']=df.index\n\npd.set_option(\"display.max_columns\", 500)\ndf.head(10)\n\n\n\n\n\n\n\n\nState\nAccount Length\nArea Code\nPhone\nInt'l Plan\nVMail Plan\nVMail Message\nDay Mins\nDay Calls\nDay Charge\nEve Mins\nEve Calls\nEve Charge\nNight Mins\nNight Calls\nNight Charge\nIntl Mins\nIntl Calls\nIntl Charge\nCustServ Calls\nChurn?\nCustomerID\n\n\n\n\n0\nPA\n163\n806\n403-2562\nno\nyes\n300\n8.162204\n3\n7.579174\n3.933035\n4\n6.508639\n4.065759\n100\n5.111624\n4.928160\n6\n5.673203\n3\nTrue.\n0\n\n\n1\nSC\n15\n836\n158-8416\nyes\nno\n0\n10.018993\n4\n4.226289\n2.325005\n0\n9.972592\n7.141040\n200\n6.436188\n3.221748\n6\n2.559749\n8\nFalse.\n1\n\n\n2\nMO\n131\n777\n896-6253\nno\nyes\n300\n4.708490\n3\n4.768160\n4.537466\n3\n4.566715\n5.363235\n100\n5.142451\n7.139023\n2\n6.254157\n4\nFalse.\n2\n\n\n3\nWY\n75\n878\n817-5729\nyes\nyes\n700\n1.268734\n3\n2.567642\n2.528748\n5\n2.333624\n3.773586\n450\n3.814413\n2.245779\n6\n1.080692\n6\nFalse.\n3\n\n\n4\nWY\n146\n878\n450-4942\nyes\nno\n0\n2.696177\n3\n5.908916\n6.015337\n3\n3.670408\n3.751673\n250\n2.796812\n6.905545\n4\n7.134343\n6\nTrue.\n4\n\n\n5\nVA\n83\n866\n454-9110\nno\nno\n0\n3.634776\n7\n4.804892\n6.051944\n5\n5.278437\n2.937880\n300\n4.817958\n4.948816\n4\n5.135323\n5\nFalse.\n5\n\n\n6\nIN\n140\n737\n331-5751\nyes\nno\n0\n3.229420\n4\n3.165082\n2.440153\n8\n0.264543\n2.352274\n300\n3.869176\n5.393439\n4\n1.784765\n4\nFalse.\n6\n\n\n7\nLA\n54\n766\n871-3612\nno\nno\n0\n0.567920\n6\n1.950098\n4.507027\n0\n4.473086\n0.688785\n400\n6.132137\n5.012747\n5\n0.417421\n8\nFalse.\n7\n\n\n8\nMO\n195\n777\n249-5723\nyes\nno\n0\n5.811116\n6\n4.331065\n8.104126\n2\n4.475034\n4.208352\n250\n5.974575\n4.750153\n7\n3.320311\n7\nTrue.\n8\n\n\n9\nAL\n104\n657\n767-7682\nyes\nno\n0\n2.714430\n7\n5.138669\n8.529944\n6\n3.321121\n2.342177\n300\n4.328966\n3.433554\n5\n5.677058\n4\nFalse.\n9\n\n\n\n\n\n\n\n\ndf.shape\n\n(5000, 22)\n\n\nBy modern standards, it’s a relatively small dataset, with only 5,000 records, where each record uses 21 attributes to describe the profile of a customer of an unknown US mobile operator. The attributes are:\n\nState: the US state in which the customer resides, indicated by a two-letter abbreviation; for example, OH or NJ\nAccount Length: the number of days that this account has been active\nArea Code: the three-digit area code of the corresponding customer’s phone number\nPhone: the remaining seven-digit phone number\nInt’l Plan: whether the customer has an international calling plan: yes/no\nVMail Plan: whether the customer has a voice mail feature: yes/no\nVMail Message: the average number of voice mail messages per month\nDay Mins: the total number of calling minutes used during the day\nDay Calls: the total number of calls placed during the day\nDay Charge: the billed cost of daytime calls\nEve Mins, Eve Calls, Eve Charge: the billed cost for calls placed during the evening\nNight Mins, Night Calls, Night Charge: the billed cost for calls placed during nighttime\nIntl Mins, Intl Calls, Intl Charge: the billed cost for international calls\nCustServ Calls: the number of calls placed to Customer Service\nChurn?: whether the customer left the service: true/false\n\nThe last attribute, Churn?, is known as the target attribute: the attribute that we want the ML model to predict. Because the target attribute is binary, our model will be performing binary prediction, also known as binary classification.\nWe have our dataset. Now we will split this dataset into three subsets * customer: customer data, and place it as a CSV file on the S3 bucket * account: accounts data, and place it as CSV on the same S3 bucket * utility: utility data, and place it as Glue tables\n\ncustomer_columns = ['CustomerID', 'State', 'Area Code', 'Phone']\naccount_columns = ['CustomerID', 'Account Length', \"Int'l Plan\", 'VMail Plan', 'Churn?']\nutility_columns = ['CustomerID', 'VMail Message', 'Day Mins', 'Day Calls', 'Day Charge', \n                   'Eve Mins', 'Eve Calls', 'Eve Charge', 'Night Mins', 'Night Calls', \n                   'Night Charge', 'Intl Mins', 'Intl Calls', 'Intl Charge', 'CustServ Calls']\n\nWe will use the default bucket associated with our SageMaker session. You may use any other bucket with proper access permissions.\n\nbucket = sess.default_bucket()\nbucket\n\n'sagemaker-us-east-1-801598032724'\n\n\nNext, we will use AWS Data Wrangler Python package (awswrangler) to create an AWS Glue database.\nawswrangler is an open source Python library maintained by AWS team, as is defined as\n\nAn AWS Professional Service open source python initiative that extends the power of Pandas library to AWS connecting DataFrames and AWS data related services. Easy integration with Athena, Glue, Redshift, Timestream, OpenSearch, Neptune, QuickSight, Chime, CloudWatchLogs, DynamoDB, EMR, SecretManager, PostgreSQL, MySQL, SQLServer and S3 (Parquet, CSV, JSON and EXCEL).\n\nYou may read more about this library here * Documentation: https://aws-data-wrangler.readthedocs.io/en/stable/what.html * Github repo: https://github.com/awslabs/aws-data-wrangler\nPlease note that AWS SageMaker session needs some additional AWS Glue permissions to create a database. If you get an error while creating a Glue database in following steps then add those permissions.\nError: AccessDeniedException: An error occurred (AccessDeniedException) when calling the GetDatabase operation: User: arn:aws:sts::801598032724:assumed-role/AmazonSageMaker-ExecutionRole-20220516T161743/SageMaker is not authorized to perform: glue:GetDatabase on resource: arn:aws:glue:us-east-1:801598032724:database/telco_db because no identity-based policy allows the glue:GetDatabase action\nFix: Go to your SageMaker Execution Role and add permission AWSGlueConsoleFullAccess\n\n##\n# define the Glue DB name\ndb_name = 'telco_db'\n\n\n##\nimport awswrangler as wr\n\n# get all the existing Glue db list\ndatabases = wr.catalog.databases()\n\n# print existing db names\nprint(\"*** existing databases ***\\n\")\nprint(databases)\n\n# if our db does not exist then create it\nif db_name not in databases.values:\n    wr.catalog.create_database(db_name, description = 'Demo DB for telco churn dataset')\n    print(\"\\n*** existing + new databases ***\\n\")\n    print(wr.catalog.databases())\nelse:\n    print(f\"Database {db_name} already exists\")\n\n*** existing databases ***\n\n                  Database Description\n0  sagemaker_data_wrangler            \n1     sagemaker_processing            \n\n*** existing + new databases ***\n\n                  Database                      Description\n0  sagemaker_data_wrangler                                 \n1     sagemaker_processing                                 \n2                 telco_db  Demo DB for telco churn dataset\n\n\n\n##\n# in case you want to delete a database using this notebook\n# wr.catalog.delete_database(db_name)\n\nSimilarly you can go to AWS Glue console to see that the new database has been created.\n\n\n\naws-glue-database\n\n\nNow we will place the three data subsets into their respective locations.\n\nsuffix = ['customer_info', 'account_info', 'utility']\nfor i, columns in enumerate([customer_columns, account_columns, utility_columns]):\n    \n    # get the data subset\n    df_tmp = df[columns]\n\n    # prepare filename and output path\n    fname = 'telco_churn_%s' % suffix[i]\n    outputpath = f's3://{bucket}/{prefix}/data/{fname}'\n    \n    print(f\"\\n*** working on {suffix[i]}***\")\n    print(f\"filename: {fname}\")\n    print(f\"output path: {outputpath}\")\n    \n    if i &gt; 1: # for utility\n        wr.s3.to_csv(\n            df=df_tmp,\n            path=outputpath,\n            dataset=True,\n            database=db_name,  # Athena/Glue database\n            table=fname,  # Athena/Glue table\n            index=False,\n            mode='overwrite')\n    else: # for customer and account\n        wr.s3.to_csv(\n            df=df_tmp,\n            path=f'{outputpath}.csv',\n            index=False)\n\n\n*** working on customer_info***\nfilename: telco_churn_customer_info\noutput path: s3://sagemaker-us-east-1-801598032724/myblog/demo-customer-churn/data/telco_churn_customer_info\n\n*** working on account_info***\nfilename: telco_churn_account_info\noutput path: s3://sagemaker-us-east-1-801598032724/myblog/demo-customer-churn/data/telco_churn_account_info\n\n*** working on utility***\nfilename: telco_churn_utility\noutput path: s3://sagemaker-us-east-1-801598032724/myblog/demo-customer-churn/data/telco_churn_utility\n\n\nWe can verify the uploaded data from the S3 bucket.\n\n\n\naws-s3-churn-data\n\n\nSimilarly, from Glue console we can verify that the utility table has been created.\n\n\n\naws-glue-churn-data\n\n\nIf you want to remain within the notebook and do the verification then that can also be done.\n\n##\n# list s3 objects\nwr.s3.list_objects('s3://sagemaker-us-east-1-801598032724/myblog/demo-customer-churn/data/')\n\n['s3://sagemaker-us-east-1-801598032724/myblog/demo-customer-churn/data/telco_churn_account_info.csv',\n 's3://sagemaker-us-east-1-801598032724/myblog/demo-customer-churn/data/telco_churn_customer_info.csv',\n 's3://sagemaker-us-east-1-801598032724/myblog/demo-customer-churn/data/telco_churn_utility/b4003acdf33e48ce989401e92146923c.csv']\n\n\n\n##\n# list glue catalog tables\nwr.catalog.tables()\n\n\n\n\n\n\n\n\nDatabase\nTable\nDescription\nTableType\nColumns\nPartitions\n\n\n\n\n0\ntelco_db\ntelco_churn_utility\n\nEXTERNAL_TABLE\ncustomerid, vmail_message, day_mins, day_calls..."
  },
  {
    "objectID": "posts/2022-05-17-aws-sagemaker-wrangler-p1.html#summary",
    "href": "posts/2022-05-17-aws-sagemaker-wrangler-p1.html#summary",
    "title": "Data Preparation with SageMaker Data Wrangler (Part 1)",
    "section": "Summary",
    "text": "Summary\nAt this point we have our dataset ready in AWS S3 and Glue, and in the next part we will use AWS SageMaker Data Wrangler to import and join this data."
  },
  {
    "objectID": "posts/2022-05-23-aws-sagemaker-wrangler-p2.html#launch-sagemaker-data-wrangler-flow",
    "href": "posts/2022-05-23-aws-sagemaker-wrangler-p2.html#launch-sagemaker-data-wrangler-flow",
    "title": "Data Preparation with SageMaker Data Wrangler (Part 2)",
    "section": "Launch SageMaker Data Wrangler Flow",
    "text": "Launch SageMaker Data Wrangler Flow\nCreate a new Data Wrangler flow by clicking on the main menu tabs File &gt; New &gt; Data Wrangler Flow.\n\n\n\ndata-wrangler-new-flow\n\n\nOnce launched SageMaker may take a minute to initialize a new flow. The reason for this is SageMaker will launch a separate machine in the background ml.m5.4xlarge with 16vCPU and 64 GiB memory for processing flow files. A flow file is a JSON file that just captures all the steps performed from the Flow UI console. When you execute the flow, the Flow engine parses this file and performs all the steps. Once a new flow file is available, rename it to customer-churn.flow.\n\n\n\ndata-wrangler-flow-ready"
  },
  {
    "objectID": "posts/2022-05-23-aws-sagemaker-wrangler-p2.html#import-data-from-sources",
    "href": "posts/2022-05-23-aws-sagemaker-wrangler-p2.html#import-data-from-sources",
    "title": "Data Preparation with SageMaker Data Wrangler (Part 2)",
    "section": "Import data from sources",
    "text": "Import data from sources\nFirst, we will create a flow to import data (created in the part-1 post) from S3 bucket. For this from the flow UI click on Amazon S3 bucket. From the next window select the bucket name S3://sagemaker-us-east-1-801598032724. In your case, it could be different where you have stored the data. From the UI select the filename “telco_churn_customer_info.csv” and click Import\n\n\n\ncustomer-churn-s3\n\n\nOnce the data is imported repeat the steps for the filename “telco_churn_account_info.csv”. If you are not seeing the “import from S3 bucket” option on the UI then check the flow UI and click on the ‘Import’ tab option. Once both files are imported, your Data Flow tab will look similar to this\n\n\n\ndata-flow-customer-account.png\n\n\nNow that we have imported data from S3, we can now work on importing data from the Athena database. For this from the Flow UI Import tab click on Amazon Athena option. From the next UI select AwsDataCatalog Data catalog option. For Databases drop down select telco_db and in the query pane write the below query.\nselect * from telco_churn_utility\nYou can also preview the data by clicking on the table preview option. Once satisfied with the results click ‘Import’. When asked about the database name write telco_churn_utility\n\n\n\nimport-athena-table.png\n\n\nAt this point, you will find all three tables imported in Data Flow UI. Against each table, a plus sign (+) will appear that you can use to add any transformations you want to apply on each table.\n\n\n\nall-tables-imported.png\n\n\nfor telco_churn_customer_info click on the plus sign and then select ‘Edit’ to change data types.\n\n\n\nedit_customer_info.png\n\n\nWe will add the following transformations * Change Area Code from Long to String * Click Preview * Then click Apply\n\n\n\ntelco_churn_customer_info_edit.png\n\n\nSimilarly for telco_churn_account_info.csv edit data types as * Change Account Length to Long * Change Int’l Plan and VMail Plan to Bool * Click Preview and then click Apply\nFor telco_churn_utility.csv edit data types as * Change custserv_calls to Long * Click Preview and then click Apply\nAt this point, we have imported the data from all three sources and have also properly transformed their column types."
  },
  {
    "objectID": "posts/2022-05-23-aws-sagemaker-wrangler-p2.html#joining-tables",
    "href": "posts/2022-05-23-aws-sagemaker-wrangler-p2.html#joining-tables",
    "title": "Data Preparation with SageMaker Data Wrangler (Part 2)",
    "section": "Joining Tables",
    "text": "Joining Tables\nNow we will join all three tables to get a full dataset. For this from the Flow UI Data flow click on the plus sign next to customer_info data type and this time select ‘Join’. From the new window select account_info as the right dataset and click Configure\n\n\n\njoin-configure.png\n\n\nFrom the next screen select * Join Type = Full Outer * Columns Left = CustomerID * Columns Right = CustomerID * Click Preview and then Add\n\n\n\njoin-preview.png\n\n\nA new join step will appear on the Data Flow UI. Click on the plus sign next to it and repeat the steps for utility table\n\n\n\nfirst-join.png\n\n\n\nJoin Type = Full Outer\nColumns Left = CustomerID_0\nColumns Right = CustomerID\nClick Preview and then Add\n\n\n\n\njoin-second.png"
  },
  {
    "objectID": "posts/2022-05-25-aws-sagemaker-wrangler-p4.html#remove-redundant-columns",
    "href": "posts/2022-05-25-aws-sagemaker-wrangler-p4.html#remove-redundant-columns",
    "title": "Data Preparation with SageMaker Data Wrangler (Part 4)",
    "section": "Remove redundant columns",
    "text": "Remove redundant columns\nWhen we made joins between tables (see part-2) it resulted in some redundant columns CustomerID_* . We will remove them first. For this click on plus sign beside 2nd Join, and select Add Transform. From the next transform UI clink Add Step and then search for transformer Manage Column. Inside Manage Columns transformer select\n\nTransform = Drop Column\nColumns to drop = CustomerID_0, CustomerID_1\n\nClick preview and Add."
  },
  {
    "objectID": "posts/2022-05-25-aws-sagemaker-wrangler-p4.html#remove-features-with-low-predictive-power",
    "href": "posts/2022-05-25-aws-sagemaker-wrangler-p4.html#remove-features-with-low-predictive-power",
    "title": "Data Preparation with SageMaker Data Wrangler (Part 4)",
    "section": "Remove features with low predictive power",
    "text": "Remove features with low predictive power\nIn part-3 we used Quick Model to get the predictive power of features. When we analyze features with low importance we find that Phone is one such feature that does not hold much information for the model. For a model, a phone number is just some random collection of numbers and does not hold any meaning. There are other features with low importance too but they still hold some information for the model. So let’s drop Phone. The steps will be same as in the last part."
  },
  {
    "objectID": "posts/2022-05-25-aws-sagemaker-wrangler-p4.html#transform-feature-values-to-correct-format",
    "href": "posts/2022-05-25-aws-sagemaker-wrangler-p4.html#transform-feature-values-to-correct-format",
    "title": "Data Preparation with SageMaker Data Wrangler (Part 4)",
    "section": "Transform feature values to correct format",
    "text": "Transform feature values to correct format\nChurn? is our target label but its value has an extra ‘.’ at the end. If we remove that symbol then it can easily be converted to a Boolean type. So let’s do that. From the transformers list this time choose Format String and select\n\nTransform = Remove Symbols\nInput Columns = Churn?\nSymbols = .\n\nClick Preview and Add.\n\n\n\nformat-strings.png\n\n\nNow that the data is in the correct format (True/False) we can apply another transformer on it to convert it to Boolean feature. So select PARSE COLUMN AS TYPE transformer and configure\n\nColumn = Churn?\nFrom = String\nTo = Boolean\n\nClick Preview and then Add."
  },
  {
    "objectID": "posts/2022-05-25-aws-sagemaker-wrangler-p4.html#encode-categorical-features",
    "href": "posts/2022-05-25-aws-sagemaker-wrangler-p4.html#encode-categorical-features",
    "title": "Data Preparation with SageMaker Data Wrangler (Part 4)",
    "section": "Encode categorical features",
    "text": "Encode categorical features\nAt this point we have only two columns with String datatype: State and Area Code. If we look at the Area Code it has high variance and little feature importance. It is better to drop this feature. So Add another transformer and drop Area Code. For State we will apply one-hot encoding. So for this select transformer Encode Categorical and configure\n\nTransform = One-hot encode\nInput Columns = State\nOutput style = Columns\n\nLeave the rest of the options as default. Click Preview and Add.\n\n\n\none-hot-encode.png"
  },
  {
    "objectID": "posts/2022-05-25-aws-sagemaker-wrangler-p4.html#move-the-target-label-to-the-start",
    "href": "posts/2022-05-25-aws-sagemaker-wrangler-p4.html#move-the-target-label-to-the-start",
    "title": "Data Preparation with SageMaker Data Wrangler (Part 4)",
    "section": "Move the target label to the start",
    "text": "Move the target label to the start\nSageMaker requires that the target label should be the first column in the dataset. So add another transformer Manage columns and configure\n\nTransform = Move column\nMove Type = Move to start\nColumn to move = Churn?\n\n\n\n\nmove-target.png"
  },
  {
    "objectID": "posts/2022-05-25-aws-sagemaker-wrangler-p4.html#evaluate-model-performance",
    "href": "posts/2022-05-25-aws-sagemaker-wrangler-p4.html#evaluate-model-performance",
    "title": "Data Preparation with SageMaker Data Wrangler (Part 4)",
    "section": "Evaluate model performance",
    "text": "Evaluate model performance\nWe have done some key transformations. We can use Quick Model again to analyze the model performance at this point. We have done a similar analysis in part-3 so let’s do it again and compare the results. From the last transformation step, click plus sign and choose Add Analysis\n\n\n\nquick_model_2.png\n\n\nWe can see from the results that these transformations have a positive impact on the model performance and the F1 score has moved up from 0.841 to 0.861."
  },
  {
    "objectID": "posts/2022-05-30-storemagic-jupyter-notebook.html#store-a-variable",
    "href": "posts/2022-05-30-storemagic-jupyter-notebook.html#store-a-variable",
    "title": "storemagic - Don’t lose your variables in Jupyter Notebook",
    "section": "Store a variable",
    "text": "Store a variable\nUse %store magic for lightweight persistence. It stores variables, aliases and macros in IPython’s database. Let’s create a variable and then store it using this magic.\n\n##\n# create a variable\nvar_hello = \"hello world!\"\n\n# store this variable\n%store var_hello\n\nStored 'var_hello' (str)\n\n\nNow I am going to intentionally restart the kernel. We can check that our created variable is now gone from the memory.\n\nvar_hello\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\n~\\AppData\\Local\\Temp/ipykernel_12100/520558475.py in &lt;module&gt;\n----&gt; 1 var_hello\n\nNameError: name 'var_hello' is not defined\n\n\n\nBut no worries. We have it stored safely using our magic. So let’s get it back.\n\n## \n# get the variable back from store\n%store -r var_hello\n\nOkay, we have our variable back and (with a sigh of relief) we can use it again.\n\nvar_hello\n\n'hello world!'\n\n\nLet’s create a few more variables and do some more magic with them.\n\n##\n# create variables\nvar_foo = [1,2,3,4]\nvar_bar = {'a':var_hello}\n\n# store multiple variables\n%store var_foo var_bar\n\nStored 'var_foo' (list)\nStored 'var_bar' (dict)"
  },
  {
    "objectID": "posts/2022-05-30-storemagic-jupyter-notebook.html#check-all-stored-varaibles",
    "href": "posts/2022-05-30-storemagic-jupyter-notebook.html#check-all-stored-varaibles",
    "title": "storemagic - Don’t lose your variables in Jupyter Notebook",
    "section": "Check all stored varaibles",
    "text": "Check all stored varaibles\nWe can check all the varaibles stored using the following magic command.\n\n%store\n\nStored variables and their in-db values:\nvar_bar               -&gt; {'a': 'hello world!'}\nvar_foo               -&gt; [1, 2, 3, 4]\nvar_hello             -&gt; 'hello world!'"
  },
  {
    "objectID": "posts/2022-05-30-storemagic-jupyter-notebook.html#remove-a-variable-from-store",
    "href": "posts/2022-05-30-storemagic-jupyter-notebook.html#remove-a-variable-from-store",
    "title": "storemagic - Don’t lose your variables in Jupyter Notebook",
    "section": "Remove a variable from store",
    "text": "Remove a variable from store\nTo remove a variable from our storage is also straight forward. Put its name after %store -d flag\n\n##\n# remove 'var_hello'\n%store -d var_hello\n\n‘var_hello’ is now gone. Forever …\n\n##\n# check the remaining variables in store\n%store\n\nStored variables and their in-db values:\nvar_bar             -&gt; {'a': 'hello world!'}\nvar_foo             -&gt; [1, 2, 3, 4]"
  },
  {
    "objectID": "posts/2022-05-30-storemagic-jupyter-notebook.html#remove-all-variables-from-store",
    "href": "posts/2022-05-30-storemagic-jupyter-notebook.html#remove-all-variables-from-store",
    "title": "storemagic - Don’t lose your variables in Jupyter Notebook",
    "section": "Remove all variables from store",
    "text": "Remove all variables from store\nIf you want to remove all the varaibles from store and start clean then use -z flag\n\n##\n# remove all variables\n%store -z\n\nKaboom! all variables are gone.\n\n##\n# check store\n%store\n\nStored variables and their in-db values:"
  },
  {
    "objectID": "posts/2022-05-30-storemagic-jupyter-notebook.html#reference",
    "href": "posts/2022-05-30-storemagic-jupyter-notebook.html#reference",
    "title": "storemagic - Don’t lose your variables in Jupyter Notebook",
    "section": "Reference",
    "text": "Reference\nCheck the official IPython storemagic documentation here: https://ipython.readthedocs.io/en/stable/config/extensions/storemagic.html"
  },
  {
    "objectID": "posts/2022-06-17-sagemaker-endpoint.html#through-sagemaker-console-ui",
    "href": "posts/2022-06-17-sagemaker-endpoint.html#through-sagemaker-console-ui",
    "title": "Serverless Inference with SageMaker Serverless Endpoints",
    "section": "Through SageMaker Console UI",
    "text": "Through SageMaker Console UI\nLet’s first deploy our serverless endpoint through SageMaker console UI. In the next section, we will do the same through SageMaker Python SDK.\nVisit the SageMaker model repository to find the registered Linear Learner model. You can find the repository on the SageMaker Inference &gt; Model page.\n\n\n\nmodel-repo\n\n\nNote the mode name linear-learner-2022-06-16-09-10-17-207 as will need it in later steps.\nClick on the model name and then Create endpoint\n\n\n\ncreate-endpoint\n\n\nThis will take you to configure endpoint page. Here do the following configurations. * Set Endpoint name to 2022-06-17-sagemaker-endpoint-serverless. You may use any other unique string here. * From Attach endpoint configuration select create a new endpoint configuration * From New endpoint configuration &gt; Endpoint configuration set * Endpoint configuration name to config-2022-06-17-sagemaker-endpoint-serverless. You may use any other name here. * Type of endpoint to Serverless * From Production variants click on Add Model and then select the model name we want to deploy. In our case it is linear-learner-2022-06-16-09-10-17-207. Click Save.\n\n\n\nadd-model\n\n\n\nThen Edit the Max Concurrency and set it to 5.\n\n\n\n\nmax-concurrency\n\n\n\nClick Create endpoint configuration\n\n\n\n\nnew-endpoint-config\n\n\n\nClick Create endpoint\n\n\n\n\nendpoint-created\n\n\nIt will take a minute for the created endpoint to become ready.\nWhile we were configuring the concurrency for our endpoint we have given it a value of 5. This is because at this point there is a limit on concurrency per account across all serverless endpoints. The maximum total concurrency for an account is 20, and if you cross this limit you will get an error as shown below.\n\n\n\nserverless-endpoints-concurrency-error"
  },
  {
    "objectID": "posts/2022-06-17-sagemaker-endpoint.html#through-sagemaker-python-sdk",
    "href": "posts/2022-06-17-sagemaker-endpoint.html#through-sagemaker-python-sdk",
    "title": "Serverless Inference with SageMaker Serverless Endpoints",
    "section": "Through SageMaker Python SDK",
    "text": "Through SageMaker Python SDK\nLet’s create another endpoint but using SageMaker SDK. Deploying a model to a serverless endpoint using SDK involves the following steps: * Get session to SageMaker API * Create a serverless endpoint deployment config * Create a reference to a model container * Deploy the model on a serverless endpoint using serverless configuration\nLet’s do it now.\n\n##\n# get a session to sagemaker api\nimport sagemaker\n\nsession = sagemaker.Session()\nrole = sagemaker.get_execution_role()\n\nprint(f\"sagemaker.__version__: {sagemaker.__version__}\")\nprint(f\"Session: {session}\")\nprint(f\"Role: {role}\")\n\nsagemaker.__version__: 2.88.1\nSession: &lt;sagemaker.session.Session object at 0x7feb1853fc10&gt;\nRole: arn:aws:iam::801598032724:role/service-role/AmazonSageMaker-ExecutionRole-20220516T161743\n\n\n\n##\n# define a serverless endpoint configuration\nfrom sagemaker.serverless import ServerlessInferenceConfig\n\nserverless_config = ServerlessInferenceConfig(\n    memory_size_in_mb=1024, max_concurrency=5\n)\n\nNote that here we are only defining the endpoint configuration. It will be created when we will deploy the model. Also, note that we have not passed any configuration name. It will default to the endpoint name. To read more about the serverless inference configuration read the documentation ServerlessInferenceConfig\nI could not find a way to give a name to endpoint configuration from SageMaker SDK. Let me know in the comments if there is a way to do it.\n\n##\n# create a SageMaker model. \n# In our case model is already registered so it will only create a reference to it\nfrom sagemaker.model import Model\n\nll_model = Model(\n    image_uri = '382416733822.dkr.ecr.us-east-1.amazonaws.com/linear-learner', # find it from the SageMaker mode repository\n    name = 'linear-learner-2022-06-16-09-10-17-207',\n    role=role\n)\n\nWhile creating a SageMaker model you need to provide its container URI, name, and role. The role gives necessary permissions to SageMaker to pull the image container from the ECR repository. To read more about the Model read the docs sagemaker.model.Model\n\n##\n# define the endpoint name\nendpoint_name = '2022-06-17-sagemaker-endpoint-serverless-sdk'\n\n\n##\n# deploy the model to serverless endpoint\nll_model.deploy(\n    endpoint_name=endpoint_name,\n    serverless_inference_config=serverless_config,\n)\n\nUsing already existing model: linear-learner-2022-06-16-09-10-17-207\n\n\n-----!\n\n\nIt will take a minute or so for the serverless endpoint to get provisioned. Once it is ready (InService) you will find it on the SageMaker Inference &gt; Endpoints page.\n\n\n\nendpoint-created-sdk\n\n\nmodel.deploy() command will also create the endpoint configuration with same name as endpoint, and it can be found on SageMaker Inference &gt; Endpoint configurations page\n\n\n\nnew-endpoint-config-sdk"
  },
  {
    "objectID": "posts/2022-07-07-sagemaker-script-mode.html#download-and-preprocess-data",
    "href": "posts/2022-07-07-sagemaker-script-mode.html#download-and-preprocess-data",
    "title": "Deploy Scikit-learn Models to Amazon SageMaker with the SageMaker Python SDK using Script mode",
    "section": "Download and preprocess data",
    "text": "Download and preprocess data\n\n##\n# download dataset\nimport boto3\nimport pandas as pd\nimport numpy as np\n\ns3 = boto3.client(\"s3\")\ns3.download_file(\n    f\"sagemaker-sample-files\", \"datasets/tabular/iris/iris.data\", \"iris.data\"\n)\n\ndf = pd.read_csv(\n    \"iris.data\",\n    header=None,\n    names=[\"sepal_len\", \"sepal_wid\", \"petal_len\", \"petal_wid\", \"class\"],\n)\ndf.head()\n\n\n\n\n\n\n\n\nsepal_len\nsepal_wid\npetal_len\npetal_wid\nclass\n\n\n\n\n0\n5.1\n3.5\n1.4\n0.2\nIris-setosa\n\n\n1\n4.9\n3.0\n1.4\n0.2\nIris-setosa\n\n\n2\n4.7\n3.2\n1.3\n0.2\nIris-setosa\n\n\n3\n4.6\n3.1\n1.5\n0.2\nIris-setosa\n\n\n4\n5.0\n3.6\n1.4\n0.2\nIris-setosa\n\n\n\n\n\n\n\n\n##\n# Convert the three classes from strings to integers in {0,1,2}\ndf[\"class_cat\"] = df[\"class\"].astype(\"category\").cat.codes\ncategories_map = dict(enumerate(df[\"class\"].astype(\"category\").cat.categories))\nprint(categories_map)\ndf.head()\n\n{0: 'Iris-setosa', 1: 'Iris-versicolor', 2: 'Iris-virginica'}\n\n\n\n\n\n\n\n\n\nsepal_len\nsepal_wid\npetal_len\npetal_wid\nclass\nclass_cat\n\n\n\n\n0\n5.1\n3.5\n1.4\n0.2\nIris-setosa\n0\n\n\n1\n4.9\n3.0\n1.4\n0.2\nIris-setosa\n0\n\n\n2\n4.7\n3.2\n1.3\n0.2\nIris-setosa\n0\n\n\n3\n4.6\n3.1\n1.5\n0.2\nIris-setosa\n0\n\n\n4\n5.0\n3.6\n1.4\n0.2\nIris-setosa\n0"
  },
  {
    "objectID": "posts/2022-07-07-sagemaker-script-mode.html#prepare-and-store-train-and-test-sets-as-csv-files",
    "href": "posts/2022-07-07-sagemaker-script-mode.html#prepare-and-store-train-and-test-sets-as-csv-files",
    "title": "Deploy Scikit-learn Models to Amazon SageMaker with the SageMaker Python SDK using Script mode",
    "section": "Prepare and store train and test sets as CSV files",
    "text": "Prepare and store train and test sets as CSV files\n\n##\n# split the data into train and test set\nfrom sklearn.model_selection import train_test_split\n\ntrain, test = train_test_split(df, test_size=0.2, random_state=42)\n\nprint(f\"train.shape: {train.shape}\")\nprint(f\"test.shape: {test.shape}\")\n\ntrain.shape: (120, 6)\ntest.shape: (30, 6)\n\n\nWe have our dataset ready. Let’s define a local directory local_path to keep all the files and artifacts related to this post. I will refer to this directory as ‘workspace’.\n\n##\n# `local_path` will be the root directory for this post.\nlocal_path = \"./datasets/2022-07-07-sagemaker-script-mode\"\n\nWe have train and test sets ready. Let’s create two more directories in our workspace and store our data in them.\n\nfrom pathlib import Path\n\n# local paths\nlocal_train_path = local_path + \"/train\"\nlocal_test_path = local_path + \"/test\"\n\n# create local directories\nPath(local_train_path).mkdir(parents=True, exist_ok=True)\nPath(local_test_path).mkdir(parents=True, exist_ok=True)\n\nprint(\"local_train_path: \", local_train_path)\nprint(\"local_test_path: \", local_test_path)\n\n# local file names\nlocal_train_file = local_train_path + \"/train.csv\"\nlocal_test_file = local_test_path + \"/test.csv\"\n\n# write train and test CSV files\ntrain.to_csv(local_train_file, index=False)\ntest.to_csv(local_test_file, index=False)\n\nprint(\"local_train_file: \", local_train_file)\nprint(\"local_test_file: \", local_test_file)\n\nlocal_train_path:  ./datasets/2022-07-07-sagemaker-script-mode/train\nlocal_test_path:  ./datasets/2022-07-07-sagemaker-script-mode/test\nlocal_train_file:  ./datasets/2022-07-07-sagemaker-script-mode/train/train.csv\nlocal_test_file:  ./datasets/2022-07-07-sagemaker-script-mode/test/test.csv"
  },
  {
    "objectID": "posts/2022-07-07-sagemaker-script-mode.html#create-sagemaker-session",
    "href": "posts/2022-07-07-sagemaker-script-mode.html#create-sagemaker-session",
    "title": "Deploy Scikit-learn Models to Amazon SageMaker with the SageMaker Python SDK using Script mode",
    "section": "Create SageMaker session",
    "text": "Create SageMaker session\n\nimport sagemaker\n\nsession = sagemaker.Session()\nrole = sagemaker.get_execution_role()\nbucket = session.default_bucket()\nregion = session.boto_region_name\n\nprint(\"sagemaker.__version__: \", sagemaker.__version__)\nprint(\"Session: \", session)\nprint(\"Role: \", role)\nprint(\"Bucket: \", bucket)\nprint(\"Region: \", region)\n\nsagemaker.__version__:  2.86.2\nSession:  &lt;sagemaker.session.Session object at 0x7f80ad720460&gt;\nRole:  arn:aws:iam::801598032724:role/service-role/AmazonSageMakerServiceCatalogProductsUseRole\nBucket:  sagemaker-us-east-1-801598032724\nRegion:  us-east-1\n\n\nWhat we have done here is * imported the SageMaker Python SDK into our runtime * get a session to work with SageMaker API and other AWS services * get the execution role associated with the user profile. It is the same profile that is available to the user to work from console UI and has AmazonSageMakerFullAccess policy attached to it. * create or get a default bucket to use and return its name. Default bucket name has the format sagemaker-{region}-{account_id}. If it doesn’t exist then our session will automatically create it. You may also use any other bucket in its place given that you have enough permission for reading and writing. * get the region name attached to our session\nNext, we will use this session to upload data to our default bucket."
  },
  {
    "objectID": "posts/2022-07-07-sagemaker-script-mode.html#upload-data-to-amazon-s3-bucket",
    "href": "posts/2022-07-07-sagemaker-script-mode.html#upload-data-to-amazon-s3-bucket",
    "title": "Deploy Scikit-learn Models to Amazon SageMaker with the SageMaker Python SDK using Script mode",
    "section": "Upload data to Amazon S3 bucket",
    "text": "Upload data to Amazon S3 bucket\n\n##\n# You may choose any other prefix for your bucket.\n# All the data related to this post will be under this prefix.\nbucket_prefix = \"2022-07-07-sagemaker-script-mode\"\n\nNow upload the data. In the output, we will get the complete path (S3 URI) for our uploaded data.\n\ns3_train_uri = session.upload_data(local_train_file, key_prefix=bucket_prefix + \"/data\")\ns3_test_uri = session.upload_data(local_test_file, key_prefix=bucket_prefix + \"/data\")\n\nprint(\"s3_train_uri: \", s3_train_uri)\nprint(\"s3_test_uri: \", s3_test_uri)\n\ns3_train_uri:  s3://sagemaker-us-east-1-801598032724/2022-07-07-sagemaker-script-mode/data/train.csv\ns3_test_uri:  s3://sagemaker-us-east-1-801598032724/2022-07-07-sagemaker-script-mode/data/test.csv\n\n\nAt this point, our data preparation step is complete. Train and test CSV files are available on the local system and in our default Amazon S3 bucket."
  },
  {
    "objectID": "posts/2022-07-07-sagemaker-script-mode.html#how-sagemaker-managed-environment-works",
    "href": "posts/2022-07-07-sagemaker-script-mode.html#how-sagemaker-managed-environment-works",
    "title": "Deploy Scikit-learn Models to Amazon SageMaker with the SageMaker Python SDK using Script mode",
    "section": "How SageMaker managed environment works?",
    "text": "How SageMaker managed environment works?\nWhen you send a request to SageMaker API (fit or deploy call) * it spins up new instances with the provided specification * loads the algorithm container * pulls the data from S3 * runs the training code * store the results and trained model artifacts to S3 * terminates the new instances\nAll this happens behind the scenes with a single line of code and is a huge advantage. Spinning up new hardware every time can be good for repeatability and security, but it can add some friction while testing and debugging our code. We can test our code on a small dataset in our local environment with SageMaker local mode and then switch seamlessly to SageMaker managed environment by changing a single line of code."
  },
  {
    "objectID": "posts/2022-07-07-sagemaker-script-mode.html#steps-to-prepare-amazon-sagemaker-local-environment",
    "href": "posts/2022-07-07-sagemaker-script-mode.html#steps-to-prepare-amazon-sagemaker-local-environment",
    "title": "Deploy Scikit-learn Models to Amazon SageMaker with the SageMaker Python SDK using Script mode",
    "section": "Steps to prepare Amazon SageMaker local environment",
    "text": "Steps to prepare Amazon SageMaker local environment\nInstall the following pre-requisites if you want to set up Amazon SageMaker on your local system. 1. Install required Python packages: pip install boto3 sagemaker pandas scikit-learn     pip install 'sagemaker[local]' 2. Docker Desktop installed and running on your computer: docker ps 3. You should have AWS credentials configured on your local machine to be able to pull the docker image from ECR.\n\nInstructions for SageMaker notebook instances\nYou can also set up SageMaker’s local environment in SageMaker notebook instances. Required Python packages and Docker service is already there. You only need to upgrade the sagemaker[local] Python package.\n\n#collapse_output\n# this is required for SageMaker notebook instances\n!pip install 'sagemaker[local]' --upgrade\n\n\nLooking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n\nRequirement already satisfied: sagemaker[local] in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (2.86.2)\n\nCollecting sagemaker[local]\n\n  Downloading sagemaker-2.99.0.tar.gz (542 kB)\n\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 542.7/542.7 KB 10.6 MB/s eta 0:00:0000:01\n\n  Preparing metadata (setup.py) ... done\n\nRequirement already satisfied: attrs&lt;22,&gt;=20.3.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from sagemaker[local]) (20.3.0)\n\nRequirement already satisfied: boto3&lt;2.0,&gt;=1.20.21 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from sagemaker[local]) (1.21.42)\n\nRequirement already satisfied: google-pasta in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from sagemaker[local]) (0.2.0)\n\nRequirement already satisfied: numpy&lt;2.0,&gt;=1.9.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from sagemaker[local]) (1.20.3)\n\nRequirement already satisfied: protobuf&lt;4.0,&gt;=3.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from sagemaker[local]) (3.19.1)\n\nRequirement already satisfied: protobuf3-to-dict&lt;1.0,&gt;=0.1.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from sagemaker[local]) (0.1.5)\n\nRequirement already satisfied: smdebug_rulesconfig==1.0.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from sagemaker[local]) (1.0.1)\n\nRequirement already satisfied: importlib-metadata&lt;5.0,&gt;=1.4.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from sagemaker[local]) (4.8.2)\n\nRequirement already satisfied: packaging&gt;=20.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from sagemaker[local]) (21.3)\n\nRequirement already satisfied: pandas in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from sagemaker[local]) (1.3.4)\n\nRequirement already satisfied: pathos in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from sagemaker[local]) (0.2.8)\n\nRequirement already satisfied: urllib3==1.26.8 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from sagemaker[local]) (1.26.8)\n\nRequirement already satisfied: docker-compose==1.29.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from sagemaker[local]) (1.29.2)\n\nRequirement already satisfied: docker~=5.0.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from sagemaker[local]) (5.0.3)\n\nRequirement already satisfied: PyYAML==5.4.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from sagemaker[local]) (5.4.1)\n\nRequirement already satisfied: texttable&lt;2,&gt;=0.9.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from docker-compose==1.29.2-&gt;sagemaker[local]) (1.6.4)\n\nRequirement already satisfied: websocket-client&lt;1,&gt;=0.32.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from docker-compose==1.29.2-&gt;sagemaker[local]) (0.59.0)\n\nRequirement already satisfied: docopt&lt;1,&gt;=0.6.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from docker-compose==1.29.2-&gt;sagemaker[local]) (0.6.2)\n\nRequirement already satisfied: jsonschema&lt;4,&gt;=2.5.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from docker-compose==1.29.2-&gt;sagemaker[local]) (3.2.0)\n\nRequirement already satisfied: dockerpty&lt;1,&gt;=0.4.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from docker-compose==1.29.2-&gt;sagemaker[local]) (0.4.1)\n\nRequirement already satisfied: distro&lt;2,&gt;=1.5.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from docker-compose==1.29.2-&gt;sagemaker[local]) (1.7.0)\n\nRequirement already satisfied: python-dotenv&lt;1,&gt;=0.13.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from docker-compose==1.29.2-&gt;sagemaker[local]) (0.20.0)\n\nRequirement already satisfied: requests&lt;3,&gt;=2.20.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from docker-compose==1.29.2-&gt;sagemaker[local]) (2.26.0)\n\nCollecting botocore&lt;1.25.0,&gt;=1.24.42\n\n  Downloading botocore-1.24.46-py3-none-any.whl (8.7 MB)\n\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 8.7/8.7 MB 34.3 MB/s eta 0:00:00:00:0100:01\n\nRequirement already satisfied: s3transfer&lt;0.6.0,&gt;=0.5.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from boto3&lt;2.0,&gt;=1.20.21-&gt;sagemaker[local]) (0.5.2)\n\nRequirement already satisfied: jmespath&lt;2.0.0,&gt;=0.7.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from boto3&lt;2.0,&gt;=1.20.21-&gt;sagemaker[local]) (0.10.0)\n\nRequirement already satisfied: zipp&gt;=0.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from importlib-metadata&lt;5.0,&gt;=1.4.0-&gt;sagemaker[local]) (3.6.0)\n\nRequirement already satisfied: pyparsing!=3.0.5,&gt;=2.0.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from packaging&gt;=20.0-&gt;sagemaker[local]) (3.0.6)\n\nRequirement already satisfied: six in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from protobuf3-to-dict&lt;1.0,&gt;=0.1.5-&gt;sagemaker[local]) (1.16.0)\n\nRequirement already satisfied: python-dateutil&gt;=2.7.3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from pandas-&gt;sagemaker[local]) (2.8.2)\n\nRequirement already satisfied: pytz&gt;=2017.3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from pandas-&gt;sagemaker[local]) (2021.3)\n\nRequirement already satisfied: multiprocess&gt;=0.70.12 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from pathos-&gt;sagemaker[local]) (0.70.12.2)\n\nRequirement already satisfied: pox&gt;=0.3.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from pathos-&gt;sagemaker[local]) (0.3.0)\n\nRequirement already satisfied: dill&gt;=0.3.4 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from pathos-&gt;sagemaker[local]) (0.3.4)\n\nRequirement already satisfied: ppft&gt;=1.6.6.4 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from pathos-&gt;sagemaker[local]) (1.6.6.4)\n\nRequirement already satisfied: paramiko&gt;=2.4.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from docker~=5.0.0-&gt;sagemaker[local]) (2.10.3)\n\nRequirement already satisfied: pyrsistent&gt;=0.14.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from jsonschema&lt;4,&gt;=2.5.1-&gt;docker-compose==1.29.2-&gt;sagemaker[local]) (0.18.0)\n\nRequirement already satisfied: setuptools in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from jsonschema&lt;4,&gt;=2.5.1-&gt;docker-compose==1.29.2-&gt;sagemaker[local]) (59.4.0)\n\nRequirement already satisfied: charset-normalizer~=2.0.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from requests&lt;3,&gt;=2.20.0-&gt;docker-compose==1.29.2-&gt;sagemaker[local]) (2.0.8)\n\nRequirement already satisfied: idna&lt;4,&gt;=2.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from requests&lt;3,&gt;=2.20.0-&gt;docker-compose==1.29.2-&gt;sagemaker[local]) (3.1)\n\nRequirement already satisfied: certifi&gt;=2017.4.17 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from requests&lt;3,&gt;=2.20.0-&gt;docker-compose==1.29.2-&gt;sagemaker[local]) (2021.10.8)\n\nRequirement already satisfied: pynacl&gt;=1.0.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from paramiko&gt;=2.4.2-&gt;docker~=5.0.0-&gt;sagemaker[local]) (1.5.0)\n\nRequirement already satisfied: cryptography&gt;=2.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from paramiko&gt;=2.4.2-&gt;docker~=5.0.0-&gt;sagemaker[local]) (36.0.0)\n\nRequirement already satisfied: bcrypt&gt;=3.1.3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from paramiko&gt;=2.4.2-&gt;docker~=5.0.0-&gt;sagemaker[local]) (3.2.0)\n\nRequirement already satisfied: cffi&gt;=1.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from bcrypt&gt;=3.1.3-&gt;paramiko&gt;=2.4.2-&gt;docker~=5.0.0-&gt;sagemaker[local]) (1.15.0)\n\nRequirement already satisfied: pycparser in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from cffi&gt;=1.1-&gt;bcrypt&gt;=3.1.3-&gt;paramiko&gt;=2.4.2-&gt;docker~=5.0.0-&gt;sagemaker[local]) (2.21)\n\nBuilding wheels for collected packages: sagemaker\n\n  Building wheel for sagemaker (setup.py) ... done\n\n  Created wheel for sagemaker: filename=sagemaker-2.99.0-py2.py3-none-any.whl size=756462 sha256=309b5159cfb7f5c739c6159b8bf309bfa7ce28d2ca402296e824f3e84bc837c1\n\n  Stored in directory: /home/ec2-user/.cache/pip/wheels/fc/df/14/14b7871f4cf108cfe8891338510d97e28cfe2da00f37114fcf\n\nSuccessfully built sagemaker\n\nInstalling collected packages: botocore, sagemaker\n\n  Attempting uninstall: botocore\n\n    Found existing installation: botocore 1.24.19\n\n    Uninstalling botocore-1.24.19:\n\n      Successfully uninstalled botocore-1.24.19\n\n  Attempting uninstall: sagemaker\n\n    Found existing installation: sagemaker 2.86.2\n\n    Uninstalling sagemaker-2.86.2:\n\n      Successfully uninstalled sagemaker-2.86.2\n\nERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n\nawscli 1.22.97 requires botocore==1.24.42, but you have botocore 1.24.46 which is incompatible.\n\naiobotocore 2.0.1 requires botocore&lt;1.22.9,&gt;=1.22.8, but you have botocore 1.24.46 which is incompatible.\n\nSuccessfully installed botocore-1.24.46 sagemaker-2.99.0\n\nWARNING: You are using pip version 22.0.4; however, version 22.1.2 is available.\n\nYou should consider upgrading via the '/home/ec2-user/anaconda3/envs/python3/bin/python -m pip install --upgrade pip' command.\n\n\n\n\n\n\n\nInstructions for SageMaker Studio environment\nNote that SageMaker local mode will not work in SageMaker Studio environment as it does not have docker service installed on the provided instances."
  },
  {
    "objectID": "posts/2022-07-07-sagemaker-script-mode.html#create-sagemaker-local-session",
    "href": "posts/2022-07-07-sagemaker-script-mode.html#create-sagemaker-local-session",
    "title": "Deploy Scikit-learn Models to Amazon SageMaker with the SageMaker Python SDK using Script mode",
    "section": "Create SageMaker local session",
    "text": "Create SageMaker local session\nSageMaker local session is required for working in a local environment. Let’s create it.\n\nfrom sagemaker.local import LocalSession\n\nsession_local = LocalSession()\nsession_local\n\n&lt;sagemaker.local.local_session.LocalSession at 0x7f80ac223910&gt;\n\n\n\n##\n# configure local session\nsession_local.config = {\"local\": {\"local_code\": True}}"
  },
  {
    "objectID": "posts/2022-07-07-sagemaker-script-mode.html#inspecting-sagemaker-sklearn-docker-image",
    "href": "posts/2022-07-07-sagemaker-script-mode.html#inspecting-sagemaker-sklearn-docker-image",
    "title": "Deploy Scikit-learn Models to Amazon SageMaker with the SageMaker Python SDK using Script mode",
    "section": "Inspecting SageMaker SKLearn docker image",
    "text": "Inspecting SageMaker SKLearn docker image\nSince the container was executed in the local environment, we can also inspect the SageMaker SKLearn local image.\n\n!docker images\n\nREPOSITORY                                                            TAG             IMAGE ID       CREATED       SIZE\n683313688378.dkr.ecr.us-east-1.amazonaws.com/sagemaker-scikit-learn   1.0-1-cpu-py3   8a6ea8272ad0   10 days ago   3.7GB\n\n\nLet’s also inspect the docker image. Notice multiple container environment variables and their default values in the output.\n\n#collapse-output\n!docker inspect 8a6ea8272ad0\n\n[\n    {\n        \"Id\": \"sha256:8a6ea8272ad003ec816569b0f879b16c770116584301161565f065aadb99436c\",\n        \"RepoTags\": [\n            \"683313688378.dkr.ecr.us-east-1.amazonaws.com/sagemaker-scikit-learn:1.0-1-cpu-py3\"\n        ],\n        \"RepoDigests\": [\n            \"683313688378.dkr.ecr.us-east-1.amazonaws.com/sagemaker-scikit-learn@sha256:fc8c3a617ff0e436c25f3b64d03e1f485f1d159478c26757f3d1d267fc849445\"\n        ],\n        \"Parent\": \"\",\n        \"Comment\": \"\",\n        \"Created\": \"2022-07-06T18:55:02.854297671Z\",\n        \"Container\": \"11b9a5fec2d61294aee63e549100ed18ceb7aa0de6a4ff198da2f556dfe3ec2f\",\n        \"ContainerConfig\": {\n            \"Hostname\": \"11b9a5fec2d6\",\n            \"Domainname\": \"\",\n            \"User\": \"\",\n            \"AttachStdin\": false,\n            \"AttachStdout\": false,\n            \"AttachStderr\": false,\n            \"ExposedPorts\": {\n                \"8080/tcp\": {}\n            },\n            \"Tty\": false,\n            \"OpenStdin\": false,\n            \"StdinOnce\": false,\n            \"Env\": [\n                \"PATH=/miniconda3/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\",\n                \"PYTHONDONTWRITEBYTECODE=1\",\n                \"PYTHONUNBUFFERED=1\",\n                \"PYTHONIOENCODING=UTF-8\",\n                \"LANG=C.UTF-8\",\n                \"LC_ALL=C.UTF-8\",\n                \"SAGEMAKER_SKLEARN_VERSION=1.0-1\",\n                \"SAGEMAKER_TRAINING_MODULE=sagemaker_sklearn_container.training:main\",\n                \"SAGEMAKER_SERVING_MODULE=sagemaker_sklearn_container.serving:main\",\n                \"SKLEARN_MMS_CONFIG=/home/model-server/config.properties\",\n                \"SM_INPUT=/opt/ml/input\",\n                \"SM_INPUT_TRAINING_CONFIG_FILE=/opt/ml/input/config/hyperparameters.json\",\n                \"SM_INPUT_DATA_CONFIG_FILE=/opt/ml/input/config/inputdataconfig.json\",\n                \"SM_CHECKPOINT_CONFIG_FILE=/opt/ml/input/config/checkpointconfig.json\",\n                \"SM_MODEL_DIR=/opt/ml/model\",\n                \"TEMP=/home/model-server/tmp\"\n            ],\n            \"Cmd\": [\n                \"/bin/sh\",\n                \"-c\",\n                \"#(nop) \",\n                \"LABEL transform_id=9be8b540-703b-4ecd-a127-c37333a0dcec_sagemaker-scikit-learn-1_0\"\n            ],\n            \"Image\": \"sha256:58b15b990d550868caed6f885423deee97a6c7f525c228a043096bf28e775d18\",\n            \"Volumes\": null,\n            \"WorkingDir\": \"\",\n            \"Entrypoint\": null,\n            \"OnBuild\": null,\n            \"Labels\": {\n                \"TRANSFORM_TYPE\": \"Aggregate-1.0\",\n                \"VERSION_SET_NAME\": \"SMFrameworksSKLearn/release-cdk\",\n                \"VERSION_SET_REVISION\": \"6086988568\",\n                \"com.amazonaws.sagemaker.capabilities.accept-bind-to-port\": \"true\",\n                \"com.amazonaws.sagemaker.capabilities.multi-models\": \"true\",\n                \"transform_id\": \"9be8b540-703b-4ecd-a127-c37333a0dcec_sagemaker-scikit-learn-1_0\"\n            }\n        },\n        \"DockerVersion\": \"20.10.15\",\n        \"Author\": \"\",\n        \"Config\": {\n            \"Hostname\": \"\",\n            \"Domainname\": \"\",\n            \"User\": \"\",\n            \"AttachStdin\": false,\n            \"AttachStdout\": false,\n            \"AttachStderr\": false,\n            \"ExposedPorts\": {\n                \"8080/tcp\": {}\n            },\n            \"Tty\": false,\n            \"OpenStdin\": false,\n            \"StdinOnce\": false,\n            \"Env\": [\n                \"PATH=/miniconda3/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\",\n                \"PYTHONDONTWRITEBYTECODE=1\",\n                \"PYTHONUNBUFFERED=1\",\n                \"PYTHONIOENCODING=UTF-8\",\n                \"LANG=C.UTF-8\",\n                \"LC_ALL=C.UTF-8\",\n                \"SAGEMAKER_SKLEARN_VERSION=1.0-1\",\n                \"SAGEMAKER_TRAINING_MODULE=sagemaker_sklearn_container.training:main\",\n                \"SAGEMAKER_SERVING_MODULE=sagemaker_sklearn_container.serving:main\",\n                \"SKLEARN_MMS_CONFIG=/home/model-server/config.properties\",\n                \"SM_INPUT=/opt/ml/input\",\n                \"SM_INPUT_TRAINING_CONFIG_FILE=/opt/ml/input/config/hyperparameters.json\",\n                \"SM_INPUT_DATA_CONFIG_FILE=/opt/ml/input/config/inputdataconfig.json\",\n                \"SM_CHECKPOINT_CONFIG_FILE=/opt/ml/input/config/checkpointconfig.json\",\n                \"SM_MODEL_DIR=/opt/ml/model\",\n                \"TEMP=/home/model-server/tmp\"\n            ],\n            \"Cmd\": [\n                \"bash\"\n            ],\n            \"Image\": \"sha256:58b15b990d550868caed6f885423deee97a6c7f525c228a043096bf28e775d18\",\n            \"Volumes\": null,\n            \"WorkingDir\": \"\",\n            \"Entrypoint\": null,\n            \"OnBuild\": null,\n            \"Labels\": {\n                \"TRANSFORM_TYPE\": \"Aggregate-1.0\",\n                \"VERSION_SET_NAME\": \"SMFrameworksSKLearn/release-cdk\",\n                \"VERSION_SET_REVISION\": \"6086988568\",\n                \"com.amazonaws.sagemaker.capabilities.accept-bind-to-port\": \"true\",\n                \"com.amazonaws.sagemaker.capabilities.multi-models\": \"true\",\n                \"transform_id\": \"9be8b540-703b-4ecd-a127-c37333a0dcec_sagemaker-scikit-learn-1_0\"\n            }\n        },\n        \"Architecture\": \"amd64\",\n        \"Os\": \"linux\",\n        \"Size\": 3699696670,\n        \"VirtualSize\": 3699696670,\n        \"GraphDriver\": {\n            \"Data\": {\n                \"LowerDir\": \"/var/lib/docker/overlay2/01a97258168fa360e9f6aa63ac0c6b2417c0ea0ebe888123edad87eb4a646765/diff:/var/lib/docker/overlay2/3b85b71e8fe52c7a27ae71ed492ff72c7e430cccdeea17046e2a361e8d7fd960/diff:/var/lib/docker/overlay2/7de8e16dd696c868ffd028a3ba1f1a80ef04237b9323229e578bc5e3aa6a29d7/diff:/var/lib/docker/overlay2/5eeb27014ab7ac7a894efdbb166d8a87fb9d4b8b739eccd82546ad6a2b53aa70/diff:/var/lib/docker/overlay2/bbd9a81a7aa5bf4c79e81ecf47670a3f8c098eee9c6682f36f88ec52db8e1946/diff:/var/lib/docker/overlay2/eb0e7f3a5bd45c1d611e4c37ba641d1e978043954312da5908fd4003c41c7e7d/diff:/var/lib/docker/overlay2/3daaedc78711e353befc51544a944ad35954327325d056094f445502bf65ce53/diff:/var/lib/docker/overlay2/9dd41e3edfb9d8f852732a968a7b179ca811e0f9d55614a0b193de753fc6aca0/diff:/var/lib/docker/overlay2/ede189a574c79eebc565041a44ebf8b586247a36a99fe3ff9588b8c940783498/diff:/var/lib/docker/overlay2/6b1d78a9c074a42d78650406b90b7b4f51eb31660a7b1e2dcc6d73cc43d29b6b/diff:/var/lib/docker/overlay2/3e0420f6740f876c9355d526cbdedd9ebde5be94ddf0d93d7dadd4f34cae351b/diff:/var/lib/docker/overlay2/de1a2da7ee1b5d9a1b4e5c3dd1adff213185dde7e1212db96c0435e512f50701/diff:/var/lib/docker/overlay2/bebca69aef394f0553634413c7875eb58228c7e6359a305a7501705e75c2b58b/diff:/var/lib/docker/overlay2/8a410db2a038a175ee6ddfb005383f8776c80b1b1901f5d2feedfc8d837ffa40/diff:/var/lib/docker/overlay2/6f6686a8cb3ccf47b214854717cbe33ba777e0985200e3d7b7f761f99231b274/diff:/var/lib/docker/overlay2/ad8b24fa9173d28a83284e4f31d830f1b3d9fe30a3fcc8cbb37895ec2fded7bf/diff:/var/lib/docker/overlay2/e8b0842f0da5b0dbb5076e350bfe1a70ef291546bbbf207fe1f90ae7ccd64517/diff\",\n                \"MergedDir\": \"/var/lib/docker/overlay2/632d2d4d01646bd8be2ec147edc70eb44f59fb262aa12b217fd560c464edd4cb/merged\",\n                \"UpperDir\": \"/var/lib/docker/overlay2/632d2d4d01646bd8be2ec147edc70eb44f59fb262aa12b217fd560c464edd4cb/diff\",\n                \"WorkDir\": \"/var/lib/docker/overlay2/632d2d4d01646bd8be2ec147edc70eb44f59fb262aa12b217fd560c464edd4cb/work\"\n            },\n            \"Name\": \"overlay2\"\n        },\n        \"RootFS\": {\n            \"Type\": \"layers\",\n            \"Layers\": [\n                \"sha256:1dc52a6b4de8561423dd3ec5a1f7f77f5309fd8cb340f80b8bc3d87fa112003e\",\n                \"sha256:b13a10ce059365d68a2113e9dbcac05b17b51f181615fca6d717a0dcf9ba8ffb\",\n                \"sha256:790d00cf365a312488151b354f0b0ae826be031edffb8a4de6a1fab048774dc7\",\n                \"sha256:323e43c53a1cd5abbd55437588f19da04f716452bc6d05486759b35f3e485390\",\n                \"sha256:c99c9d462af0bac5511ed046178ab0de79b8cdad33cd85246e9f661e098426cd\",\n                \"sha256:4a3a4d9fb4d250b1b64629b23bc0a477a45ee2659a8410d59a31a181dad70002\",\n                \"sha256:27b35f432a27e5e275038e559ebbe1aa7e91447bf417f5da01e3326739ba9366\",\n                \"sha256:ee12325fe0b7e7930b76d9a3dc81fcc37fa51a3267b311d2ed7c38703f193d75\",\n                \"sha256:7ceb40593535cdc07299efa2ce3a2c2267c2fa683161515fd6ab97f733492bf0\",\n                \"sha256:f18dbe0eec054f0aedf54a94aa29dab0d2c0f3d920fb482c99819622b0094f47\",\n                \"sha256:df2a7845ea611463f9f3282ccb45156ba883f40b15013ee49bd0a569301738d8\",\n                \"sha256:bcbd5416b87e3e37e05c22e46cbff2e3503d9caa0ec283a44931dc63e51c8cb7\",\n                \"sha256:5bcbb3ccae766c8a72d98ce494500bfd44c32e5780a1cb153139a4c5c143a8d5\",\n                \"sha256:4ecc8a8ffa902f3ea9bebb8d610e02a32ce1ca94c1a3160a31da98b73c1f55a0\",\n                \"sha256:a7a7b8b26735eb2d137fd0f91b83c73ad48cf2c4b83e9d0cadece410d6e598ba\",\n                \"sha256:ae939a0c9d32674ad6674947853ecfda4ff0530a8137960064448ae5e45fa1c5\",\n                \"sha256:6948f39c8f3cf6ec104734ccd1112fcb4af85a7c26c9c3d43495494b9b799f25\",\n                \"sha256:affd18c8e88f35e75bd02158e0418f3aeb4eec4269a208ede24cc829fa88c850\"\n            ]\n        },\n        \"Metadata\": {\n            \"LastTagTime\": \"0001-01-01T00:00:00Z\"\n        }\n    }\n]"
  },
  {
    "objectID": "posts/2022-07-07-sagemaker-script-mode.html#pass-hyperparameters-to-sklearn-estimator",
    "href": "posts/2022-07-07-sagemaker-script-mode.html#pass-hyperparameters-to-sklearn-estimator",
    "title": "Deploy Scikit-learn Models to Amazon SageMaker with the SageMaker Python SDK using Script mode",
    "section": "Pass hyperparameters to SKLearn estimator",
    "text": "Pass hyperparameters to SKLearn estimator\nLet’s pass some dummy hyperparameters to the estimator and see how it affects the output.\n\n#collapse-output\nsk_estimator = SKLearn(\n    entry_point=script_file,\n    role=role,\n    instance_count=1,\n    instance_type='local',\n    framework_version=\"1.0-1\",\n    hyperparameters={\"dummy_param_1\":\"val1\",\"dummy_param_2\":\"val2\"},\n)\n\nsk_estimator.fit()\n\n\nCreating kc4ahx6e84-algo-1-8m8ve ... \n\nCreating kc4ahx6e84-algo-1-8m8ve ... done\n\nAttaching to kc4ahx6e84-algo-1-8m8ve\n\nkc4ahx6e84-algo-1-8m8ve | 2022-07-17 15:23:46,385 sagemaker-containers INFO     Imported framework sagemaker_sklearn_container.training\n\nkc4ahx6e84-algo-1-8m8ve | 2022-07-17 15:23:46,389 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\n\nkc4ahx6e84-algo-1-8m8ve | 2022-07-17 15:23:46,398 sagemaker_sklearn_container.training INFO     Invoking user training script.\n\nkc4ahx6e84-algo-1-8m8ve | 2022-07-17 15:23:46,595 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\n\nkc4ahx6e84-algo-1-8m8ve | 2022-07-17 15:23:46,608 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\n\nkc4ahx6e84-algo-1-8m8ve | 2022-07-17 15:23:46,621 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\n\nkc4ahx6e84-algo-1-8m8ve | 2022-07-17 15:23:46,630 sagemaker-training-toolkit INFO     Invoking user script\n\nkc4ahx6e84-algo-1-8m8ve | \n\nkc4ahx6e84-algo-1-8m8ve | Training Env:\n\nkc4ahx6e84-algo-1-8m8ve | \n\nkc4ahx6e84-algo-1-8m8ve | {\n\nkc4ahx6e84-algo-1-8m8ve |     \"additional_framework_parameters\": {},\n\nkc4ahx6e84-algo-1-8m8ve |     \"channel_input_dirs\": {},\n\nkc4ahx6e84-algo-1-8m8ve |     \"current_host\": \"algo-1-8m8ve\",\n\nkc4ahx6e84-algo-1-8m8ve |     \"framework_module\": \"sagemaker_sklearn_container.training:main\",\n\nkc4ahx6e84-algo-1-8m8ve |     \"hosts\": [\n\nkc4ahx6e84-algo-1-8m8ve |         \"algo-1-8m8ve\"\n\nkc4ahx6e84-algo-1-8m8ve |     ],\n\nkc4ahx6e84-algo-1-8m8ve |     \"hyperparameters\": {\n\nkc4ahx6e84-algo-1-8m8ve |         \"dummy_param_1\": \"val1\",\n\nkc4ahx6e84-algo-1-8m8ve |         \"dummy_param_2\": \"val2\"\n\nkc4ahx6e84-algo-1-8m8ve |     },\n\nkc4ahx6e84-algo-1-8m8ve |     \"input_config_dir\": \"/opt/ml/input/config\",\n\nkc4ahx6e84-algo-1-8m8ve |     \"input_data_config\": {},\n\nkc4ahx6e84-algo-1-8m8ve |     \"input_dir\": \"/opt/ml/input\",\n\nkc4ahx6e84-algo-1-8m8ve |     \"is_master\": true,\n\nkc4ahx6e84-algo-1-8m8ve |     \"job_name\": \"sagemaker-scikit-learn-2022-07-17-15-23-44-284\",\n\nkc4ahx6e84-algo-1-8m8ve |     \"log_level\": 20,\n\nkc4ahx6e84-algo-1-8m8ve |     \"master_hostname\": \"algo-1-8m8ve\",\n\nkc4ahx6e84-algo-1-8m8ve |     \"model_dir\": \"/opt/ml/model\",\n\nkc4ahx6e84-algo-1-8m8ve |     \"module_dir\": \"s3://sagemaker-us-east-1-801598032724/sagemaker-scikit-learn-2022-07-17-15-23-44-284/source/sourcedir.tar.gz\",\n\nkc4ahx6e84-algo-1-8m8ve |     \"module_name\": \"train_and_serve\",\n\nkc4ahx6e84-algo-1-8m8ve |     \"network_interface_name\": \"eth0\",\n\nkc4ahx6e84-algo-1-8m8ve |     \"num_cpus\": 2,\n\nkc4ahx6e84-algo-1-8m8ve |     \"num_gpus\": 0,\n\nkc4ahx6e84-algo-1-8m8ve |     \"output_data_dir\": \"/opt/ml/output/data\",\n\nkc4ahx6e84-algo-1-8m8ve |     \"output_dir\": \"/opt/ml/output\",\n\nkc4ahx6e84-algo-1-8m8ve |     \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n\nkc4ahx6e84-algo-1-8m8ve |     \"resource_config\": {\n\nkc4ahx6e84-algo-1-8m8ve |         \"current_host\": \"algo-1-8m8ve\",\n\nkc4ahx6e84-algo-1-8m8ve |         \"hosts\": [\n\nkc4ahx6e84-algo-1-8m8ve |             \"algo-1-8m8ve\"\n\nkc4ahx6e84-algo-1-8m8ve |         ]\n\nkc4ahx6e84-algo-1-8m8ve |     },\n\nkc4ahx6e84-algo-1-8m8ve |     \"user_entry_point\": \"train_and_serve.py\"\n\nkc4ahx6e84-algo-1-8m8ve | }\n\nkc4ahx6e84-algo-1-8m8ve | \n\nkc4ahx6e84-algo-1-8m8ve | Environment variables:\n\nkc4ahx6e84-algo-1-8m8ve | \n\nkc4ahx6e84-algo-1-8m8ve | SM_HOSTS=[\"algo-1-8m8ve\"]\n\nkc4ahx6e84-algo-1-8m8ve | SM_NETWORK_INTERFACE_NAME=eth0\n\nkc4ahx6e84-algo-1-8m8ve | SM_HPS={\"dummy_param_1\":\"val1\",\"dummy_param_2\":\"val2\"}\n\nkc4ahx6e84-algo-1-8m8ve | SM_USER_ENTRY_POINT=train_and_serve.py\n\nkc4ahx6e84-algo-1-8m8ve | SM_FRAMEWORK_PARAMS={}\n\nkc4ahx6e84-algo-1-8m8ve | SM_RESOURCE_CONFIG={\"current_host\":\"algo-1-8m8ve\",\"hosts\":[\"algo-1-8m8ve\"]}\n\nkc4ahx6e84-algo-1-8m8ve | SM_INPUT_DATA_CONFIG={}\n\nkc4ahx6e84-algo-1-8m8ve | SM_OUTPUT_DATA_DIR=/opt/ml/output/data\n\nkc4ahx6e84-algo-1-8m8ve | SM_CHANNELS=[]\n\nkc4ahx6e84-algo-1-8m8ve | SM_CURRENT_HOST=algo-1-8m8ve\n\nkc4ahx6e84-algo-1-8m8ve | SM_MODULE_NAME=train_and_serve\n\nkc4ahx6e84-algo-1-8m8ve | SM_LOG_LEVEL=20\n\nkc4ahx6e84-algo-1-8m8ve | SM_FRAMEWORK_MODULE=sagemaker_sklearn_container.training:main\n\nkc4ahx6e84-algo-1-8m8ve | SM_INPUT_DIR=/opt/ml/input\n\nkc4ahx6e84-algo-1-8m8ve | SM_INPUT_CONFIG_DIR=/opt/ml/input/config\n\nkc4ahx6e84-algo-1-8m8ve | SM_OUTPUT_DIR=/opt/ml/output\n\nkc4ahx6e84-algo-1-8m8ve | SM_NUM_CPUS=2\n\nkc4ahx6e84-algo-1-8m8ve | SM_NUM_GPUS=0\n\nkc4ahx6e84-algo-1-8m8ve | SM_MODEL_DIR=/opt/ml/model\n\nkc4ahx6e84-algo-1-8m8ve | SM_MODULE_DIR=s3://sagemaker-us-east-1-801598032724/sagemaker-scikit-learn-2022-07-17-15-23-44-284/source/sourcedir.tar.gz\n\nkc4ahx6e84-algo-1-8m8ve | SM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{},\"current_host\":\"algo-1-8m8ve\",\"framework_module\":\"sagemaker_sklearn_container.training:main\",\"hosts\":[\"algo-1-8m8ve\"],\"hyperparameters\":{\"dummy_param_1\":\"val1\",\"dummy_param_2\":\"val2\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"sagemaker-scikit-learn-2022-07-17-15-23-44-284\",\"log_level\":20,\"master_hostname\":\"algo-1-8m8ve\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-801598032724/sagemaker-scikit-learn-2022-07-17-15-23-44-284/source/sourcedir.tar.gz\",\"module_name\":\"train_and_serve\",\"network_interface_name\":\"eth0\",\"num_cpus\":2,\"num_gpus\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1-8m8ve\",\"hosts\":[\"algo-1-8m8ve\"]},\"user_entry_point\":\"train_and_serve.py\"}\n\nkc4ahx6e84-algo-1-8m8ve | SM_USER_ARGS=[\"--dummy_param_1\",\"val1\",\"--dummy_param_2\",\"val2\"]\n\nkc4ahx6e84-algo-1-8m8ve | SM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\n\nkc4ahx6e84-algo-1-8m8ve | SM_HP_DUMMY_PARAM_1=val1\n\nkc4ahx6e84-algo-1-8m8ve | SM_HP_DUMMY_PARAM_2=val2\n\nkc4ahx6e84-algo-1-8m8ve | PYTHONPATH=/opt/ml/code:/miniconda3/bin:/miniconda3/lib/python38.zip:/miniconda3/lib/python3.8:/miniconda3/lib/python3.8/lib-dynload:/miniconda3/lib/python3.8/site-packages\n\nkc4ahx6e84-algo-1-8m8ve | \n\nkc4ahx6e84-algo-1-8m8ve | Invoking script with the following command:\n\nkc4ahx6e84-algo-1-8m8ve | \n\nkc4ahx6e84-algo-1-8m8ve | /miniconda3/bin/python train_and_serve.py --dummy_param_1 val1 --dummy_param_2 val2\n\nkc4ahx6e84-algo-1-8m8ve | \n\nkc4ahx6e84-algo-1-8m8ve | \n\nkc4ahx6e84-algo-1-8m8ve | *** Hello from the SageMaker script mode***\n\nkc4ahx6e84-algo-1-8m8ve | 2022-07-17 15:23:46,657 sagemaker-containers INFO     Reporting training SUCCESS\n\nkc4ahx6e84-algo-1-8m8ve exited with code 0\n\nAborting on container exit...\n\n===== Job Complete =====\n\n\n\n\n\n\n\nsklearn-output-hyperparams\n\n\nFrom the output we can see that our hyperparameters are passed to our training script as command line arguments. This is an important point and we will update our script using this information."
  },
  {
    "objectID": "posts/2022-07-07-sagemaker-script-mode.html#sagemaker-sklearn-container-environment-variables",
    "href": "posts/2022-07-07-sagemaker-script-mode.html#sagemaker-sklearn-container-environment-variables",
    "title": "Deploy Scikit-learn Models to Amazon SageMaker with the SageMaker Python SDK using Script mode",
    "section": "SageMaker SKLearn container environment variables",
    "text": "SageMaker SKLearn container environment variables\nLet’s now discuss some important environment variables we see in the output.\n\nSM_MODULE_DIR\nSM_MODULE_DIR=s3://sagemaker-us-east-1-801598032724/sagemaker-scikit-learn-2022-07-13-13-05-48-675/source/sourcedir.tar.gz\nSM_MODULE_DIR points to a location in the S3 bucket where SageMaker will automatically backup our source code for that particular run. SageMaker will create a separate folder in the default bucket for each new run. The default value is s3://sagemaker-{aws-region}-{aws-id}/{training-job-name}/source/sourcedir.tar.gz\nNote: We have used local_code for the SKLean estimator, then why is the source code backed up on the S3 bucket. Should it not be backed on the local system and bypass S3 altogether in local mode? Well, this should have been the default behavior, but it looks like SageMaker SDK is doing it otherwise, and even with the local mode it is using the S3 bucket for keeping source code. You can read more about this behavior in this issue ticket Model repack always uploads data to S3 bucket regardless of local mode settings\n\n\nSM_MODEL_DIR\nSM_MODEL_DIR=/opt/ml/model\nSM_MODEL_DIR points to a directory located inside the container. When the training job finishes, the container and its file system will be deleted, except for the /opt/ml/model and /opt/ml/output directories. Use /opt/ml/model to save the trained model artifacts. These artifacts are uploaded to S3 for model hosting.\n\n\nSM_OUTPUT_DATA_DIR\nSM_OUTPUT_DIR=/opt/ml/output\nSM_OUTPUT_DIR points to a directory in the container to write output artifacts. Output artifacts may include checkpoints, graphs, and other files to save, not including model artifacts. These artifacts are compressed and uploaded to S3 to the same S3 prefix as the model artifacts.\n\n\nSM_CHANNELS\nSM_CHANNELS='[\"testing\",\"training\"]'\nA channel is a named input source that training algorithms can consume. You can partition your training data into different logical “channels” when you run training. Depending on your problem, some common channel ideas are: “training”, “testing”, “evaluation” or “images” and “labels”. You can read more about the channels from SageMaker API reference Channel\n\n\nSM CHANNEL {channel_name}\nSM_CHANNEL_TRAIN='/opt/ml/input/data/train'\nSM_CHANNEL_TEST='/opt/ml/input/data/test'\nSuppose that you have passed two input channels, ‘train’ and ‘test’, to the Scikit-learn estimator’s fit() method, the following will be set, following the format SM_CHANNEL_[channel_name]: * SM_CHANNEL_TRAIN: it points to the directory in the container that has the train channel data downloaded * SM_CHANNEL_TEST: Same as above, but for the test channel\nNote that the channel names train and test are the conventions. Still, you can use any name here, and the environment variables will be created accordingly. It is important to know that the SageMaker container automatically downloads the data from the provided input channels and makes them available in the respective local directories once it starts executing. The training script can then load the data from the local container directories.\nThere are more environment variables available, and you can read about them from Environment variables"
  },
  {
    "objectID": "posts/2022-08-10-sagemaker-fastai-classifier.html",
    "href": "posts/2022-08-10-sagemaker-fastai-classifier.html",
    "title": "Train an Image Classifier using Fastai (Deep Dive Analysis)",
    "section": "",
    "text": "Introduction\nIn this notebook, we will build an image classifier to categorize images for different types of balls, including cricket, tennis, basketball, and soccer. Data is collected from the DuckDuckGo search engine. Unlike a getting started tutorial, we will take an alternate approach. Our focus is NOT to quickly train a model/prototype with fastai library. It may take only 5 lines of code to prepare an image classifier using this library. Instead, let’s aim to understand each line and function involved in the process. We have a lot to uncover, so let’s get started.\n\n\nEnvironment\nThis notebook is prepared using Amazon SageMaker studio’s Python 3 (PyTorch 1.10 Python 3.8 GPU Optimized) kernel running on the ml.t3.medium instance. I have also tried other images, including Data Science and Data Science 2.0, but fastai does not work smoothly with them. ml.t3.medium does not have any GPU attached to it but somehow Python 3 (PyTorch 1.10 Python 3.8 CPU Optimized) kernel was giving slow performance compare to GPU optimized.\nYou may run this notebook on any other system without any issue.\n\n!aws --version\n\naws-cli/1.22.68 Python/3.8.10 Linux/4.14.287-215.504.amzn2.x86_64 botocore/1.24.13\n\n\n\n!python3 --version\n\nPython 3.8.10\n\n\n\n\nFastai setup\nIntall fastai library. I have used IPython magic cell %%capture to capture and discard output of this cell. You may read more about this magic command from Capturing Output With %%capture\n\n%%capture\n!pip install -U fastai\n\nNext, I will install the library “duckduckgo_search” that can be used to “Search for words, documents, images, news, maps and text translation using the DuckDuckGo.com search engine.”\n\n%%capture\n!pip install -Uqq duckduckgo_search\n\nChecking the version of the installed fastai library.\n\nimport fastai\n\nfastai.__version__\n\n'2.7.9'\n\n\n\n\nPrepare data set\nWe will download training data images from DuckDuckGo search engine. The following function uses the search engine to “query” for an image and returns searched image URLs.\n\n##\n# Step: Define a function to search images and return URLs\n# Use 'duckduckgo' search engine to find image URLs\n\nfrom duckduckgo_search import ddg_images\nfrom fastcore.all import *\n\ndef search_images(term, max_images=200):\n    print(f\"Searching for '{term}'\")\n    return L(ddg_images(term, max_results=max_images)).itemgot('image')\n\nNow let’s define the search strings to download images for tennis, cricket, soccer, and basketball balls.\n\n##\n# Step: Define the search strings to find images on the Internet\n\nsearches = {\n    \"tennis\": \"tennis ball photo\",\n    \"cricket\": \"cricket hard ball photo\",\n    \"soccer\": \"soccer ball photo\",\n    \"basketball\": \"basketball ball photos\",\n}\n\nsearches\n\n{'tennis': 'tennis ball photo',\n 'cricket': 'cricket hard ball photo',\n 'soccer': 'soccer ball photo',\n 'basketball': 'basketball ball photos'}\n\n\nSearch and get the URL for a tennis ball image.\n\n##\n# Step: Search an image URL for tennis balls\n\nurls = search_images(searches['tennis'], max_images=1)\nurls[0]\n\nSearching for 'tennis ball photo'\n\n\n'https://www.thoughtco.com/thmb/rC73Tl0nBlYStXTVXrCRAnhPaq8=/3888x2592/filters:fill(auto,1)/tennis-ball-on-tennis-court-125847528-58db9de83df78c5162dba2ee.jpg'\n\n\nDefine a local path to store all the downloaded images.\n\n##\n# Step: Define a local path that will be root directory for this project\n# All the artifacts related to this post will be stored under this folder\n \nlocal_path = \"./datasets/2022-08-10-sagemaker-fastai-classifier\"\n\nDownload a sample image and view it.\n\n##\n# Step: Download an image using its URL, and show it as a thumbnail\n\nfrom fastai.vision.all import *\nfrom fastdownload import download_url\n\ndest = f'{local_path}/sample/tennis_ball.jpg'\ndownload_url(urls[0], dest, show_progress=True)\n\nim = Image.open(dest)\nim.to_thumb(256,256)\n\n/opt/conda/lib/python3.8/site-packages/torchvision/io/image.py:11: UserWarning: Failed to load image Python extension: \n  warn(f\"Failed to load image Python extension: {e}\")\n\n\n\n\n\n\n\n    \n      \n      100.57% [1015808/1010004 00:00&lt;00:00]\n    \n    \n\n\n\n\n\n\n\n\n\nWe have now downloaded a sample image from the Internet using the DuckDuckGo search engine and shown it as a thumbnail. For this, we have also used a few functions. So let’s deconstruct them to understand them better.\nThe first function we have used is download_url. Let’s check its documentation.\n\n#collapse_output\n??download_url\n\n\nSignature: download_url(url, dest=None, timeout=None, show_progress=True)\nSource:   \ndef download_url(url, dest=None, timeout=None, show_progress=True):\n    \"Download `url` to `dest` and show progress\"\n    pbar = progress_bar([])\n    def progress(count=1, bsize=1, tsize=None):\n        pbar.total = tsize\n        pbar.update(count*bsize)\n    return urlsave(url, dest, reporthook=progress if show_progress else None, timeout=timeout)\nFile:      /opt/conda/lib/python3.8/site-packages/fastdownload/core.py\nType:      function\n\n\n\n\nThis tells us that download_url function is from fastdownload library. This library is also published by the fastai team. Its purpose is “Easily download, verify, and extract archives”. You may read more about this library from its documentation site fastdownload.fast.ai\nThe next function we have used is Image.open. Let’s check its documentation too.\n\n#collapse_output\n?Image.open\n\n\nSignature: Image.open(fp, mode='r', formats=None)\nDocstring:\nOpens and identifies the given image file.\nThis is a lazy operation; this function identifies the file, but\nthe file remains open and the actual image data is not read from\nthe file until you try to process the data (or call the\n:py:meth:`~PIL.Image.Image.load` method).  See\n:py:func:`~PIL.Image.new`. See :ref:`file-handling`.\n:param fp: A filename (string), pathlib.Path object or a file object.\n   The file object must implement ``file.read``,\n   ``file.seek``, and ``file.tell`` methods,\n   and be opened in binary mode.\n:param mode: The mode.  If given, this argument must be \"r\".\n:param formats: A list or tuple of formats to attempt to load the file in.\n   This can be used to restrict the set of formats checked.\n   Pass ``None`` to try all supported formats. You can print the set of\n   available formats by running ``python3 -m PIL`` or using\n   the :py:func:`PIL.features.pilinfo` function.\n:returns: An :py:class:`~PIL.Image.Image` object.\n:exception FileNotFoundError: If the file cannot be found.\n:exception PIL.UnidentifiedImageError: If the image cannot be opened and\n   identified.\n:exception ValueError: If the ``mode`` is not \"r\", or if a ``StringIO``\n   instance is used for ``fp``.\n:exception TypeError: If ``formats`` is not ``None``, a list or a tuple.\nFile:      /opt/conda/lib/python3.8/site-packages/PIL/Image.py\nType:      function\n\n\n\n\nIt tells us that this function is from Python Pillow library and is used to open and read an image file. Refer to this library documentation for more information python-pillow.org\nThe next function that we used is Image.to_thumb. Let’s check its documentation.\n\n#collapse_output\n??Image.Image.to_thumb\n\n\nSignature: Image.Image.to_thumb(self: 'Image.Image', h, w=None)\nSource:   \n@patch\ndef to_thumb(self:Image.Image, h, w=None):\n    \"Same as `thumbnail`, but uses a copy\"\n    if w is None: w=h\n    im = self.copy()\n    im.thumbnail((w,h))\n    return im\nFile:      /opt/conda/lib/python3.8/site-packages/fastai/vision/core.py\nType:      function\n\n\n\n\nSo this function is actually from the fastai library, and its docstring tells us that it is the same as the Pillow library thumbnail function but uses a copy of the image.\nLet’s check Pillow thumbnail function documentation as well.\n\n#collapse_output\n?Image.Image.thumbnail\n\n\nSignature: Image.Image.thumbnail(self, size, resample=3, reducing_gap=2.0)\nDocstring:\nMake this image into a thumbnail.  This method modifies the\nimage to contain a thumbnail version of itself, no larger than\nthe given size.  This method calculates an appropriate thumbnail\nsize to preserve the aspect of the image, calls the\n:py:meth:`~PIL.Image.Image.draft` method to configure the file reader\n(where applicable), and finally resizes the image.\nNote that this function modifies the :py:class:`~PIL.Image.Image`\nobject in place.  If you need to use the full resolution image as well,\napply this method to a :py:meth:`~PIL.Image.Image.copy` of the original\nimage.\n:param size: Requested size.\n:param resample: Optional resampling filter.  This can be one\n   of :py:data:`PIL.Image.NEAREST`, :py:data:`PIL.Image.BOX`,\n   :py:data:`PIL.Image.BILINEAR`, :py:data:`PIL.Image.HAMMING`,\n   :py:data:`PIL.Image.BICUBIC` or :py:data:`PIL.Image.LANCZOS`.\n   If omitted, it defaults to :py:data:`PIL.Image.BICUBIC`.\n   (was :py:data:`PIL.Image.NEAREST` prior to version 2.5.0).\n   See: :ref:`concept-filters`.\n:param reducing_gap: Apply optimization by resizing the image\n   in two steps. First, reducing the image by integer times\n   using :py:meth:`~PIL.Image.Image.reduce` or\n   :py:meth:`~PIL.Image.Image.draft` for JPEG images.\n   Second, resizing using regular resampling. The last step\n   changes size no less than by ``reducing_gap`` times.\n   ``reducing_gap`` may be None (no first step is performed)\n   or should be greater than 1.0. The bigger ``reducing_gap``,\n   the closer the result to the fair resampling.\n   The smaller ``reducing_gap``, the faster resizing.\n   With ``reducing_gap`` greater or equal to 3.0, the result is\n   indistinguishable from fair resampling in most cases.\n   The default value is 2.0 (very close to fair resampling\n   while still being faster in many cases).\n:returns: None\nFile:      /opt/conda/lib/python3.8/site-packages/PIL/Image.py\nType:      function\n\n\n\n\nSummary of the functions used till now\nFunctions we have seen till now are summarized in this section.\n\ndownload_url: Download url to dest and show progress. This function is from module fastdownload\\core.py. Fastdownload is a separate library from the fastai team, and its use is to “Easily download, verify, and extract archives”.\n\nDocumentation: fastdownload.fast.ai\nSource code: fastai/fastdownload\n\nImage.open: Opens and identifies the given image file. This is a lazy operation; this function identifies the file, but the file remains open, and the actual image data is not read from the file until you try to process the data. this function is from pil\\image.py or Python Pillow Image library. Fastai installs this library for us. Fastai vision module internally loads Pillow for us. It has defined wrapper functions to make it easier to use this library in machine learning work.\n\nCheck fastai dependencies: settings.ini It contains external dependent libraries that fastai installs for us. Some important libraries from that list include\n\nfastdownload\nfastcore\nfastprogress\nmatplotlib\npandas\npillow\nscikit-learn\npytorch\n\nPillow image library documentation: https://pillow.readthedocs.io/en/stable/index.html\nImage.open documentation link: PIL.Image.open\n\nImage.to_thumb: Same as thumbnail, but uses a copy. This function is from fastai\\vision\\core.py module. It is a wrapper function around the PIL thumbnail function.\n\nSource code: 07_vision.core.ipynb\n\nImage.thumbnail: Make this image into a thumbnail. This method modifies the image to contain a thumbnail version of itself, no larger than the given size. This method calculates an appropriate thumbnail size to preserve the image’s aspect, calls the draft() method to configure the file reader (where applicable), and finally resizes the image. Note that this function modifies the Image object in place. If you need to use the full resolution image, apply this method to a copy() of the original image. This function is from pil\\image.py\n\nDocumentation: PIL.Image.Image.thumbnail\n\n\nSummary of the steps performed till now\n\nDefined a function to search images and return their URLs\nDefined the search strings to find images on the Internet\nDownloaded an image using its URL and showed it as a thumbnail\n\nLet’s proceed with our work. First, define a filesystem “images” path where files from the search engine will be downloaded.\n\n##\n# Step: Define a folder path where downloaded images will be stored\n\npath = Path(f'{local_path}/images')\n\nWe have used a Path class so let’s check its documentation.\n\n#collapse_output\n?Path\n\n\nInit signature: Path(*args, **kwargs)\nDocstring:     \nPurePath subclass that can make system calls.\nPath represents a filesystem path but unlike PurePath, also offers\nmethods to do system calls on path objects. Depending on your system,\ninstantiating a Path will return either a PosixPath or a WindowsPath\nobject. You can also instantiate a PosixPath or WindowsPath directly,\nbut cannot instantiate a WindowsPath on a POSIX system or vice versa.\nFile:           /opt/conda/lib/python3.8/pathlib.py\nType:           type\nSubclasses:     PosixPath, WindowsPath\n\n\n\n\nFrom the documentation, we find that the Path class is from Python standard library module pathlib. Python’s official documentation page for this module is pathlib.html. We have not loaded this module, so it begs the question, who loaded it for us? When I checked the fastai code repository, I found a file named fastai/imports.py. This file has many imports defined in it, and fastai is loading it for us behind the scenes. This imports.py file is loaded in many core fastai modules (data, vision, tabular, etc.). A typical loading sequence is as follows\n\nfrom fast.vision.all import * loads fastai.vision.core.py\n\nsource code for fast.vision.all\n\nfastai.vision.core.py loads fastai.torch_basics.py\n\nsource code for fastai.vision.core.py\n\nfastai.torch_basics.py loads fastai.imports.py\n\nsource code for fastai.torch_basics.py\n\nfastai.imports.py loads from pathlib import Path\n\nsource code for fastai.imports.py\n\n\nOkay, we have understood how Path library is loaded implicitly for us. So let’s continue with our work and download images using the search strings to the Path folder.\n\n##\n# Step: Use each search string to search and download images\n\nfor key, value in searches.items():\n    dest = (path/key)\n    dest.mkdir(exist_ok=True, parents=True)\n    download_images(dest, urls=search_images(value))\n    resize_images(path/key, max_size=400, dest=path/key)\n\n/opt/conda/lib/python3.8/site-packages/PIL/Image.py:945: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n  warnings.warn(\n/opt/conda/lib/python3.8/site-packages/PIL/Image.py:945: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n  warnings.warn(\n/opt/conda/lib/python3.8/site-packages/PIL/Image.py:945: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n  warnings.warn(\n/opt/conda/lib/python3.8/site-packages/PIL/Image.py:945: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n  warnings.warn(\n\n\nIn the last cell, we have introduced another function resize_images. So let’s check its docs too.\n\n#collapse_output\n?resize_images\n\n\nSignature:\nresize_images(\n    path,\n    max_workers=2,\n    max_size=None,\n    recurse=False,\n    dest=Path('.'),\n    n_channels=3,\n    ext=None,\n    img_format=None,\n    resample=2,\n    resume=None,\n    **kwargs,\n)\nDocstring: Resize files on path recursively to dest to max_size\nFile:      /opt/conda/lib/python3.8/site-packages/fastai/vision/utils.py\nType:      function\n\n\n\n\nIt tells us that this function is from the fastai vision module. Its purpose is to resize images to a given MAX size and store them in a destination folder. If any image size exceeds the max size, this function will resize it to a given size. Otherwise (smaller than max size), the image is left as it is.\nSo our images are now downloaded. Let’s check them.\n\n##\n# Step: Check the downloaded image files\n\nfns = get_image_files(path)\nfns\n\n(#757) [Path('datasets/2022-08-10-sagemaker-fastai-classifier/images/tennis/ee4bb413-b78d-4f2a-8e41-3f32bbad79a2.jpeg'),Path('datasets/2022-08-10-sagemaker-fastai-classifier/images/tennis/647709da-50e5-4344-8cd5-9895448d47dc.jpg'),Path('datasets/2022-08-10-sagemaker-fastai-classifier/images/tennis/8a9378aa-6c9d-4456-8a8e-19ddc1a00354.jpeg'),Path('datasets/2022-08-10-sagemaker-fastai-classifier/images/tennis/84fd1455-607d-40c9-9454-8d06c18c8eab.jpg'),Path('datasets/2022-08-10-sagemaker-fastai-classifier/images/tennis/20711509-3b5e-4b06-93e1-2013a1d40279.jpeg'),Path('datasets/2022-08-10-sagemaker-fastai-classifier/images/tennis/ae3b15ca-748b-4fdd-868d-e602bf37c79e.jpg'),Path('datasets/2022-08-10-sagemaker-fastai-classifier/images/tennis/df19cf58-1da4-4dbc-8c9c-0224c6ffa32b.jpg'),Path('datasets/2022-08-10-sagemaker-fastai-classifier/images/tennis/5775d55d-1c61-4477-aab5-0dd0feb9c5ba.jpg'),Path('datasets/2022-08-10-sagemaker-fastai-classifier/images/tennis/31dfd24b-35ca-43fa-8cb9-5227b6ee9dac.jpg'),Path('datasets/2022-08-10-sagemaker-fastai-classifier/images/tennis/bf9ec4b8-f237-4030-b77b-ca02fcbf96c2.jpg')...]\n\n\nChecking the documentation for get_image_files\n\n#collapse_output\n??get_image_files\n\n\nSignature: get_image_files(path, recurse=True, folders=None)\nSource:   \ndef get_image_files(path, recurse=True, folders=None):\n    \"Get image files in `path` recursively, only in `folders`, if specified.\"\n    return get_files(path, extensions=image_extensions, recurse=recurse, folders=folders)\nFile:      /opt/conda/lib/python3.8/site-packages/fastai/data/transforms.py\nType:      function\n\n\n\n\nIt tells us that this function is from the fastai data module. It gets image files in the directory recursively. Internally it is calling another function get_files. Let’s check it as well.\n\n#collapse_output\n??get_files\n\n\nSignature:\nget_files(\n    path,\n    extensions=None,\n    recurse=True,\n    folders=None,\n    followlinks=True,\n)\nSource:   \ndef get_files(path, extensions=None, recurse=True, folders=None, followlinks=True):\n    \"Get all the files in `path` with optional `extensions`, optionally with `recurse`, only in `folders`, if specified.\"\n    path = Path(path)\n    folders=L(folders)\n    extensions = setify(extensions)\n    extensions = {e.lower() for e in extensions}\n    if recurse:\n        res = []\n        for i,(p,d,f) in enumerate(os.walk(path, followlinks=followlinks)): # returns (dirpath, dirnames, filenames)\n            if len(folders) !=0 and i==0: d[:] = [o for o in d if o in folders]\n            else:                         d[:] = [o for o in d if not o.startswith('.')]\n            if len(folders) !=0 and i==0 and '.' not in folders: continue\n            res += _get_files(p, f, extensions)\n    else:\n        f = [o.name for o in os.scandir(path) if o.is_file()]\n        res = _get_files(path, f, extensions)\n    return L(res)\nFile:      /opt/conda/lib/python3.8/site-packages/fastai/data/transforms.py\nType:      function\n\n\n\n\nThis tells us that this function is also from the fastai data module. It reads the files from the folder and returns their paths as “L” class objects. L class is new to us so let’s check what this class is about.\n\n#collapse_output\n?L\n\n\nInit signature: L(items=None, *rest, use_list=False, match=None)\nDocstring:      Behaves like a list of `items` but can also index with list of indices or masks\nFile:           /opt/conda/lib/python3.8/site-packages/fastcore/foundation.py\nType:           _L_Meta\nSubclasses:     TfmdLists, MultiCategory, LabeledBBox\n\n\n\n\nIt tells us that L class is from a separate library fastcore, also released by the fastai team. The purpose of this library is defined as Python supercharged for the fastai library. An important takeaway from this class documentation is that it extends the Python list functionality and calls it the L class. You may read more on this library from fastcore.fast.ai\n\n##\n# Step: Verify the downloaded images\n# If any image is corrupt then remove it.\nfailed = verify_images(fns)\nfailed.map(Path.unlink)\nlen(failed)\n\n3\n\n\nA new function, verify_images is used in the above cell. First, let’s check its documentation.\n\n#collapse_output\n?verify_images\n\n\nSignature: verify_images(fns)\nDocstring: Find images in `fns` that can't be opened\nFile:      /opt/conda/lib/python3.8/site-packages/fastai/vision/utils.py\nType:      function\n\n\n\n\nThis tells us that verify_images is from fastai vision module, and it simply returns images (Paths) that cannot be opened. So we removed these corrupt image files to make our data clean.\n\n##\n# Step: print count of images downloaded against each search string\n\nfor search in searches:\n    fns = get_image_files(path/search)\n    print(search, \"images count: \", len(fns))\n\ntennis images count:  189\ncricket images count:  185\nsoccer images count:  193\nbasketball images count:  190\n\n\nSummary of the functions used till now\n\nPath: Is from Python standard library pathlib module. fastai loads this module for us\n\nDocumentation: pathlib.html\n\nresize_images: Is from fastai library vision module. Its purpose is to resize images to a given MAX size and store them in a destination folder\n\nDocumentation: vision.utils.html#resize_image\nSouce code: 09b_vision.utils.ipynb\n\nget_image_files: Is from fastai library data module. It returns a list (L class) of image file paths. Internally it calls get_files function\n\nDocumentation: data.transforms.html#get_image_files\nSouce code: 05_data.transforms.ipynb\n\nget_files: This is also from fastai library data module. It returns L class list of file paths\n\nDocumentation: data.transforms.html#get_files\nSouce code: 05_data.transforms.ipynb\n\nL class object: Is from fastcore library. It extends Python list object features\n\nDocumentation: fastcore.fast.ai/#l\nSouce code: 02_foundation.ipynb\n\nverify_images: Is from fastai library vision module. It verifies images and returns paths of images that cannot be opened\n\nDocumentation: vision.utils.html#verify_images\nSource code: 09b_vision.utils.ipynb\n\n\nSummary of the steps performed till now\n\nUse each search string to search and download images to a local folder. Resize the downloaded images to a given size.\nGet the downloaded images file paths as a list of type L\nVerify the images and remove any corrupted image\n\n\n\nCreate a data block\nWe have our training data (images) downloaded in a folder. So let’s continue with our work and create a DataBlock on them.\n\n##\n# Step: Create a data block\n\nballs = DataBlock(\n    blocks=(ImageBlock, CategoryBlock), \n    get_items=get_image_files, \n    splitter=RandomSplitter(valid_pct=0.2, seed=42),\n    get_y=parent_label,\n    item_tfms=[Resize(128, method='squish')]\n)\n\nA “DataBlock” is like a package or a pipeline with instructions that tell the type of data we are dealing with and how we want to process it. It is like a blueprint that defines how we want to process our data. If you are coming from scikit-learn world, you may think of it as a sklearn Pipeline and ColumnTransformer. Let’s check this class documentation for a better understanding.\n\n#collapse_output\n?DataBlock\n\n\nInit signature:\nDataBlock(\n    blocks: 'list' = None,\n    dl_type: 'TfmdDL' = None,\n    getters: 'list' = None,\n    n_inp: 'int' = None,\n    item_tfms: 'list' = None,\n    batch_tfms: 'list' = None,\n    *,\n    get_items=None,\n    splitter=None,\n    get_y=None,\n    get_x=None,\n)\nDocstring:      Generic container to quickly build `Datasets` and `DataLoaders`.\nFile:           /opt/conda/lib/python3.8/site-packages/fastai/data/block.py\nType:           type\nSubclasses:     \n\n\n\n\nIt tells us that it is from fastai data module. On its usage, it says it is a Generic container to build DataLoaders. Let’s deconstruct the arguments we have passed to this class to understand them.\nThe first argument that we have passed is the blocks.\nblocks=(ImageBlock, CategoryBlock)\nblocks are themselves predefined domain-specific containers (or pipelines) with default transformations defined for common use cases. For example, CategoryBlock is for “single-label categorical targets”. And the default transformations for this type of data are\n\nEncode categories like one-hot-encoding\nsort them\nfill empty values\n\nSimilary, many other built-in blocks are available in the fastai library for various data types and domains. All these blocks are like pipelines with default transformations defined (or simply generic containers with a set of instructions). So, by blocks=(ImageBlock, CategoryBlock), we are defining a blueprint and saying that for X and y of our data, treat them with ImageBlock and CategoryBlock, respectively. Here X is our training data (or images), and y is our labels i.e. tennis ball, cricket ball, etc.\nThe next argument in DataBlock is get_items\nget_items=get_image_files\nIt tells the blueprint on how to get the data items. Data items in our case are images, and we are telling it to use “get_image_files” function to get the image paths.\nThen we pass the argument “splitter”\nsplitter=RandomSplitter(valid_pct=0.2, seed=42),\nHere we are telling the DataBlock how to split the data.\nNote that each DataBlock blueprint requires four things: the types of your input/labels (or blocks), and at least two functions: get_items and splitter.\nWe have passed a class RandomSplitter for a splitting strategy. Let’s check this class doc for more clarity.\n\n#collapse_output\n?RandomSplitter\n\n\nSignature: RandomSplitter(valid_pct=0.2, seed=None)\nDocstring: Create function that splits `items` between train/val with `valid_pct` randomly.\nFile:      /opt/conda/lib/python3.8/site-packages/fastai/data/transforms.py\nType:      function\n\n\n\n\nThis class is also from the fastai data module, and it randomly splits the data into two sets: train and validation.\nIn the DataBlock constructor, we also passed an argument get_y that defined how to get the labels. Note that we have not defined anything for X, so the library will automatically take the data found in get_items as X. for get_y we have passed a class parent_label. Let’s check its documentation to understand what it is for.\nget_y=parent_label\n\n#collapse_output\n?parent_label\n\n\nSignature: parent_label(o)\nDocstring: Label `item` with the parent folder name.\nFile:      /opt/conda/lib/python3.8/site-packages/fastai/data/transforms.py\nType:      function\n\n\n\n\nIt tells that it is another class from the fastai data module and returns labels derived from the parent folder name. Meaning that all my cricket ball images are placed in a folder name cricket, so this class will automatically label all the images under it as cricket.\nThe last argument we have passed is item_tfms (item transformations). This argument defines if we want to apply any other transformation besides those described in blocks on each data point. Here we are telling it to use the Resize method on each item (from X). What does this function do? Let’s check the docs.\nitem_tfms=[Resize(128, method='squish')]\n\n#collapse_output\n??Resize\n\n\nInit signature:\nResize(\n    self,\n    size: 'int | tuple',\n    method: 'ResizeMethod' = 'crop',\n    pad_mode: 'PadMode' = 'reflection',\n    resamples=(2, 0),\n    **kwargs,\n)\nDocstring:      A transform that before_call its state at each `__call__`\nSource:        \nclass Resize(RandTransform):\n    split_idx,mode,mode_mask,order = None,BILINEAR,NEAREST,1\n    \"Resize image to `size` using `method`\"\n    def __init__(self, \n        size:int|tuple, # Size to resize to, duplicated if one value is specified\n        method:ResizeMethod=ResizeMethod.Crop, # A `ResizeMethod`\n        pad_mode:PadMode=PadMode.Reflection, # A `PadMode`\n        resamples=(BILINEAR, NEAREST), # Pillow `Image` resamples mode, resamples[1] for mask\n        **kwargs\n    ):\n        size = _process_sz(size)\n        store_attr()\n        super().__init__(**kwargs)\n        self.mode,self.mode_mask = resamples\n    def before_call(self, \n        b, \n        split_idx:int # Index of the train/valid dataset\n    ):\n        if self.method==ResizeMethod.Squish: return\n        self.pcts = (0.5,0.5) if split_idx else (random.random(),random.random())\n    def encodes(self, x:Image.Image|TensorBBox|TensorPoint):\n        orig_sz = _get_sz(x)\n        if self.method==ResizeMethod.Squish:\n            return x.crop_pad(orig_sz, fastuple(0,0), orig_sz=orig_sz, pad_mode=self.pad_mode,\n                   resize_mode=self.mode_mask if isinstance(x,PILMask) else self.mode, resize_to=self.size)\n        w,h = orig_sz\n        op = (operator.lt,operator.gt)[self.method==ResizeMethod.Pad]\n        m = w/self.size[0] if op(w/self.size[0],h/self.size[1]) else h/self.size[1]\n        cp_sz = (int(m*self.size[0]),int(m*self.size[1]))\n        tl = fastuple(int(self.pcts[0]*(w-cp_sz[0])), int(self.pcts[1]*(h-cp_sz[1])))\n        return x.crop_pad(cp_sz, tl, orig_sz=orig_sz, pad_mode=self.pad_mode,\n                   resize_mode=self.mode_mask if isinstance(x,PILMask) else self.mode, resize_to=self.size)\nFile:           /opt/conda/lib/python3.8/site-packages/fastai/vision/augment.py\nType:           _TfmMeta\nSubclasses:     \n\n\n\n\nIt tells us that this class is from the fastai vision module, but the docstring is not very helpful. Overall the impression is that this function internally uses the Pillow library and helps resize images. It utilizes multiple techniques to make all images of the same size like padding, cropping, reflection, squish, etc. Here we want all the pictures of the exact same size. Previously we have seen a similar function while downloading images (resize_images), which applied to the images max size.\n\n\nCreate a data loader and show batch\nIn the last section, we created a blueprint that defines the type of data we are dealing with and some transformations for it. We call it DataBlock. But a DataBlock is just a set of instructions as it does not point to any data. When we group a DataBlock with the actual data, we get a Dataset. But in machine learning workloads, we commonly deal with batches of data from the same dataset. For this, we have an iterator class over “Dataset” that creates batches from the given dataset for us. We call it DataLoaders. Both these concepts come from Pytorch on which fastai has its roots. So let’s first learn about these new concepts directly from Pytorch documentation.\n\nCode for processing data samples can get messy and hard to maintain; we ideally want our dataset code to be decoupled from our model training code for better readability and modularity. PyTorch provides two data primitives: torch.utils.data.DataLoader and torch.utils.data.Dataset that allow you to use pre-loaded datasets as well as your own data. Dataset stores the samples and their corresponding labels, and DataLoader wraps an iterable around the Dataset to enable easy access to the samples.\n\nRead more about it from Pytorch documentation data_tutorial.html\nWhat fastai provides is that it builds on these Pytorch constructs and extends their functionality. You can say that while working with Pytorch fastai team found many repeated steps for everyday tasks in their machine learning work. So they created higher-level functions in fastai that do many of these repeated and common tasks for us (though internally, it is still using Pytorch). This way, we developers can achieve more with fewer lines of code in fastai. So using fastai, we don’t need to create a Dataset and then a Dataloader. Instead, we can get a Dataloader by pointing our DataBlock to data and asking it to return a DataLoader. In the next cell, we are doing that.\n\n##\n# Step: Create a DataLoader\n\ndls = balls.dataloaders(path)\n\nTo complete the picture, let’s also check the documentation of Datasets from the fastai library.\n\n#collapse-output\n?Datasets\n\n\nInit signature:\nDatasets(\n    items: 'list' = None,\n    tfms: 'list | Pipeline' = None,\n    tls: 'TfmdLists' = None,\n    n_inp: 'int' = None,\n    dl_type=None,\n    *,\n    use_list: 'bool' = None,\n    do_setup: 'bool' = True,\n    split_idx: 'int' = None,\n    train_setup: 'bool' = True,\n    splits: 'list' = None,\n    types=None,\n    verbose: 'bool' = False,\n)\nDocstring:      A dataset that creates a tuple from each `tfms`\nFile:           /opt/conda/lib/python3.8/site-packages/fastai/data/core.py\nType:           type\nSubclasses:     \n\n\n\n\nIt tells us that it is defined in fastai data module.\n\nDocumentation: data.core.html#datasets\nSource code: 03_data.core.ipynb\n\nLet’s do the same for DataLoaders and check its documentation.\n\n#collapse-output\n?DataLoaders\n\n\nInit signature: DataLoaders(*loaders, path: 'str | Path' = '.', device=None)\nDocstring:      Basic wrapper around several `DataLoader`s.\nFile:           /opt/conda/lib/python3.8/site-packages/fastai/data/core.py\nType:           type\nSubclasses:     ImageDataLoaders, SegmentationDataLoaders\n\n\n\n\nIt tells us that DataLoaders class is also defined in fastai library data module.\n\nDocumentation: data.core.html#dataloaders\nSource code: 03_data.core.ipynb\n\nImportant notes from DataLoaders documentation:\n\nDataLoaders.train: Training DataLoader\nDataLoaders.valid: Validation DataLoader\nDataLoaders.train_ds: Training Dataset\nDataLoaders.valid_ds: Validation Dataset\n\nWe can also check the data loader object’s data type to confirm its class name and origin.\n\n##\n# dataloader object type\ntype(dls)\n\nfastai.data.core.DataLoaders\n\n\n\n##\n# check types for dataloaders.train and dataloaders.valid\nprint(type(dls.train))\nprint(type(dls.valid))\n\n&lt;class 'fastai.data.core.TfmdDL'&gt;\n&lt;class 'fastai.data.core.TfmdDL'&gt;\n\n\nData type for dls.train and dls.valid are of a different class. Let’s check the documentation for TfmdDL to get more clarity.\n\n#collapse-output\n??TfmdDL\n\n\nInit signature:\nTfmdDL(\n    dataset,\n    bs: 'int' = 64,\n    shuffle: 'bool' = False,\n    num_workers: 'int' = None,\n    verbose: 'bool' = False,\n    do_setup: 'bool' = True,\n    *,\n    pin_memory=False,\n    timeout=0,\n    batch_size=None,\n    drop_last=False,\n    indexed=None,\n    n=None,\n    device=None,\n    persistent_workers=False,\n    pin_memory_device='',\n    wif=None,\n    before_iter=None,\n    after_item=None,\n    before_batch=None,\n    after_batch=None,\n    after_iter=None,\n    create_batches=None,\n    create_item=None,\n    create_batch=None,\n    retain=None,\n    get_idxs=None,\n    sample=None,\n    shuffle_fn=None,\n    do_batch=None,\n)\nSource:        \nclass TfmdDL(DataLoader):\n    \"Transformed `DataLoader`\"\n    def __init__(self, \n        dataset, # Map- or iterable-style dataset from which to load the data\n        bs:int=64, # Size of batch\n        shuffle:bool=False, # Whether to shuffle data\n        num_workers:int=None, # Number of CPU cores to use in parallel (default: All available up to 16)\n        verbose:bool=False, # Whether to print verbose logs\n        do_setup:bool=True, # Whether to run `setup()` for batch transform(s)\n        **kwargs\n    ):\n        if num_workers is None: num_workers = min(16, defaults.cpus)\n        for nm in _batch_tfms: kwargs[nm] = Pipeline(kwargs.get(nm,None))\n        super().__init__(dataset, bs=bs, shuffle=shuffle, num_workers=num_workers, **kwargs)\n        if do_setup:\n            for nm in _batch_tfms:\n                pv(f\"Setting up {nm}: {kwargs[nm]}\", verbose)\n                kwargs[nm].setup(self)\n    def _one_pass(self):\n        b = self.do_batch([self.do_item(None)])\n        if self.device is not None: b = to_device(b, self.device)\n        its = self.after_batch(b)\n        self._n_inp = 1 if not isinstance(its, (list,tuple)) or len(its)==1 else len(its)-1\n        self._types = explode_types(its)\n    def _retain_dl(self,b):\n        if not getattr(self, '_types', None): self._one_pass()\n        return retain_types(b, typs=self._types)\n    @delegates(DataLoader.new)\n    def new(self, \n        dataset=None, # Map- or iterable-style dataset from which to load the data\n        cls=None, # Class of the newly created `DataLoader` object\n        **kwargs\n    ):\n        res = super().new(dataset, cls, do_setup=False, **kwargs)\n        if not hasattr(self, '_n_inp') or not hasattr(self, '_types'):\n            try:\n                self._one_pass()\n                res._n_inp,res._types = self._n_inp,self._types\n            except Exception as e: \n                print(\"Could not do one pass in your dataloader, there is something wrong in it. Please see the stack trace below:\")\n                raise\n        else: res._n_inp,res._types = self._n_inp,self._types\n        return res\n    def before_iter(self):\n        super().before_iter()\n        split_idx = getattr(self.dataset, 'split_idx', None)\n        for nm in _batch_tfms:\n            f = getattr(self,nm)\n            if isinstance(f,Pipeline): f.split_idx=split_idx\n    def decode(self, \n        b # Batch to decode\n    ):\n        return to_cpu(self.after_batch.decode(self._retain_dl(b)))\n    def decode_batch(self, \n        b, # Batch to decode\n        max_n:int=9, # Maximum number of items to decode\n        full:bool=True # Whether to decode all transforms. If `False`, decode up to the point the item knows how to show itself\n    ): \n        return self._decode_batch(self.decode(b), max_n, full)\n    def _decode_batch(self, b, max_n=9, full=True):\n        f = self.after_item.decode\n        f1 = self.before_batch.decode\n        f = compose(f1, f, partial(getcallable(self.dataset,'decode'), full = full))\n        return L(batch_to_samples(b, max_n=max_n)).map(f)\n    def _pre_show_batch(self, b, max_n=9):\n        \"Decode `b` to be ready for `show_batch`\"\n        b = self.decode(b)\n        if hasattr(b, 'show'): return b,None,None\n        its = self._decode_batch(b, max_n, full=False)\n        if not is_listy(b): b,its = [b],L((o,) for o in its)\n        return detuplify(b[:self.n_inp]),detuplify(b[self.n_inp:]),its\n    def show_batch(self,\n        b=None, # Batch to show\n        max_n:int=9, # Maximum number of items to show\n        ctxs=None, # List of `ctx` objects to show data. Could be matplotlib axis, DataFrame etc\n        show:bool=True, # Whether to display data\n        unique:bool=False, # Whether to show only one \n        **kwargs\n    ):\n        \"Show `max_n` input(s) and target(s) from the batch.\"\n        if unique:\n            old_get_idxs = self.get_idxs\n            self.get_idxs = lambda: Inf.zeros\n        if b is None: b = self.one_batch()\n        if not show: return self._pre_show_batch(b, max_n=max_n)\n        show_batch(*self._pre_show_batch(b, max_n=max_n), ctxs=ctxs, max_n=max_n, **kwargs)\n        if unique: self.get_idxs = old_get_idxs\n    def show_results(self, \n        b, # Batch to show results for\n        out, # Predicted output from model for the batch\n        max_n:int=9, # Maximum number of items to show\n        ctxs=None, # List of `ctx` objects to show data. Could be matplotlib axis, DataFrame etc\n        show:bool=True, # Whether to display data\n        **kwargs\n    ):\n        \"Show `max_n` results with input(s), target(s) and prediction(s).\"\n        x,y,its = self.show_batch(b, max_n=max_n, show=False)\n        b_out = type(b)(b[:self.n_inp] + (tuple(out) if is_listy(out) else (out,)))\n        x1,y1,outs = self.show_batch(b_out, max_n=max_n, show=False)\n        res = (x,x1,None,None) if its is None else (x, y, its, outs.itemgot(slice(self.n_inp,None)))\n        if not show: return res\n        show_results(*res, ctxs=ctxs, max_n=max_n, **kwargs)\n    @property\n    def n_inp(self) -&gt; int:\n        \"Number of elements in `Datasets` or `TfmdDL` tuple to be considered part of input.\"\n        if hasattr(self.dataset, 'n_inp'): return self.dataset.n_inp\n        if not hasattr(self, '_n_inp'): self._one_pass()\n        return self._n_inp\n    def to(self, \n        device # Device to put `DataLoader` and transforms\n    ):\n        self.device = device\n        for tfm in self.after_batch.fs:\n            for a in L(getattr(tfm, 'parameters', None)): setattr(tfm, a, getattr(tfm, a).to(device))\n        return self\nFile:           /opt/conda/lib/python3.8/site-packages/fastai/data/core.py\nType:           type\nSubclasses:     TabDataLoader, WeightedDL, PartialDL\n\n\n\n\nIt tells us that it is a class derived from Dataloader and calls it Transformed DataLoader. It is defined in fastai data module. Its purpose is defined as\n\nA TfmdDL is a DataLoader that creates Pipeline from a list of Transforms for the callbacks after_item, before_batch and after_batch. As a result, it can decode or show a processed batch\n\n\nDocumentation: data.core.html#tfmddl\nSource code: 03_data.core.ipynb\n\nTo clarify, DataLoaders and DataLoader are two separate classes.\n\nour object “dls” is of DataLoaders origin. This class is a wrapper around several DataLoaders. We have checked this class doc before\ndls attributes “dls.train” and “dls.valid” are of DataLoader origin. TfmdDL (transformed data loader) is one of its kind, and we have seen its docs in the last cell.\n\nLet’s also visit the documentation for the DataLoader class from which TfmdDL is derived.\n\n#collapse-output\n?DataLoader\n\n\nInit signature:\nDataLoader(\n    dataset=None,\n    bs=None,\n    num_workers=0,\n    pin_memory=False,\n    timeout=0,\n    batch_size=None,\n    shuffle=False,\n    drop_last=False,\n    indexed=None,\n    n=None,\n    device=None,\n    persistent_workers=False,\n    pin_memory_device='',\n    *,\n    wif=None,\n    before_iter=None,\n    after_item=None,\n    before_batch=None,\n    after_batch=None,\n    after_iter=None,\n    create_batches=None,\n    create_item=None,\n    create_batch=None,\n    retain=None,\n    get_idxs=None,\n    sample=None,\n    shuffle_fn=None,\n    do_batch=None,\n)\nDocstring:      API compatible with PyTorch DataLoader, with a lot more callbacks and flexibility\nFile:           /opt/conda/lib/python3.8/site-packages/fastai/data/load.py\nType:           type\nSubclasses:     TfmdDL\n\n\n\n\nIt tells us that it is an extension to PyTorch DataLoader with more flexibility (or functionality). This class is defined in fastai data module.\n\nDocumentation: data.load.html#dataloader\nSource code: 02_data.load.ipynb\n\nLet us continue with our work and visualize a small batch from our validation set.\n\n##\n# Step: Show a small batch from validation set\ndls.valid.show_batch(max_n=9, nrows=3)\n\n\n\n\n\n\n\n\n\n##\n# Step: Show a small batch from training set\ndls.train.show_batch(max_n=9, nrows=3)\n\n\n\n\n\n\n\n\nshow_batch is a very convenient function from fastai with which you can quickly verify a sample from the training dataset. You can also check that all the images are of the same size and have been appropriately labeled. Let’s quickly review the docs for this function.\n\n#collapse-output\n?TfmdDL.show_batch\n\n\nSignature:\nTfmdDL.show_batch(\n    self,\n    b=None,\n    max_n: 'int' = 9,\n    ctxs=None,\n    show: 'bool' = True,\n    unique: 'bool' = False,\n    **kwargs,\n)\nDocstring: Show `b` (defaults to `one_batch`), a list of lists of pipeline outputs (i.e. output of a `DataLoader`)\nFile:      /opt/conda/lib/python3.8/site-packages/fastai/data/core.py\nType:      function\n\n\n\n\nSummary of the functions used till now\n\nDataBlock: Is from fastai data module. It is a generic container to build DataLoaders\n\nDocumentation: data.block.html#datablock\nSource code: 06_data.block.ipynb\n\nDataBlock.dataloaders(Path): to create a DataLoaders object from DataBlock\nDataLoaders: Is from fastai library data module. It is a basic wrapper around several DataLoaders\n\nDocumentation: data.core.html#dataloaders\nSource code: 03_data.core.ipynb\n\nDataLoader: Is an extension to PyTorch DataLoader with more flexibility (or functionality). This class is defined in fastai data module.\n\nDocumentation: data.load.html#dataloader\nSource code: 02_data.load.ipynb\n\nTfmdDL: A transformed DataLoader\n\nDocumentation: data.core.html#tfmddl\nSource code: 03_data.core.ipynb\n\n\nSummary of the steps till now\n\nCreated a DataBlock object\nCreated a DataLoaders object\nViewed a small batch using show_batch function\n\n\n\nTrain an image classifier\nAt this point, our data is ready for training. Let’s train an image classifier on this data.\n\n##\n# Step: Train an image classifier\nlearn = vision_learner(dls, resnet18, metrics=error_rate)\nlearn.fine_tune(3)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n\n\n0\n1.900133\n0.625038\n0.192053\n00:26\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n\n\n0\n0.623933\n0.692287\n0.192053\n00:37\n\n\n1\n0.440623\n0.726005\n0.192053\n00:36\n\n\n2\n0.325829\n0.649467\n0.192053\n00:36\n\n\n\n\n\nIn the last cell, we have performed two steps:\n\nCreated a vision learner using a vision_learner() method\nThen used the learner object fine_tune() method to train a model\n\nLet’s deconstruct these two steps. First, check the documentation on the vision_learner function.\n\n#collapse-output\n?vision_learner\n\n\nSignature:\nvision_learner(\n    dls,\n    arch,\n    normalize=True,\n    n_out=None,\n    pretrained=True,\n    loss_func=None,\n    opt_func=&lt;function Adam at 0x7ff9747445e0&gt;,\n    lr=0.001,\n    splitter=None,\n    cbs=None,\n    metrics=None,\n    path=None,\n    model_dir='models',\n    wd=None,\n    wd_bn_bias=False,\n    train_bn=True,\n    moms=(0.95, 0.85, 0.95),\n    cut=None,\n    init=&lt;function kaiming_normal_ at 0x7ff9912824c0&gt;,\n    custom_head=None,\n    concat_pool=True,\n    pool=True,\n    lin_ftrs=None,\n    ps=0.5,\n    first_bn=True,\n    bn_final=False,\n    lin_first=False,\n    y_range=None,\n    *,\n    n_in=3,\n)\nDocstring: Build a vision learner from `dls` and `arch`\nFile:      /opt/conda/lib/python3.8/site-packages/fastai/vision/learner.py\nType:      function\n\n\n\n\nIt tells us that this method is from the fastai vision module, and the docstring says that its purpose is to “Build a vision learner from dls and arch”. We are getting limited information here. So let’s check the documentation pages vision.learner.html#learner-convenience-functions. From the documentation page, we get the following information\n\nIt is a “convenience function” for Learner class objects\nfastai “vision learner” module purpose is defined as “All the functions necessary to build Learner suitable for transfer learning in computer vision”\n\nThe most essential functions of this module are vision_learner and unet_learner.\nThey will help you define a Learner using a pre-trained model.\n\nArguments passed to this function\n\ndls refers to DataLoaders object\narch refers to model architecture to use\n\n\nSo basically, vision_learner is a helper function for Learner class to create its object. This means we need to understand the Learner concept to complete the picture.\n\n#collapse-output\n?Learner\n\n\nInit signature:\nLearner(\n    dls,\n    model: 'callable',\n    loss_func: 'callable | None' = None,\n    opt_func=&lt;function Adam at 0x7ff9747445e0&gt;,\n    lr=0.001,\n    splitter: 'callable' = &lt;function trainable_params at 0x7ff9774ea790&gt;,\n    cbs=None,\n    metrics=None,\n    path=None,\n    model_dir='models',\n    wd=None,\n    wd_bn_bias=False,\n    train_bn=True,\n    moms=(0.95, 0.85, 0.95),\n    default_cbs: 'bool' = True,\n)\nDocstring:      Group together a `model`, some `dls` and a `loss_func` to handle training\nFile:           /opt/conda/lib/python3.8/site-packages/fastai/learner.py\nType:           type\nSubclasses:     \n\n\n\n\nIt tells us that a Learner “Group together a model, some dls and a loss_func to handle training”. It is also a container that combines data, model, and a loss function into a single pipeline; we get this pipeline using a helper function vision_learner. Let’s also check the type of our “learn” object.\n\ntype(learn)\n\nfastai.learner.Learner\n\n\nOkay, the concept of vision_learner is clear. Let’s also discuss the arguments we have passed to it.\nlearn = vision_learner(dls, resnet18, metrics=error_rate)\n\ndls we already know that it refers to DataLoaders object\nresnet18 we know it is the arch or model architecture we want to use. But from which library/class are we using this object?\nerror_rate we know that it refers to the error metric we want to use for training. But again, from which library/class are we using this object?\n\nLet’s check resnet18 first.\n\n#collapse-output\n?resnet18\n\n\nSignature: resnet18(pretrained: bool = False, progress: bool = True, **kwargs: Any) -&gt; torchvision.models.resnet.ResNet\nDocstring:\nResNet-18 model from\n`\"Deep Residual Learning for Image Recognition\" &lt;https://arxiv.org/pdf/1512.03385.pdf&gt;`_.\nArgs:\n    pretrained (bool): If True, returns a model pre-trained on ImageNet\n    progress (bool): If True, displays a progress bar of the download to stderr\nFile:      /opt/conda/lib/python3.8/site-packages/torchvision/models/resnet.py\nType:      function\n\n\n\n\nIt tells us that this model architecture is defined in torchvision library or PyTorch vision module. From the docstring, we get that it is a pre-trained “ResNet-18 model for Deep Residual Learning for Image Recognition”.\nLet’s check the next argument, error_rate.\n\n#collapse-output\n?error_rate\n\n\nSignature: error_rate(inp, targ, axis=-1)\nDocstring: 1 - `accuracy`\nFile:      /opt/conda/lib/python3.8/site-packages/fastai/metrics.py\nType:      function\n\n\n\n\nIt tells us that error_rate is defined in fastai library metrics module.\nNow let’s go back to the image classifier training step and comprehend the second line.\nlearn.fine_tune(3)\n\n#collapse-output\n?Learner.fine_tune\n\n\nSignature:\nLearner.fine_tune(\n    self: 'Learner',\n    epochs,\n    base_lr=0.002,\n    freeze_epochs=1,\n    lr_mult=100,\n    pct_start=0.3,\n    div=5.0,\n    *,\n    lr_max=None,\n    div_final=100000.0,\n    wd=None,\n    moms=None,\n    cbs=None,\n    reset_opt=False,\n    start_epoch=0,\n)\nDocstring: Fine tune with `Learner.freeze` for `freeze_epochs`, then with `Learner.unfreeze` for `epochs`, using discriminative LR.\nFile:      /opt/conda/lib/python3.8/site-packages/fastai/callback/schedule.py\nType:      function\n\n\n\n\nIt tells us that it is from fastai callback module, and its purpose is defined as “Fine tune with Learner.freeze for freeze_epochs, then with Learner.unfreeze for epochs, using discriminative LR.”\nSummary of the functions used till now\n\nvision_learner: Is a helper function from the fastai Learner class. It is used to conveniently create a learner object\n\nDocumentation: vision.learner.html#vision_learner\nSource code: 21_vision.learner.ipynb\n\nLearner: Is a fastai class that combines data, model, and error function into a single pipeline.\n\nDocumentation: learner.html#learner\nSource code: 21_vision.learner.ipynb\n\nresnet18: Is a Pytorch pre-trained model with ResNet-18 architecture for image recognition.\n\nDocumentation: pytorch.org/vision\n\nerror_rate: Is a fastai metric function\nLearner.fine_tune: Is a Learner class function that trains a model\n\nDocumentation: callback.schedule.html#learner.fine_tune\nSource code: 14_callback.schedule.ipynb\n\n\nSummary of the steps till now\n\nCreate an image classifier Learner object\nTrain a model using Learner.fine_tune method\n\n\n\nInterpret the trained model\nIn the last section, we trained an image classifier that can distinguish between 4 different ball images: cricket, tennis, soccer, and basketball. But how good is our model? A confusion matrix is the best tool to interpret a model for classification problems. It visualizes and summarizes the performance of a classification algorithm. Let’s do it next.\n\n##\n# Step: Interpret the model using confusion matrix\n\ninterp = ClassificationInterpretation.from_learner(learn)\ninterp.plot_confusion_matrix()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAgain, we would like to know more about class ClassificationInterpretation as to which library it belongs. For this, let’s check the documentation.\n\n?ClassificationInterpretation\n\n\nInit signature:\nClassificationInterpretation(\n    learn: 'Learner',\n    dl: 'DataLoader',\n    losses: 'TensorBase',\n    act=None,\n)\nDocstring:      Interpretation methods for classification models.\nFile:           /opt/conda/lib/python3.8/site-packages/fastai/interpret.py\nType:           type\nSubclasses:     \n\n\n\n\nIt tells us that this method is from the fastai interpret module. Its purpose is defined as “Interpretation methods for classification models”. We can check the module documentation interpret.html#interpretation to get more details. There it says\n\nInterpretation is a helper base class for exploring predictions from trained models. It can be inherited for task specific interpretation classes, such as ClassificationInterpretation. Interpretation is memory efficient and should be able to process any sized dataset, provided the hardware could train the same model.\n\nAnd to get an Interpretation object we are Interpretation.from_learner method. Its purpose is to “Construct interpretation object from a learner”. Read more about this method from docs interpret.html#interpretation.from_learner\nThere are many useful functions in fastai Interpretation class, and I would encourage you to check them from the documentation. Once such method is plot_top_losses. Its purpose is “Show k largest(/smallest) preds and losses” from the validation set and plot the data points where our model has the high loss score. Usually, a model makes a wrong prediction when it has low confidence (or the model can be very confident in a wrong prediction). So this function can be valuable in understanding a model’s performance.\nLet’s use this function next. You may read more about this function from docs interpret.html#interpretation.plot_top_losses\n\n##\n# Step: Plot top losses\n\ninterp.plot_top_losses(9, nrows=9)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLet’s analyze the results from the last cell. It outputs model performance as “Prediction/Actual/Loss/Confidence”\n\nPrediction: What label has our model predicted for a given image\nActual: What label does our validation data have for that image\nLoss: A quantitative measure of how wrong was the model prediction. Making a wrong prediction with high confidence gets a high score\nConfidence: The confidence/probability that the model thinks a given image has the predicted label\n\nNow let’s check the output images.\n\nFirst image gets the highest loss. Our model thinks (very confidently) that this image is a tennis ball. But in our validation set, it is a cricket ball with green color. This makes sense because our model has learned that green balls are tennis balls.\nThe second image model again thinks it is a tennis ball. It seems right as it looks like a tennis ball to the human eye. We can remove this image from validation if we are unsure about it.\nIn the third image, our model thinks it is a cricket ball though it is a soccer ball. What do we learn from this? Our model has not seen soccer ball images before that have green and red colors in it. So we may include more soccer ball images with that color.\n\nSimilarly, we can analyze the remaining results and take action accordingly.\n\n\nExport and load a trained model\nSo we have trained our image classifier and are happy with the results. The next phase in ML work is to copy/transfer this artifact to a server where it can be hosted for inference work. Remember, a trained model consists of the architecture and the trained parameters. So to store a model, we need to save both these parts, and no information is lost. For this, we can use Learner.export method to keep the model in a pickle binary format.\n\nYou can learn more about Learner.export from documentation learner.html#learner.export\nSource code: 13a_learner.ipynb\nTo read about pickle Python object serialization module check the official docs pickle.html\n\n\n##\n# Step: Define export file name and path\n\nexport_file = local_path + \"/export_model.pkl\"\nexport_file\n\n'./datasets/2022-08-10-sagemaker-fastai-classifier/export_model.pkl'\n\n\n\n##\n# Step: Export the model\n\nlearn.export(export_file)\n\nCheck the local directory contents if the model pickle file is present.\n\n##\n# Step: Check local directory for .pkl file\n\np = Path(local_path)\np.ls(file_exts=\".pkl\")\n\n(#1) [Path('datasets/2022-08-10-sagemaker-fastai-classifier/export_model.pkl')]\n\n\nNow let’s load the exported model again using load_learner function. Read its docs learner.html#load_learner\n\n##\n# Step: Load a model\n\nlearn_inf = load_learner(export_file)\n\nRecall that we downloaded a sample tennis ball image at the start of this notebook. Let’s use that to make a prediction on the loaded model.\n\n##\n# Step: Sample tennis ball image\n\nim = Image.open(dest)\nim.to_thumb(256,256)\n\n\n\n\n\n\n\n\n\n##\n# Step: Make a prediction\n\nlearn_inf.predict(dest)\n\n\n\n\n\n\n\n\n('tennis',\n TensorBase(3),\n TensorBase([1.0414e-05, 1.6653e-05, 2.1281e-06, 9.9997e-01]))\n\n\nIt has returned three items:\n\nThe predicted category in the format we provided the labels\nThe index of the predicted category “3”. Indexes start from 0\nAnd the probabilities of each category\n\nIf you want to get a list of all the categories, then you can access DataLoaders attribute of the Learner.\n\n##\n# Step: Get a list of all categories from Learner\n\nlearn_inf.dls.vocab\n\n['basketball', 'cricket', 'soccer', 'tennis']"
  },
  {
    "objectID": "posts/2022-10-11-pytorch-two-class-logistic-regression.html#introduction",
    "href": "posts/2022-10-11-pytorch-two-class-logistic-regression.html#introduction",
    "title": "Two Class (Binary) Logistic Regression in Pytorch",
    "section": "Introduction",
    "text": "Introduction\nIn this notebook, we will train a logistic regression model using PyTorch. Given below is the summary of the steps followed in this notebook.\n\nCreate a synthetic binary class dataset\nSplit the data into Train and Validation datasets. Then convert them into mini-batches using PyTorch DataLoader class\nCreate a Neural Net model configuration, an SGD optimizer, and a loss function\nCreate a pipeline that will train the model on given data and update the weights based on the loss\nCompare the results with a scikit-learn logistic regression model"
  },
  {
    "objectID": "posts/2022-10-11-pytorch-two-class-logistic-regression.html#environment",
    "href": "posts/2022-10-11-pytorch-two-class-logistic-regression.html#environment",
    "title": "Two Class (Binary) Logistic Regression in Pytorch",
    "section": "Environment",
    "text": "Environment\nThis notebook is prepared with Google Colab.\n\nGitHub: 2022-10-11-pytorch-two-class-logistic-regression.ipynb\nOpen In Colab: \n\n\n\nCode\nfrom platform import python_version\nimport sklearn, numpy, matplotlib, pandas, torch\n\nprint(\"python==\" + python_version())\nprint(\"sklearn==\" + sklearn.__version__)\nprint(\"numpy==\" + numpy.__version__)\nprint(\"torch==\" + torch.__version__)\nprint(\"matplotlib==\" + matplotlib.__version__)\n\n\npython==3.7.14\nsklearn==1.0.2\nnumpy==1.21.6\ntorch==1.12.1+cu113\nmatplotlib==3.2.2"
  },
  {
    "objectID": "posts/2022-10-11-pytorch-two-class-logistic-regression.html#credits",
    "href": "posts/2022-10-11-pytorch-two-class-logistic-regression.html#credits",
    "title": "Two Class (Binary) Logistic Regression in Pytorch",
    "section": "Credits",
    "text": "Credits\nThis notebook takes inspiration from the book “Deep Learning with PyTorch Step-by-Step” by “Daniel Voigt Godoy”. You can get the book from its website: pytorchstepbystep. In addition, the GitHub repository for this book has valuable notebooks and can be used independently: github.com/dvgodoy/PyTorchStepByStep. Parts of the code you see in this notebook are taken from chapter 3 notebook of the same book."
  },
  {
    "objectID": "posts/2022-10-11-pytorch-two-class-logistic-regression.html#generate-synthetic-data",
    "href": "posts/2022-10-11-pytorch-two-class-logistic-regression.html#generate-synthetic-data",
    "title": "Two Class (Binary) Logistic Regression in Pytorch",
    "section": "Generate synthetic data",
    "text": "Generate synthetic data\nIn this section, we will generate some data representing two interleaving half-circles using sklearn.datasets.make_moons. The purpose of make_moons function is defined as\n\nMake two interleaving half circles. A simple toy dataset to visualize clustering and classification algorithms … It generates 2d binary classification datasets that are challenging to certain algorithms (e.g. centroid-based clustering or linear classification), including optional Gaussian noise.\n\n\n## \n# Synthetic data generation\nfrom sklearn.datasets import make_moons\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\n\nX, y = make_moons(n_samples=100, noise=0.3, random_state=0)\n\n# split data into train-validation sets using 80-20 ratio\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=.2, random_state=13)\n\n# standardize data\nsc = StandardScaler()\nsc.fit(X_train)\n\nX_train = sc.transform(X_train)\nX_val = sc.transform(X_val)\n\nLet’s view the first ten elements of the generated data. Note that X_train has two features (2 columns), and y_train has 0,1 classes as labels.\n\nX_train[:10], y_train[:10]\n\n(array([[-0.59635346, -0.51713419],\n        [ 0.3937561 , -1.35813138],\n        [ 1.33167696, -1.16636502],\n        [-1.52208256, -0.33314461],\n        [-1.20280449,  0.64649722],\n        [-0.65443973,  0.48658224],\n        [ 1.00612744, -1.81018492],\n        [-0.28996374, -1.5477782 ],\n        [ 0.03349394, -0.65113935],\n        [-0.94744907,  0.76650095]]), array([1, 1, 1, 0, 0, 1, 1, 1, 1, 0]))\n\n\nLet’s plot our generated data to see how it looks.\n\nimport matplotlib.pyplot as plt\n\nfigure, axes = plt.subplots(1, 3, figsize=(15,5))\nfigure.suptitle('Train and Validation Dataset')\n\naxes[0].set_title('Training Data')\naxes[0].scatter(X_train[:, 0], X_train[:, 1], c=y_train)\n\naxes[1].set_title('Validation Data')\naxes[1].scatter(X_val[:, 0], X_val[:, 1], c=y_val)\n\naxes[2].set_title('Combined Data')\naxes[2].scatter(X_train[:, 0], X_train[:, 1], c=y_train)\naxes[2].scatter(X_val[:, 0], X_val[:, 1], c=y_val)\nplt.show()"
  },
  {
    "objectID": "posts/2022-10-11-pytorch-two-class-logistic-regression.html#load-generated-data-into-pytorch-dataset-and-dataloader-class",
    "href": "posts/2022-10-11-pytorch-two-class-logistic-regression.html#load-generated-data-into-pytorch-dataset-and-dataloader-class",
    "title": "Two Class (Binary) Logistic Regression in Pytorch",
    "section": "Load generated data into PyTorch Dataset and DataLoader class",
    "text": "Load generated data into PyTorch Dataset and DataLoader class\nIn this section, we will load our data in PyTorch helper classes Dataset and DataLoader. PyTorch documentation defines them as: [see basics/data_tutorial]\n\nCode for processing data samples can get messy and hard to maintain; we ideally want our dataset code to be decoupled from our model training code for better readability and modularity. PyTorch provides two data primitives: torch.utils.data.DataLoader and torch.utils.data.Dataset that allow you to use pre-loaded datasets as well as your own data. Dataset stores the samples and their corresponding labels, and DataLoader wraps an iterable around the Dataset to enable easy access to the samples.\n\nFor this, we first need to convert NumPy data arrays to PyTorch tensors.\n\nimport torch\n\n# Builds tensors from numpy arrays\nx_train_tensor = torch.as_tensor(X_train).float()\ny_train_tensor = torch.as_tensor(y_train.reshape(-1, 1)).float()\n\nx_val_tensor = torch.as_tensor(X_val).float()\ny_val_tensor = torch.as_tensor(y_val.reshape(-1, 1)).float()\n\nNow load the tensors into Dataset and DataLoader class. PyTorch Dataset is a helper class that converts data and labels into a list of tuples. DataLoader is another helper class to create batches from Dataset tuples. batch_size means the number of tuples we want in a single batch. We have used 16 here since our data is small. So each fetch from DataLoader will give us a list of 16 tuples.\n\n## \n# Load tensors into Dataset and DataLoader\nfrom torch.utils.data import DataLoader, TensorDataset\n\n# Builds dataset containing ALL data points\ntrain_dataset = TensorDataset(x_train_tensor, y_train_tensor)\nval_dataset = TensorDataset(x_val_tensor, y_val_tensor)\n\n# Builds a loader of each set\ntrain_loader = DataLoader(dataset=train_dataset, batch_size=16, shuffle=True)\nval_loader = DataLoader(dataset=val_dataset, batch_size=16)"
  },
  {
    "objectID": "posts/2022-10-11-pytorch-two-class-logistic-regression.html#define-a-class-to-implement-training-validation-and-mini-batch-processing-pipeline",
    "href": "posts/2022-10-11-pytorch-two-class-logistic-regression.html#define-a-class-to-implement-training-validation-and-mini-batch-processing-pipeline",
    "title": "Two Class (Binary) Logistic Regression in Pytorch",
    "section": "Define a class to implement training, validation, and mini-batch processing pipeline",
    "text": "Define a class to implement training, validation, and mini-batch processing pipeline\nIn this section we will implement a class that encapsulates all the usual steps required in training a PyTorch model. This way we can focus more on the model architecture and performance, and less concerned about the boilerplate training loop. Important parts of this class are\n\n__init__: Class constructor to define the main actors in a training cycle including model, optimizer, loss function, training and validation DataLoaders\n_make_train_step_fn: Training pipeline is usually called “training step” which includes the following steps\n\nCompute our model’s predicted output - the forward pass\nCompute the loss\nCompute gradients i.e., find the direction and scale to update the weights to reduce the loss\nUpdate weight parameters using gradients and the learning rate\n\n_make_val_step_fn: Validation pipeline is usually called the “validation step” which includes the following steps\n\nCompute our model’s predicted output - the forward pass\nCompute the loss\nNote that during validation, we are only concerned about the loss, i.e., how well our model performs on the validation dataset. Therefore, we don’t use it to calculate the gradients.\n\n_mini_batch: It defines the steps to process a single minibatch in a helper function. For a mini-batch processing, we want to\n\nGet the next batch of data and labels (x, y) from the DataLoader iterator\nPerform a step on the batch. A step can be either training or validation\nCompute the average batch loss\n\ntrain: Execute training and validation steps for given number of epoch\npredict: Make a prediction from model on provided data\n\n\nimport numpy as np\nimport datetime\nfrom torch.utils.tensorboard import SummaryWriter\n\nclass DeepLearningPipeline(object):\n    def __init__(self, model, loss_fn, optimizer):\n        # Here we define the attributes of our class\n        \n        # We start by storing the arguments as attributes \n        # to use them later\n        self.model = model\n        self.loss_fn = loss_fn\n        self.optimizer = optimizer\n        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n        # Let's send the model to the specified device right away\n        self.model.to(self.device)\n\n        # These attributes are defined here, but since they are\n        # not informed at the moment of creation, we keep them None\n        self.train_loader = None\n        self.val_loader = None\n        self.writer = None\n        \n        # These attributes are going to be computed internally\n        self.losses = []\n        self.val_losses = []\n        self.total_epochs = 0\n\n        # Creates the train_step function for our model, \n        # loss function and optimizer\n        # Note: there are NO ARGS there! It makes use of the class\n        # attributes directly\n        self.train_step_fn = self._make_train_step_fn()\n        # Creates the val_step function for our model and loss\n        self.val_step_fn = self._make_val_step_fn()\n\n    def set_loaders(self, train_loader, val_loader=None):\n        # This method allows the user to define which train_loader (and val_loader, optionally) to use\n        # Both loaders are then assigned to attributes of the class\n        # So they can be referred to later\n        self.train_loader = train_loader\n        self.val_loader = val_loader\n\n    def _make_train_step_fn(self):\n        # This method does not need ARGS... it can refer to\n        # the attributes: self.model, self.loss_fn and self.optimizer\n        \n        # Builds function that performs a step in the train loop\n        def perform_train_step_fn(x, y):\n            # Sets model to TRAIN mode\n            self.model.train()\n\n            # Step 1 - Computes our model's predicted output - forward pass\n            yhat = self.model(x)\n            # Step 2 - Computes the loss\n            loss = self.loss_fn(yhat, y)\n            # Step 3 - Computes gradients for both \"a\" and \"b\" parameters\n            loss.backward()\n            # Step 4 - Updates parameters using gradients and the learning rate\n            self.optimizer.step()\n            self.optimizer.zero_grad()\n\n            # Returns the loss\n            return loss.item()\n\n        # Returns the function that will be called inside the train loop\n        return perform_train_step_fn\n    \n    def _make_val_step_fn(self):\n        # Builds function that performs a step in the validation loop\n        def perform_val_step_fn(x, y):\n            # Sets model to EVAL mode\n            self.model.eval()\n\n            # Step 1 - Computes our model's predicted output - forward pass\n            yhat = self.model(x)\n            # Step 2 - Computes the loss\n            loss = self.loss_fn(yhat, y)\n            # There is no need to compute Steps 3 and 4, \n            # since we don't update parameters during evaluation\n            return loss.item()\n\n        return perform_val_step_fn\n            \n    def _mini_batch(self, validation=False):\n        # The mini-batch can be used with both loaders\n        # The argument `validation`defines which loader and \n        # corresponding step function is going to be used\n        if validation:\n            data_loader = self.val_loader\n            step_fn = self.val_step_fn\n        else:\n            data_loader = self.train_loader\n            step_fn = self.train_step_fn\n\n        if data_loader is None:\n            return None\n            \n        # Once the data loader and step function, this is the \n        # same mini-batch loop we had before\n        mini_batch_losses = []\n        for x_batch, y_batch in data_loader:\n            x_batch = x_batch.to(self.device)\n            y_batch = y_batch.to(self.device)\n\n            mini_batch_loss = step_fn(x_batch, y_batch)\n            mini_batch_losses.append(mini_batch_loss)\n\n        loss = np.mean(mini_batch_losses)\n        return loss\n\n    def set_seed(self, seed=42):\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False    \n        torch.manual_seed(seed)\n        np.random.seed(seed)\n    \n    def train(self, n_epochs, seed=42):\n        # To ensure reproducibility of the training process\n        self.set_seed(seed)\n\n        for epoch in range(n_epochs):\n            # Keeps track of the numbers of epochs\n            # by updating the corresponding attribute\n            self.total_epochs += 1\n\n            # inner loop\n            # Performs training using mini-batches\n            loss = self._mini_batch(validation=False)\n            self.losses.append(loss)\n\n            # VALIDATION\n            # no gradients in validation!\n            with torch.no_grad():\n                # Performs evaluation using mini-batches\n                val_loss = self._mini_batch(validation=True)\n                self.val_losses.append(val_loss)\n\n            # If a SummaryWriter has been set...\n            if self.writer:\n                scalars = {'training': loss}\n                if val_loss is not None:\n                    scalars.update({'validation': val_loss})\n                # Records both losses for each epoch under the main tag \"loss\"\n                self.writer.add_scalars(main_tag='loss',\n                                        tag_scalar_dict=scalars,\n                                        global_step=epoch)\n\n        if self.writer:\n            # Closes the writer\n            self.writer.close()\n\n    def predict(self, x):\n        # Set is to evaluation mode for predictions\n        self.model.eval() \n        # Takes aNumpy input and make it a float tensor\n        x_tensor = torch.as_tensor(x).float()\n        # Send input to device and uses model for prediction\n        y_hat_tensor = self.model(x_tensor.to(self.device))\n        # Set it back to train mode\n        self.model.train()\n        # Detaches it, brings it to CPU and back to Numpy\n        return y_hat_tensor.detach().cpu().numpy()\n\n    def plot_losses(self):\n        fig = plt.figure(figsize=(10, 4))\n        plt.plot(self.losses, label='Training Loss', c='b')\n        plt.plot(self.val_losses, label='Validation Loss', c='r')\n        plt.yscale('log')\n        plt.xlabel('Epochs')\n        plt.ylabel('Loss')\n        plt.legend()\n        plt.tight_layout()\n        return fig"
  },
  {
    "objectID": "posts/2022-10-11-pytorch-two-class-logistic-regression.html#create-model-configuration",
    "href": "posts/2022-10-11-pytorch-two-class-logistic-regression.html#create-model-configuration",
    "title": "Two Class (Binary) Logistic Regression in Pytorch",
    "section": "Create model configuration",
    "text": "Create model configuration\nIn this section, we will configure the model for training, define a loss function, and an optimizer to update the weights.\nThere are two ways in which we can define our logistic classifier.\nFirst Approach: A model with a single linear layer and no activation function at the end. In this case, the output from the model will not be probabilities, and the loss function we use is nn.BCEWithLogitsLoss. This way, our model is similar to a linear regression model but with a different loss function. ‘BCEWithLogitsLoss’ is a variant of Binary Cross Entropy loss function (nn.BCELoss) and is defined as ‘numerically more stable’ [see docs torch.nn.BCEWithLogitsLoss.html]\n\nThis loss combines a Sigmoid layer and the BCELoss in one single class. This version is more numerically stable than using a plain Sigmoid followed by a BCELoss as, by combining the operations into one layer, we take advantage of the log-sum-exp trick for numerical stability.\n\nIn this approach, we train a model without ‘Sigmoid’ layer. But at the time of classification, we pass the output (called as logits) from the model to ‘Sigmoid’ function to get class probabilities.\nSecond Approach: Here we have an activation function (nn.Sigmoid) after the Linear layer. In this case, we have probabilities as an output. The loss function we use in this case is torch.nn.BCELoss.\nLet’s try both these approaches.\n\nLogistic regression model without Sigmoid layer\n\n##\n# Logistic model configuration without Sigmoid layer\nimport torch.nn as nn\nimport torch.optim as optim\n\nlr = 0.1\n\ntorch.manual_seed(42)\nmodel_1 = nn.Sequential()\nmodel_1.add_module('linear', nn.Linear(2, 1))\n\n# Defines a SGD optimizer to update the parameters\noptimizer_1 = optim.SGD(model_1.parameters(), lr=lr)\n\n# Defines a BCE loss function\nloss_fn_1 = nn.BCEWithLogitsLoss()\n\nNow let’s train our model for 100 epochs.\n\nn_epochs = 100\n\ndlp_1 = DeepLearningPipeline(model_1, loss_fn_1, optimizer_1)\ndlp_1.set_loaders(train_loader, val_loader)\ndlp_1.train(n_epochs)\n\nLet’s see how our training and validation loss looks like.\n\nfig = dlp_1.plot_losses()\n\n\n\n\n\n\n\n\nLet’s also print the weights learned by our model. Note that there are two weights in the linear layer, as there were two features (or columns) for our X_train data.\n\nprint(model_1.state_dict())\n\nOrderedDict([('linear.weight', tensor([[ 1.1806, -1.8693]])), ('linear.bias', tensor([-0.0591]))])\n\n\nLet’s also create a confusion matrix for our validation data. Note that here we have used torch.sigmoid to convert the output from the model into probabilities.\n\nfrom sklearn.metrics import confusion_matrix\n\nlogits_val = dlp_1.predict(X_val)\nlogits_val_tensor = torch.from_numpy(logits_val)\nprobabilities_val = torch.sigmoid(logits_val_tensor).squeeze()\ncm_thresh50 = confusion_matrix(y_val, (probabilities_val &gt;= 0.5))\ncm_thresh50\n\narray([[ 7,  2],\n       [ 1, 10]])\n\n\nLet’s also print the output from model for five validation data points. Again, remember that the output is logits, not probabilities.\n\nlogits_val[:5]\n\narray([[-0.37522304],\n       [ 0.7390274 ],\n       [-2.5800889 ],\n       [-0.93623203],\n       [-1.6819004 ]], dtype=float32)\n\n\n\n\nLogistic regression model with Sigmoid layer\nNow let’s again create our model, but this time ‘Sigmoid’ layer is attached at the end.\n\n##\n# Logistic model configuration with Sigmoid layer\nmodel_2 = nn.Sequential()\nmodel_2.add_module('linear', nn.Linear(2, 1))\nmodel_2.add_module('sigmoid', nn.Sigmoid())\n\n# Defines a SGD optimizer to update the parameters\noptimizer_2 = optim.SGD(model_2.parameters(), lr=lr)\n\n# Defines a BCE loss function\nloss_fn_2 = nn.BCELoss(reduction='mean')\n\n\nn_epochs = 100\n\ndlp_2 = DeepLearningPipeline(model_2, loss_fn_2, optimizer_2)\ndlp_2.set_loaders(train_loader, val_loader)\ndlp_2.train(n_epochs)\n\n\nfig = dlp_2.plot_losses()\n\n\n\n\n\n\n\n\nLet’s print the learned weights. They are slightly different from the last model.\n\nprint(model_2.state_dict())\n\nOrderedDict([('linear.weight', tensor([[ 1.1794, -1.8716]])), ('linear.bias', tensor([-0.0604]))])\n\n\nLet’s also create a confusion matrix for comparison. Note that the results are same as from the first model.\n\nfrom sklearn.metrics import confusion_matrix\n\nprobabilities_val = dlp_2.predict(X_val).squeeze()\ncm_thresh50 = confusion_matrix(y_val, (probabilities_val &gt;= 0.5))\ncm_thresh50\n\narray([[ 7,  2],\n       [ 1, 10]])\n\n\nLet’s also print the model output for five validation data points. Each output shows the probability of a point belonging to class 0 or 1. The points with a probability greater than 0.5 are put into class 1, and the remaining are placed into class 0. So from the below output\n\n0.4070907 –&gt; class 0\n0.6768992 –&gt; class 1\n0.0704094 –&gt; class 0\n\n\nprobabilities_val[:5]\n\narray([0.4070907 , 0.67689914, 0.07040942, 0.28118652, 0.15702702],\n      dtype=float32)\n\n\n\n\nComparison with Scikit-learn LogisticRegression model\nLet’s also compare our model with sklearn logistic regression and see how our neural net model compares to it.\n\n##\n# Comparison with sklearn logistic regression\n\nfrom sklearn.linear_model import LogisticRegression\nlogreg = LogisticRegression()\nlogreg.fit(X_train, y_train)\n\nLogisticRegression()\n\n\nLet’s see the weights learned by the model.\n\nlogreg.coef_ , logreg.intercept_\n\n(array([[ 1.03955962, -1.60876812]]), array([-0.05857131]))\n\n\nLet’s create a confusion matrix for comparison. Note that the results are identical.\n\npredictions = logreg.predict(X_val)\nprobabilities_val = logreg.predict_proba(X_val)\ncm_thresh50 = confusion_matrix(y_val, predictions)\ncm_thresh50\n\narray([[ 7,  2],\n       [ 1, 10]])\n\n\nLet’s also print the output from model for five validation data points. Again, remember the output is probabilities, and the sum of each row is 1.\nNote that the scikit-learn model output has two columns. Both show the probabilities of a point for class 0 or 1 (left to right). PyTorch model only outputs the probability of a point belonging to class 1 (right column). Since the sum of probabilities is equal to 1, we can find the other class probability (for binary classifiers only) if we have one class probability.\n\nprobabilities_val[:5]\n\narray([[0.58380158, 0.41619842],\n       [0.34949557, 0.65050443],\n       [0.90582984, 0.09417016],\n       [0.69290104, 0.30709896],\n       [0.81695121, 0.18304879]])"
  },
  {
    "objectID": "posts/2022-10-11-pytorch-two-class-logistic-regression.html#why-classification-is-called-logistic-regression",
    "href": "posts/2022-10-11-pytorch-two-class-logistic-regression.html#why-classification-is-called-logistic-regression",
    "title": "Two Class (Binary) Logistic Regression in Pytorch",
    "section": "Why classification is called logistic regression",
    "text": "Why classification is called logistic regression\nWhat classification has to do with regression, and why do we call it logistic regression? Remember that our binary classifier model has a linear layer with a logistic function at the end. A logistic function is an S-shaped curve function wiki/Logistic_function.\n\n\n\nlogistic_model.png\n\n\nImage taken from PyTorchStepByStep GitHub repository.\nWhen doing linear regression, we are trying to find a line that best fits our data points. We measure our error by finding the distance between each data point and our fitted line. If the distance between points and the line is minimum, we say we have found the best line that fits our data. Otherwise, we wiggle it up and down slowly till our errors are minimum.\nIn the case of logistic regression, we are also fitting a line on our data such that it can separate them into distinct categories. Points on one side of the line belong to class A, and points on the other side belong to class B. We usually call this line a decision boundary.\nHow do we measure our error in this case? We can measure error by counting how many points we have correctly classified. But just using a count is a very rough measurement. Because there can be multiple angles on which we can place a line in data and still be able to classify the points by the exact count. There should be a better way to tell us that a particular line angle is better than all others. For this, we use probabilities from a sigmoid (logistic) function. This helps us to capture the errors in a better way.\nIf a data point is on one side of the line (decision boundary) but is further away from it, we say it has a high probability of being in class A. If a point is close to the line, we say it has a low probability of being in class A. And we can extend this logic for points on the other side of the line. If a data point is on the other side of the decision boundary but close to the line, we give it a low probability of being in class B. And if a point is on the other side but farther away, we give it a high probability of being in class B.\n\nDuring training, we are trying to find a line that maximizes the certainty of data points for being in their correct classes.\n\nLet’s create a plot to see how our decision boundary looks like for our trained models.\n\n\nCode\ndef plot_decision_boundary(model, X_train, X_val):\n    x_min, x_max = -2, 2\n    y_min, y_max = -2, 2\n    h = .02\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n\n    # Check if model is PyTorch model\n    if type(model) == torch.nn.modules.container.Sequential:\n        # get predictions\n        logits = model(torch.as_tensor(np.c_[xx.ravel(), yy.ravel()]).float())\n        \n        # check if model's last layer is Linear, then apply Sigmoid on predictions to get probabilities\n        if type(model[-1]) == torch.nn.modules.linear.Linear:\n            logits_proba = torch.sigmoid(logits).squeeze()\n        else:\n            logits_proba = logits.clone()\n        \n        logits = logits.detach().cpu().numpy().reshape(xx.shape)\n        logits_proba = logits_proba.detach().cpu().numpy().reshape(xx.shape)\n    else: # Sklean model\n        logits = logreg.predict_proba(np.c_[xx.ravel(), yy.ravel()])\n        logits = logits[:,1]\n        logits = logits.reshape(xx.shape)\n        logits_proba = logits\n\n    fig = plt.figure(figsize=(14,7))\n    fig.suptitle('Plot Model Decision Boundary')\n\n    # Plot 1\n    ax1 = plt.subplot(1,3,1)\n    ax1.set_title('In 2D Plane')\n    # ax1.scatter(X_train[:, 0], X_train[:, 1], c=y_train)\n    ax1.scatter(X_val[:, 0], X_val[:, 1], c=y_val)\n    ax1.contour(xx, yy, logits_proba, levels=[0.5], cmap=\"Greys\", vmin=0, vmax=1)\n\n    # Plot 2\n    ax2 = plt.subplot(1,3,2, projection='3d')\n    ax2.set_title('In 3D Plane - View 1')\n    # ax2.scatter(X_train[:, 0], X_train[:, 1], c=y_train)\n    ax2.scatter(X_val[:, 0], X_val[:, 1], c=y_val)\n    ax2.plot_surface(xx, yy, logits, color='lightgreen')\n    ax2.view_init(10, 10)\n\n    # Plot 3\n    ax3 = plt.subplot(1,3,3, projection='3d')\n    ax3.set_title('3D Plane - View 2')\n    # ax3.scatter(X_train[:, 0], X_train[:, 1], c=y_train)\n    ax3.scatter(X_val[:, 0], X_val[:, 1], c=y_val)\n    ax3.plot_surface(xx, yy, logits, color='lightgreen')\n    \n    plt.show()\n\n\n\nDecision boundary for logistic regression model without Sigmoid\nLet’s view the decision boundary for the first model we built. That model has only a Linear layer and no Sigmoid function at the end. Notice that the decision boundary is a straight line (or a plane in 3D) that cuts the data in a way that maximizes the certainty of data for being in one class or the other.\n\nplot_decision_boundary(model_1, X_train, X_val)\n\n\n\n\n\n\n\n\n\n\nDecision boundary of a logistic regression model with Sigmoid\nLet’s draw the decision boundary for our classification model with Sigmoid as the last layer.\nNote that it is not a line this time when we view the decision boundary in 3D space. The 3D plane is more like an S-shaped curve. Because now our model output is probabilities coming out of a logistic function. Think of it as an extra step that is now added to the output of the first model (without sigmoid). The points on the extreme right side of the first model linear plane are given the lowest probabilities (almost zero). And as we move to the left, the possibilities gradually increase with 0.5 at the middle and highest when we reach the extreme left (almost 1).\n\nplot_decision_boundary(model_2, X_train, X_val)\n\n\n\n\n\n\n\n\n\n\nDecision boundary of sklearn logistic regression model\nLet’s now plot the same for sklearn model. The plot is similar to the last model.\n\nplot_decision_boundary(logreg, X_train, X_val)"
  },
  {
    "objectID": "posts/2022-10-11-pytorch-two-class-logistic-regression.html#can-we-do-better",
    "href": "posts/2022-10-11-pytorch-two-class-logistic-regression.html#can-we-do-better",
    "title": "Two Class (Binary) Logistic Regression in Pytorch",
    "section": "Can we do better?",
    "text": "Can we do better?\nWe have seen that our models’ decision boundaries are linear (a line), and on the validation set they misclassify 3 points (two purple dots below the line and one yellow dot above the line). What if we can bend our decision boundary a little? Will it be able to better capture the classes in validation data? Let’s test it.\nWe introduce non-linearity to our decision boundary by placing a non-linear activation function between our neural net layers. Let’s create another model but this time use two hidden layers with a non-linear activation function (ReLU) in between them.\n\n##\n# Logistic model configuration with two hidden layers and ReLU in between\n# No Sigmoid layer at the end\nmodel_3 = nn.Sequential()\nmodel_3.add_module('linear1', nn.Linear(2, 10))\nmodel_3.add_module('activation1', nn.ReLU())\nmodel_3.add_module('linear2', nn.Linear(10, 1))\n\n# Defines a SGD optimizer to update the parameters\noptimizer_3 = optim.SGD(model_3.parameters(), lr=lr)\n\n# Defines a BCE loss function\nloss_fn_3 = nn.BCEWithLogitsLoss()\n\nTrain the model for 100 epochs.\n\nn_epochs = 100\n\ndlp_3 = DeepLearningPipeline(model_3, loss_fn_3, optimizer_3)\ndlp_3.set_loaders(train_loader, val_loader)\ndlp_3.train(n_epochs)\n\nLet’s view model’s performance.\n\nfig = dlp_3.plot_losses()\n\n\n\n\n\n\n\n\nLet’s print the learned model weights. They are many now.\n\nprint(model_3.state_dict())\n\nOrderedDict([('linear1.weight', tensor([[-0.6459,  0.2700],\n        [-1.0339, -0.3841],\n        [ 0.2376, -0.9286],\n        [-0.2675, -0.0527],\n        [ 0.5548,  0.7580],\n        [ 0.9121, -0.9306],\n        [-0.4046,  1.0968],\n        [ 0.2245, -0.0907],\n        [ 0.0439, -0.4252],\n        [-0.2865,  0.9936]])), ('linear1.bias', tensor([-0.3062, -0.6352, -0.4185, -0.5086,  0.2363, -0.1485,  0.2418,  0.3736,\n        -0.7792,  0.4397])), ('linear2.weight', tensor([[-0.5564, -1.0559,  0.6160,  0.1544, -0.4395,  1.0762, -0.9444,  0.2832,\n         -0.2357, -0.8370]])), ('linear2.bias', tensor([0.7808]))])\n\n\nLet’s create a confusion matrix. Notice that our error has slightly improved, and the model misclassifies only two purple data point from the validation set.\n\nfrom sklearn.metrics import confusion_matrix\n\nlogits_val_3 = dlp_3.predict(X_val)\nlogits_val_tensor_3 = torch.from_numpy(logits_val_3)\nprobabilities_val = torch.sigmoid(logits_val_tensor_3).squeeze()\ncm_thresh50 = confusion_matrix(y_val, (probabilities_val &gt;= 0.5))\ncm_thresh50\n\narray([[ 7,  2],\n       [ 0, 11]])\n\n\nLet’s plot the model’s decision boundary. First, notice that it is not linear and has a bend. This effect is due to placing a non-linear activation function after a linear layer. And, it has correctly captured a yellow point and barely missed purple points.\n\nplot_decision_boundary(model_3, X_train, X_val)\n\n\n\n\n\n\n\n\n\n##\n# Confusion matrix plot\nfrom sklearn.metrics import ConfusionMatrixDisplay\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm_thresh50, display_labels=[0,1])\ndisp.plot()\nplt.show()"
  },
  {
    "objectID": "posts/2022-10-23-pytorch-vanishing-gradients-deep-neural-networks.html#credits",
    "href": "posts/2022-10-23-pytorch-vanishing-gradients-deep-neural-networks.html#credits",
    "title": "Detect Vanishing Gradients in Deep Neural Networks by Plotting Gradient Distributions",
    "section": "Credits",
    "text": "Credits\nThis notebook takes inspiration and ideas from the following sources.\n\nA great post from MATLAB company MathWorks with the same title: Detect Vanishing Gradients in Deep Neural Networks by Plotting Gradient Distributions. That post is written for the MATLAB audience, and I have tried translating its ideas for Python and PyTorch community.\nThe outstanding book “Deep Learning with PyTorch Step-by-Step” by “Daniel Voigt Godoy”. You can get the book from its website: pytorchstepbystep. In addition, the GitHub repository for this book has valuable notebooks and can be used independently: github.com/dvgodoy/PyTorchStepByStep. Parts of the code you see in this notebook are taken from chapter 3 notebook of the same book.\nUniversity of Amsterdam (UvA) Deep Learning Course series. uvadlc.github.io. Their lecture on “activation functions and gradients” discusses the same topic. Here is the link: tutorial3/Activation_Functions.html. The course is outstanding, and lectures and notebooks are also openly shared. From the course site “This course is taught in the MSc program in Artificial Intelligence of the University of Amsterdam. In this course we study the theory of deep learning, namely of modern, multi-layered neural networks trained on big data.”"
  },
  {
    "objectID": "posts/2022-10-23-pytorch-vanishing-gradients-deep-neural-networks.html#environment",
    "href": "posts/2022-10-23-pytorch-vanishing-gradients-deep-neural-networks.html#environment",
    "title": "Detect Vanishing Gradients in Deep Neural Networks by Plotting Gradient Distributions",
    "section": "Environment",
    "text": "Environment\nThis notebook is prepared with Google Colab.\n\nGitHub: 2022-10-23-pytorch-vanishing-gradients-deep-neural-networks.ipynb\nOpen In Colab: \n\n\n\nCode\nfrom platform import python_version\nimport numpy, matplotlib, pandas, torch, seaborn\n\nprint(\"python==\" + python_version())\nprint(\"numpy==\" + numpy.__version__)\nprint(\"torch==\" + torch.__version__)\nprint(\"matplotlib==\" + matplotlib.__version__)\nprint(\"seaborn==\" + seaborn.__version__)\n\n\npython==3.7.15\nnumpy==1.21.6\ntorch==1.12.1+cu113\nmatplotlib==3.2.2\nseaborn==0.11.2"
  },
  {
    "objectID": "posts/2022-10-23-pytorch-vanishing-gradients-deep-neural-networks.html#introduction",
    "href": "posts/2022-10-23-pytorch-vanishing-gradients-deep-neural-networks.html#introduction",
    "title": "Detect Vanishing Gradients in Deep Neural Networks by Plotting Gradient Distributions",
    "section": "Introduction",
    "text": "Introduction\nMathWorks post explains vanishing gradients problem in deep neural networks really well, and I am sharing a passage from it.\n\nA common problem in deep network training is vanishing gradients. Deep learning training algorithms aim to minimize the loss by adjusting the network’s learnable parameters (i.e., weights) during training. Gradient-based training algorithms determine the level of adjustment using the gradients of the loss function with respect to the current learnable parameters. The gradient computation uses the propagated gradients from the previous layers for earlier layers (i.e., from the output layer to the input layer). Therefore, when a network contains activation functions that always produce gradient values less than 1 (e.g., Sigmoid), the value of the gradients can become increasingly small as the updating algorithm moves toward the initial layers. As a result, early layers in the network can receive a vanishingly small gradient, and therefore, the network is unable to learn. However, if the gradient of the activation function is always greater than or equal to 1 (e.g., ReLU), the gradients can flow through the network, reducing the chance of vanishing gradients.\n\nLet’s understand it better by visualizing the gradients produced by Sigmoid and ReLU.\n\nCompare activation functions\nIn this section we will compare the properties of two popular activation functions: Sigmoid and ReLU.\n\nSigmoid\nSigmoid function is normally used to refer specifically to the logistic function, also called the logistic sigmoid function. It is defined as\n\\[\nSigmoid(x) =  \\frac{\\mathrm{1} }{\\mathrm{1} + e^{-x} }\n\\]\n\n\nReLU\nReLU function is defined as\n\\[ Relu(x) = max(0,x)\n\\]\n\n\n\nVisualising activation functions and their gradients\nLet’s plot both these functions’ outputs and visualize their gradients. In the next cell, I have created two PyTorch classes that define Sigmoid and ReLU activations.\n\n\nCode\nimport torch.nn as nn\n\n# A class representing Sigmoid activation function\nclass SigmoidAct(nn.Module):\n    def __init__(self):\n        super().__init__()\n        pass\n\n    def forward(self, x):\n        return 1 / (1 + torch.exp(-x))\n\n# A class representing ReLU activation function\nclass ReluAct(nn.Module):\n    def __init__(self):\n        super().__init__()\n        pass\n\n    def forward(self, x):\n        return x * (x &gt; 0).float()\n\n# initialize sigmoid activation function\nsigmoid_fn = SigmoidAct()\n# initialize relu activation function\nrelu_fn = ReluAct()\n\n\nI have defined a helper function to calculate the gradients for these activation functions.\n\n\nCode\nimport matplotlib.pyplot as plt\n\n# A helper function to computes the gradients of an activation function at specified positions.\ndef get_grads(act_fn, x):\n    x = x.clone().requires_grad_()  # Mark the input as tensor for which we want to store gradients\n    out = act_fn(x)\n    out.sum().backward()  # Summing results in an equal gradient flow to each element in x\n    return x.grad  # Accessing the gradients of x by \"x.grad\"\n\n# A helper function to plot the activation function and its gradient\ndef vis_act_fn(act_fn, fn_name, ax, x):\n    # Run activation function\n    y = act_fn(x)\n    y_grads = get_grads(act_fn, x)\n    # Push x, y and gradients back to cpu for plotting\n    x, y, y_grads = x.cpu().numpy(), y.cpu().numpy(), y_grads.cpu().numpy()\n    # Plotting\n    ax.plot(x, y, linewidth=2, label=\"Activation function\")\n    ax.plot(x, y_grads, linewidth=2, label=\"Gradient\")\n    ax.set_title(fn_name)\n    ax.legend()\n    ax.set_ylim(-1.5, x.max())\n\n\nlet’s plot the gradients.\n\n\nCode\nimport torch\n\nx = torch.linspace(-5, 5, 1000)\n\nfig, ax = plt.subplots(1, 2, figsize=(15, 7))\nvis_act_fn(sigmoid_fn, 'sigmoid', ax[0], x)\nvis_act_fn(relu_fn, 'relu', ax[1], x)\nfig.subplots_adjust(hspace=0.3)\nplt.show()\n\n\n\n\n\n\n\n\n\nWe can take the following explanations from the above plots.\n\nThe sigmoid output is bounded, as it remains between 0 and 1. The gradients are the highest when the input is close to zero and diminishes as the input value moves away from it. Notice that the sigmoid gradient curve is less than 1 for the entire range; therefore, a network containing sigmoid activation functions can suffer from a vanishing gradients problem.\nThe output value for ReLU is not bounded. It keeps on increasing on the positive side. And for positive output values, the gradients do not diminish but remain constant at 1. As the gradient is not decreasing, it reduces the chances of the vanishing gradients problem. However, the gradient is zero for negative values, and this state is sometimes referred to as dead ReLU. It means that if ReLU ends up in this situation, it is improbable that it will recover from it.\n\nWhy will it not recover? Because the gradient from the activation function is zero for negative inputs, it will also not update the weights during the backward pass, thus leaving the weights in that perpetual state.\n\n\nIn the coming sections, we will build networks and try to visualize how gradients flow between different layers and the effect of activation functions on them."
  },
  {
    "objectID": "posts/2022-10-23-pytorch-vanishing-gradients-deep-neural-networks.html#download-mnist-dataset",
    "href": "posts/2022-10-23-pytorch-vanishing-gradients-deep-neural-networks.html#download-mnist-dataset",
    "title": "Detect Vanishing Gradients in Deep Neural Networks by Plotting Gradient Distributions",
    "section": "Download MNIST dataset",
    "text": "Download MNIST dataset\nMNIST dataset can be downloaded easily from PyTorch built-in datasets provided under torchvision.datasets. In this section, we will download it, split it into train and test datasets, and then convert them into PyTorch tensors.\n\nRead more about the PyTorch MNIST dataset here\ntorchvision.transforms.Compose is like a container to hold a list of transformations you intend to apply. Read more about it here\ntorchvision.transforms.ToTensor converts a PIL Image or numpy.ndarray to tensor. It converts a PIL Image or numpy.ndarray (H x W x C) in the range [0, 255] to a torch.FloatTensor of shape (C x H x W) in the range [0.0, 1.0]. Here C=Channel, H=Height, W=Width. Read more about this transformation here\n\n\n#collapse-output\nimport torchvision\nimport numpy as np\n\ntrain_dataset = torchvision.datasets.MNIST('classifier_data', train=True, download=True)\ntest_dataset  = torchvision.datasets.MNIST('classifier_data', train=False, download=True)\n\ntransform = torchvision.transforms.Compose([\n    torchvision.transforms.ToTensor()\n])\n\ntrain_dataset.transform=transform\ntest_dataset.transform=transform\n\nprint(f\"Total training images: {len(train_dataset)}\")\nprint(f\"Shape of an image: {np.shape(train_dataset.data[7])}\")\n\nDownloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\nDownloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to classifier_data/MNIST/raw/train-images-idx3-ubyte.gz\n\n\n\n\n\nExtracting classifier_data/MNIST/raw/train-images-idx3-ubyte.gz to classifier_data/MNIST/raw\n\nDownloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\nDownloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to classifier_data/MNIST/raw/train-labels-idx1-ubyte.gz\n\n\n\n\n\nExtracting classifier_data/MNIST/raw/train-labels-idx1-ubyte.gz to classifier_data/MNIST/raw\n\nDownloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\nDownloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to classifier_data/MNIST/raw/t10k-images-idx3-ubyte.gz\n\n\n\n\n\nExtracting classifier_data/MNIST/raw/t10k-images-idx3-ubyte.gz to classifier_data/MNIST/raw\n\nDownloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\nDownloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to classifier_data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n\n\n\n\n\nExtracting classifier_data/MNIST/raw/t10k-labels-idx1-ubyte.gz to classifier_data/MNIST/raw\n\nTotal training images: 60000\nShape of an image: torch.Size([28, 28])\n\n\nFrom the above cell output, there are 60,000 training images. The shape of each image is 28 x 28, which means it is a 2D matrix.\n\n##\n# plot a single image\nimport matplotlib.pyplot as plt\n\nplt.imshow(train_dataset.data[7], cmap='gray')"
  },
  {
    "objectID": "posts/2022-10-23-pytorch-vanishing-gradients-deep-neural-networks.html#load-generated-data-into-pytorch-dataset-and-dataloader-class",
    "href": "posts/2022-10-23-pytorch-vanishing-gradients-deep-neural-networks.html#load-generated-data-into-pytorch-dataset-and-dataloader-class",
    "title": "Detect Vanishing Gradients in Deep Neural Networks by Plotting Gradient Distributions",
    "section": "Load generated data into PyTorch Dataset and DataLoader class",
    "text": "Load generated data into PyTorch Dataset and DataLoader class\nNow let’s load our data into Dataset and DataLoader classes. PyTorch Dataset is a helper class that converts data and labels into a list of tuples. DataLoader is another helper class to create batches from Dataset tuples. batch_size means the number of tuples we want in a single batch. We have used 128 here, so each fetch from DataLoader will give us a list of 128 tuples.\n\nimport torch\nfrom torch.utils.data import Dataset, DataLoader, random_split\n\ntrain_size=len(train_dataset)\n\n# Randomly split the data into non-overlapping train and validation set\n# train size = 70% and validation size = 30%\ntrain_data, val_data = random_split(train_dataset, [int(train_size*0.7), int(train_size - train_size*0.7)])\n\nbatch_size=128\n\n# Load data into DataLoader class\ntrain_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size)\nvalid_loader = torch.utils.data.DataLoader(val_data, batch_size=batch_size)\n\nprint(f\"Batches in Train Loader: {len(train_loader)}\")\nprint(f\"Batches in Valid Loader: {len(valid_loader)}\")\n\nprint(f\"Examples in Train Loader: {len(train_loader.sampler)}\")\nprint(f\"Examples in Valid Loader: {len(valid_loader.sampler)}\")\n\nBatches in Train Loader: 469\nBatches in Valid Loader: 141\nExamples in Train Loader: 60000\nExamples in Valid Loader: 18000"
  },
  {
    "objectID": "posts/2022-10-23-pytorch-vanishing-gradients-deep-neural-networks.html#define-a-class-to-implement-training-validation-and-mini-batch-processing-pipeline",
    "href": "posts/2022-10-23-pytorch-vanishing-gradients-deep-neural-networks.html#define-a-class-to-implement-training-validation-and-mini-batch-processing-pipeline",
    "title": "Detect Vanishing Gradients in Deep Neural Networks by Plotting Gradient Distributions",
    "section": "Define a class to implement training, validation, and mini-batch processing pipeline",
    "text": "Define a class to implement training, validation, and mini-batch processing pipeline\nIn this section we will implement a class that encapsulates all the usual steps required in training a PyTorch model. This way we can focus more on the model architecture and performance, and less concerned about the boilerplate training loop. Important parts of this class are\n\n__init__: Class constructor to define the main actors in a training cycle including model, optimizer, loss function, training and validation DataLoaders\n_make_train_step_fn: Training pipeline is usually called “training step” which includes the following steps\n\nCompute our model’s predicted output - the forward pass\nCompute the loss\nCompute gradients i.e., find the direction and scale to update the weights to reduce the loss\nUpdate weight parameters using gradients and the learning rate\n\n_make_val_step_fn: Validation pipeline is usually called the “validation step” which includes the following steps\n\nCompute our model’s predicted output - the forward pass\nCompute the loss\nNote that during validation, we are only concerned about the loss, i.e., how well our model performs on the validation dataset. Therefore, we don’t use it to calculate the gradients.\n\n_mini_batch: It defines the steps to process a single minibatch in a helper function. For a mini-batch processing, we want to\n\nGet the next batch of data and labels (x, y) from the DataLoader iterator\nPerform a step on the batch. A step can be either training or validation\nCompute the average batch loss\n\ntrain: Execute training and validation steps for given number of epoch\npredict: Make a prediction from model on provided data\n\n\n\nCode\nclass DeepLearningPipeline(object):\n    def __init__(self, model, loss_fn, optimizer):\n        # Here we define the attributes of our class\n        \n        # We start by storing the arguments as attributes \n        # to use them later\n        self.model = model\n        self.loss_fn = loss_fn\n        self.optimizer = optimizer\n        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n        # Let's send the model to the specified device right away\n        self.model.to(self.device)\n\n        # These attributes are defined here, but since they are\n        # not informed at the moment of creation, we keep them None\n        self.train_loader = None\n        self.val_loader = None\n        self.writer = None\n        \n        # These attributes are going to be computed internally\n        self.losses = []\n        self.val_losses = []\n        self.total_epochs = 0\n        self.grad = []\n\n        # Creates the train_step function for our model, \n        # loss function and optimizer\n        # Note: there are NO ARGS there! It makes use of the class\n        # attributes directly\n        self.train_step_fn = self._make_train_step_fn()\n        # Creates the val_step function for our model and loss\n        self.val_step_fn = self._make_val_step_fn()\n\n    def set_loaders(self, train_loader, val_loader=None):\n        # This method allows the user to define which train_loader (and val_loader, optionally) to use\n        # Both loaders are then assigned to attributes of the class\n        # So they can be referred to later\n        self.train_loader = train_loader\n        self.val_loader = val_loader\n\n    def _make_train_step_fn(self):\n        # This method does not need ARGS... it can refer to\n        # the attributes: self.model, self.loss_fn and self.optimizer\n        \n        # Builds function that performs a step in the train loop\n        def perform_train_step_fn(x, y):\n            # Sets model to TRAIN mode\n            self.model.train()\n\n            # Step 1 - Computes our model's predicted output - forward pass\n            yhat = self.model(x)\n            # Step 2 - Computes the loss\n            loss = self.loss_fn(yhat, y)\n            # Step 3 - Computes gradients for both \"a\" and \"b\" parameters\n            loss.backward()\n            # Step 4 - Updates parameters using gradients and the learning rate\n            self.optimizer.step()\n            self.optimizer.zero_grad()\n\n            # Returns the loss\n            return loss.item()\n\n        # Returns the function that will be called inside the train loop\n        return perform_train_step_fn\n    \n    def _make_val_step_fn(self):\n        # Builds function that performs a step in the validation loop\n        def perform_val_step_fn(x, y):\n            # Sets model to EVAL mode\n            self.model.eval()\n\n            # Step 1 - Computes our model's predicted output - forward pass\n            yhat = self.model(x)\n            # Step 2 - Computes the loss\n            loss = self.loss_fn(yhat, y)\n            # There is no need to compute Steps 3 and 4, \n            # since we don't update parameters during evaluation\n            return loss.item()\n\n        return perform_val_step_fn\n            \n    def _mini_batch(self, validation=False):\n        # The mini-batch can be used with both loaders\n        # The argument `validation`defines which loader and \n        # corresponding step function is going to be used\n        if validation:\n            data_loader = self.val_loader\n            step_fn = self.val_step_fn\n        else:\n            data_loader = self.train_loader\n            step_fn = self.train_step_fn\n\n        if data_loader is None:\n            return None\n            \n        # Once the data loader and step function, this is the \n        # same mini-batch loop we had before\n        mini_batch_losses = []\n        for x_batch, y_batch in data_loader:\n            x_batch = x_batch.to(self.device)\n            y_batch = y_batch.to(self.device)\n\n            mini_batch_loss = step_fn(x_batch, y_batch)\n            mini_batch_losses.append(mini_batch_loss)\n\n        loss = np.mean(mini_batch_losses)\n        return loss\n\n    def set_seed(self, seed=42):\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False    \n        torch.manual_seed(seed)\n        np.random.seed(seed)\n    \n    def train(self, n_epochs, seed=42):\n        # To ensure reproducibility of the training process\n        self.set_seed(seed)\n\n        for epoch in range(n_epochs):\n            # Keeps track of the numbers of epochs\n            # by updating the corresponding attribute\n            self.total_epochs += 1\n\n            # inner loop\n            # Performs training using mini-batches\n            loss = self._mini_batch(validation=False)\n            self.losses.append(loss)\n\n            ##########################\n            # get grad at the end of each epoch\n            imgs, labels = next(iter(self.train_loader))\n            imgs, labels = imgs.to(device), labels.to(device)\n\n            # Pass one batch through the network, and calculate the gradients for the weights\n            self.model.zero_grad()\n            preds = self.model(imgs)\n            loss = torch.nn.functional.cross_entropy(preds, labels)\n            loss.backward()\n            # We limit our visualization to the weight parameters and exclude the bias to reduce the number of plots\n            grads = {\n                name: params.grad.data.view(-1).cpu().clone().numpy()\n                for name, params in self.model.named_parameters()\n                if \"weight\" in name\n            }\n            self.model.zero_grad()\n            self.grad.append(grads)\n            ##########################\n\n            # VALIDATION\n            # no gradients in validation!\n            with torch.no_grad():\n                # Performs evaluation using mini-batches\n                val_loss = self._mini_batch(validation=True)\n                self.val_losses.append(val_loss)\n\n            # If a SummaryWriter has been set...\n            if self.writer:\n                scalars = {'training': loss}\n                if val_loss is not None:\n                    scalars.update({'validation': val_loss})\n                # Records both losses for each epoch under the main tag \"loss\"\n                self.writer.add_scalars(main_tag='loss',\n                                        tag_scalar_dict=scalars,\n                                        global_step=epoch)\n            \n            print(f\"epoch: {epoch:3}, train loss: {loss:.5f}, valid loss: {val_loss:.5f}\")\n\n        if self.writer:\n            # Closes the writer\n            self.writer.close()\n\n    def predict(self, x):\n        # Set is to evaluation mode for predictions\n        self.model.eval() \n        # Takes aNumpy input and make it a float tensor\n        x_tensor = torch.as_tensor(x).float()\n        # Send input to device and uses model for prediction\n        y_hat_tensor = self.model(x_tensor.to(self.device))\n        # Set it back to train mode\n        self.model.train()\n        # Detaches it, brings it to CPU and back to Numpy\n        return y_hat_tensor.detach().cpu().numpy()\n\n    def plot_losses(self):\n        fig = plt.figure(figsize=(10, 4))\n        plt.plot(self.losses, label='Training Loss', c='b')\n        plt.plot(self.val_losses, label='Validation Loss', c='r')\n        plt.yscale('log')\n        plt.xlabel('Epochs')\n        plt.ylabel('Loss')\n        plt.legend()\n        plt.tight_layout()\n        return fig"
  },
  {
    "objectID": "posts/2022-10-23-pytorch-vanishing-gradients-deep-neural-networks.html#create-a-model-with-sigmoid-activations",
    "href": "posts/2022-10-23-pytorch-vanishing-gradients-deep-neural-networks.html#create-a-model-with-sigmoid-activations",
    "title": "Detect Vanishing Gradients in Deep Neural Networks by Plotting Gradient Distributions",
    "section": "Create a model with sigmoid activations",
    "text": "Create a model with sigmoid activations\nLet’s define a fully connected 4 layers model with only sigmoid activations.\n\nimport torch.nn as nn\n\nSigmoidNet = nn.Sequential()\nSigmoidNet.add_module(\"F\", nn.Flatten())\nSigmoidNet.add_module(\"L1\", nn.Linear(28*28, 10, bias=False))\nSigmoidNet.add_module(\"S1\", nn.Sigmoid())\nSigmoidNet.add_module(\"L2\", nn.Linear(10, 10, bias=False))\nSigmoidNet.add_module(\"S2\", nn.Sigmoid())\nSigmoidNet.add_module(\"L3\", nn.Linear(10, 10, bias=False))\nSigmoidNet.add_module(\"S3\", nn.Sigmoid())\nSigmoidNet.add_module(\"L4\", nn.Linear(10, 10, bias=False))\n\nPrint model’s summary.\n\ntorch.manual_seed(42)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\n#collapse-output\nfrom torchsummary import summary\n\nmodel_sigmoid = SigmoidNet\nmodel_sigmoid = model_sigmoid.to(device)\n\nsummary(model_sigmoid, (1, 28*28))\n\n----------------------------------------------------------------\n        Layer (type)               Output Shape         Param #\n================================================================\n           Flatten-1                  [-1, 784]               0\n            Linear-2                   [-1, 10]           7,840\n           Sigmoid-3                   [-1, 10]               0\n            Linear-4                   [-1, 10]             100\n           Sigmoid-5                   [-1, 10]               0\n            Linear-6                   [-1, 10]             100\n           Sigmoid-7                   [-1, 10]               0\n            Linear-8                   [-1, 10]             100\n================================================================\nTotal params: 8,140\nTrainable params: 8,140\nNon-trainable params: 0\n----------------------------------------------------------------\nInput size (MB): 0.00\nForward/backward pass size (MB): 0.01\nParams size (MB): 0.03\nEstimated Total Size (MB): 0.04\n----------------------------------------------------------------\n\n\nCreate an optimizer and a loss function.\n\nimport torch.optim as optim\n\n# learning rate\nlr = 0.001\n\n# Defines a SGD optimizer to update the parameters\noptimizer_sigmoid = optim.SGD(model_sigmoid.parameters(), lr=lr)\n\n# Defines a BCE loss function\nloss_fn = nn.CrossEntropyLoss()\n\nTrain our model for 15 epochs.\n\n#collapse-output\nn_epochs = 15\n\ndlp_sigmoid = DeepLearningPipeline(model_sigmoid, loss_fn, optimizer_sigmoid)\ndlp_sigmoid.set_loaders(train_loader, valid_loader)\ndlp_sigmoid.train(n_epochs)\n\nepoch:   0, train loss: 2.38803, valid loss: 2.34602\nepoch:   1, train loss: 2.37269, valid loss: 2.33644\nepoch:   2, train loss: 2.36004, valid loss: 2.32891\nepoch:   3, train loss: 2.34957, valid loss: 2.32299\nepoch:   4, train loss: 2.34085, valid loss: 2.31831\nepoch:   5, train loss: 2.33357, valid loss: 2.31463\nepoch:   6, train loss: 2.32746, valid loss: 2.31172\nepoch:   7, train loss: 2.32232, valid loss: 2.30943\nepoch:   8, train loss: 2.31798, valid loss: 2.30762\nepoch:   9, train loss: 2.31431, valid loss: 2.30620\nepoch:  10, train loss: 2.31119, valid loss: 2.30508\nepoch:  11, train loss: 2.30853, valid loss: 2.30420\nepoch:  12, train loss: 2.30627, valid loss: 2.30351\nepoch:  13, train loss: 2.30433, valid loss: 2.30297\nepoch:  14, train loss: 2.30266, valid loss: 2.30254\n\n\nLet’s see how our training and validation loss looks like.\n\nfig = dlp_sigmoid.plot_losses()"
  },
  {
    "objectID": "posts/2022-10-23-pytorch-vanishing-gradients-deep-neural-networks.html#create-a-model-with-relu-activations",
    "href": "posts/2022-10-23-pytorch-vanishing-gradients-deep-neural-networks.html#create-a-model-with-relu-activations",
    "title": "Detect Vanishing Gradients in Deep Neural Networks by Plotting Gradient Distributions",
    "section": "Create a model with ReLU activations",
    "text": "Create a model with ReLU activations\nThis time let’s define the same model with ReLU activation functions.\n\nReluNet = nn.Sequential()\nReluNet.add_module(\"F\", nn.Flatten())\nReluNet.add_module(\"L1\", nn.Linear(28*28, 10, bias=False))\nReluNet.add_module(\"S1\", nn.ReLU())\nReluNet.add_module(\"L2\", nn.Linear(10, 10, bias=False))\nReluNet.add_module(\"S2\", nn.ReLU())\nReluNet.add_module(\"L3\", nn.Linear(10, 10, bias=False))\nReluNet.add_module(\"S3\", nn.ReLU())\nReluNet.add_module(\"L4\", nn.Linear(10, 10, bias=False))\n\nPrint the model’s summary.\n\n#collapse-output\nmodel_relu = ReluNet\nmodel_relu = model_relu.to(device)\n\nsummary(model_relu, (1, 28*28))\n\n----------------------------------------------------------------\n        Layer (type)               Output Shape         Param #\n================================================================\n           Flatten-1                  [-1, 784]               0\n            Linear-2                   [-1, 10]           7,840\n              ReLU-3                   [-1, 10]               0\n            Linear-4                   [-1, 10]             100\n              ReLU-5                   [-1, 10]               0\n            Linear-6                   [-1, 10]             100\n              ReLU-7                   [-1, 10]               0\n            Linear-8                   [-1, 10]             100\n================================================================\nTotal params: 8,140\nTrainable params: 8,140\nNon-trainable params: 0\n----------------------------------------------------------------\nInput size (MB): 0.00\nForward/backward pass size (MB): 0.01\nParams size (MB): 0.03\nEstimated Total Size (MB): 0.04\n----------------------------------------------------------------\n\n\nCreate an optimizer and a loss function.\n\nlr = 0.001\n\n# Defines a SGD optimizer to update the parameters\noptimizer_relu = optim.SGD(model_relu.parameters(), lr=lr)\n\n# Defines a BCE loss function\nloss_fn = nn.CrossEntropyLoss()\n\nTrain the model for 15 epochs.\n\n#collapse-output\nn_epochs = 15\n\ndlp_relu = DeepLearningPipeline(model_relu, loss_fn, optimizer_relu)\ndlp_relu.set_loaders(train_loader, valid_loader)\ndlp_relu.train(n_epochs)\n\nepoch:   0, train loss: 2.30268, valid loss: 2.30202\nepoch:   1, train loss: 2.30229, valid loss: 2.30165\nepoch:   2, train loss: 2.30193, valid loss: 2.30122\nepoch:   3, train loss: 2.30147, valid loss: 2.30069\nepoch:   4, train loss: 2.30086, valid loss: 2.29998\nepoch:   5, train loss: 2.30012, valid loss: 2.29905\nepoch:   6, train loss: 2.29906, valid loss: 2.29793\nepoch:   7, train loss: 2.29775, valid loss: 2.29667\nepoch:   8, train loss: 2.29621, valid loss: 2.29525\nepoch:   9, train loss: 2.29440, valid loss: 2.29363\nepoch:  10, train loss: 2.29227, valid loss: 2.29176\nepoch:  11, train loss: 2.28972, valid loss: 2.28957\nepoch:  12, train loss: 2.28673, valid loss: 2.28703\nepoch:  13, train loss: 2.28323, valid loss: 2.28408\nepoch:  14, train loss: 2.27912, valid loss: 2.28062\n\n\nNow plot the model losses.\n\nfig = dlp_relu.plot_losses()"
  },
  {
    "objectID": "posts/2022-10-23-pytorch-vanishing-gradients-deep-neural-networks.html#gradients-for-a-model-with-sigmoid-activations",
    "href": "posts/2022-10-23-pytorch-vanishing-gradients-deep-neural-networks.html#gradients-for-a-model-with-sigmoid-activations",
    "title": "Detect Vanishing Gradients in Deep Neural Networks by Plotting Gradient Distributions",
    "section": "Gradients for a model with Sigmoid activations",
    "text": "Gradients for a model with Sigmoid activations\nLet’s create a helper function that will plot the gradients for all the weights from each epoch. Note that our models have 4 layers, with L1 being the input layer and L4 being the output layer. Information flows from L1 to L4 during the forward pass. During the backward pass, gradients are calculated from the output layer (L4) and move toward the input layer (L1).\n\n\nCode\nimport seaborn as sns\n\ndef plot_gradients(grads, epoch=0):\n    \"\"\"\n    Args:\n        net: Object of class BaseNetwork\n        color: Color in which we want to visualize the histogram (for easier separation of activation functions)\n    \"\"\"\n    grads = grads\n\n    # Plotting\n    columns = len(grads)\n    fig, ax = plt.subplots(1, columns, figsize=(columns * 3.5, 2.5))\n    fig_index = 0\n    for key in grads:\n        key_ax = ax[fig_index % columns]\n        sns.histplot(data=grads[key], bins=30, ax=key_ax, kde=True)\n        key_ax.set_title(str(key))\n        key_ax.set_xlabel(\"Grad magnitude\", fontsize=11)\n        fig_index += 1\n    fig.suptitle(\n        f\"Epoch: {epoch}\", fontsize=16, y=1.05\n    )\n    fig.subplots_adjust(wspace=0.45)\n    plt.show()\n    plt.close()\n\n\nPlot the gradients for model with sigmoid activations.\n\nfor i in range(len(dlp_sigmoid.grad)):\n    plot_gradients(dlp_sigmoid.grad[i], i)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhat do we get from these plots?\n\nIssues\nIn this section, we will discuss the issues we can identify from these gradient plots for our SigmoidNet\nIssue 1: Look closely at how the Grad magnitude scale changes from L4 to L1 in an epoch. It is diminishing at an exponential rate. This tells us that the gradient is very high at the output layer but diminishes till it reaches L1.\nIssue 2: Gradient is also spread out and not smoother. This is a bad sign because it shows that different areas of the weight layer produce gradients in different ranges. For example, from L4 plot in the last epoch, we can see that the gradients are making clusters around -0.02, 0, and 0.02. We can also find that the gradient mean is not centered around 0. Either it is on the left of 0, or right, or has multiple peaks.\n\n\nPossible explanations\nIssue 1 Reason: Our network is facing Diminishing Gradient problem. I have pasted below another plot for Sigmoid. Image source\n\n\n\nsigmoid.png\n\n\nFrom this plot, we can see that the highest gradient produced by sigmoid is 0.25. So during backpropagation, when we calculate derivatives for deeper layers (L1, L2), there is a chain reaction where smaller and smaller numbers (less than 0.25) are multiplied to produce even smaller numbers. The result is that gradients diminish, and the weights are barely updated in deeper layers during the backward pass. We can avoid this by using a different activation function (e.g., ReLU) in hidden layers. We will do that in Section II.\nIssue 2 Reason: This is due to an initial weight initialization mismatch with the activation function used. By default, PyTorch uses kaiming initialization (Ref here) that works well for ReLU but not for sigmoid. It’s recommended to use Tanh or Xavior for sigmoid.\n\n\n\nCan we improve our SigmoidNet?\nLet’s train another SigmoidNet with the same configuration but with xavior initialization , and observe how it behaves now.\n\n#collapse-output\nmodel_sigmoid_v1 = SigmoidNet\nmodel_sigmoid_v1 = model_sigmoid_v1.to(device)\n\n# define a function to initialize weight with xavier uniform distribution\ndef init_weights(m):\n    if isinstance(m, nn.Linear):\n        torch.nn.init.xavier_uniform_(m.weight)\n        # m.bias.data.fill_(0.01)\n\n# reset model weights with xavier\nwith torch.no_grad():\n    model_sigmoid_v1.apply(init_weights)\n\nlr = 0.001\noptimizer_sigmoid = optim.SGD(model_sigmoid_v1.parameters(), lr=lr)\nloss_fn = nn.CrossEntropyLoss()\n\nn_epochs = 15\n\ndlp_sigmoid_v1 = DeepLearningPipeline(model_sigmoid_v1, loss_fn, optimizer_sigmoid)\ndlp_sigmoid_v1.set_loaders(train_loader, valid_loader)\ndlp_sigmoid_v1.train(n_epochs)\n\nepoch:   0, train loss: 2.49727, valid loss: 2.45576\nepoch:   1, train loss: 2.46252, valid loss: 2.42681\nepoch:   2, train loss: 2.43418, valid loss: 2.40356\nepoch:   3, train loss: 2.41090, valid loss: 2.38474\nepoch:   4, train loss: 2.39164, valid loss: 2.36943\nepoch:   5, train loss: 2.37565, valid loss: 2.35693\nepoch:   6, train loss: 2.36232, valid loss: 2.34670\nepoch:   7, train loss: 2.35119, valid loss: 2.33831\nepoch:   8, train loss: 2.34186, valid loss: 2.33143\nepoch:   9, train loss: 2.33404, valid loss: 2.32578\nepoch:  10, train loss: 2.32746, valid loss: 2.32113\nepoch:  11, train loss: 2.32192, valid loss: 2.31731\nepoch:  12, train loss: 2.31724, valid loss: 2.31417\nepoch:  13, train loss: 2.31328, valid loss: 2.31159\nepoch:  14, train loss: 2.30993, valid loss: 2.30946\n\n\n\nfig = dlp_sigmoid_v1.plot_losses()\n\n\n\n\n\n\n\n\n\n##\n#  SigmoidNet gradients with xavior initialization\nfor i in range(len(dlp_sigmoid_v1.grad)):\n    plot_gradients(dlp_sigmoid_v1.grad[i], i)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNow the plots are much smoother. Though the network is still suffering from diminishing gradients.\n\n\nRidge plot for gradients\nIn this section, we will use ridge plots for gradients. They provide a better perspective on how gradients evolve during epochs.\n\n\nCode\nimport pandas as pd\n\n# A helper function to get gradients of a weight layer for all epochs.\ndef get_layer_gradients(layer_name, layer_grads):\n    df = pd.DataFrame(columns=['x','g'])\n    for i in range(len(layer_grads)):\n        temp = {\n            'x': layer_grads[i][layer_name], # x --&gt; gradients\n            'g': i # g --&gt; epochs\n        }\n        epoch_df = pd.DataFrame(temp)\n        df = df.append(epoch_df, ignore_index=True)\n\n    return df\n\n\nPrint the names for the model weight layers.\n\nweight_layers_sigmoid_v1 = list(dlp_sigmoid_v1.grad[0].keys())\nweight_layers_sigmoid_v1\n\n['L1.weight', 'L2.weight', 'L3.weight', 'L4.weight']\n\n\nStore the gradients for each layer in a separate DataFrame. Each DataFrame has two columns\n\nx: for the gradient value\ng: for epoch\n\n\ndf0_sigmoid_v1 = get_layer_gradients(weight_layers_sigmoid_v1[0], dlp_sigmoid_v1.grad)\ndf1_sigmoid_v1 = get_layer_gradients(weight_layers_sigmoid_v1[1], dlp_sigmoid_v1.grad)\ndf2_sigmoid_v1 = get_layer_gradients(weight_layers_sigmoid_v1[2], dlp_sigmoid_v1.grad)\ndf3_sigmoid_v1 = get_layer_gradients(weight_layers_sigmoid_v1[3], dlp_sigmoid_v1.grad)\n\ndf3_sigmoid_v1.head()\n\n\n    \n      \n\n\n\n\n\n\nx\ng\n\n\n\n\n0\n-0.003590\n0\n\n\n1\n-0.002747\n0\n\n\n2\n-0.002291\n0\n\n\n3\n-0.006168\n0\n\n\n4\n-0.002711\n0\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n\n\n\n\nCode\n# Another helper function to create the ridge plots\ndef plot_gradients_ridge_v1(df, layer_name):\n    sns.set_theme(style=\"white\", rc={\"axes.facecolor\": (0, 0, 0, 0)})\n\n    # Initialize the FacetGrid object\n    pal = sns.cubehelix_palette(10, rot=-.25, light=.7)\n    g = sns.FacetGrid(df, row=\"g\", hue=\"g\", aspect=15, height=.5, palette=pal)\n\n    # Draw the densities in a few steps\n    g.map(sns.kdeplot, \"x\",\n        bw_adjust=.5, clip_on=False,\n        fill=True, alpha=1, linewidth=1.5)\n    g.map(sns.kdeplot, \"x\", clip_on=False, color=\"w\", lw=2, bw_adjust=.5)\n\n    # passing color=None to refline() uses the hue mapping\n    g.refline(y=0, linewidth=2, linestyle=\"-\", color=None, clip_on=False)\n\n\n    # Define and use a simple function to label the plot in axes coordinates\n    def label(x, color, label):\n        ax = plt.gca()\n        ax.text(0, .2, label, fontweight=\"bold\", color=color,\n                ha=\"left\", va=\"center\", transform=ax.transAxes)\n\n\n    g.map(label, \"x\")\n\n    # Set the subplots to overlap\n    g.figure.subplots_adjust(hspace=-.25)\n\n    g.fig.suptitle(layer_name, ha='left', fontsize=16, fontweight=16)\n\n    # Remove axes details that don't play well with overlap\n    g.set_titles(\"\")\n    g.set(yticks=[], ylabel=\"\")\n    g.despine(bottom=True, left=True)\n\n    return g\n\n\nCreate plots for all weight layer.\n\n\nCode\n# https://stackoverflow.com/questions/35042255/how-to-plot-multiple-seaborn-jointplot-in-subplot\nimport warnings\nimport matplotlib.image as mpimg\nwarnings.filterwarnings(\"ignore\")\n\ng1 = plot_gradients_ridge_v1(df0_sigmoid_v1, weight_layers_sigmoid_v1[0])\ng2 = plot_gradients_ridge_v1(df1_sigmoid_v1, weight_layers_sigmoid_v1[1])\ng3 = plot_gradients_ridge_v1(df2_sigmoid_v1, weight_layers_sigmoid_v1[2])\ng4 = plot_gradients_ridge_v1(df3_sigmoid_v1, weight_layers_sigmoid_v1[3])\n\ng1.savefig('g1.png')\nplt.close(g1.fig)\n\ng2.savefig('g2.png')\nplt.close(g2.fig)\n\ng3.savefig('g3.png')\nplt.close(g3.fig)\n\ng4.savefig('g4.png')\nplt.close(g4.fig)\n\n############### 3. CREATE YOUR SUBPLOTS FROM TEMPORAL IMAGES\nf, axarr = plt.subplots(2, 2, figsize=(25, 16))\n\naxarr[0,0].imshow(mpimg.imread('g1.png'))\naxarr[0,1].imshow(mpimg.imread('g2.png'))\naxarr[1,0].imshow(mpimg.imread('g3.png'))\naxarr[1,1].imshow(mpimg.imread('g4.png'))\n\n\n# turn off x and y axis\n[ax.set_axis_off() for ax in axarr.ravel()]\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\nWhat do we get from these ridge plots?\n\nIt also shows that the network is suffering from diminishing gradients. gradient scale ‘x’ is decreasing exponentially between weight layers\nIt shows that gradients are very spread out at the start. They keep on saturating till around epoch 10. After that, instead of getting more saturated around zero, multiple peaks start to emerge. This could be due to our learning rate. In the later epochs, our weights start to oscillate around zero. We can avoid this by using an adaptive learning rate that decreases when weights are near the global minimum."
  },
  {
    "objectID": "posts/2022-10-23-pytorch-vanishing-gradients-deep-neural-networks.html#gradients-for-a-model-with-relu-activations",
    "href": "posts/2022-10-23-pytorch-vanishing-gradients-deep-neural-networks.html#gradients-for-a-model-with-relu-activations",
    "title": "Detect Vanishing Gradients in Deep Neural Networks by Plotting Gradient Distributions",
    "section": "Gradients for a model with ReLU activations",
    "text": "Gradients for a model with ReLU activations\nIn this section we will visualize gradients for our model with ReLU activations.\n\nfor i in range(len(dlp_relu.grad)):\n    plot_gradients(dlp_relu.grad[i], i)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhat do we get from these plots?\n\nThis time, the gradient curves are nice and smooth. They are not diminishing between weight layers.\nGradients are shrinking between epochs. This means that learning is slowing down in later epochs, and we need to increase the learning rate.\n\n\n\nRidge plot for gradients\nLet’s also do ridge plots for ReluNet.\n\n\nCode\n# a helper function relu ridge plots. \n# same as 'plot_gradients_ridge_v1' but with added limits\ndef plot_gradients_ridge_v2(df, layer_name):\n    sns.set_theme(style=\"white\", rc={\"axes.facecolor\": (0, 0, 0, 0)})\n\n    # Initialize the FacetGrid object\n    pal = sns.cubehelix_palette(10, rot=-.25, light=.7)\n    g = sns.FacetGrid(df, row=\"g\", hue=\"g\", aspect=15, height=3.5, palette=pal, sharex=False, xlim=(-0.01,0.01)) ## works best\n    # g = sns.FacetGrid(df, row=\"g\", hue=\"g\", aspect=7, height=3.5, palette=pal)#, sharex=False, xlim=(-0.01,0.01)) ## works good\n\n    # Draw the densities in a few steps\n    g.map(sns.kdeplot, \"x\", bw_adjust=.5, clip_on=True, fill=True, alpha=1, linewidth=1.5)\n    g.map(sns.kdeplot, \"x\", clip_on=True, color=\"w\", lw=2, bw_adjust=.5)\n\n    # passing color=None to refline() uses the hue mapping\n    g.refline(y=0, linewidth=2, linestyle=\"-\", color=None, clip_on=True)\n\n\n    # Define and use a simple function to label the plot in axes coordinates\n    def label(x, color, label):\n        ax = plt.gca()\n        ax.text(0, .2, label, fontsize=40, fontweight=26, color=color, ha=\"left\", va=\"center\", transform=ax.transAxes)\n\n\n    g.map(label, \"x\")\n\n    # Set the subplots to overlap\n    g.figure.subplots_adjust(hspace=-.25)\n\n    g.fig.suptitle(layer_name, ha='left', fontsize=40, fontweight=26)\n\n    # Remove axes details that don't play well with overlap\n    g.set_titles(\"\")\n    g.set(yticks=[], ylabel=\"\")\n    g.despine(bottom=True, left=True)\n    plt.xticks(fontsize=35)\n\n    return g\n\n\n\n\nCode\nweight_layers_relu = list(dlp_relu.grad[0].keys())\n\ndf0_relu = get_layer_gradients(weight_layers_relu[0], dlp_relu.grad)\ndf1_relu = get_layer_gradients(weight_layers_relu[1], dlp_relu.grad)\ndf2_relu = get_layer_gradients(weight_layers_relu[2], dlp_relu.grad)\ndf3_relu = get_layer_gradients(weight_layers_relu[3], dlp_relu.grad)\n\n\n\n\nCode\nimport matplotlib.image as mpimg\nwarnings.filterwarnings(\"ignore\")\n\ng1 = plot_gradients_ridge_v2(df0_relu, weight_layers_relu[0])\ng2 = plot_gradients_ridge_v2(df1_relu, weight_layers_relu[1])\ng3 = plot_gradients_ridge_v2(df2_relu, weight_layers_relu[2])\ng4 = plot_gradients_ridge_v2(df3_relu, weight_layers_relu[3])\n\ng1.savefig('g1.png')\nplt.close(g1.fig)\n\ng2.savefig('g2.png')\nplt.close(g2.fig)\n\ng3.savefig('g3.png')\nplt.close(g3.fig)\n\ng4.savefig('g4.png')\nplt.close(g4.fig)\n\n############### 3. CREATE YOUR SUBPLOTS FROM TEMPORAL IMAGES\nf, axarr = plt.subplots(2, 2, figsize=(25, 16))\n\naxarr[0,0].imshow(mpimg.imread('g1.png'))\naxarr[0,1].imshow(mpimg.imread('g2.png'))\naxarr[1,0].imshow(mpimg.imread('g3.png'))\naxarr[1,1].imshow(mpimg.imread('g4.png'))\n\n\n# turn off x and y axis\n[ax.set_axis_off() for ax in axarr.ravel()]\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\nWhat do we get from these ridge plots?\nThe results are consistent with earlier plots.\n\nGradients are not diminishing between weight layers.\nGradients are shrinking between epochs, and we need to increase the learning rate for later epochs."
  },
  {
    "objectID": "posts/2022-11-09-pytorch-lstm-imdb-sentiment-prediction.html#credits",
    "href": "posts/2022-11-09-pytorch-lstm-imdb-sentiment-prediction.html#credits",
    "title": "Predicting the Sentiment of IMDB Movie Reviews using LSTM in PyTorch",
    "section": "Credits",
    "text": "Credits\nThis notebook takes inspiration and ideas from the following sources.\n\n“Machine learning with PyTorch and Scikit-Learn” by “Sebastian Raschka, Yuxi (Hayden) Liu, and Vahid Mirjalili”. You can get the book from its website: Machine learning with PyTorch and Scikit-Learn. In addition, the GitHub repository for this book has valuable notebooks: github.com/rasbt/machine-learning-book. Parts of the code you see in this notebook are taken from chapter 15 notebook of the same book.\n“Intro to Deep Learning and Generative Models Course” lecture series from “Sebastian Raschka”. Course website: stat453-ss2021. YouTube Link: Intro to Deep Learning and Generative Models Course. Lectures that are related to this post are L15.5 Long Short-Term Memory and L15.7 An RNN Sentiment Classifier in PyTorch"
  },
  {
    "objectID": "posts/2022-11-09-pytorch-lstm-imdb-sentiment-prediction.html#environment",
    "href": "posts/2022-11-09-pytorch-lstm-imdb-sentiment-prediction.html#environment",
    "title": "Predicting the Sentiment of IMDB Movie Reviews using LSTM in PyTorch",
    "section": "Environment",
    "text": "Environment\nThis notebook is prepared with Google Colab.\n\nGitHub: 2022-11-09-pytorch-lstm-imdb-sentiment-prediction.ipynb\nOpen In Colab: \n\nFor “runtime type” choose hardware accelerator as “GPU”. It will take a long time to complete the training without any GPU.\nThis notebook also depends on the PyTorch library TorchText. We will use this library to fetch IMDB review data. While using the torchtext latest version, I found more dependencies on other libraries like torchdata. Even after resolving them, it threw strange encoding errors while fetching IMDB data. So I have downgraded this library till the version I found working without external dependencies. Consequently, torch is also downgraded to a compatible version, but I did not find any issue while working with a lower version of PyTorch for this notebook. It is preferred to restart the runtime after the library installation is complete.\n\n#collapse-output\n! pip install torchtext==0.11.0\n\nLooking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\nRequirement already satisfied: torchtext==0.11.0 in /usr/local/lib/python3.7/dist-packages (0.11.0)\nRequirement already satisfied: torch==1.10.0 in /usr/local/lib/python3.7/dist-packages (from torchtext==0.11.0) (1.10.0)\nRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchtext==0.11.0) (1.21.6)\nRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchtext==0.11.0) (2.23.0)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torchtext==0.11.0) (4.64.1)\nRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.10.0-&gt;torchtext==0.11.0) (4.1.1)\nRequirement already satisfied: certifi&gt;=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests-&gt;torchtext==0.11.0) (2022.9.24)\nRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,&lt;1.26,&gt;=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests-&gt;torchtext==0.11.0) (1.24.3)\nRequirement already satisfied: idna&lt;3,&gt;=2.5 in /usr/local/lib/python3.7/dist-packages (from requests-&gt;torchtext==0.11.0) (2.10)\nRequirement already satisfied: chardet&lt;4,&gt;=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests-&gt;torchtext==0.11.0) (3.0.4)\n\n\n\n\nCode\nfrom platform import python_version\nimport numpy, matplotlib, pandas, torch, torchtext\n\nprint(\"python==\" + python_version())\nprint(\"numpy==\" + numpy.__version__)\nprint(\"torch==\" + torch.__version__)\nprint(\"torchtext==\" + torchtext.__version__)\nprint(\"matplotlib==\" + matplotlib.__version__)\n\n\npython==3.7.15\nnumpy==1.21.6\ntorch==1.10.0+cu102\ntorchtext==0.11.0\nmatplotlib==3.2.2"
  },
  {
    "objectID": "posts/2022-11-09-pytorch-lstm-imdb-sentiment-prediction.html#data-preparation",
    "href": "posts/2022-11-09-pytorch-lstm-imdb-sentiment-prediction.html#data-preparation",
    "title": "Predicting the Sentiment of IMDB Movie Reviews using LSTM in PyTorch",
    "section": "Data Preparation",
    "text": "Data Preparation\n\nDownload data\nLet’s download our movie review dataset. This dataset is also known as Large Movie Review Dataset, and can also be obtained in a compressed zip file from this link. Using the torchtext library makes downloading, extracting, and reading files a lot easier. ‘torchtext.datasets’ comes with many more NLP related datasets, and a full list can be found here.\n\nfrom torchtext.datasets import IMDB\nfrom torch.utils.data.dataset import random_split\n\ntorch.manual_seed(1)\n\ntrain_dataset_raw = IMDB(split=\"train\")\ntest_dataset_raw = IMDB(split=\"test\")\n\nCheck the size of the downloaded data.\n\nprint(\"Train dataset size: \", len(train_dataset_raw))\nprint(\"Test dataset size: \", len(test_dataset_raw))\n\nTrain dataset size:  25000\nTest dataset size:  25000\n\n\n\n\nSplit train data further into train and validation set\nBoth train and test datasets have 25000 reviews. Therefore, we can split the training set further into the train and validation sets.\n\ntrain_set_size = 20000\nvalid_set_size = 5000\n\ntrain_dataset, valid_dataset = random_split(list(train_dataset_raw), [20000, 5000])\n\n\n\nHow does this data look?\nThe data we have is in the form of tuples. The first index has the sentiment label, and the second contains the review text. Let’s check the first element in our training dataset.\n\ntrain_dataset[0]\n\n('pos',\n 'An extra is called upon to play a general in a movie about the Russian Revolution. However, he is not any ordinary extra. He is Serguis Alexander, former commanding general of the Russia armies who is now being forced to relive the same scene, which he suffered professional and personal tragedy in, to satisfy the director who was once a revolutionist in Russia and was humiliated by Alexander. It can now be the time for this broken man to finally \"win\" his penultimate battle. This is one powerful movie with meticulous direction by Von Sternberg, providing the greatest irony in Alexander\\'s character in every way he can. Jannings deserved his Oscar for the role with a very moving performance playing the general at his peak and at his deepest valley. Powell lends a sinister support as the revenge minded director and Brent is perfect in her role with her face and movements showing so much expression as Jannings\\' love. All around brilliance. Rating, 10.')\n\n\nCheck the first index of the validation set.\n\nvalid_dataset[0]\n\n('neg',\n 'The Dereks did seem to struggle to find rolls for Bo after \"10\".&lt;br /&gt;&lt;br /&gt;I used to work for a marine park in the Florida Keys. One day, the script for \"Ghosts Can\\'t Do It\" was circulating among the trainers in the \"fish house\" where food was prepared for the dolphins. There was one scene where a -dolphin- supposedly propositions Bo (or Bo the dolphin), asking to \"go make eggs.\" Reading the script, we -lauuughed-...&lt;br /&gt;&lt;br /&gt;We did not end up doing any portion of this movie at our facility, although our dolphins -were- in \"The Big Blue!\"&lt;br /&gt;&lt;br /&gt;This must have been very close to the end of Anthony Quinn\\'s life. I hope he had fun in this film, as it certainly didn\\'t do anything for his legacy.')\n\n\n\n\nData preprocessing steps\nFrom these two reviews, we can deduce that\n\nWe have two labels. ‘pos’ for a positive and ‘neg’ for a negative review\nFrom the second review (from valid_dataset), we also get that text may contain HTML tags, special characters, and emoticons besides normal English words. It will require some preprocessing to remove them for proper word tokenization.\nReviews can have varying text lengths. It will require some padding to make all review texts the same size.\n\nLet’s take a simple text example and apply these steps to understand why these steps are essential in preprocessing. In the last step, we will create tokens from the preprocessed text.\n\nexample_text = '''This is awesome movie &lt;br /&gt;&lt;br /&gt;. I loved it so much :-) I\\'m goona watch it again :)'''\nexample_text\n\n\"This is awesome movie &lt;br /&gt;&lt;br /&gt;. I loved it so much :-) I'm goona watch it again :)\"\n\n\n\n##\n# step 1. remove HTML tags. they are not helpful in understanding the sentiments of a review\nimport re\n\ntext = re.sub('&lt;[^&gt;]*&gt;', '', example_text)\ntext\n\n\"This is awesome movie . I loved it so much :-) I'm goona watch it again :)\"\n\n\n\n##\n# step 2: use lowercase for all text to keep symmetry\ntext = text.lower()\ntext\n\n\"this is awesome movie . i loved it so much :-) i'm goona watch it again :)\"\n\n\n\n##\n# step 3: extract emoticons. keep them as they are important sentiment signals\nemoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text)\nemoticons\n\n[':-)', ':)']\n\n\n\n##\n# step 4: remove punctuation marks\ntext = re.sub('[\\W]+', ' ', text)\ntext\n\n'this is awesome movie i loved it so much i m goona watch it again '\n\n\n\n##\n# step 5: put back emoticons\ntext = text + ' '.join(emoticons).replace('-', '')\ntext\n\n'this is awesome movie i loved it so much i m goona watch it again :) :)'\n\n\n\n##\n# step 6: generate word tokens\ntext = text.split()\ntext\n\n['this',\n 'is',\n 'awesome',\n 'movie',\n 'i',\n 'loved',\n 'it',\n 'so',\n 'much',\n 'i',\n 'm',\n 'goona',\n 'watch',\n 'it',\n 'again',\n ':)',\n ':)']\n\n\nLet’s put all the preprocessing steps in a nice function and give it a name.\n\ndef tokenizer(text):\n    # step 1. remove HTML tags. they are not helpful in understanding the sentiments of a review\n    # step 2: use lowercase for all text to keep symmetry\n    # step 3: extract emoticons. keep them as they are important sentiment signals\n    # step 4: remove punctuation marks\n    # step 5: put back emoticons\n    # step 6: generate word tokens\n    text = re.sub(\"&lt;[^&gt;]*&gt;\", \"\", text)\n    text = text.lower()\n    emoticons = re.findall(\"(?::|;|=)(?:-)?(?:\\)|\\(|D|P)\", text)\n    text = re.sub(\"[\\W]+\", \" \", text)\n    text = text + \" \".join(emoticons).replace(\"-\", \"\")\n    tokenized = text.split()\n    return tokenized\n\nApply tokenizer on the example_text to verify the output.\n\nexample_tokens = tokenizer(example_text)\nexample_tokens\n\n['this',\n 'is',\n 'awesome',\n 'movie',\n 'i',\n 'loved',\n 'it',\n 'so',\n 'much',\n 'i',\n 'm',\n 'goona',\n 'watch',\n 'it',\n 'again',\n ':)',\n ':)']\n\n\n\n\nPreparing data dictionary\nWe are successful in creating word tokens from our example_text. But there is one more problem. Some of the tokens are repeating. If we can convert these tokens into a dictionary along with their frequency count, we can significantly reduce the generated token size from these reviews. Let’s do that.\n\nfrom collections import Counter\n\ntoken_counts = Counter()\ntoken_counts.update(example_tokens)\ntoken_counts\n\nCounter({'this': 1,\n         'is': 1,\n         'awesome': 1,\n         'movie': 1,\n         'i': 2,\n         'loved': 1,\n         'it': 2,\n         'so': 1,\n         'much': 1,\n         'm': 1,\n         'goona': 1,\n         'watch': 1,\n         'again': 1,\n         ':)': 2})\n\n\nLet’s sort the output to have the most common words at the top.\n\nsorted_by_freq_tuples = sorted(token_counts.items(), key=lambda x: x[1], reverse=True)\nsorted_by_freq_tuples\n\n[('i', 2),\n ('it', 2),\n (':)', 2),\n ('this', 1),\n ('is', 1),\n ('awesome', 1),\n ('movie', 1),\n ('loved', 1),\n ('so', 1),\n ('much', 1),\n ('m', 1),\n ('goona', 1),\n ('watch', 1),\n ('again', 1)]\n\n\nIt shows that in our example text, the top place is taken by pronouns (i and it) followed by the emoticon. Though our data is now correctly processed, it needs to be prepared to be fed to a model. Because [machine] models love math and work with numbers exclusively. To convert our dictionary of word tokens into integers, we can take help from torchtext.vocab. Its purpose in the official documentation is defined as link here\n\nFactory method for creating a vocab object which maps tokens to indices.\n\n\nNote that the ordering in which key value pairs were inserted in the ordered_dict will be respected when building the vocab. Therefore if sorting by token frequency is important to the user, the ordered_dict should be created in a way to reflect this.\n\nIt highlights three points:\n\nIt maps tokens to indices\nIt requires an ordered dictionary (OrderedDict) to work\nTokens in vocab at the starting indices reflect higher frequency\n\n\n##\n# step 1: convert our sorted list of tokens to OrderedDict\nfrom collections import OrderedDict\n\nordered_dict = OrderedDict(sorted_by_freq_tuples)\nordered_dict\n\nOrderedDict([('i', 2),\n             ('it', 2),\n             (':)', 2),\n             ('this', 1),\n             ('is', 1),\n             ('awesome', 1),\n             ('movie', 1),\n             ('loved', 1),\n             ('so', 1),\n             ('much', 1),\n             ('m', 1),\n             ('goona', 1),\n             ('watch', 1),\n             ('again', 1)])\n\n\n\n##\n# Check the length of our dictionary\nlen(ordered_dict)\n\n14\n\n\n\n##\n# step 2: convert the ordered dict to torchtext.vocab\nfrom torchtext.vocab import vocab\n\nvb = vocab(ordered_dict)\nvb.get_stoi()\n\n{'goona': 11,\n 'much': 9,\n 'm': 10,\n 'loved': 7,\n 'watch': 12,\n 'so': 8,\n 'movie': 6,\n 'it': 1,\n 'again': 13,\n 'this': 3,\n 'i': 0,\n 'awesome': 5,\n ':)': 2,\n 'is': 4}\n\n\nThis generated vocabulary shows that tokens with higher frequency (i, it) have been assigned lower indices (or integers). This vocabulary will act as a lookup table for us, and during training for each word token, we will find a corresponding index from this vocab and pass it to our model.\nWe have done many steps while processing our example_text. Let’s summarize them here before moving further\n\nSummary of data dictionary preparation steps\n\nGenerate tokens from text using the function tokenizer\nFind the frequency of tokens using Python collections.Counter\nSort the tokens based on their frequency in descending order\nPut the sorted tokens in Python collections.OrderedDict\nConvert the tokens into integers using torchtext.vocab\n\nLet’s apply all these steps on our IMDB reviews training dataset.\n\n##\n# step 1: convert reviews into tokens\n# step 2: find frequency of tokens\n\ntoken_counts = Counter()\n\nfor label, line in train_dataset:\n    tokens = tokenizer(line)\n    token_counts.update(tokens)\n \nprint('IMDB vocab size:', len(token_counts))\n\nIMDB vocab size: 69023\n\n\nAfter tokenizing IMDB reviews, we find that there 69023 unique tokens.\n\n##\n# step 3: sort the token based on their frequency\n# step 4: put the sorted tokens in OrderedDict\n# step 5: convert token to integers using vocab object\n\nsorted_by_freq_tuples = sorted(token_counts.items(), key=lambda x: x[1], reverse=True)\nordered_dict = OrderedDict(sorted_by_freq_tuples)\n\nvb = vocab(ordered_dict)\n\nvb.insert_token(\"&lt;pad&gt;\", 0)  # special token for padding\nvb.insert_token(\"&lt;unk&gt;\", 1)  # special token for unknown words\nvb.set_default_index(1)\n\n# print some token indexes from vocab\nfor token in [\"this\", \"is\", \"an\", \"example\"]:\n    print(token, \" --&gt; \", vb[token])\n\nthis  --&gt;  11\nis  --&gt;  7\nan  --&gt;  35\nexample  --&gt;  457\n\n\nWe have added two extra tokens to our vocabulary.\n\n“pad” for padding. This token will come in handy when we pad our reviews to make them of the same length\n“unk” for unknown. This token will come in handy if we find any token in the validation or test set that was not part of the train set\n\nLet’s also print the tokens present at the first ten indices of our vocab object.\n\nvb.get_itos()[:10]\n\n['&lt;pad&gt;', '&lt;unk&gt;', 'the', 'and', 'a', 'of', 'to', 'is', 'it', 'in']\n\n\nIt shows that articles, prepositions, and pronouns are the most common words in the training dataset. So let’s also check the least common words.\n\nvb.get_itos()[-10:]\n\n['hairband',\n 'ratt',\n 'bettiefile',\n 'queueing',\n 'johansen',\n 'hemmed',\n 'jardine',\n 'morland',\n 'seriousuly',\n 'fictive']\n\n\nThe least common words seem to be people or place names or misspelled words like ‘queueing’ and ‘seriousuly’."
  },
  {
    "objectID": "posts/2022-11-09-pytorch-lstm-imdb-sentiment-prediction.html#define-data-processing-pipelines",
    "href": "posts/2022-11-09-pytorch-lstm-imdb-sentiment-prediction.html#define-data-processing-pipelines",
    "title": "Predicting the Sentiment of IMDB Movie Reviews using LSTM in PyTorch",
    "section": "Define data processing pipelines",
    "text": "Define data processing pipelines\nAt this point, we have our tokenizer function and vocabulary lookup ready. For each review item from the dataset, we are supposed to perform the following preprocessing steps:\nFor review text\n\nCreate tokens from the review text\nAssign a unique integer to each token from the vocab lookup\n\nFor review label\n\nAssign 1 for pos and 0 for neg label\n\nLet’s create two simple functions (inline lambda) for review text and label processing.\n\n##\n# inline lambda functions for text and label precessing\ntext_pipeline = lambda x: [vb[token] for token in tokenizer(x)]\nlabel_pipeline = lambda x: 1.0 if x == \"pos\" else 0.0\n\n\n##\n# apply text_pipeline to example_text\ntext_pipeline(example_text)\n\n[11, 7, 1166, 18, 10, 450, 8, 37, 74, 10, 142, 1, 104, 8, 174, 2287, 2287]\n\n\nInstead of processing a single review at a time, we always prefer to work with a batch of them during model training. For each review item in the batch, we will be doing the same preprocessing steps i.e. review text processing and label processing. For handling preprocessing steps at a batch level, we can create another higher-level function that applies preprocessing steps at a batch level.\n\n##\n# setting device on GPU if available, else CPU\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint('Using device:', device)\n\nUsing device: cuda\n\n\n\n##\n# a function to apply pre-processing steps at a batch level\nimport torch.nn as nn\n\ndef collate_batch(batch):\n    label_list, text_list, lengths = [], [], []\n\n    # iterate over all reviews in a batch\n    for _label, _text in batch:\n        # label preprocessing\n        label_list.append(label_pipeline(_label))\n        # text preprocessing\n        processed_text = torch.tensor(text_pipeline(_text), dtype=torch.int64)\n\n        # store the processed text in a list\n        text_list.append(processed_text)\n        \n        # store the length of processed text\n        # this will come handy in future when we want to know the original size of a text (without padding)\n        lengths.append(processed_text.size(0))\n    \n    label_list = torch.tensor(label_list)\n    lengths = torch.tensor(lengths)\n    \n    # pad the processed reviews to make their lengths consistant\n    padded_text_list = nn.utils.rnn.pad_sequence(\n        text_list, batch_first=True)\n    \n    # return\n    # 1. a list of processed and padded review texts\n    # 2. a list of processed labels\n    # 3. a list of review text original lengths (before padding)\n    return padded_text_list.to(device), label_list.to(device), lengths.to(device)\n\n\nSequence padding\nIn the above collate_batch function, I added one extra padding step.\nadded_text_list = nn.utils.rnn.pad_sequence(text_list, batch_first=True)\nWe intend to make all review texts in a batch of the same length. For this, we take the maximum length of a text in a batch, all pad all the smaller text with extra dummy tokens (‘pad’) to make their sizes equal. Finally, with all the data in a batch of the same dimension, we convert it into a tensor matrix for faster processing.\nTo understand how PyTorch utility nn.utils.rnn.pad_sequence works, we can take a simple example of three tensors (a, b, c) of varying sizes (1, 3, 5).\n\n##\n# initialize three tensors of varying sizes\na = torch.tensor([1])\nb = torch.tensor([2, 3, 4])\nc = torch.tensor([5, 6, 7, 8, 9])\na, b, c\n\n(tensor([1]), tensor([2, 3, 4]), tensor([5, 6, 7, 8, 9]))\n\n\nNow let’s pad them to make sizes consistant.\n\n##\n# apply padding on tensors\npad_seq = nn.utils.rnn.pad_sequence([a, b, c])\npad_seq\n\ntensor([[1, 2, 5],\n        [0, 3, 6],\n        [0, 4, 7],\n        [0, 0, 8],\n        [0, 0, 9]])\n\n\n\n\nSequence packing\nFrom the above output, we can see that after padding tensors of varying sizes, we can convert them into a single matrix for faster processing. But the drawback of this approach is that we can have many, many padded tokens in our matrix. They are not helping us in any way, instead of occupying a lot of machine memory. To avoid this, we can also squish these matrixes into a much condensed form called packed padded sequences using PyTorch utility nn.utils.rnn.pack_padded_sequence.\n\npack_pad_seq = nn.utils.rnn.pack_padded_sequence(\n    pad_seq, [1, 3, 5], enforce_sorted=False, batch_first=False\n)\npack_pad_seq.data\n\ntensor([5, 2, 1, 6, 3, 7, 4, 8, 9])\n\n\nHere the tensor still holds all the original tensor values (1 to 9) but is very condensed and has no extra padded token. So how does this tensor know which tokens belong to which token? For this, it stores some additional information.\n\nbatch sizes (or original tensor length)\ntensor indices\n\nWe can move back and forth between the padded pack and unpacked sequences using this information.\n\npack_pad_seq\n\nPackedSequence(data=tensor([5, 2, 1, 6, 3, 7, 4, 8, 9]), batch_sizes=tensor([3, 2, 2, 1, 1]), sorted_indices=tensor([2, 1, 0]), unsorted_indices=tensor([2, 1, 0]))\n\n\n\n\nRun data preprocessing pipelines on an example batch\nLet’s load our data in the PyTorch DataLoader class and create a small batch of 4 reviews. Preprocess the entire set with collate_batch function.\n\nfrom torch.utils.data import DataLoader\n\ndataloader = DataLoader(\n    train_dataset, batch_size=4, shuffle=False, collate_fn=collate_batch\n)\ntext_batch, label_batch, length_batch = next(iter(dataloader))\n\nprint(\"text_batch.shape: \", text_batch.shape)\nprint(\"label_batch: \", label_batch)\nprint(\"length_batch: \", length_batch)\n\ntext_batch.shape:  torch.Size([4, 218])\nlabel_batch:  tensor([1., 1., 1., 0.], device='cuda:0')\nlength_batch:  tensor([165,  86, 218, 145], device='cuda:0')\n\n\n\ntext_batch.shape: torch.Size([4, 218]) tells us that in this batch, there are four reviews (or their tokens) and all have the same length of 218\nlabel_batch:  tensor([1., 1., 1., 0.]) tells us that the first three reviews are positive and the last is negative\nlength_batch:  tensor([165,  86, 218, 145]) tells us that before padding the original length of review tokens\n\nLet’s check what the first review in this batch looks like after preprocessing and padding.\n\nprint(text_batch[0])\n\ntensor([   35,  1739,     7,   449,   721,     6,   301,     4,   787,     9,\n            4,    18,    44,     2,  1705,  2460,   186,    25,     7,    24,\n          100,  1874,  1739,    25,     7, 34415,  3568,  1103,  7517,   787,\n            5,     2,  4991, 12401,    36,     7,   148,   111,   939,     6,\n        11598,     2,   172,   135,    62,    25,  3199,  1602,     3,   928,\n         1500,     9,     6,  4601,     2,   155,    36,    14,   274,     4,\n        42945,     9,  4991,     3,    14, 10296,    34,  3568,     8,    51,\n          148,    30,     2,    58,    16,    11,  1893,   125,     6,   420,\n         1214,    27, 14542,   940,    11,     7,    29,   951,    18,    17,\n        15994,   459,    34,  2480, 15211,  3713,     2,   840,  3200,     9,\n         3568,    13,   107,     9,   175,    94,    25,    51, 10297,  1796,\n           27,   712,    16,     2,   220,    17,     4,    54,   722,   238,\n          395,     2,   787,    32,    27,  5236,     3,    32,    27,  7252,\n         5118,  2461,  6390,     4,  2873,  1495,    15,     2,  1054,  2874,\n          155,     3,  7015,     7,   409,     9,    41,   220,    17,    41,\n          390,     3,  3925,   807,    37,    74,  2858,    15, 10297,   115,\n           31,   189,  3506,   667,   163,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0],\n       device='cuda:0')\n\n\nTo complete the picture, I have re-printed the original text of the first review and manually processed a part of it. You can verify that the tokens match.\n\n##\n# first review\ntrain_dataset[0]\n\n('pos',\n 'An extra is called upon to play a general in a movie about the Russian Revolution. However, he is not any ordinary extra. He is Serguis Alexander, former commanding general of the Russia armies who is now being forced to relive the same scene, which he suffered professional and personal tragedy in, to satisfy the director who was once a revolutionist in Russia and was humiliated by Alexander. It can now be the time for this broken man to finally \"win\" his penultimate battle. This is one powerful movie with meticulous direction by Von Sternberg, providing the greatest irony in Alexander\\'s character in every way he can. Jannings deserved his Oscar for the role with a very moving performance playing the general at his peak and at his deepest valley. Powell lends a sinister support as the revenge minded director and Brent is perfect in her role with her face and movements showing so much expression as Jannings\\' love. All around brilliance. Rating, 10.')\n\n\n\n##\n# manually preprocessing a part of review text\n# notice that the generated tokens match\ntext = 'An extra is called upon to play a general in a movie about the Russian Revolution'\n[vb[token] for token in tokenizer(text)]\n\n[35, 1739, 7, 449, 721, 6, 301, 4, 787, 9, 4, 18, 44, 2, 1705, 2460]"
  },
  {
    "objectID": "posts/2022-11-09-pytorch-lstm-imdb-sentiment-prediction.html#batching-the-training-validation-and-test-dataset",
    "href": "posts/2022-11-09-pytorch-lstm-imdb-sentiment-prediction.html#batching-the-training-validation-and-test-dataset",
    "title": "Predicting the Sentiment of IMDB Movie Reviews using LSTM in PyTorch",
    "section": "Batching the training, validation, and test dataset",
    "text": "Batching the training, validation, and test dataset\nLet’s proceed on creating DataLoaders for train, valid, and test data with batch_size = 32\n\nbatch_size = 32\n\ntrain_dl = DataLoader(\n    train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_batch\n)\nvalid_dl = DataLoader(\n    valid_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_batch\n)\ntest_dl = DataLoader(\n    test_dataset_raw, batch_size=batch_size, shuffle=False, collate_fn=collate_batch\n)"
  },
  {
    "objectID": "posts/2022-11-09-pytorch-lstm-imdb-sentiment-prediction.html#define-model-training-and-evaluation-pipelines",
    "href": "posts/2022-11-09-pytorch-lstm-imdb-sentiment-prediction.html#define-model-training-and-evaluation-pipelines",
    "title": "Predicting the Sentiment of IMDB Movie Reviews using LSTM in PyTorch",
    "section": "Define model training and evaluation pipelines",
    "text": "Define model training and evaluation pipelines\nI have defined two simple functions to train and evaluate the model in this section.\n\n##\n# model training pipeline\n# https://github.com/rasbt/machine-learning-book/blob/main/ch15/ch15_part2.ipynb\ndef train(dataloader):\n    model.train()\n    total_acc, total_loss = 0, 0\n    for text_batch, label_batch, lengths in dataloader:\n        optimizer.zero_grad()\n        pred = model(text_batch, lengths)[:, 0]\n        loss = loss_fn(pred, label_batch)\n        loss.backward()\n        optimizer.step()\n        total_acc += ((pred &gt;= 0.5).float() == label_batch).float().sum().item()\n        total_loss += loss.item() * label_batch.size(0)\n    return total_acc / len(dataloader.dataset), total_loss / len(dataloader.dataset)\n\n\n# model evaluation pipeline\ndef evaluate(dataloader):\n    model.eval()\n    total_acc, total_loss = 0, 0\n    with torch.no_grad():\n        for text_batch, label_batch, lengths in dataloader:\n            pred = model(text_batch, lengths)[:, 0]\n            loss = loss_fn(pred, label_batch)\n            total_acc += ((pred &gt;= 0.5).float() == label_batch).float().sum().item()\n            total_loss += loss.item() * label_batch.size(0)\n    return total_acc / len(dataloader.dataset), total_loss / len(dataloader.dataset)"
  },
  {
    "objectID": "posts/2022-11-09-pytorch-lstm-imdb-sentiment-prediction.html#rnn-model-configuration-loss-function-and-optimizer",
    "href": "posts/2022-11-09-pytorch-lstm-imdb-sentiment-prediction.html#rnn-model-configuration-loss-function-and-optimizer",
    "title": "Predicting the Sentiment of IMDB Movie Reviews using LSTM in PyTorch",
    "section": "RNN model configuration, loss function, and optimizer",
    "text": "RNN model configuration, loss function, and optimizer\nWe have seen the review text, which can be long sequences. We will use the LSTM layer for capturing the long-term dependencies. Our sentiment analysis model is composed of the following layers\n\nStart with an Embedding layer. Placing the embedding layer is similar to one-hot-encoding, where each word token is converted to a separate feature (or vector or column). But this can lead to too many features (curse of dimensionality or dimensional explosion). To avoid this, we try to map tokens to fixed-size vectors (or columns). In such a feature matrix, different elements denote different tokens. Tokens that are closed are also placed together. Further, during training, we also learn and update the positioning of tokens. Similar tokens are placed into closer and closer locations. Such a matrix layer is termed an embedding layer.\nAfter the embedding layer, there is the RNN layer (LSTM to be specific).\nThen we have a fully connected layer followed by activation and another fully connected layer.\nFinally, we have a logistic sigmoid layer for prediction\n\n\n##\n# https://github.com/rasbt/machine-learning-book/blob/main/ch15/ch15_part2.ipynb\nclass RNN(nn.Module):\n    def __init__(self, vocab_size, embed_dim, rnn_hidden_size, fc_hidden_size):\n        super().__init__()\n        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n        self.rnn = nn.LSTM(embed_dim, rnn_hidden_size, batch_first=True)\n        self.fc1 = nn.Linear(rnn_hidden_size, fc_hidden_size)\n        self.relu = nn.ReLU()\n        self.fc2 = nn.Linear(fc_hidden_size, 1)\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, text, lengths):\n        out = self.embedding(text)\n        out = nn.utils.rnn.pack_padded_sequence(\n            out, lengths.cpu().numpy(), enforce_sorted=False, batch_first=True\n        )\n        out, (hidden, cell) = self.rnn(out)\n        out = hidden[-1, :, :]\n        out = self.fc1(out)\n        out = self.relu(out)\n        out = self.fc2(out)\n        out = self.sigmoid(out)\n        return out\n\n\nvocab_size = len(vb)\nembed_dim = 20\nrnn_hidden_size = 64\nfc_hidden_size = 64\n\ntorch.manual_seed(1)\nmodel = RNN(vocab_size, embed_dim, rnn_hidden_size, fc_hidden_size)\nmodel = model.to(device)\n\n\nDefine model loss function and optimizer\nFor loss function (or criterion), I have used Binary Cross Entropy, and for loss optimization, I have used Adam algorithm\n\ntorch.manual_seed(1)\n\nloss_fn = nn.BCELoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
  },
  {
    "objectID": "posts/2022-11-09-pytorch-lstm-imdb-sentiment-prediction.html#model-training-and-evaluation",
    "href": "posts/2022-11-09-pytorch-lstm-imdb-sentiment-prediction.html#model-training-and-evaluation",
    "title": "Predicting the Sentiment of IMDB Movie Reviews using LSTM in PyTorch",
    "section": "Model training and evaluation",
    "text": "Model training and evaluation\nLet’s run the pipeline for ten epochs and compare the training and validation accuracy.\n\nnum_epochs = 10\nfor epoch in range(num_epochs):\n    acc_train, loss_train = train(train_dl)\n    acc_valid, loss_valid = evaluate(valid_dl)\n    print(\n        f\"Epoch {epoch} train accuracy: {acc_train:.4f}; val accuracy: {acc_valid:.4f}\"\n    )\n\nEpoch 0 train accuracy: 0.6085; val accuracy: 0.6502\nEpoch 1 train accuracy: 0.7206; val accuracy: 0.7462\nEpoch 2 train accuracy: 0.7613; val accuracy: 0.6250\nEpoch 3 train accuracy: 0.8235; val accuracy: 0.8232\nEpoch 4 train accuracy: 0.8819; val accuracy: 0.8482\nEpoch 5 train accuracy: 0.9132; val accuracy: 0.8526\nEpoch 6 train accuracy: 0.9321; val accuracy: 0.8374\nEpoch 7 train accuracy: 0.9504; val accuracy: 0.8502\nEpoch 8 train accuracy: 0.9643; val accuracy: 0.8608\nEpoch 9 train accuracy: 0.9747; val accuracy: 0.8636\n\n\n\nEvaluate sentiments on random texts\nLet’s create another helper method to evaluate sentiments on random texts.\n\ndef classify_review(text):\n    text_list, lengths = [], []\n\n    # process review text with text_pipeline\n    # note: \"text_pipeline\" has dependency on data vocabulary\n    processed_text = torch.tensor(text_pipeline(text), dtype=torch.int64)\n    text_list.append(processed_text)\n\n    # get processed review tokens length\n    lengths.append(processed_text.size(0))\n    lengths = torch.tensor(lengths)\n        \n    # change the dimensions from (torch.Size([8]), torch.Size([1, 8]))\n    # nn.utils.rnn.pad_sequence(text_list, batch_first=True) does this too\n    padded_text_list = torch.unsqueeze(processed_text, 0)\n\n    # move tensors to correct device\n    padded_text_list = padded_text_list.to(device)\n    lengths = lengths.to(device)\n\n    # get prediction\n    model.eval()\n    pred = model(padded_text_list, lengths)\n    print(\"model pred: \", pred)\n\n    # positive or negative review\n    review_class = 'negative' # else case\n    if (pred&gt;=0.5) == 1:\n        review_class = \"positive\"\n\n    print(\"review type: \", review_class)\n\n\n##\n# create two random texts with strong positive and negative sentiments\npos_review = 'i love this movie. it was so good.'\nneg_review = 'slow and boring. waste of time.'\n\n\nclassify_review(pos_review)\n\nmodel pred:  tensor([[0.9388]], device='cuda:0', grad_fn=&lt;SigmoidBackward0&gt;)\nreview type:  positive\n\n\n\nclassify_review(neg_review)\n\nmodel pred:  tensor([[0.0057]], device='cuda:0', grad_fn=&lt;SigmoidBackward0&gt;)\nreview type:  negative"
  },
  {
    "objectID": "posts/2022-12-02-pytorch-word2vec-embedding.html#credits",
    "href": "posts/2022-12-02-pytorch-word2vec-embedding.html#credits",
    "title": "Implementing Word2Vec with PyTorch",
    "section": "Credits",
    "text": "Credits\nThis notebook takes inspiration and ideas from the following sources.\n\nAn excellent word2vec introduction from “Jay Alammar”: illustrated-word2vec.\nBlog post by “Musashi (sometimes Jacobs-) Harukawa” with the same title. You can find the original post here: word2vec-from-scratch. Parts of the code you see in this notebook are taken from this post.\nAnother very detailed and well-explained blog post by “Olga Chernytska”. You can find the original post here: word2vec-with-pytorch-implementing-original-paper. Parts of the code you see in this notebook are taken from this post."
  },
  {
    "objectID": "posts/2022-12-02-pytorch-word2vec-embedding.html#environment",
    "href": "posts/2022-12-02-pytorch-word2vec-embedding.html#environment",
    "title": "Implementing Word2Vec with PyTorch",
    "section": "Environment",
    "text": "Environment\nThis notebook is prepared with Google Colab.\n\nGitHub: 2022-12-02-pytorch-word2vec-embedding.ipynb\nOpen In Colab: \n\nHugging Face Datasets library is required for this post.\n\n# install hugging face datasets\n! pip install datasets\n\n\n\nCode\nfrom platform import python_version\nimport numpy, matplotlib, pandas, torch, datasets\n\nprint(\"python==\" + python_version())\nprint(\"numpy==\" + numpy.__version__)\nprint(\"torch==\" + torch.__version__)\nprint(\"matplotlib==\" + matplotlib.__version__)\nprint(\"datasets==\" + datasets.__version__)\n\n\npython==3.8.15\nnumpy==1.21.6\ntorch==1.12.1+cu113\nmatplotlib==3.2.2\ndatasets==2.7.1"
  },
  {
    "objectID": "posts/2022-12-02-pytorch-word2vec-embedding.html#introduction",
    "href": "posts/2022-12-02-pytorch-word2vec-embedding.html#introduction",
    "title": "Implementing Word2Vec with PyTorch",
    "section": "Introduction",
    "text": "Introduction\n\nFollowing are the word2vec and word embedding explanations provided by OpenAI ChatGPT. If unsatisfied, I suggest using the links provided in the “Credits” section (illustrated-word2vec from Jay Alammar).\n\nprompt &gt; word2vec blog introduction\nWord2vec\nWord2vec is a widely-used natural language processing (NLP) algorithm that uses deep learning to learn the relationships between words in a corpus (a large collection of text data). A team of researchers developed it at Google, led by Tomas Mikolov in 2013.\nWord2vec uses a neural network to learn the relationships between words by training on a large corpus of text data. The algorithm uses a shallow neural network with a single hidden layer to predict the surrounding words of a given word based on its context. The network’s hidden layer is trained to capture the relationships between words, and the resulting vectors (or “word embeddings”) can be used to represent the words in the corpus in a lower-dimensional space.\nThe benefits of using word2vec include the ability to capture the relationships between words in a corpus and the ability to handle large amounts of data efficiently. This makes it a useful tool for many NLP tasks, such as text classification, language translation and named entity recognition.\nOverall, word2vec is a powerful and effective algorithm for learning the relationships between words in a corpus and has become a cornerstone of modern NLP techniques.\nprompt &gt; word embedding short description\nWord Embedding\nA word embedding represents a word in a low-dimensional, continuous vector space. This representation captures the semantic relationships between words in a corpus, allowing for more accurate and efficient natural language processing. Word embeddings are typically learned using neural network models, such as the word2vec algorithm. These learned embeddings can then be used in various NLP tasks, such as text classification and language translation.\nSummary of the steps followed in this notebook\n\nDownload hugging face tweets_hate_speech_detection dataset\nPreprocess the data to remove special characters and normalize the text\nCreate tokens from the text and their vocabulary, and finally encode the tokens\nCreate training dataset, model configuration, loss, and optimizer\nTrain the model and extract the embedding layer weights\nAnalyse the word embeddings learned from the tweets"
  },
  {
    "objectID": "posts/2022-12-02-pytorch-word2vec-embedding.html#data-preparation",
    "href": "posts/2022-12-02-pytorch-word2vec-embedding.html#data-preparation",
    "title": "Implementing Word2Vec with PyTorch",
    "section": "Data Preparation",
    "text": "Data Preparation\n\nLoad Hugging Face Dataset\nFor this notebook, I will use Hugging Face Twitter Hate Speech Dataset. It contains training data with approximately 32K tweets divided into two groups: hate speech and not a hate speech. This dataset is originally for classification tasks, but we may use it to learn word embeddings (or word contexts). This approach can also be used to identify inherent biases present in the data.\n\nimport datasets\n\ndataset = datasets.load_dataset(\"tweets_hate_speech_detection\")\n\nWARNING:datasets.builder:Found cached dataset tweets_hate_speech_detection (/root/.cache/huggingface/datasets/tweets_hate_speech_detection/default/0.0.0/c6b6f41e91ac9113e1c032c5ecf7a49b4e1e9dc8699ded3c2d8425c9217568b2)\n\n\n\n\n\n\n# Let's check the downloaded object\ndataset\n\nDatasetDict({\n    train: Dataset({\n        features: ['label', 'tweet'],\n        num_rows: 31962\n    })\n})\n\n\nIt shows that we have only a training set, and each element from the set has a label and tweet text.\n\ntrain_ds = dataset[\"train\"]\ntrain_ds.features\n\n{'label': ClassLabel(names=['no-hate-speech', 'hate-speech'], id=None),\n 'tweet': Value(dtype='string', id=None)}\n\n\nLet’s check what the raw data looks like.\n\n# print a few labels\nprint(\"** labels **\\n\", train_ds[\"label\"][:5])\n\n# print a few tweet texts\nprint(\"\\n** tweets **\")\nfor t in train_ds[\"tweet\"][:5]:\n    print(t)\n\n** labels **\n [0, 0, 0, 0, 0]\n\n** tweets **\n@user when a father is dysfunctional and is so selfish he drags his kids into his dysfunction.   #run\n@user @user thanks for #lyft credit i can't use cause they don't offer wheelchair vans in pdx.    #disapointed #getthanked\nbihday your majesty\n#model   i love u take with u all the time in urð±!!! ððððð¦ð¦ð¦  \nfactsguide: society now    #motivation\n\n\nThis raw data shows that the labels are encoded as 0 and 1 for no-hate and hate speech, respectively. Therefore, we can improve our view by putting labels and tweets in Pandas’s DataFrame and analyzing them side by side.\n\nimport pandas as pd\n\npd.set_option(\"display.max_colwidth\", None)\n\ntrain_ds.set_format(type=\"pandas\")\ndf = train_ds[:]\n\n# a function to convert label codes to string value\ndef label_int2str(row):\n    return train_ds.features[\"label\"].int2str(row)\n\n\ndf[\"label_name\"] = df[\"label\"].apply(label_int2str)\ndf.head(10)\n\n\n    \n      \n\n\n\n\n\n\nlabel\ntweet\nlabel_name\n\n\n\n\n0\n0\n@user when a father is dysfunctional and is so selfish he drags his kids into his dysfunction. #run\nno-hate-speech\n\n\n1\n0\n@user @user thanks for #lyft credit i can't use cause they don't offer wheelchair vans in pdx. #disapointed #getthanked\nno-hate-speech\n\n\n2\n0\nbihday your majesty\nno-hate-speech\n\n\n3\n0\n#model i love u take with u all the time in urðŸ“±!!! ðŸ˜™ðŸ˜ŽðŸ‘„ðŸ‘…ðŸ’¦ðŸ’¦ðŸ’¦\nno-hate-speech\n\n\n4\n0\nfactsguide: society now #motivation\nno-hate-speech\n\n\n5\n0\n[2/2] huge fan fare and big talking before they leave. chaos and pay disputes when they get there. #allshowandnogo\nno-hate-speech\n\n\n6\n0\n@user camping tomorrow @user @user @user @user @user @user @user dannyâ€¦\nno-hate-speech\n\n\n7\n0\nthe next school year is the year for exams.ðŸ˜¯ can't think about that ðŸ˜­ #school #exams #hate #imagine #actorslife #revolutionschool #girl\nno-hate-speech\n\n\n8\n0\nwe won!!! love the land!!! #allin #cavs #champions #cleveland #clevelandcavaliers â€¦\nno-hate-speech\n\n\n9\n0\n@user @user welcome here ! i'm it's so #gr8 !\nno-hate-speech\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n\n\nAll these tweets are labeled as no-hate-speech. Let’s view some of the tweets from the other class.\n\ndf[df[\"label\"]==1].head(10)\n\n\n    \n      \n\n\n\n\n\n\nlabel\ntweet\nlabel_name\n\n\n\n\n13\n1\n@user #cnn calls #michigan middle school 'build the wall' chant '' #tcot\nhate-speech\n\n\n14\n1\nno comment! in #australia #opkillingbay #seashepherd #helpcovedolphins #thecove #helpcovedolphins\nhate-speech\n\n\n17\n1\nretweet if you agree!\nhate-speech\n\n\n23\n1\n@user @user lumpy says i am a . prove it lumpy.\nhate-speech\n\n\n34\n1\nit's unbelievable that in the 21st century we'd need something like this. again. #neverump #xenophobia\nhate-speech\n\n\n56\n1\n@user lets fight against #love #peace\nhate-speech\n\n\n68\n1\nðŸ˜©the white establishment can't have blk folx running around loving themselves and promoting our greatness\nhate-speech\n\n\n77\n1\n@user hey, white people: you can call people 'white' by @user #race #identity #medâ€¦\nhate-speech\n\n\n82\n1\nhow the #altright uses &amp; insecurity to lure men into #whitesupremacy\nhate-speech\n\n\n111\n1\n@user i'm not interested in a #linguistics that doesn't address #race &amp; . racism is about #power. #raciolinguistics bringsâ€¦\nhate-speech\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n\n\n\n# reset dataset to its original format\ntrain_ds.reset_format()\n\n\n\nPreprocess data\nWe can not use raw text directly to train a model because machines understand numbers and not alphabets. But to properly encode our text (convert it into numbers), we need to do some extra steps.\n\ntweets also contain special characters from emoticons or emojis. However, they do not help learn word embeddings. So we need to strip them from the text.\nsplit the tweet text into proper words. Even though we can use all the words from the tweet text, experience has shown that not all are useful in learning embeddings. Words that are commonly omitted are either uncommon or rare words, or stopwords (commonly used words)\nCreate a dictionary or vocabulary to filter words and encode them. This vocabulary helps move between encoded (integer) and character (or string) representations of words.\n\nFor some of these preprocessing tasks, we will use Natural Language Toolkit (NLTK library).\n\nNLTK is a leading platform for building Python programs to work with human language data. It provides easy-to-use interfaces to over 50 corpora and lexical resources such as WordNet, along with a suite of text processing libraries for classification, tokenization, stemming, tagging, parsing, and semantic reasoning, wrappers for industrial-strength NLP libraries.\n\n\nimport nltk\n\nnltk.download(\"stopwords\")  # for filtering common words\nnltk.download(\"wordnet\")  # for lemmatization of words\nnltk.download(\"omw-1.4\")  # required for 'wordnet'\n\n[nltk_data] Downloading package stopwords to /root/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package wordnet to /root/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n[nltk_data]   Package omw-1.4 is already up-to-date!\n\n\nTrue\n\n\nI have created a helper function in the next cell to preprocess and split the tweet text into tokens.\n\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\n\nimport re\n\nsw = stopwords.words(\"english\")\nwl = WordNetLemmatizer()\n\n\ndef split_tokens(row):\n    # step 1: lower the text\n    t = row[\"tweet\"].lower()\n    # step 2: remove all other characters except alphabets, numbers, and a space\n    t = re.sub(r\"[^a-z 0-9]\", \"\", t)\n    # step 3: split the text into words or tokens. split is made at each \"space\" character\n    t = re.split(r\" +\", t)\n    # step 4: remove stop words\n    t = [i for i in t if (i not in sw) and (i not in [\"user\"]) and len(i)]\n    # step 5: lemmatize words\n    t = [wl.lemmatize(i) for i in t]\n\n    row[\"all_tokens\"] = t\n    return row\n\nLet’s use a sample tweet text to uncover the working of this function.\n\nsample_tweet = train_ds[0][\"tweet\"]\nsample_tweet\n\n'@user when a father is dysfunctional and is so selfish he drags his kids into his dysfunction.   #run'\n\n\nStep 1: lower the text\n\nt = sample_tweet.lower()\nt\n\n'@user when a father is dysfunctional and is so selfish he drags his kids into his dysfunction.   #run'\n\n\nStep 2: remove all other characters except alphabets, numbers, and spaces\n\nt = re.sub(r\"[^a-z 0-9]\", \"\",t)\nt\n\n'user when a father is dysfunctional and is so selfish he drags his kids into his dysfunction   run'\n\n\nStep 3: split the text into words or tokens. split is made at each “space” character\n\nt = re.split(r\" +\", t)\nt\n\n['user',\n 'when',\n 'a',\n 'father',\n 'is',\n 'dysfunctional',\n 'and',\n 'is',\n 'so',\n 'selfish',\n 'he',\n 'drags',\n 'his',\n 'kids',\n 'into',\n 'his',\n 'dysfunction',\n 'run']\n\n\nStep 4: remove stop words Besides stop words, I have also filtered “user” from the text. This is because in the original tweet text, any reference to a Twitter user (e.g., @hassaanbinaslam) is replaced with @user to hide identity.\n\nt = [i for i in t if (i not in sw) and (i not in [\"user\"]) and len(i)]\nt\n\n['father', 'dysfunctional', 'selfish', 'drags', 'kids', 'dysfunction', 'run']\n\n\nEnglish language stop words taken from the NLTK library include the following list.\n\n# englist language stopwords from NLTK\nprint(sw)\n\n['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n\n\nStep 5: lemmatize words What is lemmatization anyway? Lemmatization is applied to normalize the text. It considers the context and converts the word to its meaningful base form, which is called Lemma. to read more about it, refer to what-is-the-difference-between-lemmatization-vs-stemming\n\nt = [wl.lemmatize(i) for i in t]\nt\n\n['father', 'dysfunctional', 'selfish', 'drag', 'kid', 'dysfunction', 'run']\n\n\nLet’s use the helper function split_tokens and compare our output.\n\nsplit_tokens(train_ds[0])\n\n{'label': 0,\n 'tweet': '@user when a father is dysfunctional and is so selfish he drags his kids into his dysfunction.   #run',\n 'all_tokens': ['father',\n  'dysfunctional',\n  'selfish',\n  'drag',\n  'kid',\n  'dysfunction',\n  'run']}\n\n\nOur preprocessing steps and tokenize function is ready. So let’s apply it to our entire tweet dataset.\n\n# tokenize tweet dataset\ntrain_ds_all_tokens = train_ds.map(split_tokens)\ntrain_ds_all_tokens\n\nWARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/tweets_hate_speech_detection/default/0.0.0/c6b6f41e91ac9113e1c032c5ecf7a49b4e1e9dc8699ded3c2d8425c9217568b2/cache-4f6808d01b1a8972.arrow\n\n\nDataset({\n    features: ['label', 'tweet', 'all_tokens'],\n    num_rows: 31962\n})\n\n\n\n\nCreate vocabulary\nA vocabulary is a dictionary where each key represents the token (word ) string form. The value obtained from the dictionary defines the token’s unique integer or encoded form. To create our vocabulary, we will follow the following approach.\n\nIterate through all the tweets (entire dataset) and count the frequency of all the tokens.\nRemove the rare or uncommon tokens. This helps in reducing the size of the vocabulary. However, these uncommon tokens also include words created with typos e.g., while tweeting someone incorrectly typed fahtr instead of father. In our case, we will remove all tokens with a frequency of less than 10.\nAfter that, we will put our filtered tokens in an ordered dictionary\nPass the ordered dictionary to Pytorch Vocab class. A vocab object automatically maps tokens to indices.\n\n\n# create a frequency map for all toekns\nfrom collections import Counter\n\ntoken_count = Counter()\nfor row_tokens in train_ds_all_tokens[\"all_tokens\"]:\n    token_count.update(row_tokens)\n\nprint(\"Number of tokens found: \", len(token_count))\n\nNumber of tokens found:  38988\n\n\n\n# remove uncommon tokens\nmin_token_freq = 10\ntoken_count_filtered = {k: v for k, v in token_count.items() if v &gt; min_token_freq}\n\nprint(\"Number of tokens after filtering: \", len(token_count_filtered))\n\nNumber of tokens after filtering:  3007\n\n\nNotice that by removing uncommon tokens, we have significantly reduced the size of our vocabulary (almost 13x less). In the next step, we will sort them, convert them to OrdreredDict, and pass them to torchtext.vocab.\n\nfrom torchtext.vocab import vocab\nfrom collections import OrderedDict\n\n# sort the tokens based on their frequency\nsorted_by_freq_tuples = sorted(\n    token_count_filtered.items(), key=lambda x: x[1], reverse=True\n)\n# create a dictionary of tokens\nordered_dict = OrderedDict(sorted_by_freq_tuples)\n# convert the dictionary into a vocabulary\nvb = vocab(ordered_dict)\n\nUsing the following methods, we can use the vocabulary to move between the token’s integer and string form.\n\nvb['love'] to get the id ‘0’ for token ‘love’\nvb.get_stoi()['love'] to get id ‘0’ for token ‘love’\nvb.lookup_token('love') to get id ‘0’ for token ‘love’\nvb.lookup_tokens(['love', 'happy']) to get id ‘0’ and ‘2’ for token ‘love’ and ‘happy’\nvb.get_itos()[0] to get token ‘love’ from id ‘0’\nvb.lookup_indices([0, 2]) to get token ‘love’ and ‘happy’ from id ‘0’ and ‘2’\n\n\nvb[\"love\"], vb.get_stoi()[\"love\"], vb.get_itos()[0]\n\n(0, 0, 'love')\n\n\n\n# let's check a few more tokens\nfor i in range(5):\n    token = vb.get_itos()[i]\n    print(f\"{i} ---&gt; {token}\")\n    print(f\"{token} ---&gt; {vb.get_stoi()[token]}\")\n    print()\n\n0 ---&gt; love\nlove ---&gt; 0\n\n1 ---&gt; day\nday ---&gt; 1\n\n2 ---&gt; happy\nhappy ---&gt; 2\n\n3 ---&gt; u\nu ---&gt; 3\n\n4 ---&gt; amp\namp ---&gt; 4\n\n\n\nAlright, we have our vocabulary ready. Remember that we have filtered our vocabulary to remove uncommon tokens. Let’s use this vocabulary to do the same for our tweet text tokens.\n\n# use vocabulary to filter uncommon tokens from tweets\ndef remove_rare_tokens(row):\n    row[\"tokens\"] = [t for t in row[\"all_tokens\"] if t in vb]\n    return row\n\n\ntrain_ds_tokens = train_ds_all_tokens.map(remove_rare_tokens)\n\nWARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/tweets_hate_speech_detection/default/0.0.0/c6b6f41e91ac9113e1c032c5ecf7a49b4e1e9dc8699ded3c2d8425c9217568b2/cache-84d95f24a0e6d681.arrow\n\n\nIn the following cell output, note that only three tokens are left for a sample tweet after removing uncommon tokens.\n\n# verify the dataset after filtering\ntrain_ds_tokens[0]\n\n{'label': 0,\n 'tweet': '@user when a father is dysfunctional and is so selfish he drags his kids into his dysfunction.   #run',\n 'all_tokens': ['father',\n  'dysfunctional',\n  'selfish',\n  'drag',\n  'kid',\n  'dysfunction',\n  'run'],\n 'tokens': ['father', 'kid', 'run']}\n\n\n\n\nCreate training samples using skip-gram\nThe next step is to create training samples using a ” skip-gram ” technique. To understand it, let’s take an example of a sample tweet text: i get to see my daddy today!. After applying preprocessing steps, the word tokens produced from the tweet are: ['get', 'see', 'daddy', 'today']. The vocabulary indices (integer value) for these tokens are provided below.\n\n\n\nTokens\nIndices\n\n\n\n\nget\n10\n\n\nsee\n22\n\n\ndaddy\n404\n\n\ntoday\n9\n\n\n\nFor creating training samples using skip-gram, we take each token as an input and a surrounding token as the label. But we also need to decide the window size, meaning how many surrounding tokens to create training samples. Suppose we take window_size=2; then the training sample for this tweet will be.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntext\n\n\n\n\ninput\noutput\n\ninput\noutput\n\n\n\n\nget\nsee\ndaddy\ntoday\n—&gt;\nget\nsee\n—&gt;\n10\n22\n\n\n\n\n\n\n\nget\ndaddy\n\n10\n404\n\n\nget\nsee\ndaddy\ntoday\n—&gt;\nsee\nget\n—&gt;\n22\n10\n\n\n\n\n\n\n\nsee\ndaddy\n\n22\n404\n\n\n\n\n\n\n\nsee\ntoday\n\n22\n9\n\n\nget\nsee\ndaddy\ntoday\n—&gt;\ndaddy\nget\n—&gt;\n404\n10\n\n\n\n\n\n\n\ndaddy\nsee\n\n404\n22\n\n\n\n\n\n\n\ndaddy\ntoday\n\n404\n9\n\n\nget\nsee\ndaddy\ntoday\n—&gt;\ntoday\nsee\n—&gt;\n9\n22\n\n\n\n\n\n\n\ntoday\ndaddy\n\n9\n404\n\n\n\nIn the above table, we have generated ten training samples from 4 tokens with a window size of 2. In each sample, we have two tokens\n\ninput or the training data\noutput or the label value\n\nGiven the tokens and window size, let’s create a function to create training samples for us.\n\n# code source: https://muhark.github.io/python/ml/nlp/2021/10/21/word2vec-from-scratch.html\ndef windowizer(row, wsize=3):\n    # default 'window size' (wsize) is three\n    doc = row[\"tokens\"]\n    wsize = wsize\n    out = []\n    for i, wd in enumerate(doc):\n        target = vb[wd]\n        window = [\n            i + j\n            for j in range(-wsize, wsize + 1, 1)\n            if (i + j &gt;= 0) & (i + j &lt; len(doc)) & (j != 0)\n        ]\n\n        out += [(target, vb[doc[w]]) for w in window]\n    row[\"moving_window\"] = out\n    return row\n\n\ntrain_ds_tokens = train_ds_tokens.map(windowizer)\n\nWARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/tweets_hate_speech_detection/default/0.0.0/c6b6f41e91ac9113e1c032c5ecf7a49b4e1e9dc8699ded3c2d8425c9217568b2/cache-e1806b88a0961df3.arrow\n\n\nLet’s check a sample tweet with generated training samples.\n\n# note that we have used skip-gram window size=3\ntrain_ds_tokens[12]\n\n{'label': 0,\n 'tweet': 'i get to see my daddy today!!   #80days #gettingfed',\n 'all_tokens': ['get', 'see', 'daddy', 'today', '80days', 'gettingfed'],\n 'tokens': ['get', 'see', 'daddy', 'today'],\n 'moving_window': [[10, 22],\n  [10, 404],\n  [10, 9],\n  [22, 10],\n  [22, 404],\n  [22, 9],\n  [404, 10],\n  [404, 22],\n  [404, 9],\n  [9, 10],\n  [9, 22],\n  [9, 404]]}\n\n\n\n\nCreate dataset and dataloader\nThe preprocessing part of the data is complete. Now we only need to load this data into the Pytorch Dataset class and create batches for model training using DataLoader class. Both are utilities or helper classes provided by PyTorch to make our dataset (data preparation) code decoupled from our model training code for better readability and modularity.\n\ntorch.utils.data.Dataset stores the samples and their corresponding labels\ntorch.utils.data.DataLoader wraps an iterable around the Dataset to enable easy access to the samples.\n\n\n# source: https://muhark.github.io/python/ml/nlp/2021/10/21/word2vec-from-scratch.html\n# Helper class to make our data compatible with PyTorch Dataset\nfrom torch.utils.data import Dataset\n\nclass Word2VecDataset(Dataset):\n    def __init__(self, dataset, vocab_size):\n        self.dataset = dataset\n        self.vocab_size = vocab_size\n        self.data = [i for s in dataset[\"moving_window\"] for i in s]\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        return self.data[idx]\n\n\nvb_size = len(vb)\n\nword2vec_ds = Word2VecDataset(train_ds_tokens, vocab_size=vb_size)\n\nprint(\n    f\"a word2vec_ds entry: {word2vec_ds[0]}\",\n    f\"\\nnumber of word2vec_ds entries: {len(word2vec_ds)}\",\n)\n\na word2vec_ds entry: [13, 123] \nnumber of word2vec_ds entries: 761044\n\n\nword2vec_ds is our training dataset, and a single entry from it is of shape (input, label). Notice that there are many entries (or training samples) in word2vec_ds, and these samples represent all of the tweets tokens. For training, we need to create batches from them for faster processing. So let’s do that next.\n\nfrom torch.utils.data import DataLoader\n\nBATCH_SIZE = 2 ** 14\nN_LOADER_PROCS = 10\n\ndataloader = DataLoader(\n    word2vec_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=N_LOADER_PROCS\n)\n\nprint(f\"number of training batches: {len(dataloader)}\")\n\nnumber of training batches: 47\n\n\n/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:563: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n\n\n\ntmp = next(iter(dataloader))\ntmp\n\n[tensor([  46, 1743,    0,  ...,   28,    8,  122]),\n tensor([  15, 2197,  197,  ...,  179,  846,    0])]\n\n\nIn the last cell, I fetched a single batch from the dataloader. Its output is two long tensors.\n\nThe first tensor has all the entries for the input data\nThe second tensor has all the entries for the labels\n\nThe size of both these tensors is the same."
  },
  {
    "objectID": "posts/2022-12-02-pytorch-word2vec-embedding.html#model-configuration",
    "href": "posts/2022-12-02-pytorch-word2vec-embedding.html#model-configuration",
    "title": "Implementing Word2Vec with PyTorch",
    "section": "Model Configuration",
    "text": "Model Configuration\nNow we will configure the model to be used for training.\nWhat are we trying to solve with our model?\nWe are trying to train a model that can predict the surrounding words of a given input. And this is how we have designed our training data too. Each training sample has (input, label). Where input is the given token, and label is some nearby token. It is like forcing the model to predict the context of an input word.\nHow can such a model be helpful to us?\nSuch a model that can predict the context of a word is not helpful to us in any actual scenario. So we are not going to use it. Instead, we are only interested in the learned weights of such a model, and we call them word embedding. It is like faking a problem (creating a pseudo-problem), training a model, and then using the learned weights for other tasks.\n\n# source: https://muhark.github.io/python/ml/nlp/2021/10/21/word2vec-from-scratch.html\nimport torch.nn as nn\n\nclass Word2Vec(nn.Module):\n    def __init__(self, vocab_size, embedding_size):\n        super().__init__()\n        self.embed = nn.Embedding(vocab_size, embedding_size)\n        self.expand = nn.Linear(embedding_size, vocab_size, bias=False)\n\n    def forward(self, input):\n        # Encode input to lower-dimensional representation\n        hidden = self.embed(input)\n        # Expand hidden layer to predictions\n        logits = self.expand(hidden)\n        return logits\n\nNotice that in the model configuration, the size of the output layer (nn.Linear) is equal to our vocabulary size. So, our model is configured for a multi-classification problem.\n\nEMBED_SIZE = 100\nmodel = Word2Vec(vb_size, EMBED_SIZE)\n\n# Relevant if you have a GPU\ndevice = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\nprint(device)\n\nmodel.to(device)\n\ncuda\n\n\nWord2Vec(\n  (embed): Embedding(3007, 100)\n  (expand): Linear(in_features=100, out_features=3007, bias=False)\n)\n\n\nNow configure the loss function and a training optimizer.\n\nimport torch\n\nLR = 3e-4  # learning rate\nloss_fn = nn.CrossEntropyLoss()\noptimizer = torch.optim.AdamW(model.parameters(), lr=LR)"
  },
  {
    "objectID": "posts/2022-12-02-pytorch-word2vec-embedding.html#model-training",
    "href": "posts/2022-12-02-pytorch-word2vec-embedding.html#model-training",
    "title": "Implementing Word2Vec with PyTorch",
    "section": "Model Training",
    "text": "Model Training\nIn this section we are going to train our model for 100 epochs.\n\n# source: https://muhark.github.io/python/ml/nlp/2021/10/21/word2vec-from-scratch.html\nfrom tqdm import tqdm  # For progress bar. https://github.com/tqdm/tqdm\n\nEPOCHS = 100\nprogress_bar = tqdm(range(EPOCHS * len(dataloader)))\nrunning_loss = []\n\nfor epoch in range(EPOCHS):\n    epoch_loss = 0\n    for center, context in dataloader:\n        center, context = center.to(device), context.to(device)\n\n        optimizer.zero_grad()\n        logits = model(input=context)\n        loss = loss_fn(logits, center)\n        epoch_loss += loss.item()\n        loss.backward()\n        optimizer.step()\n\n        progress_bar.update(1)\n    epoch_loss /= len(dataloader)\n    running_loss.append(epoch_loss)\n\n100%|█████████▉| 4699/4700 [05:59&lt;00:00, 40.21it/s]\n\n\nLet’s plot the training loss.\n\nimport matplotlib.pyplot as plt\nplt.plot(running_loss)"
  },
  {
    "objectID": "posts/2022-12-02-pytorch-word2vec-embedding.html#analyze-word-embeddings-word-vectors",
    "href": "posts/2022-12-02-pytorch-word2vec-embedding.html#analyze-word-embeddings-word-vectors",
    "title": "Implementing Word2Vec with PyTorch",
    "section": "Analyze Word Embeddings (Word Vectors)",
    "text": "Analyze Word Embeddings (Word Vectors)\nTraining is complete, and our model has learned something. In this section, we will analyze the learned weights and their quality.\nSo, let’s extract the weights.\nWe have weights in two layers: embedding and linear. Both have similar dimensions, so which should we use as embeddings? I have experimented with weights from both these layers, and here are some suggestions.\n\nEmbeddings from the layer closer to the output layer give better results. In our case, it is the Linear layer. Our linear layer has the same dimensions as the embedding layer, and we will use weights from this layer.\nIf the dimensions of the layers closer to the output layer are different than the Embedding layer, then we should stick to the embedding layer. Sometimes, people prefer to concatenate weights from multiple layers to create the final output.\n\n\n# embedding = model.embed.weight.cpu().detach().numpy()\nembedding = model.expand.weight.cpu().detach().numpy()\nembedding.shape\n\n(3007, 100)\n\n\nNext, I have created some functions to calculate the distance between these word vectors. We are doing this to find a word similarity score with other words (cosine similarity). A smaller distance between two vectors means that these words are often used in similar contexts.\n\n# source: https://muhark.github.io/python/ml/nlp/2021/10/21/word2vec-from-scratch.html\nfrom scipy.spatial import distance\nimport numpy as np\n\ndef get_distance_matrix(embedding, metric):\n    dist_matrix = distance.squareform(distance.pdist(embedding, metric))\n    return dist_matrix\n\n\ndef get_k_similar_words(word, dist_matrix, k=10):\n    idx = vb[word]\n    dists = dist_matrix[idx]\n    ind = np.argpartition(dists, k)[: k + 1]\n    ind = ind[np.argsort(dists[ind])][1:]\n    out = [(i, vb.lookup_token(i), dists[i]) for i in ind]\n    return out\n\n\n# calculate 2d distance matrix\ndmat = get_distance_matrix(embedding, \"cosine\")\ndmat.shape\n\n(3007, 3007)\n\n\nAnother helper function is to print similar words identified based on their distance.\n\ndef similar_words(tokens):\n    for word in tokens:\n        print(word, [t[1] for t in get_k_similar_words(word, dmat)], \"\\n\")\n\nNext, I selected some tokens and used embeddings to find words most similar to them.\n\ntokens = [\"father\", \"mother\", \"boy\", \"girl\"]  # tokens for relations\ntokens += [\"job\", \"sad\", \"happy\", \"hate\"]  # for emotions\ntokens += [\"america\", \"england\", \"india\"]  # for regions\ntokens += [\"football\", \"swimming\", \"cycling\"]  # for sports\ntokens += [\"exercise\", \"health\", \"fitness\"]  # for health\n\nsimilar_words(tokens)\n\nfather ['dad', 'fathersday', 'day', 'happy', 'love', 'one', 'bihday', 'kid', 'u', 'today'] \n\nmother ['care', 'child', 'son', 'died', 'father', 'kill', 'men', 'ten', 'born', 'wife'] \n\nboy ['girl', 'guy', 'man', 'smile', 'little', 'family', 'love', 'selfie', 'day', 'face'] \n\ngirl ['smile', 'summer', 'love', 'guy', 'happy', 'fun', 'friend', 'boy', 'beautiful', 'today'] \n\njob ['one', 'amp', 'im', 'getting', 'got', 'going', 'first', 'even', 'get', 'cant'] \n\nsad ['make', 'people', 'life', 'know', 'dont', 'help', 'world', 'amp', 'like', 'say'] \n\nhappy ['day', 'today', 'love', 'great', 'life', 'make', 'amp', 'weekend', 'smile', 'one'] \n\nhate ['people', 'america', 'say', 'dont', 'even', 'many', 'still', 'world', 'much', 'amp'] \n\namerica ['hate', 'orlando', 'many', 'trump', 'people', 'say', 'even', 'american', 'still', 'shooting'] \n\nengland ['eng', 'football', 'euro2016', 'v', 'wale', 'soccer', 'match', 'russia', 'player', 'clinton'] \n\nindia ['received', 'test', 'sign', 'local', 'em', 'rude', 'amount', 'forget', 'called', 'shocking'] \n\nfootball ['england', 'euro2016', 'france', 'fan', 'v', 'making', 'review', 'match', 'game', 'award'] \n\nswimming ['swim', 'bbq', 'sunglass', 'ink', 'lovemylife', 'ceremony', 'placement', 'follow4follow', 'brunette', 'pool'] \n\ncycling ['pub', 'nutrition', 'musictherapy', 'exploring', 'letsgo', 'niece', 'cook', 'taste', 'pougal', 'mount'] \n\nexercise ['wellness', 'weightloss', 'lifeisgood', 'fitfam', 'inspire', 'madrid', 'namaste', 'runner', 'yoga', 'workout'] \n\nhealth ['healthy', 'food', 'happiness', 'amazing', 'fitness', 'beauty', 'gym', 'positive', 'run', 'lifestyle'] \n\nfitness ['workout', 'gym', 'fit', 'food', 'running', 'run', 'yoga', 'health', 'selfie', 'monday'] \n\n\n\n\nVisualize embeddings\nWe can also analyze the embeddings by visualizing them on a plot. Similar word vectors should appear closer in the plot. But each word vector has 300 dimensions, so how can we plot them on a 2D surface? For this, we can take the help of the dimension reduction technique t-SNE to reduce the word vectors dimensions from 300 to 2. PCA can also be applied to achieve similar results.\nWe need first to normalize the weights to get better results.\n\n# source: https://github.com/OlgaChernytska/word2vec-pytorch/blob/main/notebooks/Inference.ipynb\n# normalization of weights\nnorms = (embedding ** 2).sum(axis=1) ** (1 / 2)\nnorms = np.reshape(norms, (len(norms), 1))\nembedding_norm = embedding / norms\n\nembedding_norm.shape\n\n(3007, 100)\n\n\nNow apply t-distributed Stochastic Neighbor Embedding (t-SNE) to reduce the dimensions of our embedding.\n\n# source: https://github.com/OlgaChernytska/word2vec-pytorch/blob/main/notebooks/Inference.ipynb\nfrom sklearn.manifold import TSNE\n\n# embeddings DataFrame\nembedding_df = pd.DataFrame(embedding)\n\n# t-SNE transform\ntsne = TSNE(n_components=2)\nembedding_df_trans = tsne.fit_transform(embedding_df)\nembedding_df_trans = pd.DataFrame(embedding_df_trans)\n\n/usr/local/lib/python3.8/dist-packages/sklearn/manifold/_t_sne.py:780: FutureWarning: The default initialization in TSNE will change from 'random' to 'pca' in 1.2.\n  warnings.warn(\n/usr/local/lib/python3.8/dist-packages/sklearn/manifold/_t_sne.py:790: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n  warnings.warn(\n100%|██████████| 4700/4700 [06:10&lt;00:00, 40.21it/s]\n\n\nWe have some tokens selected in the last section. Let’s highlight them in the plot to locate them easily. For this, I have created a color coding for their indices.\n\n# get token order\nembedding_df_trans.index = vb.get_itos()\n\n# create color codes for selected tokens\ncolor_codes = []\nfor s in embedding_df_trans.index:\n    if s in tokens:\n        color_codes.append(True)\n    else:\n        color_codes.append(False)\n\nNow create a scatter plot of these embeddings using Plotly.\n\n# source: https://github.com/OlgaChernytska/word2vec-pytorch/blob/main/notebooks/Inference.ipynb\nimport plotly.graph_objects as go\n\ncolor = np.where(color_codes, \"red\", \"grey\")\nfig = go.Figure()\n\nfig.add_trace(\n    go.Scatter(\n        x=embedding_df_trans[0],\n        y=embedding_df_trans[1],\n        mode=\"text\",\n        text=embedding_df_trans.index,\n        textposition=\"middle center\",\n        textfont=dict(color=color),\n    )\n)\n\n\nI am zooming in on some of the highlighted tokens at the top right corner.\n\nFurther zooming."
  },
  {
    "objectID": "posts/2023-03-01-glue-partition-indexes.html",
    "href": "posts/2023-03-01-glue-partition-indexes.html",
    "title": "Working with partition indexes in AWS Glue",
    "section": "",
    "text": "image source: krea.ai/prompt/c4ddd913-2d07-4b5e-a034-9110654019b8"
  },
  {
    "objectID": "posts/2023-03-01-glue-partition-indexes.html#credits",
    "href": "posts/2023-03-01-glue-partition-indexes.html#credits",
    "title": "Working with partition indexes in AWS Glue",
    "section": "Credits",
    "text": "Credits\nThis post is adapted from AWS Glue Immersions Day Workshop. For a complete list of all the hands-on labs covered during the workshop, visit the official workshop page. In addition, I have shared here helpful links related to the topic.\n\nAWS Glue Immersions Day Workshop\nAWS blog post on the same topic. Improve query performance using AWS Glue partition indexes\nAnother AWS post focused on Athena query performance. Improve Amazon Athena query performance using AWS Glue Data Catalog partition indexes\nAthena documentation page on creating partitions and indexes on tables. Partitioning data in Athena\nGlue documentation page on partition indexes. Working with partition indexes in AWS Glue\n\n\nIn this post, I have explained how to create partitions and indexes on data lake tables. Then we will test the effect of partition indexes on query performance from the Glue ETL job using Apache Spark SQL.\nBelow is a summary of the steps followed in this post.\n\nCreate AWS access keys for an IAM user. We will use these keys to access AWS resources from the AWS Cloud9 environment\nCreate AWS Cloud9 environment. We will use this environment to perform all the steps mentioned in this post\nCreate an S3 bucket and clone some sample data\nDefine required policies and roles for Glue crawlers\nDefine the Glue database and crawlers, and then prepare the data catalog by running crawlers on an S3 bucket\nCreate partition indexes on tables. This step is where we apply partition indexes to the data catalog tables. All the steps before this are pre-requisites to creating a proper environment where we can test the working and performance of partition indexes\nCompare query performance on tables with partition indexes and without it"
  },
  {
    "objectID": "posts/2023-03-01-glue-partition-indexes.html#step-1-create-aws-access-keys-for-an-iam-user",
    "href": "posts/2023-03-01-glue-partition-indexes.html#step-1-create-aws-access-keys-for-an-iam-user",
    "title": "Working with partition indexes in AWS Glue",
    "section": "Step 1: Create AWS access keys for an IAM user",
    "text": "Step 1: Create AWS access keys for an IAM user\nThe first step is to create an AWS access key, and we will use it in the Cloud9 environment (created in the next step) to access other AWS resources. If you already have a working key, you may use it; otherwise, you can create it by following the step provided in the AWS docs Managing access keys (console)"
  },
  {
    "objectID": "posts/2023-03-01-glue-partition-indexes.html#step-2-create-aws-cloud9-environment-and-configure-aws-cli-credentials",
    "href": "posts/2023-03-01-glue-partition-indexes.html#step-2-create-aws-cloud9-environment-and-configure-aws-cli-credentials",
    "title": "Working with partition indexes in AWS Glue",
    "section": "Step 2: Create AWS Cloud9 environment and configure AWS CLI credentials",
    "text": "Step 2: Create AWS Cloud9 environment and configure AWS CLI credentials\nIn this step, we will create AWS Cloud9 environment. Cloud9 is a cloud-based integrated development environment (IDE) that lets you write, run, and debug your code with just a browser.\n\n\n\n\n\n\nTip\n\n\n\n\n\nI will be working in us-east-1 (US East - N. Virginia), but you may follow these steps in your region of choice.\n\n\n\n\nGo to cloud9 console https://console.aws.amazon.com/cloud9/, select your region, and click create environment\nFrom the Create Environment settings page\n\nGive a name to the environment e.g. glue-cloud9\nKeep the rest of the setting as default and click create\nBy default, it uses a t2-micro EC2 instance for the environment, which is free tier eligible and has enough processing power to complete all the steps in this post\nYou may take help from the AWS docs for detailed instructions on creating this environment. Create an EC2 environment with the console\n\nOpen the Cloud9 IDE once it is ready\n\n\n\n\nopen-cloud9-environment.png\n\n\n\n\nOnce the IDE loads, follow these steps from inside Cloud9 IDE.\n\nDisable the AWS managed temporary credentials\n\nBy default, the EC2 machine provisioned for the cloud9 environment has a role assigned that can be used to get temporary credentials to access other AWS resources. But this access is limited and can only be used to perform some of the tasks in this post. Therefore, we will disable these temporary credentials and use IAM access keys (created in the previous step) to access other AWS resources\n\nClicking the Preferences cog button on the top right corner\nDisable Preferences &gt; AWS Settings &gt; AWS managed temporary credentials\n\n\n\n\ncloud9-disable-temp-cred.png\n\n\n\n\n\nUse the bash terminal to configure the AWS CLI using the command aws configure. For more detailed instructions on configuring AWS CLI, use the AWS docs AWS CLI Configuration basics\n\n\n\n\nconfigure-aws-cli.png\n\n\n\n\n\n\n\n\n\n\nCaution\n\n\n\n\n\nNever expose your AWS access keys, never! Even though I masked the key in the above image, as a precaution, I deleted it once I was done with it."
  },
  {
    "objectID": "posts/2023-03-01-glue-partition-indexes.html#step-3-create-s3-bucket-and-clone-sample-data",
    "href": "posts/2023-03-01-glue-partition-indexes.html#step-3-create-s3-bucket-and-clone-sample-data",
    "title": "Working with partition indexes in AWS Glue",
    "section": "Step 3: Create S3 bucket and clone sample data",
    "text": "Step 3: Create S3 bucket and clone sample data\nIn this step, we will create an AWS S3 bucket and then populate some data. Later we will create a table on it and use it in a query. We will first create a unique name for a bucket using aws account-id and region. For this, paste the following commands in the bash terminal.\n\n\n\n\n\n\nNote\n\n\n\n\n\nS3 bucket names are globally unique. If I have created a bucket with the name ‘s3://amazing-bucket’, no other bucket can have that name again till I delete my bucket.\n\n\n\nAWS_ACCOUNT_ID=`aws sts get-caller-identity --query Account --output text`\nAWS_REGION=`aws configure get region`\nBUCKET_NAME=glueworkshop-${AWS_ACCOUNT_ID}-${AWS_REGION}\necho ${BUCKET_NAME}\nNow create a bucket using s3 make bucket (mb) command.\naws s3 mb s3://${BUCKET_NAME}\n\n\n\n\nmake-bucket-command.png\n\n\n\nOnce our bucket is ready, we can clone sample data using the following three commands. Each command will take about a minute to complete. These commands will synchronize the contents of our bucket with another bucket (containing the sample data). To read more about s3 sync command use the AWS docs s3 sync objects\naws s3 sync s3://redshift-immersionday-labs/data/clickstream/uservisits_parquet1/customer=1/ s3://${BUCKET_NAME}/input/syncstreamingdata/customer=1/ --exclude \"*\" --include \"visitYearMonth=1992*\"\naws s3 sync s3://redshift-immersionday-labs/data/clickstream/uservisits_parquet1/customer=2/ s3://${BUCKET_NAME}/input/syncstreamingdata/customer=2/ --exclude \"*\" --include \"visitYearMonth=1992*\"\naws s3 sync s3://redshift-immersionday-labs/data/clickstream/uservisits_parquet1/customer=3/ s3://${BUCKET_NAME}/input/syncstreamingdata/customer=3/ --exclude \"*\" --include \"visitYearMonth=1992*\"\n\n\n\n\ns3-sync-command.png\n\n\n\nThe data is copied into the bucket under the prefix /input/syncstreamingdata/ with partitions on\n\n/customer=x\n/visitYearMonth=xxxxxx\n\nYou can view all the partitions (and objects under them) with the s3 list command, and each partition has one parquet file under it.\naws s3 ls s3://${BUCKET_NAME}/input/syncstreamingdata/ --recursive\n\n\n\n\ns3-partitions.png\n\n\n\n\n\n\n\n\n\nTip\n\n\n\n\n\nNote that S3 bucket data partitions are only logical in the sense that.\n\nIt does not change the way the objects are stored in S3\nIt is only a data organization scheme where we change a part of the objects’ S3 URI (or prefix) while keeping the rest the same. For example, in the following example, we have used a scheme where we are changing the s3 objects prefix after the customer= and visitYearMonth= words. When using such a prefix scheme, we call them data partitions and can be compared to folders in a hierarchical file system.\n\ns3://bucket/input/syncstreamingdata/customer=1/visitYearMonth=199201/302125b7984f.parquet\ns3://bucket/input/syncstreamingdata/customer=1/visitYearMonth=199202/302125b7984f.parquet\ns3://bucket/input/syncstreamingdata/customer=1/visitYearMonth=199203/302125b7984f.parquet\ns3://bucket/input/syncstreamingdata/customer=2/visitYearMonth=199201/302125b7984f.parquet\ns3://bucket/input/syncstreamingdata/customer=2/visitYearMonth=199202/302125b7984f.parquet\ns3://bucket/input/syncstreamingdata/customer=2/visitYearMonth=199203/302125b7984f.parquet"
  },
  {
    "objectID": "posts/2023-03-01-glue-partition-indexes.html#step-4-create-iam-policies-and-roles-for-glue-crawlers",
    "href": "posts/2023-03-01-glue-partition-indexes.html#step-4-create-iam-policies-and-roles-for-glue-crawlers",
    "title": "Working with partition indexes in AWS Glue",
    "section": "Step 4: Create IAM policies and roles for Glue crawlers",
    "text": "Step 4: Create IAM policies and roles for Glue crawlers\nIn this step, we will create some IAM roles and IAM policies that Glue crawlers will use in the next step.\n\nUsing cloud9 environment, create a file with name iam-policy.json with the following contents.\n\n{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Sid\": \"passRoleGlue\",\n            \"Effect\": \"Allow\",\n            \"Action\": \"iam:PassRole\",\n            \"Resource\": \"*\"\n        }\n    ]\n}\nUsing this file, we will now create an IAM policy named passrole-glue-policy. Later we will assign this policy to a role. This policy allows an AWS user or service to transfer its role to other services. To read more about this policy, use the AWS docs page Granting a user permissions to pass a role to an AWS service. To create the policy use command\naws iam create-policy --policy-name passrole-glue-policy --policy-document file://iam-policy.json\n\n\n\n\niam-policy.png\n\n\n\nCopy the Arn of the policy created. We will require it in the following steps.\nNext, we will create an IAM role and assign it some policies. For this, create a file named iam-assume-policy.json and put the following contents.\n{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n        \"Effect\": \"Allow\",\n        \"Principal\": {\n        \"Service\": \"glue.amazonaws.com\"\n        },\n        \"Action\": \"sts:AssumeRole\"\n        }\n    ]\n}\nNow create the role AWSGlueServiceRole-glueworkshop and assign required AWS managed policies policies to it with the following commands.\nGLUE_ROLENAME='AWSGlueServiceRole-glueworkshop'\naws iam create-role --role-name ${GLUE_ROLENAME} --assume-role-policy-document file://iam-assume-policy.json\naws iam attach-role-policy --role-name ${GLUE_ROLENAME} --policy-arn arn:aws:iam::aws:policy/AmazonS3FullAccess\naws iam attach-role-policy --role-name ${GLUE_ROLENAME} --policy-arn arn:aws:iam::aws:policy/service-role/AWSGlueServiceRole\nNext, we will assign the policy we created in the last step to this role.\n\n\n\n\n\n\nNote\n\n\n\n\n\nIn the below command, replace the arn with the value you saved in the last step.\n\n\n\naws iam attach-role-policy --role-name ${GLUE_ROLENAME} --policy-arn arn:aws:iam::801598032724:policy/passrole-glue-policy\n\n\n\n\niam-role.png"
  },
  {
    "objectID": "posts/2023-03-01-glue-partition-indexes.html#step-5-define-glue-database-and-crawlers",
    "href": "posts/2023-03-01-glue-partition-indexes.html#step-5-define-glue-database-and-crawlers",
    "title": "Working with partition indexes in AWS Glue",
    "section": "Step 5: Define Glue database and crawlers",
    "text": "Step 5: Define Glue database and crawlers\nCreate AWS Glue database with the name partition_index_glueworkshop by the command\naws glue create-database --database-input \"{\\\"Name\\\":\\\"partition_index_glueworkshop\\\", \\\"Description\\\":\\\"This database is created using AWS CLI for partition index\\\"}\"\nYou can verify that the database has been created with the command.\naws glue get-database --name partition_index_glueworkshop\n\n\n\n\nglue-database.png\n\n\n\nNext, we will create two crawlers. These crawlers will crawl the S3 bucket and create tables from the identified objects. Both crawlers will be same except that the tables created by one crawler will be given the prefix without_index_ and the other given with_index_. To create them use the commands below.\nCrawler 1: crawler-without-partition-index\naws glue create-crawler \\\n--name crawler-without-partition-index \\\n--role AWSGlueServiceRole-glueworkshop \\\n--database-name partition_index_glueworkshop \\\n--table-prefix without_index_ \\\n--targets \"{\\\"S3Targets\\\": [{\\\"Path\\\": \\\"s3://${BUCKET_NAME}/input/syncstreamingdata/\\\"} ]}\"\nCrawler 2: crawler-with-partition-index\naws glue create-crawler \\\n--name crawler-with-partition-index \\\n--role AWSGlueServiceRole-glueworkshop \\\n--database-name partition_index_glueworkshop \\\n--table-prefix with_index_ \\\n--targets \"{\\\"S3Targets\\\": [{\\\"Path\\\": \\\"s3://${BUCKET_NAME}/input/syncstreamingdata/\\\"} ]}\"\n\n\n\n\ncreate-glue-crawlers.png\n\n\n\nWe can verify that both crawlers are created and ready to start from the AWS Glue console.\n\n\n\n\ncrawlers-console.png\n\n\n\nNow we will start both these crawlers.\naws glue start-crawler --name crawler-without-partition-index\naws glue start-crawler --name crawler-with-partition-index\n\n\n\n\nglue-start-crawlers.png\n\n\n\nThey will take around a minute to scan the S3 bucket and populate tables in Glue Data Catalog. These tables are the same; their names are the only difference.\n\n\n\n\nglue-tables.png"
  },
  {
    "objectID": "posts/2023-03-01-glue-partition-indexes.html#step-6-create-a-partition-index-on-the-table",
    "href": "posts/2023-03-01-glue-partition-indexes.html#step-6-create-a-partition-index-on-the-table",
    "title": "Working with partition indexes in AWS Glue",
    "section": "Step 6: Create a partition index on the table",
    "text": "Step 6: Create a partition index on the table\nFinally, we have reached a point where we can define the partition index on table with_index_syncstreamingdata. Adding an index does not affect the availability of a table, as the table continues to be available while indexes are being created. To read more about the glue create-partition-index command use the AWS docs glue/create-partition-index.html\naws glue create-partition-index \\\n--database-name partition_index_glueworkshop \\\n--table-name with_index_syncstreamingdata \\\n--partition-index Keys=customer,visityearmonth,IndexName=idxbycustvym\n\n\n\n\npartition-index.png"
  },
  {
    "objectID": "posts/2023-03-01-glue-partition-indexes.html#step-7-query-performance-with-partition-index",
    "href": "posts/2023-03-01-glue-partition-indexes.html#step-7-query-performance-with-partition-index",
    "title": "Working with partition indexes in AWS Glue",
    "section": "Step 7: Query performance with partition index",
    "text": "Step 7: Query performance with partition index\nNow to see if there is any benefit in query performance by defining partition indexes on a table, we will use a Spark Notebook from AWS Glue ETL job to query both tables (with and without index).\nYou can use the sample notebook provided here. It already includes the queries on the tables. Download it from the provided link. download_link\nNow follow the below steps to use the provided notebook for the Glue ETL job.\n\nGo to AWS Glue Studio console , click Jobs on the left side menu, under Create job choose Jupyter Notebook and select the option Upload and edit an existing notebook. Click the Choose file button to upload the downloaded notebook file.\n\n\n\n\nglue-studio.png\n\n\n\nOn the next Notebook setup screen provide\n\nJob name = glue-notebook\nIAM Role = AWSGlueServiceRole-glueworkshop\nKernel = Spark\n\n\n\n\nglue-notebook.png\n\n\n\n\nOnce the notebook starts, run all the cells. I have given the details of each cell here.\n\nCell 1 : It creates a Spark session\nCell 2 : It will query the table without any index on it. When I ran the query, the execution time on this table was around 32 seconds\n\n\n\n\nquery-1-results.png\n\n\n\nCell 3 : It will query the table with partition index on it. When I ran the query, the execution time on this table was around 10 seconds\n\n\n\n\nquery-2-results.png\n\n\n\nCell 4 : Calculates the difference in query execution time between the two tables. In my case query on table with partition indexes was faster by 22 seconds .\n\n\n\n\nqueries-diff.png"
  },
  {
    "objectID": "posts/2023-03-01-glue-partition-indexes.html#conclusion",
    "href": "posts/2023-03-01-glue-partition-indexes.html#conclusion",
    "title": "Working with partition indexes in AWS Glue",
    "section": "Conclusion",
    "text": "Conclusion\nBy defining partition indexes on large highly partitioned tables can give significant boost in query performance. Partition indexes are available for queries in Amazon EMR, Amazon Redshift Spectrum, and AWS Glue extract, transform, and load (ETL) jobs (Spark DataFrame and Glue DynamicFrame). When partition indexes are enabled on the heavily partitioned AWS Glue Data Catalog tables, all these query engines are accelerated."
  },
  {
    "objectID": "posts/2023-05-10-amazon-jumpstart-text2img-stablediffusion.html",
    "href": "posts/2023-05-10-amazon-jumpstart-text2img-stablediffusion.html",
    "title": "Build your own Generative AI Art Studio with Amazon SageMaker JumpStart",
    "section": "",
    "text": "image source: https://prompthero.com/prompt/967d64692e0"
  },
  {
    "objectID": "posts/2023-05-10-amazon-jumpstart-text2img-stablediffusion.html#credits",
    "href": "posts/2023-05-10-amazon-jumpstart-text2img-stablediffusion.html#credits",
    "title": "Build your own Generative AI Art Studio with Amazon SageMaker JumpStart",
    "section": "Credits",
    "text": "Credits\nThis notebook took inspiration from the AWS Machine Learning Blog post when they announced the availability of Stable Diffusion V1 and Stable Diffusion V2 models on SageMaker JumpStart. You can find the original post here Generate images from text with the stable diffusion model on Amazon SageMaker JumpStart."
  },
  {
    "objectID": "posts/2023-05-10-amazon-jumpstart-text2img-stablediffusion.html#introduction",
    "href": "posts/2023-05-10-amazon-jumpstart-text2img-stablediffusion.html#introduction",
    "title": "Build your own Generative AI Art Studio with Amazon SageMaker JumpStart",
    "section": "Introduction",
    "text": "Introduction\n\nWhat Is Amazon SageMaker?\nAmazon SageMaker is a fully managed machine learning service. With SageMaker, data scientists and developers can quickly and easily build and train machine learning models, and then directly deploy them into a production-ready hosted environment. It provides an integrated Jupyter authoring notebook instance for easy access to your data sources for exploration and analysis, so you don’t have to manage servers. It also provides common machine learning algorithms that are optimized to run efficiently against extremely large data in a distributed environment. With native support for bring-your-own-algorithms and frameworks, SageMaker offers flexible distributed training options that adjust to your specific workflows. You can deploy a model into a secure and scalable environment by launching it with a few clicks from SageMaker Studio or the SageMaker console.\n\n\n\n\n\n\nNote\n\n\n\nAmazon SageMaker introduction is taken from SageMaker Developer Guide. You may use Developer Guide for more details including Get Started with Amazon SageMaker.\n\n\n\n\nWhat is SageMaker JumpStart?\nSageMaker JumpStart is the machine learning (ML) hub of SageMaker that provides hundreds of built-in algorithms, pre-trained models, and end-to-end solution templates to help you quickly get started with ML. JumpStart also provides solution templates that set up infrastructure for common use cases, and executable example notebooks for machine learning with SageMaker.\n\n\n\n\n\n\nNote\n\n\n\nSageMaker JumpStart introduction is taken from SageMaker JumpStart Developer Guide. You may use Developer Guide for more details including Get Started and one-click, end-to-end Solution Templates for many common machine learning use cases.\n\n\n\n\nWhat is Stable Diffusion?\nStable Diffusion is a text-to-image model that enables you to create photorealistic images from just a text prompt. A diffusion model trains by learning to remove noise that was added to a real image. This de-noising process generates a realistic image. These models can also generate images from text alone by conditioning the generation process on the text. For instance, Stable Diffusion is a latent diffusion where the model learns to recognize shapes in a pure noise image and gradually brings these shapes into focus if the shapes match the words in the input text.\n\nHow does JumpStart simplify it?\nTraining, deploying, and running inference on large models such as Stable Diffusion is often challenging. It includes issues such as CUDA being out of memory, the payload size limit being exceeded, and so on. JumpStart simplifies this process by providing ready-to-use scripts that have been robustly tested. Furthermore, it provides guidance on each step of the process, including the recommended instance types, how to select parameters to guide the image generation process, prompt engineering, etc. Moreover, you can deploy and run inference on any of the 80+ Diffusion models from JumpStart without having to write any piece of your own code.\n\n\n\n\n\n\nNote\n\n\n\nStable Diffusion (SD) introduction is taken from Amazon JumpStart Text To Image notebook. For a more in-depth discussion on SD architecture and working, I suggest reading Jay Alammar The Illustrated Stable Diffusion guide."
  },
  {
    "objectID": "posts/2023-05-10-amazon-jumpstart-text2img-stablediffusion.html#environment",
    "href": "posts/2023-05-10-amazon-jumpstart-text2img-stablediffusion.html#environment",
    "title": "Build your own Generative AI Art Studio with Amazon SageMaker JumpStart",
    "section": "Environment",
    "text": "Environment\nThis notebook is created with Amazon SageMaker Studio running on ml.t3.medium instance with Python 3 (Base Python 2.0) kernel.\n\nGitHub: 2023-05-10-amazon-jumpstart-text2img-stablediffusion.ipynb\n\n\nFor model deployment and inference, I recommend using the ml.p3.2xlarge or ml.g4dn.2xlarge instance. I have relied on the ml.p3.2xlarge instance for this notebook. For generating multiple images per prompt, ml.g4dn.2xlarge can be slow, and you will get timeout errors as highlighted below.\n\n\n\n\n\n\nImportant\n\n\n\nBy default, both ml.p3.2xlarge and ml.g4dn.2xlarge may not be available in your AWS account. To get access, you need to generate a Request quota increase ticket from Service Quotas &gt; AWS services &gt; Amazon SageMaker &gt; ml.p3.2xlarge for endpoint usage. A service request may take up to 24 hours to get approved.\n\n\n\n\n\n\n\n\n\nTip\n\n\n\n\n\nWhy this timeout exception?\nYou get an API endpoint when you deploy a model into production using Amazon SageMaker hosting services. Your client applications use this API to get inferences from the model hosted at the specified endpoint. There is a 60 seconds hard limit on these API endpoints.\nA customer’s model containers must respond to requests within 60 seconds. The model itself can have a maximum processing time of 60 seconds before responding to invocations. If your model is going to take 50-60 seconds of processing time, the SDK socket timeout should be set to be 70 seconds.\nTo read more about it, refer to the documentation SageMakerRuntime.Client.invoke_endpoint\nWhat to do when your model requires more than 60 seconds for inference?\nFor such cases, AWS recommends using Amazon SageMaker Asynchronous Inference. This option is ideal for inferences with large payload sizes (up to 1GB) or long processing times (up to 15 minutes). To read more about it, use the following references.\n\nMy blog post to run StableDiffusion inference on SageMaker in Async mode - COMING SOON!\nAmazon SageMaker Asynchronous Inference announcement\nHow does Asynchronous inference work?\nGitHub Issue: Increasing the timeout for SageMaker InvokeEndpoint"
  },
  {
    "objectID": "posts/2023-05-10-amazon-jumpstart-text2img-stablediffusion.html#set-up-the-environment",
    "href": "posts/2023-05-10-amazon-jumpstart-text2img-stablediffusion.html#set-up-the-environment",
    "title": "Build your own Generative AI Art Studio with Amazon SageMaker JumpStart",
    "section": "Set up the environment",
    "text": "Set up the environment\nThere are some initial steps required to execute this notebook. They mainly involve installing the needed packages and initializing the SageMaker session.\n\n%%capture\n!pip install --upgrade sagemaker\n!pip install matplotlib\n!pip install watermark\n\n# 1. Get the latest version of SageMaker Python SDK. https://github.com/aws/sagemaker-python-sdk\n# 2. Install matplotlib. https://github.com/matplotlib/matplotlib\n# 3. Install watermark. An IPython magic extension for printing date and time stamps, version numbers, and hardware information. https://github.com/rasbt/watermark\n\nwatermark extension is a great utility to expose packages, kernel, and hardware information. Though this is optional, and you may skip this step, it is a great way to report execution environment information and make it more transparent.\n\n%load_ext watermark\n\n# To load the watermark magic, execute the following line in your IPython notebook or current IPython shell\n# to learn more about the usage: https://github.com/rasbt/watermark/blob/master/docs/watermark.ipynb\n\n\n%watermark -v -m -p numpy,matplotlib,boto3,json,sagemaker\n\n# watermark the notebook environment\n# watermark step is optional. This is done to make the environment details more transpaent\n\nPython implementation: CPython\nPython version       : 3.8.12\nIPython version      : 8.12.0\n\nnumpy     : 1.24.3\nmatplotlib: 3.7.1\nboto3     : 1.26.111\njson      : 2.0.9\nsagemaker : 2.153.0\n\nCompiler    : GCC 10.2.1 20210110\nOS          : Linux\nRelease     : 4.14.311-233.529.amzn2.x86_64\nMachine     : x86_64\nProcessor   : \nCPU cores   : 2\nArchitecture: 64bit\n\n\n\nNext, we will initialize the SageMaker session. This session manages interactions with the Amazon SageMaker APIs and any other AWS services needed. It provides convenient methods for manipulating entities and resources that Amazon SageMaker uses, such as training jobs, endpoints, and input datasets in S3. AWS service calls are delegated to an underlying Boto3 session, which is initialized using the AWS configuration chain by default.\nTo read more about SageMaker Session refer to the documentation sagemaker.session.Session\n\nimport sagemaker, boto3\nfrom sagemaker import get_execution_role\n\naws_role = get_execution_role()\naws_region = boto3.Session().region_name\nsagemaker_session = sagemaker.Session()\n\naws_region\n\n'us-east-1'"
  },
  {
    "objectID": "posts/2023-05-10-amazon-jumpstart-text2img-stablediffusion.html#define-functions-to-deploy-models-and-get-inference-endpoints",
    "href": "posts/2023-05-10-amazon-jumpstart-text2img-stablediffusion.html#define-functions-to-deploy-models-and-get-inference-endpoints",
    "title": "Build your own Generative AI Art Studio with Amazon SageMaker JumpStart",
    "section": "Define functions to deploy models and get inference endpoints",
    "text": "Define functions to deploy models and get inference endpoints\nIn this section, we will define some functions that will make it easy for us to deploy JumpStart pre-trained models and get inference endpoints against them.\n\n\nShow the code\nfrom sagemaker import image_uris, model_uris\nfrom sagemaker.model import Model\nfrom sagemaker.predictor import Predictor\nfrom sagemaker.utils import name_from_base\n\n\ndef get_model_endpoint(model_id, sagemaker_session, instance_type=\"ml.p3.2xlarge\"):\n    \"\"\"Deploy the model on the provided instance type are return the inference endpoint\"\"\"\n\n    # Get the endpoint name from the provided 'model_id'\n    endpoint_name = name_from_base(f\"jumpstart-example-{model_id}\")\n\n    # recommended `inference_instance_type` are\n    # \"ml.g4dn.2xlarge\"\n    # \"ml.g5.2xlarge\"\n    # \"ml.p3.2xlarge\"\n    inference_instance_type = instance_type\n\n    # Retrieve the inference docker container uri.\n    # This is the base HuggingFace container image for the default model above.\n    deploy_image_uri = image_uris.retrieve(\n        region=None,\n        framework=None,  # automatically inferred from model_id\n        image_scope=\"inference\",\n        model_id=model_id,\n        model_version=\"*\",  # '*' means get the latest version\n        instance_type=inference_instance_type,\n    )\n\n    # Retrieve the model uri. This includes the pre-trained model and parameters as well as the inference scripts.\n    # This includes all dependencies and scripts for model loading, inference handling etc..\n    model_uri = model_uris.retrieve(\n        model_id=model_id, model_version=\"*\", model_scope=\"inference\"\n    )\n\n    # To increase the maximum response size from the endpoint.\n    # Response in our case will be generated images\n    env = {\n        \"MMS_MAX_RESPONSE_SIZE\": \"20000000\",\n    }\n\n    # Create the SageMaker model instance\n    model = Model(\n        image_uri=deploy_image_uri,\n        model_data=model_uri,\n        role=aws_role,\n        predictor_cls=Predictor,\n        name=endpoint_name,\n        env=env,\n    )\n\n    # Deploy the Model and return Inference endpoint. Note that we need to pass Predictor class when we deploy model through Model class,\n    # for being able to run inference through the sagemaker API.\n    return model.deploy(\n        initial_instance_count=1,\n        instance_type=inference_instance_type,\n        predictor_cls=Predictor,\n        endpoint_name=endpoint_name,\n        sagemaker_session=sagemaker_session,\n    )\n\n\ndef remove_model_endpoint(model_predictor):\n    \"\"\"Remove the model and deployed inference endpoint\"\"\"\n    model_predictor.delete_model()\n    model_predictor.delete_endpoint()"
  },
  {
    "objectID": "posts/2023-05-10-amazon-jumpstart-text2img-stablediffusion.html#define-functions-to-query-endpoints-and-display-results",
    "href": "posts/2023-05-10-amazon-jumpstart-text2img-stablediffusion.html#define-functions-to-query-endpoints-and-display-results",
    "title": "Build your own Generative AI Art Studio with Amazon SageMaker JumpStart",
    "section": "Define functions to query endpoints and display results",
    "text": "Define functions to query endpoints and display results\nIn the next section, we will define some functions that we will use to query the inference endpoint and display the results.\n\n\nShow the code\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom PIL import Image\nfrom io import BytesIO\nimport base64\nimport json\nimport re\nimport os\n\n# Define a path to save the generated images\nimage_path = \"./images/2023-05-10-amazon-jumpstart-text2img-stablediffusion/generated/\"\nos.makedirs(image_path, exist_ok=True) # create folder if not present\n\n\ndef display_and_save_img(image, filename):\n    \"\"\"Display and save the hallucinated image.\"\"\"\n\n    plt.figure(figsize=(7, 7), frameon=False)\n    plt.imshow(np.array(image))\n    plt.axis(\"off\")\n    plt.savefig(\n        image_path + filename, bbox_inches=\"tight\", transparent=True, pad_inches=0\n    )  # comment it to NOT save generated images\n    plt.show()\n\n\ndef query_endpoint_with_json_payload(model_predictor, payload, content_type, accept):\n    \"\"\"Query the model predictor with json payload.\"\"\"\n\n    encoded_payload = json.dumps(payload).encode(\"utf-8\")\n\n    query_response = model_predictor.predict(\n        encoded_payload, {\"ContentType\": content_type, \"Accept\": accept,},\n    )\n    return query_response\n\n\ndef display_encoded_images(generated_images, prompt):\n    \"\"\"Decode the images and convert to RGB format and display\n\n    Args:\n    generated_images: are a list of jpeg images as bytes with b64 encoding.\n    prompt: text string used to generate the images\n    \"\"\"\n\n    for count, generated_image in enumerate(generated_images):\n        generated_image_decoded = BytesIO(base64.b64decode(generated_image.encode()))\n        generated_image_rgb = Image.open(generated_image_decoded).convert(\"RGB\")\n\n        # prepare filename to store the image from the prompt \n        temp = re.sub(\n            r\"[^a-zA-Z0-9\\s]+\", \"\", prompt\n        )  # remove special chars from prompt\n\n        temp = temp.replace(\" \", \"-\")  # turn spaces to '-'\n        temp = temp[:50]  # limit the lenght of string upto 100 chars\n        \n        filename = (\n            temp + str(count) + \".jpg\"\n        )  # add count and extension to the image name\n\n        # display the generated image\n        display_and_save_img(generated_image_rgb, filename)\n\n\ndef parse_response_multiple_images(query_response):\n    \"\"\"Parse response and return generated image and the prompt\"\"\"\n\n    response_dict = json.loads(query_response)\n    return response_dict[\"generated_images\"], response_dict[\"prompt\"]\n\n\ndef query_model_and_display(payload, model_predictor):\n    query_response = query_endpoint_with_json_payload(\n        model_predictor, payload, \"application/json\", \"application/json;jpeg\"\n    )\n    generated_images, prompt = parse_response_multiple_images(query_response)\n\n    display_encoded_images(generated_images, prompt)\n\n\n\nSupported Inference parameters\nWe have to pass along some advanced parameters in the request to get inferences from the model API. These include\n\nprompt: prompt to guide the image generation. It must be specified and can be a string or a list of strings.\nwidth: width of the generated image. If specified, it must be a positive integer divisible by 8.\nheight: height of the generated image. If specified, it must be a positive integer divisible by 8.\nnum_inference_steps: Number of denoising steps during image generation. More steps lead to a higher-quality image. If specified, it must be a positive integer.\nguidance_scale: Higher guidance scale results in an image closely related to the prompt at the expense of image quality. If specified, it must be a float. guidance_scale&lt;=1 is ignored.\nnegative_prompt: guide image generation against this prompt. If specified, it must be a string or a list of strings used with guidance_scale. If guidance_scale is disabled, this is also disabled. Moreover, if a prompt is a list of strings, then negative_prompt must also be a list of strings.\nnum_images_per_prompt: number of images returned per prompt. If specified, it must be a positive integer.\nseed: Fix the randomized state for reproducibility. If specified, it must be an integer.\n\nAn example request payload is provided below. The effect of these parameters will become apparent when we generate images using them.\npayload = {\n    \"prompt\": \"a portrait of a man\",\n    \"negative_prompt\": \"beard\"\n    \"width\": 512,\n    \"height\": 512,\n    \"num_images_per_prompt\": 1,\n    \"num_inference_steps\": 50,\n    \"guidance_scale\": 7.5,\n    \"seed\": 1,\n}\n\n\n\n\n\n\nTip\n\n\n\n\n\n\nprompt and negative_prompt: Stable Diffusion models are not good at understanding negative words ‘without’, ‘except’, ‘exclude’, ‘not’ etc in the prompt statement. For example, the prompt “a portrait of a man without a beard” may still generate an image of a man with a beard. However, including “negative_promt” with the word “beard” has much more influence on the model.\nheight and width: The model often performs best when the generated image has dimensions similar to the training data dimension used for model training. It is recommended to read about the model to get the correct dimensions.\nnum_images_per_prompt: From experience, a number between 1 and 5 works best for real-time inference endpoint.\nnum_inference_steps: When experimenting with prompts, I tend to keep the “inference_steps” under 50. At 20, you will get a black-and-white image but get some idea of the image being produced. If I find an output with fine patterns, I try higher inference_steps between 100 and 150 to improve the quality further."
  },
  {
    "objectID": "posts/2023-05-10-amazon-jumpstart-text2img-stablediffusion.html#select-sagemaker-pre-trained-diffusion-model-and-prompt-engineering",
    "href": "posts/2023-05-10-amazon-jumpstart-text2img-stablediffusion.html#select-sagemaker-pre-trained-diffusion-model-and-prompt-engineering",
    "title": "Build your own Generative AI Art Studio with Amazon SageMaker JumpStart",
    "section": "Select SageMaker pre-trained Diffusion model and Prompt Engineering",
    "text": "Select SageMaker pre-trained Diffusion model and Prompt Engineering\n\nModel selection\nSageMaker JumpStart provides many pre-trained models. Use the following link to search and select the correct Model ID. Built-in Algorithms with pre-trained Model Table. We are only interested in text-to-image models, and we can filter them using txt2img string in the search bar.\n\nAt the time of writing this notebook, 86 txt2img models are available on JumpStart. We may use any of them to generate the images. I have selected a few from them that are the more well-known. However, you are welcome to experiment with anyone of them. Following in the list of the models that we will use in the later part of this notebook.\n\nmodel-txt2img-stabilityai-stable-diffusion-v2-1-base\nhuggingface-txt2img-prompthero-openjourney\nhuggingface-txt2img-andite-anything-v4-0 \nhuggingface-txt2img-nitrosocke-mo-di-diffusion\nhuggingface-txt2img-envvi-inkpunk-diffusion\n\n\n\nPrompt engineering\nWriting a good prompt can be an art. Predicting whether a particular prompt will yield a satisfactory image with a given model is often difficult. However, specific templates have been observed to work. Broadly, a prompt can be roughly broken down into three pieces:\n\nType of image (photograph/sketch/painting, etc.)\nDescription (subject/object/environment/scene, etc.)\nThe style of the image (realistic/artistic/type of art, etc.)\n\nYou can change each of the three parts individually to generate variations of an image. Adjectives have been known to play a significant role in the image-generation process. Also, adding more details about the scene helps in the generation process. Here are some suggestions that you may follow to generate good prompts.\n\nTo generate a realistic image, you can use phrases such as “a photo of,” “a photograph of,” “realistic,” or “hyper-realistic.”\nTo generate images by artists, you can use phrases like “by Pablo Picasso,” “oil painting by Rembrandt,” “landscape art by Frederic Edwin Church,” or “pencil drawing by Albrecht Dürer.”\nYou can combine different artists as well. For example, to generate artistic images by category, you can add the art category in the prompt such as “lion on a beach, abstract.”\nSome other types include “oil painting,” “pencil drawing, “pop art,” “digital art,” “anime,” “cartoon,” “futurism,” “watercolor,” “manga,” etc.\nYou can also include details such as lighting or camera lenses such as 35mm wide lens or 85mm wide lens, and information on the framing (portrait/landscape/close up, etc.).\n\n\n\n\n\n\n\nTip\n\n\n\n\n\nThe above concise prompt engineering outline is taken from AWS Blog Post. For a more in-depth discussion and techniques to write good prompts, you may consult below resources.\n\nStable Diffusion Art: a definitive prompt guide. An excellent beginner-level guide.\nBest Stable Diffusion Negative Prompt List To Use. It highlights techniques to improve results by using negative prompts.\nOpenArt.ai Prompt Book. A very detailed guide covering many techniques and areas."
  },
  {
    "objectID": "posts/2023-05-10-amazon-jumpstart-text2img-stablediffusion.html#stable-diffusion-v2-1",
    "href": "posts/2023-05-10-amazon-jumpstart-text2img-stablediffusion.html#stable-diffusion-v2-1",
    "title": "Build your own Generative AI Art Studio with Amazon SageMaker JumpStart",
    "section": "Stable Diffusion v2-1",
    "text": "Stable Diffusion v2-1\nLet’s get our first model up and running stable-diffusion-2-1.\n\nmodel_id = \"model-txt2img-stabilityai-stable-diffusion-v2-1-base\"\nmodel_predictor = get_model_endpoint(model_id, sagemaker_session)\n\n---------!\n\n\nThis base model is trained from scratch and can be applied to various use cases. However, to use this model, it is also important to understand some of the limitations.\n\nThe model does not achieve perfect photorealism\nThe model cannot render legible text\nThe model does not perform well on more difficult tasks which involve compositionality, such as rendering an image corresponding to “A red cube on top of a blue sphere”\nFaces and people in general may not be generated properly\nThe model was trained mainly with English captions and will not work as well in other languages\n\n\nRealistic people\nLet’s see how good is this model to generate real people images.\n\n\n\n\n\n\nPrompt\n\n\n\nPrompt = photo of young woman, highlight hair, sitting outside restaurant, wearing dress, rim lighting, studio lighting, looking at the camera, dslr, ultra quality, sharp focus, tack sharp, dof, film grain, Fujifilm XT3, crystal clear, 8K UHD, highly detailed glossy eyes, high detailed skin, skin pores\nNegative = disfigured, ugly, bad, immature, cartoon, anime, 3d, painting, b&w\n\n\n\n\nShow the code\nprompt = \"photo of young woman, highlight hair, sitting outside restaurant, wearing dress, rim lighting, studio lighting, looking at the camera, dslr, ultra quality, sharp focus, tack sharp, dof, film grain, Fujifilm XT3, crystal clear, 8K UHD, highly detailed glossy eyes, high detailed skin, skin pores\"\nnegative_prompt = \"disfigured, ugly, bad, immature, cartoon, anime, 3d, painting, b&w\"\n\npayload = {\n    \"prompt\": prompt,\n    \"negative_prompt\": negative_prompt,\n    \"width\": 512,\n    \"height\": 512,\n    \"num_images_per_prompt\": 5,\n    \"num_inference_steps\": 50,\n    \"guidance_scale\": 7.5,\n    \"seed\": 1,\n}\n\nquery_model_and_display(payload, model_predictor)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe model can generate images of faces with good symmetry but still struggles with hands and fingers. This is visible in images 3 and 5, where the fingers are distorted. Otherwise, it was able to capture the instructions from the prompt.\n\nphoto of a young woman\nhighlight hair\nsitting outside the restaurant\nwearing dress\n\nLet’s try another prompt; this time, we will focus more on the body shape, parts, and aesthetics.\n\n\n\n\n\n\nPrompt\n\n\n\nPrompt = sitting on green grass, bodybuilder posing, bodycam footage, man esthete with disgust face, overlooking a dark street, black and white photograph, sitting on a bench, dirty concrete wall, very sexy outfit, perfect face mode, sport photography, black business suit\nNegative = disfigured, ugly, bad\n\n\nThe results could be better, again highlighting that the Stable Diffusion v2.1 model still needs to improve at generating proper people faces and shapes.\n\n\nShow the code\nprompt = \"sitting on green grass, bodybuilder posing, bodycam footage, man esthete with disgust face, overlooking a dark street, black and white photograph, sitting on a bench, dirty concrete wall, very sexy outfit, perfect face mode, sport photography, black business suit\"\nnegative_prompt = \"disfigured, ugly, bad\"\n\npayload = {\n    \"prompt\": prompt,\n    \"negative_prompt\": negative_prompt,\n    \"width\": 512,\n    \"height\": 512,\n    \"num_images_per_prompt\": 5,\n    \"num_inference_steps\": 50,\n    \"guidance_scale\": 7.5,\n    \"seed\": 1,\n}\n\nquery_model_and_display(payload, model_predictor)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArchitecture\nOkay, we have seen that model struggles with depicting accurate body shapes. So let’s try something more complex and see how well the model understands building shapes and architectures.\n\n\n\n\n\n\nPrompt\n\n\n\nA Hyperrealistic photograph of Italian architectural modern home in Italy, lens flares, cinematic, hdri, matte painting, concept art, celestial, soft render, highly detailed, octane render, architectural HD, HQ, 4k, 8k\n\n\n\n\nShow the code\nprompt = \"A Hyperrealistic photograph of Italian architectural modern home in Italy, lens flares, cinematic, hdri, matte painting, concept art, celestial, soft render, highly detailed, octane render, architectural HD, HQ, 4k, 8k\"\n\npayload = {\n    \"prompt\": prompt,\n    \"width\": 512,\n    \"height\": 512,\n    \"num_images_per_prompt\": 5,\n    \"num_inference_steps\": 50,\n    \"guidance_scale\": 7.5,\n    \"seed\": 1,\n}\n\nquery_model_and_display(payload, model_predictor)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThese are very impressive results. The model is adept at understanding how modern buildings should look like. It can intelligently place windows, balconies, chairs, trees, shadows, and even reflections at appropriate locations. The design is also modern, meaning it follows the instructions well.\nLet’s try another difficult prompt focusing on a city’s complex geometry and formations.\n\n\n\n\n\n\nPrompt\n\n\n\nA beautiful digital illustration of BARCELONA, 4k, crystal clear, ultra HD, high definition, detailed, trending in artstation, fantasy\n\n\n\n\nShow the code\nprompt = \"A beautiful digital illustration of BARCELONA, 4k, crystal clear, ultra HD, high definition, detailed, trending in artstation, fantasy\"\n\npayload = {\n    \"prompt\": prompt,\n    \"width\": 512,\n    \"height\": 512,\n    \"num_images_per_prompt\": 3,\n    \"num_inference_steps\": 120,\n    \"guidance_scale\": 7.5,\n    \"seed\": 1,\n}\n\nquery_model_and_display(payload, model_predictor)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe results are pretty good. The model can understand that structures at the front are expansive, and those at the far back slowly decreases in size. It also understands that roads run through and divides a typical city layout. The last image is impressive, containing a green belt having trees in the center. After that, there is a road busy with vehicles. And then, at the peripheries, there are building with intricate layouts.\n\n\nAbstract art and styles\nLet’s move to abstract art, styles, and artists and see how well the model portrays them.\n\n\n\n\n\n\nPrompt\n\n\n\nA surreal 3D render of a cityscape with twisted buildings, upside-down structures, and floating objects, creating a dreamlike and disorienting atmosphere, by Simon Stålenhag\n\n\n\n\nShow the code\nprompt = \"A surreal 3D render of a cityscape with twisted buildings, upside-down structures, and floating objects, creating a dreamlike and disorienting atmosphere, by Simon Stålenhag\"\n\npayload = {\n    \"prompt\": prompt,\n    \"width\": 512,\n    \"height\": 512,\n    \"num_images_per_prompt\": 5,\n    \"num_inference_steps\": 50,\n    \"guidance_scale\": 7.5,\n    \"seed\": 1,\n}\n\nquery_model_and_display(payload, model_predictor)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n“Twisted upside-down buildings and floating objects” is very unusual, and hard to imagine what such a world would look like. But it is not difficult for the model to produce it. So let’s try some more prompts.\n\n\n\n\n\n\nPrompt\n\n\n\nA futuristic space station orbiting a distant planet, with a sleek design and advanced technology visible through the windows, Nuremberg Chronicle, 1493, Liber Chronicarum, Michael Wolgemut, detailed architecture\n\n\n\n\nShow the code\nprompt = \"A futuristic space station orbiting a distant planet, with sleek design and advanced technology visible through the windows, Nuremberg Chronicle, 1493, Liber Chronicarum, Michael Wolgemut, detailed archtitecture\"\nnegative_prompt = \"writings, ugly, bad, immature, anime, b&w, lines, latin\"\n\npayload = {\n    \"prompt\": prompt,\n    \"negative_prompt\": negative_prompt,\n    \"width\": 512,\n    \"height\": 512,\n    \"num_images_per_prompt\": 3,\n    \"num_inference_steps\": 50,\n    \"guidance_scale\": 7.5,\n    \"seed\": 1,\n}\n\nquery_model_and_display(payload, model_predictor)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPrompt\n\n\n\nRealistic 3D render of an isometric hospital in a low poly art style, medical workers, doctors, nurses, patients, beds and equipment.\n\n\n\n\nShow the code\nprompt = \"Realistic 3D render of an isometric hospital in a low poly art style, medical workers, doctors, nurses, patients, beds and equipment.\"\n\npayload = {\n    \"prompt\": prompt,\n    \"width\": 512,\n    \"height\": 512,\n    \"num_images_per_prompt\": 3,\n    \"num_inference_steps\": 50,\n    \"guidance_scale\": 7.5,\n    \"seed\": 1,\n}\n\nquery_model_and_display(payload, model_predictor)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLandscapes\nLet’s move to a more natural setting. The model can generate beautiful scenes involving meadows, wildlands, mountains, rivers, etc. The depth and details in the results are remarkable. Some prompts that I tried are provided here.\n\n\n\n\n\n\nPrompt\n\n\n\nThe green idyllic Arcadian prairie with sheep by Thomas Cole, Breath-taking digital painting with placid colors, amazing art, artstation 3, cottagecore.\n\n\n\n\nShow the code\nprompt = \"The green idyllic Arcadian prairie with sheep by Thomas Cole, Breath-taking digital painting with placid colors, amazing art, artstation 3, cottagecore.\"\n\npayload = {\n    \"prompt\": prompt,\n    \"width\": 512,\n    \"height\": 512,\n    \"num_images_per_prompt\": 3,\n    \"num_inference_steps\": 120,\n    \"guidance_scale\": 7.5,\n    \"seed\": 1,\n}\n\nquery_model_and_display(payload, model_predictor)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPrompt\n\n\n\n\n\n\nNote\n\n\n\nA beautiful castle beside a waterfall in the woods, by Josef Thoma, matte painting, trending on artstation HQ, full view\n\n\n\n\nShow the code\nprompt = \"A beautiful castle beside a waterfall in the woods, by Josef Thoma, matte painting, trending on artstation HQ, full view\"\n\npayload = {\n    \"prompt\": prompt,\n    \"width\": 512,\n    \"height\": 512,\n    \"num_images_per_prompt\": 3,\n    \"num_inference_steps\": 120,\n    \"guidance_scale\": 7.5,\n    \"seed\": 1,\n}\n\nquery_model_and_display(payload, model_predictor)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPortraits and paintings\nI also tried generating portraits and paintings of imaginative scenarios. Stable Diffusion is again very good at it. But similar to its limitations in creating accurate human body shapes, it struggles with them in paintings.\n\n\n\n\n\n\nPrompt\n\n\n\nPrompt = A painting of a pirate ship in the ocean, a digital painting, renaissance, color study, portrait, castle, three masts, davinci, whole page illustration, small boat\nNegative = logos, icons, writings, description\n\n\n\n\nShow the code\nprompt = \"A painting of a pirate ship in the ocean, a digital painting, renaissance, color study, portrait, castle, three masts, davinci, whole page illustration, small boat\"\nnegative_prompt = \"logos, icons, writings, description\"\n\npayload = {\n    \"prompt\": prompt,\n    \"negative_prompt\": negative_prompt,\n    \"width\": 512,\n    \"height\": 512,\n    \"num_images_per_prompt\": 3,\n    \"num_inference_steps\": 50,\n    \"guidance_scale\": 7.5,\n    \"seed\": 1,\n}\n\nquery_model_and_display(payload, model_predictor)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPrompt\n\n\n\nComplex detailed medieval painting of a fantasy medieval cleric woman walking through a field of flowers, cleric wearing a tiara and a ornate embroidered hooded cloak, bright blue eyes, sharp focus, close shot, long blonde hair, high contrast, low saturation, lush flower field and clouds background, backlighting\n\n\n\n\nShow the code\nprompt = \"Complex detailed medieval painting of a fantasy medieval cleric woman walking through a field of flowers, cleric wearing a tiara and a ornate embroidered hooded cloak, bright blue eyes, sharp focus, close shot, long blonde hair, high contrast, low saturation, lush flower field and clouds background, backlighting\"\n\npayload = {\n    \"prompt\": prompt,\n    \"width\": 512,\n    \"height\": 512,\n    \"num_images_per_prompt\": 3,\n    \"num_inference_steps\": 120,\n    \"guidance_scale\": 7.5,\n    \"seed\": 1,\n}\n\nquery_model_and_display(payload, model_predictor)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPlease delete the model endpoint once it is no longer required to avoid unforeseen costs.\n\nremove_model_endpoint(model_predictor)\n\n\n\n\n\n\n\nTip\n\n\n\n\n\nIf your notebook kernel has crashed, manually delete the model endpoint by going to SageMaker &gt; Inference &gt; Endpoints. Make sure that you are in the right region in the Management Console."
  },
  {
    "objectID": "posts/2023-05-10-amazon-jumpstart-text2img-stablediffusion.html#prompthero-openjourney",
    "href": "posts/2023-05-10-amazon-jumpstart-text2img-stablediffusion.html#prompthero-openjourney",
    "title": "Build your own Generative AI Art Studio with Amazon SageMaker JumpStart",
    "section": "PromptHero Openjourney",
    "text": "PromptHero Openjourney\nBesides the Stable Diffusion base models, people have also released models with unique styles and tastes. One such model is OpenJourney. Openjourney is based on Stable Diffusion v1.5 but fine-tuned with many Midjourney images. The result is a model capable of generating very detailed and colorful photos. But to invoke the model’s unique style, we must append the prompt with the text modern-v4 style.\nLet’s get this model up and running.\n\nmodel_id = \"huggingface-txt2img-prompthero-openjourney\"\nmodel_predictor = get_model_endpoint(model_id, sagemaker_session) \n\n---------!\n\n\n\nRealistic people and portraits\nRemember that this OpenJourney model is based on an older version of stable diffusion, but I found it equally good at generating faces and body shapes with acceptable symmetry.\n\n\n\n\n\n\nPrompt\n\n\n\nPrompt = mdjrny-v4 style old, female robot, metal, rust, wisible wires, destroyed, sad, dark, dirty, looking at viewer, portrait, photography, detailed skin, realistic, photo-realistic, 8k, highly detailed, full length frame, High detail RAW color art, piercing, diffused soft lighting, shallow depth of field, sharp focus, hyperrealism, cinematic lighting\nNegative = logo, CGI, 3d, lowres, text, error, cropped, worst quality, low quality, jpeg artifacts, ugly, duplicate, morbid, mutilated, out of frame, extra fingers, mutated hands, poorly drawn hands, poorly drawn face, mutation, deformed, blurry, dehydrated, bad anatomy, bad proportions, extra limbs, cloned face, disfigured, gross proportions, malformed limbs, missing arms, missing legs, extra arms, extra legs, fused fingers, too many fingers, long neck, username, watermark, signature\n\n\n\n\nShow the code\nprompt = \"mdjrny-v4 style old, female robot, metal, rust, wisible wires, destroyed, sad, dark, dirty, looking at viewer, portrait, photography, detailed skin, realistic, photo-realistic, 8k, highly detailed, full length frame, High detail RAW color art, piercing, diffused soft lighting, shallow depth of field, sharp focus, hyperrealism, cinematic lighting\"\nnegative_prompt = \"logo, CGI, 3d, lowres, text, error, cropped, worst quality, low quality, jpeg artifacts, ugly, duplicate, morbid, mutilated, out of frame, extra fingers, mutated hands, poorly drawn hands, poorly drawn face, mutation, deformed, blurry, dehydrated, bad anatomy, bad proportions, extra limbs, cloned face, disfigured, gross proportions, malformed limbs, missing arms, missing legs, extra arms, extra legs, fused fingers, too many fingers, long neck, username, watermark, signature\"\n\npayload = {\n    \"prompt\": prompt,\n    \"negative_prompt\": negative_prompt,\n    \"width\": 512,\n    \"height\": 512,\n    \"num_images_per_prompt\": 3,\n    \"num_inference_steps\": 120,\n    \"guidance_scale\": 14,\n    \"seed\": 1,\n}\n\nquery_model_and_display(payload, model_predictor)\n# https://prompthero.com/prompt/4c1a987fec7\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPrompt\n\n\n\nPrompt = mdjrny-v4 style portrait photograph of Madison Beer as Pocahontas, young beautiful native american woman, perfect symmetrical face, feather jewelry, traditional handmade dress, armed female hunter warrior, wild west environment, Utah landscape, ultra realistic, concept art, elegant, intricate, highly detailed, depth of field, professionally color graded, 8k, art by artgerm and greg rutkowski and alphonse mucha\nNegative = backlight, dark face, white fur, gold fabric, cropped head, out of frame, deformed, cripple, ugly, additional arms, additional legs, additional head, two heads, multiple people, group of people\n\n\n\n\nShow the code\nprompt = \"mdjrny-v4 style portrait photograph of Madison Beer as Pocahontas, young beautiful native american woman, perfect symmetrical face, feather jewelry, traditional handmade dress, armed female hunter warrior, wild west environment, Utah landscape, ultra realistic, concept art, elegant, intricate, highly detailed, depth of field, professionally color graded, 8k, art by artgerm and greg rutkowski and alphonse mucha\"\nnegative_prompt = \"backlight, dark face, white fur, gold fabric, cropped head, out of frame, deformed, cripple, ugly, additional arms, additional legs, additional head, two heads, multiple people, group of people\"\npayload = {\n    \"prompt\": prompt,\n    \"negative_prompt\": negative_prompt,\n    \"width\": 512,\n    \"height\": 512,\n    \"num_images_per_prompt\": 3,\n    \"num_inference_steps\": 120,\n    \"guidance_scale\": 14,\n    \"seed\": 1,\n}\nquery_model_and_display(payload, model_predictor)\n# https://prompthero.com/prompt/d4f36f0dcdf\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPrompt\n\n\n\nPrompt = mdjrny-v4 style Incredibly detailed technical diagram of the profile of a woman with the top and back of her head split into complex geometric shapes and flowers growing from the shapes, Chiaroscuro lighting, fine intricate details\nNegative = long neck, skinny\n\n\n\n\nShow the code\nprompt = \"mdjrny-v4 style Incredibly detailed technical diagram of the profile of a woman with the top and back of her head split into complex geometric shapes and flowers growing from the shapes, Chiaroscuro lighting, fine intricate details\"\nnegative_prompt = \"long neck, skinny\"\npayload = {\n    \"prompt\": prompt,\n    \"negative_prompt\": negative_prompt,\n    \"width\": 512,\n    \"height\": 512,\n    \"num_images_per_prompt\": 3,\n    \"num_inference_steps\": 120,\n    \"guidance_scale\": 14,\n    \"seed\": 1,\n}\nquery_model_and_display(payload, model_predictor)\n# https://prompthero.com/prompt/0c8536f3b23\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFairy Landscapes\nYou can also create very detailed and colorful photos of landscapes.\n\n\n\n\n\n\nPrompt\n\n\n\nmdjrny-v4 style painting of a fairy forest, dream light, full of colors, mushroom tree, dim light, super detailed, unreal engine 5, hdr, 12k, by Vincent Van Goth\n\n\n\n\nShow the code\nprompt = \"mdjrny-v4 style painting of a fairy forest, dream light, full of colors, mushroom tree, dim light, super detailed, unreal engine 5, hdr, 12k, by Vincent Van Goth\"\npayload = {\n    \"prompt\": prompt,\n    \"width\": 512,\n    \"height\": 512,\n    \"num_images_per_prompt\": 3,\n    \"num_inference_steps\": 120,\n    \"guidance_scale\": 14,\n    \"seed\": 1,\n}\nquery_model_and_display(payload, model_predictor)\n# https://prompthero.com/prompt/19c0ad9cac1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOther digital art prompts\nSome other prompt examples where the results are very impressive.\n\n\n\n\n\n\nPrompt\n\n\n\nmdjrny-v4 style little dragon staring at magic spell book sitting on a table in the catacombs, hypermaximalist, insanely detailed and intricate, octane render, unreal engine, 8k, by greg rutkowski and Peter Mohrbacher and magali villeneuve\n\n\n\n\nShow the code\nprompt = \"mdjrny-v4 style little dragon staring at magic spell book sitting on a table in the catacombs, hypermaximalist, insanely detailed and intricate, octane render, unreal engine, 8k, by greg rutkowski and Peter Mohrbacher and magali villeneuve\"\npayload = {\n    \"prompt\": prompt,\n    \"width\": 512,\n    \"height\": 512,\n    \"num_images_per_prompt\": 3,\n    \"num_inference_steps\": 120,\n    \"guidance_scale\": 14,\n    \"seed\": 1,\n}\nquery_model_and_display(payload, model_predictor)\n# https://prompthero.com/prompt/63c7ed7865f\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPrompt\n\n\n\nmdjrny-v4 style, magic spell book sitting on a table in the catacombs, hypermaximalist, insanely detailed and intricate, octane render, unreal engine, 8k, by greg rutkowski and Peter Mohrbacher and magali villeneuve\n\n\n\n\nShow the code\nprompt = \"mdjrny-v4 style, magic spell book sitting on a table in the catacombs, hypermaximalist, insanely detailed and intricate, octane render, unreal engine, 8k, by greg rutkowski and Peter Mohrbacher and magali villeneuve\"\npayload = {\n    \"prompt\": prompt,\n    \"width\": 512,\n    \"height\": 512,\n    \"num_images_per_prompt\": 3,\n    \"num_inference_steps\": 120,\n    \"guidance_scale\": 14,\n    \"seed\": 1,\n}\nquery_model_and_display(payload, model_predictor)\n# https://prompthero.com/prompt/4c1a987fec7\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nremove_model_endpoint(model_predictor)"
  },
  {
    "objectID": "posts/2023-05-10-amazon-jumpstart-text2img-stablediffusion.html#fantasyai-anything-v4.0",
    "href": "posts/2023-05-10-amazon-jumpstart-text2img-stablediffusion.html#fantasyai-anything-v4.0",
    "title": "Build your own Generative AI Art Studio with Amazon SageMaker JumpStart",
    "section": "FantasyAI Anything V4.0",
    "text": "FantasyAI Anything V4.0\nanything-v4.0 is another model with a particular style. It is also based on Stable Diffusion and is good at producing high-quality, highly detailed anime styles with just a few prompts.\nLet’s get this model up and running.\n\nmodel_id = \"huggingface-txt2img-andite-anything-v4-0\"\nmodel_predictor = get_model_endpoint(model_id, sagemaker_session)\n\n---------!\n\n\n\nRealistic people\nI used a simple prompt to get a portrait of a casual boy sitting in the coffee shop. From the results, you can see that it has a notable anime and manga taste. In addition, the boy’s eyes, hairstyle, clothes, and lighting all represent anime fashion.\n\n\n\n\n\n\nPrompt\n\n\n\n1boy, bishounen, casual, indoors, sitting, coffee shop, bokeh\n\n\n\n\nShow the code\nprompt = \"1boy, bishounen, casual, indoors, sitting, coffee shop, bokeh\"\npayload = {\n    \"prompt\": prompt,\n    \"width\": 512,\n    \"height\": 512,\n    \"num_images_per_prompt\": 3,\n    \"num_inference_steps\": 120,\n    \"guidance_scale\": 7.5,\n    \"seed\": 1,\n}\nquery_model_and_display(payload, model_predictor)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLet’s also try the same prompt we used to generate portraits of a young woman. But this time, all the generated images are in anime style.\n\n\n\n\n\n\nPrompt\n\n\n\nPrompt = photo of young woman, highlight hair, sitting outside restaurant, wearing dress, rim lighting, studio lighting, looking at the camera, dslr, ultra quality, sharp focus, tack sharp, dof, film grain, Fujifilm XT3, crystal clear, 8K UHD, highly detailed glossy eyes, high detailed skin, skin pores\nNegative = disfigured, ugly, bad, immature, cartoon, anime, 3d, painting, b&w\n\n\n\n\nShow the code\nprompt = \"photo of young woman, highlight hair, sitting outside restaurant, wearing dress, rim lighting, studio lighting, looking at the camera, dslr, ultra quality, sharp focus, tack sharp, dof, film grain, Fujifilm XT3, crystal clear, 8K UHD, highly detailed glossy eyes, high detailed skin, skin pores\"\nnegative_prompt = \"disfigured, ugly, bad, immature, cartoon, anime, 3d, painting, b&w\"\npayload = {\n    \"prompt\": prompt,\n    \"negative_prompt\": negative_prompt,\n    \"width\": 512,\n    \"height\": 512,\n    \"num_images_per_prompt\": 3,\n    \"num_inference_steps\": 120,\n    \"guidance_scale\": 7.5,\n    \"seed\": 1,\n}\nquery_model_and_display(payload, model_predictor)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nI have found that “anything-v4.0” model has a tendency to generate nude images. It could be because their creators have also used such images while finetuning the model. You have to rely on the negative prompt to restrict the model from generating such images.\n\n\n\n\nLandscapes\nLet’s also try our castle prompt. Again the generative images are representative of anime and manga styles.\n\n\n\n\n\n\nPrompt\n\n\n\nA beautiful castle beside a waterfall in the woods, by Josef Thoma, matte painting, trending on artstation HQ\n\n\n\n\nShow the code\nprompt = \"A beautiful castle beside a waterfall in the woods, by Josef Thoma, matte painting, trending on artstation HQ\"\npayload = {\n    \"prompt\": prompt,\n    \"width\": 512,\n    \"height\": 512,\n    \"num_images_per_prompt\": 3,\n    \"num_inference_steps\": 120,\n    \"guidance_scale\": 7.5,\n    \"seed\": 1,\n}\nquery_model_and_display(payload, model_predictor)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nremove_model_endpoint(model_predictor)"
  },
  {
    "objectID": "posts/2023-05-10-amazon-jumpstart-text2img-stablediffusion.html#mo-di-diffusion",
    "href": "posts/2023-05-10-amazon-jumpstart-text2img-stablediffusion.html#mo-di-diffusion",
    "title": "Build your own Generative AI Art Studio with Amazon SageMaker JumpStart",
    "section": "Mo Di Diffusion",
    "text": "Mo Di Diffusion\nmo-di-diffusion is another model good at expressing Disney animation studio style. It is based on Stable Diffusion 1.5 and fine-tuned on screenshots from Disney animation studio. However, users must use the tokens modern disney style in the prompts to get the desired effect.\n\nmodel_id = \"huggingface-txt2img-nitrosocke-mo-di-diffusion\"\nmodel_predictor = get_model_endpoint(model_id, sagemaker_session)\n\n----------!\n\n\n\nRealistic people\nLet’s start with our same “young boy in a coffee shop” prompt. This time the young boy is an attractive youthful adult characteristic of the Disney Animation world. The generated photographs have a very close resemblance to Kristoff from Frozen or Tadashi from Big Hero 6.\n\n\n\n\n\n\nPrompt\n\n\n\nmodern disney style 1boy, bishounen, casual, indoors, sitting, coffee shop, bokeh\n\n\n\n\nShow the code\nprompt = \"modern disney style 1boy, bishounen, casual, indoors, sitting, coffee shop, bokeh\"\npayload = {\n    \"prompt\": prompt,\n    \"width\": 512,\n    \"height\": 512,\n    \"num_images_per_prompt\": 3,\n    \"num_inference_steps\": 120,\n    \"guidance_scale\": 7.5,\n    \"seed\": 1,\n}\nquery_model_and_display(payload, model_predictor)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe same is the case with a photo of young women. They remind me of Princess Moana from Disney World. But, unfortunately, the last image is distorted because MoDi is based on Stable Diffusion, which has these inherent limitations.\n\n\n\n\n\n\nPrompt\n\n\n\nPrompt = modern disney style photo of young woman, highlight hair, sitting outside restaurant, wearing dress, rim lighting, studio lighting, looking at the camera, dslr, ultra quality, sharp focus, tack sharp, dof, film grain, Fujifilm XT3, crystal clear, 8K UHD, highly detailed glossy eyes, high detailed skin, skin pores\nNegative = disfigured, ugly, bad, immature, cartoon, anime, 3d, painting, b&w\n\n\n\n\nShow the code\nprompt = \"modern disney style photo of young woman, highlight hair, sitting outside restaurant, wearing dress, rim lighting, studio lighting, looking at the camera, dslr, ultra quality, sharp focus, tack sharp, dof, film grain, Fujifilm XT3, crystal clear, 8K UHD, highly detailed glossy eyes, high detailed skin, skin pores\"\nnegative_prompt = \"disfigured, ugly, bad, immature, crowd, people\"\npayload = {\n    \"prompt\": prompt,\n    \"negative_prompt\": negative_prompt,\n    \"width\": 512,\n    \"height\": 512,\n    \"num_images_per_prompt\": 3,\n    \"num_inference_steps\": 120,\n    \"guidance_scale\": 7.5,\n    \"seed\": 1,\n}\nquery_model_and_display(payload, model_predictor)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLandscapes and architectures\nI have also tried generating landscapes and building architectures. Both cases have a heavy touch of Disney World animation style.\n\n\n\n\n\n\nPrompt\n\n\n\n“modern disney style A beautiful castle beside a waterfall in the woods, by Josef Thoma, matte painting, trending on artstation HQ\n\n\n\n\nShow the code\nprompt = \"modern disney style A beautiful castle beside a waterfall in the woods, by Josef Thoma, matte painting, trending on artstation HQ\"\npayload = {\n    \"prompt\": prompt,\n    \"width\": 512,\n    \"height\": 512,\n    \"num_images_per_prompt\": 3,\n    \"num_inference_steps\": 120,\n    \"guidance_scale\": 7.5,\n    \"seed\": 1,\n}\nquery_model_and_display(payload, model_predictor)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPrompt\n\n\n\nmodern disney style A Hyperrealistic photograph of Italian architectural modern home in Italy, lens flares, cinematic, hdri, matte painting, concept art, celestial, soft render, highly detailed, octane render, architectural HD, HQ, 4k, 8k\n\n\n\n\nShow the code\nprompt = \"modern disney style A Hyperrealistic photograph of Italian architectural modern home in Italy, lens flares, cinematic, hdri, matte painting, concept art, celestial, soft render, highly detailed, octane render, architectural HD, HQ, 4k, 8k\"\npayload = {\n    \"prompt\": prompt,\n    \"width\": 512,\n    \"height\": 512,\n    \"num_images_per_prompt\": 3,\n    \"num_inference_steps\": 120,\n    \"guidance_scale\": 7.5,\n    \"seed\": 1,\n}\nquery_model_and_display(payload, model_predictor)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nremove_model_endpoint(model_predictor)"
  },
  {
    "objectID": "posts/2023-05-10-amazon-jumpstart-text2img-stablediffusion.html#inkpunk-diffusion",
    "href": "posts/2023-05-10-amazon-jumpstart-text2img-stablediffusion.html#inkpunk-diffusion",
    "title": "Build your own Generative AI Art Studio with Amazon SageMaker JumpStart",
    "section": "Inkpunk Diffusion",
    "text": "Inkpunk Diffusion\nOur last model to try is Inkpunk-Diffusion. It is also based on Stable Diffusion and fine-tuned for “Inky” and “Punky” styles. We have to use nvinkpunk in our prompts to summon this style. If you find that the InkPunk style is very dominating in the outcomes, then try experimenting without this token to get a lighter version of this style.\nLet’s get this model up and running.\n\nmodel_id = \"huggingface-txt2img-envvi-inkpunk-diffusion\"\nmodel_predictor = get_model_endpoint(model_id, sagemaker_session)\n\n----------!\n\n\n\nRealistic people and portraits\nI asked for a “medieval cleric woman”. In the InkPunk world, the lady turned out to be a super-rocky astro-woman from the century 2100. But the style is fantastic.\n\n\n\n\n\n\nPrompt\n\n\n\nnvinkpunk Complex detailed painting of a fantasy medieval cleric woman walking through a field of flowers by rembrandt, cleric wearing a tiara and a ornate embroidered hooded cloak, bright blue eyes, evocative, sharp focus, close shot, long blonde hair, high contrast, low saturation, lush flower field and clouds background, backlighting.\n\n\n\n\nShow the code\nprompt = \"nvinkpunk Complex detailed painting of a fantasy medieval cleric woman walking through a field of flowers by rembrandt, cleric wearing a tiara and a ornate embroidered hooded cloak, bright blue eyes, evocative, sharp focus, close shot, long blonde hair, high contrast, low saturation, lush flower field and clouds background, backlighting.\"\npayload = {\n    \"prompt\": prompt,\n    \"width\": 512,\n    \"height\": 512,\n    \"num_images_per_prompt\": 3,\n    \"num_inference_steps\": 120,\n    \"guidance_scale\": 7.5,\n    \"seed\": 1,\n}\nquery_model_and_display(payload, model_predictor)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLandscapes and architectures\n\n\n\n\n\n\nPrompt\n\n\n\nnvinkpunk A Hyperrealistic photograph of Italian architectural modern home in Italy, lens flares, cinematic, hdri, matte painting, concept art, celestial, soft render, highly detailed, octane render, architectural HD, HQ, 4k, 8k\n\n\n\n\nShow the code\nprompt = \"nvinkpunk A Hyperrealistic photograph of Italian architectural modern home in Italy, lens flares, cinematic, hdri, matte painting, concept art, celestial, soft render, highly detailed, octane render, architectural HD, HQ, 4k, 8k\"\npayload = {\n    \"prompt\": prompt,\n    \"width\": 512,\n    \"height\": 512,\n    \"num_images_per_prompt\": 3,\n    \"num_inference_steps\": 120,\n    \"guidance_scale\": 7.5,\n    \"seed\": 1,\n}\nquery_model_and_display(payload, model_predictor)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOther digital art prompts\n\n\n\n\n\n\nPrompt\n\n\n\nnvinkpunk bomb diffuser robot with assault rifle strapped to the top, killer robot\n\n\n\n\nShow the code\nprompt = \"nvinkpunk bomb diffuser robot with assault rifle strapped to the top, killer robot\"\npayload = {\n    \"prompt\": prompt,\n    \"width\": 512,\n    \"height\": 512,\n    \"num_images_per_prompt\": 3,\n    \"num_inference_steps\": 120,\n    \"guidance_scale\": 7.5,\n    \"seed\": 1,\n}\nquery_model_and_display(payload, model_predictor)\n# https://prompthero.com/prompt/67bc77c123f\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPrompt\n\n\n\nnvinkpunk f-18 Fighter Jet flying over a city, comic book style, bold colors and lines\n\n\n\n\nShow the code\nprompt = \"nvinkpunk f-18 Fighter Jet flying over a city, comic book style, bold colors and lines\"\npayload = {\n    \"prompt\": prompt,\n    \"width\": 512,\n    \"height\": 512,\n    \"num_images_per_prompt\": 3,\n    \"num_inference_steps\": 120,\n    \"guidance_scale\": 7.5,\n    \"seed\": 1,\n}\nquery_model_and_display(payload, model_predictor)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPrompt\n\n\n\nnvinkpunk steampunk style blimp, Jules Verne, mechanical\n\n\n\n\nShow the code\nprompt = \"nvinkpunk steampunk style blimp, Jules Verne, mechanical\"\npayload = {\n    \"prompt\": prompt,\n    \"width\": 512,\n    \"height\": 512,\n    \"num_images_per_prompt\": 3,\n    \"num_inference_steps\": 120,\n    \"guidance_scale\": 7.5,\n    \"seed\": 1,\n}\nquery_model_and_display(payload, model_predictor)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nremove_model_endpoint(model_predictor)"
  },
  {
    "objectID": "posts/2023-05-29-tips-aws-cert-pearsonvue-online-pakistan.html",
    "href": "posts/2023-05-29-tips-aws-cert-pearsonvue-online-pakistan.html",
    "title": "Tips to Register for an AWS Certification Exam with Pearson VUE from Pakistan",
    "section": "",
    "text": "image source: https://openart.ai/discovery/sd-1006914875638960178"
  },
  {
    "objectID": "posts/2023-05-29-tips-aws-cert-pearsonvue-online-pakistan.html#challenges-with-pearson-vue-exam-registration",
    "href": "posts/2023-05-29-tips-aws-cert-pearsonvue-online-pakistan.html#challenges-with-pearson-vue-exam-registration",
    "title": "Tips to Register for an AWS Certification Exam with Pearson VUE from Pakistan",
    "section": "Challenges with Pearson VUE Exam Registration",
    "text": "Challenges with Pearson VUE Exam Registration\nNavigating the exam registration process with Pearson VUE can present some difficulties. When you reach the checkout page, you will encounter a few crucial steps that might pose obstacles:\n\nPayment Method Selection: Pearson VUE requires you to choose a payment method, such as VISA, MasterCard, or American Express, for paying the exam fees.\nVouchers and Promotion Codes: You have the option to apply any vouchers or promotion codes you may have. If you have a voucher that covers the full exam fee, it will be deducted automatically, and you can proceed with registration without any issues.\nEntering Payment and Billing Information: You need to provide accurate payment and billing information that matches the details associated with your selected payment method.\n\nHowever, the problem arises when you click ‘Next’ to move forward. At this point, Pearson VUE displays an error message stating, The credit card payment option is not available for this order. If you need additional information, please contact customer service. This error leaves you stuck and unable to proceed with the registration process.\n\n\nIf you find yourself facing this predicament, I recommend closing the tab and following the steps outlined below, which can help you overcome this issue smoothly.\n\nDrawbacks of Pearson VUE\nThere are certain limitations associated with Pearson VUE that you should be aware of. One major limitation is the inability to register for an onsite test center exam through Pearson VUE. They do not accept card payments for this option from Pakistan, and I could not find a workaround for this option within their system. Consequently, the only viable alternative is to opt for the Online Proctoring exam."
  },
  {
    "objectID": "posts/2023-05-29-tips-aws-cert-pearsonvue-online-pakistan.html#step-by-step-guide-to-registering-for-the-online-proctoring-exam",
    "href": "posts/2023-05-29-tips-aws-cert-pearsonvue-online-pakistan.html#step-by-step-guide-to-registering-for-the-online-proctoring-exam",
    "title": "Tips to Register for an AWS Certification Exam with Pearson VUE from Pakistan",
    "section": "Step-by-Step Guide to Registering for the Online Proctoring Exam",
    "text": "Step-by-Step Guide to Registering for the Online Proctoring Exam\nIn this section, I will provide you with a detailed walkthrough of the steps involved in registering for your exam using the Online Proctoring option. By following these steps, you can successfully complete your registration process.\n\nStep 1: Accessing Your AWS Certification Account with CertMetrics\nTo begin the registration process, you need to access your AWS Certification account, which is managed through CertMetrics, a trusted third-party service provider. Follow these steps:\n\nGo to the AWS Certification website.\nLogin to your AWS Certification account using your credentials.\n\nOnce logged in, you will land on your account’s homepage, which will resemble the example shown below:\n\n\nNavigate to your profile page by clicking on “My profile” in the menu bar.\n\nOn the profile page, locate the section titled Mailing address - This section requires input in English (Roman) alphabet. This is where you have provided your address details.\n\nUnder this section, modify the country from “Pakistan” to “Sri Lanka,” as depicted in the screenshot below. Then, click the “Update/Confirm” button at the bottom to save the changes.\n\n\n\n\n\n\n\n\nTip\n\n\n\nYou only need to change the country, and the rest of the details can remain unchanged. There is no need to modify any other information.\n\n\nNow, you may be wondering why you need to change only the country to “Sri Lanka” and why it matters:\n\nPearson VUE uses the country value from this field to verify your country of origin. If it remains as “Pakistan,” card payments will not be accepted. Although Pearson VUE shows the country field again on the “Enter payment and billing information” page, modifying it there will not have any effect, and you will still encounter issues during registration. Therefore, it is crucial to update your country from the CertMetrics profile before proceeding with exam registration through Pearson VUE.\nWhy specifically “Sri Lanka”? While you can attempt other countries, it’s important to note that some countries have taxes applicable to exam fees, which would be added to the total cost. During my own exploration, I tested various options such as India, UAE, Ireland, and Afghanistan before settling on “Sri Lanka.” India, UAE, and Ireland have additional taxes, while Pearson VUE does not accept card payments from Afghanistan. Hence, “Sri Lanka” emerged as a suitable choice without any tax implications.\n\n\n\nStep 2: Registering for the Exam with Pearson VUE\nNow that you have successfully updated your profile, you can proceed with registering for your exam through Pearson VUE without encountering any issues. Remember, this method applies specifically to the Online Proctoring option, as you may still encounter difficulties with the Test Center option.\nHere’s a helpful tip to keep in mind during the registration process:\nOn the “Find an appointment” page, you have the flexibility to select a suitable date and test-taking time of your choice. If the recommended time does not align with your schedule, you can explore additional time slots by clicking the “Explore more times” button. In case you’re unable to find a suitable window, you can try the following approach:\n\nChange the selected exam date to a different random date.\nChange it back to the original desired date.\nTry the “Explore more times” option again.\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nThe available time slots are not fixed and may change periodically. By changing the dates, you can prompt the page to update the available options for you. Repeat this process a few times if necessary until you find a suitable time window for your exam.\n\n\nOnce you reach the “Enter payment and billing” page, follow these steps:\n\nAdd any voucher or promotion codes you have, if applicable.\nSelect the desired payment type.\nVerify your billing information and ensure that the country name in the Location/Region field is reverted to “Pakistan.”\n\nFinally, click “Next,” and this time you will be able to proceed to the final payment page without encountering any obstacles.\n\n\n\nStep 3: Completing the Registration and Updating Your CertMetrics Profile\nAfter successfully completing the payment and registering for the exam, there is one final step to consider. You should return to your CertMetrics profile and revert the country back to “Pakistan.” Follow these instructions:\n\nOnce you have completed the registration process and received confirmation of your exam booking, access your CertMetrics profile again.\nLocate the country field in your profile and change it back to “Pakistan.”\nSave the changes by updating your profile.\n\nBy updating your country back to “Pakistan,” you ensure that your CertMetrics profile reflects your accurate country information.\nBest of luck with your upcoming exam!"
  },
  {
    "objectID": "posts/2025-01-21-streamlit-deployment-aws.html",
    "href": "posts/2025-01-21-streamlit-deployment-aws.html",
    "title": "My Streamlit Deployment Journey on AWS : What Worked, What Didn’t",
    "section": "",
    "text": "image source: https://www.artbreeder.com/image/5172f176d52800751b4463f4d554"
  },
  {
    "objectID": "posts/2025-01-21-streamlit-deployment-aws.html#introduction",
    "href": "posts/2025-01-21-streamlit-deployment-aws.html#introduction",
    "title": "My Streamlit Deployment Journey on AWS : What Worked, What Didn’t",
    "section": "Introduction",
    "text": "Introduction\nI recently embarked on a journey to rapidly prototype a personal LLM application. Like many developers, I was drawn to Streamlit for its simplicity and Python-first approach. The ability to quickly design interactive UIs without wrestling with HTML or CSS made it the perfect fit. However, the road to deploying that Streamlit app on AWS turned out to be much more challenging than I initially anticipated. I naively thought that containerizing my app with Docker would be the golden ticket, and I’d be up and running in no time. I was wrong! This post will take you through my learning experience, detailing the various approaches I tried to host the Streamlit application on the AWS cloud, and share what I learned through the process. From serverless lambda functions to dedicated virtual machines, I’ll describe what worked, what didn’t, and most importantly, why. By the end of the post, I hope that my experience helps you avoid the pitfalls and saves you from many days of frustration. Let’s begin!\n\nEnvironment Details\n\n\nShow the code\nfrom platform import python_version\nimport streamlit\n\nprint(\"python==\" + python_version())\nprint(\"streamlit==\" + streamlit.__version__)\n\n\npython==3.12.8\nstreamlit==1.41.1\n\n\n\n\n\n\n\n\nCode Samples\n\n\n\nAll the code examples used in this post can be found on the GitHub repo 2025-01-21-streamlit-deployment-aws"
  },
  {
    "objectID": "posts/2025-01-21-streamlit-deployment-aws.html#the-initial-assumption-docker-cloud-easy-deployment",
    "href": "posts/2025-01-21-streamlit-deployment-aws.html#the-initial-assumption-docker-cloud-easy-deployment",
    "title": "My Streamlit Deployment Journey on AWS : What Worked, What Didn’t",
    "section": "The Initial Assumption: Docker + Cloud = Easy Deployment",
    "text": "The Initial Assumption: Docker + Cloud = Easy Deployment\nWith my Streamlit application working perfectly on my local machine, I confidently assumed the next step would be straightforward. “Dockerize it, and deploy it to the cloud,” I thought. After all, Docker simplifies application packaging, and cloud platforms like AWS are built for containerized apps, right? My understanding was that the cloud provided a wide array of services to host containerized apps, automatically managing network configurations, and quickly exposing my app through a public URL. I naively believed it would be a seamless and fast process.\nTo understand where I went wrong, it’s helpful to understand the internal architecture of Streamlit. As shown in the diagram below (inspired by the post: Streamlit vs HTMX), a Streamlit application consists of three main parts:\n\na React-based frontend\na Tornado web server\nand the Streamlit library.\n\nThe React frontend is responsible for rendering the user interface, while the Tornado web server manages the application’s logic and communication with the frontend. What’s crucial is that the frontend and the Tornado web server communicate through the WebSocket protocol, enabling real-time, bi-directional communication. This is a key detail which is easy to overlook when starting out.\n\n\nDiving Deep: Streamlit Network Communication\nTo truly grasp the importance of WebSockets for Streamlit, let’s take a look at what’s happening under the hood during its network communication. I have used Wireshark to capture the network communication that happens when a simple Streamlit app (shown below) is loaded in a browser.\n\nAs you can see, the app simply displays “Hello From Streamlit” and is running on localhost:8501.\n\n\n\nShow the code\n\napp.py\n\nimport streamlit as st\n\nst.title(\"Hello From Streamlit\")\n\n\n\nThe screenshot below shows the Wireshark capture of the complete network communication trace.\n\nInitially, the communication starts with the standard HTTP protocol, and it can be seen by the green lines. The browser sends GET requests to fetch resources like JavaScript and CSS files. Then, after initial handshakes, the protocol switches to WebSockets which can be seen in the blue lines. The communication over WebSockets is for the actual application logic. This screenshot highlights the critical role WebSockets play in Streamlit’s operation.\nLet’s take a closer look.\nStage 1: First Contact with Server\nThe first request from the browser is a simple GET request for the main page. The server then sends back an HTML document. This is shown in the below image.\n\nHere, we can see that the server identifies itself as a “TornadoServer” and the content type is text/html. This confirms that the underlying web server in Streamlit is indeed Tornado.\nStage 2: Initial HTML Payload from Server\nThe response body contains the HTML page that needs to be rendered, and the browser then parses it and starts loading the associated CSS and JS files.\n\nThe following code block is the HTML page that is returned by the server.\n\n\nShow the code\n&lt;!--\n Copyright (c) Streamlit Inc. (2018-2022) Snowflake Inc. (2022-2024)\n\n Licensed under the Apache License, Version 2.0 (the \"License\");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n\n     http://www.apache.org/licenses/LICENSE-2.0\n\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an \"AS IS\" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n--&gt;\n\n&lt;!DOCTYPE html&gt;\n&lt;html lang=\"en\"&gt;\n\n&lt;head&gt;\n   &lt;meta charset=\"UTF-8\" /&gt;\n   &lt;meta name=\"viewport\" content=\"width=device-width, initial-scale=1, shrink-to-fit=no\" /&gt;\n   &lt;link rel=\"shortcut icon\" href=\"./favicon.png\" /&gt;\n   &lt;link rel=\"preload\" href=\"./static/media/SourceSansPro-Regular.DZLUzqI4.woff2\" as=\"font\" type=\"font/woff2\"\n       crossorigin /&gt;\n   &lt;link rel=\"preload\" href=\"./static/media/SourceSansPro-SemiBold.sKQIyTMz.woff2\" as=\"font\" type=\"font/woff2\"\n       crossorigin /&gt;\n   &lt;link rel=\"preload\" href=\"./static/media/SourceSansPro-Bold.-6c9oR8J.woff2\" as=\"font\" type=\"font/woff2\"\n       crossorigin /&gt;\n\n   &lt;title&gt;Streamlit&lt;/title&gt;\n\n   &lt;!-- initialize window.prerenderReady to false and then set to true in React app when app is ready for indexing --&gt;\n   &lt;script&gt;\n       window.prerenderReady = false\n   &lt;/script&gt;\n   &lt;script type=\"module\" crossorigin src=\"./static/js/index.Phesr84n.js\"&gt;&lt;/script&gt;\n   &lt;link rel=\"stylesheet\" crossorigin href=\"./static/css/index.CcCsR6ah.css\"&gt;\n&lt;/head&gt;\n\n&lt;body&gt;\n   &lt;noscript&gt;You need to enable JavaScript to run this app.&lt;/noscript&gt;\n   &lt;div id=\"root\"&gt;&lt;/div&gt;\n&lt;/body&gt;\n\n&lt;/html&gt;\n\n\nAfter loading the HTML page, the embedded preload links starts making subsequent HTTP requests for fetching CSS and JavaScript files. Once these files are loaded, the client is ready to switch to the WebSocket protocol.\nStage 3: Switching of Protocols\nThe next image shows that the browser requests the server to switch from HTTP to WebSocket using the 101 Switching Protocols message.\n\nAfter this handshake, the communication from client and server happens using the WebSocket protocol. The image below shows how the application data is sent from the server to the client using WebSockets.\nStage 4: Application Logic\n\nThis shows how the data containing the message “Hello From Streamlit” is exchanged through WebSockets. After that, most of the communication between the client and server consists of ping/pong messages, which indicates that the connection is active. This pattern of sending data using WebSockets is what makes Streamlit apps interactive."
  },
  {
    "objectID": "posts/2025-01-21-streamlit-deployment-aws.html#first-attempt-the-serverless-approach-with-aws-lambda",
    "href": "posts/2025-01-21-streamlit-deployment-aws.html#first-attempt-the-serverless-approach-with-aws-lambda",
    "title": "My Streamlit Deployment Journey on AWS : What Worked, What Didn’t",
    "section": "First Attempt: The Serverless Approach with AWS Lambda",
    "text": "First Attempt: The Serverless Approach with AWS Lambda\nGiven my initial goal of quick prototyping and a preference for serverless architectures, AWS Lambda seemed like the ideal first choice. It promised a cost-effective and low-maintenance solution where I could just upload my code and let AWS handle the infrastructure. I envisioned deploying the Streamlit app as a Lambda function and then expose it to the public internet through an endpoint. My understanding was that this would be the easiest, cheapest, and fastest way to make my app accessible to others.\nTo make this happen, I used the Lambda Web Adapter, which is designed to allow Lambda functions to respond to HTTP requests like a typical web server. As illustrated in the diagram below, the client browser would send requests to the Lambda Function URL.\n\nThe Lambda function, through the Lambda Web Adapter, would then pass these requests to the Docker container running my Streamlit app, which was pushed to Amazon ECR. Following is the Dockerfile I used to build the app container.\n\n\n\nShow the code\n\nDockerfile\n\n# Use the AWS Lambda Python base image\nFROM public.ecr.aws/docker/library/python:3.12.1-slim\n\n# Add the AWS Lambda Web Adapter\nCOPY --from=public.ecr.aws/awsguru/aws-lambda-adapter:0.8.4 /lambda-adapter /opt/extensions/lambda-adapter\n\n# Set the working directory\nWORKDIR /var/task\n\n# Copy app code and dependencies\nCOPY app.py requirements.txt ./\n\n# Install dependencies\nRUN pip install --no-cache-dir -r requirements.txt\n\n# Command to start Streamlit\nCMD [\"streamlit\", \"run\", \"app.py\", \"--server.port=8080\"]\n\n\n\nWhen I deployed my app and loaded the page, to my utter frustration, it was just an empty page. No errors, just a blank screen! I refreshed the page, checked my code, and looked at the logs, and everything seemed to be fine. I was able to get a response from the server, indicating that my request was reaching the Lambda function and that my Streamlit app was running inside a container. My browser was connecting to the server, and the server was responding to HTTP requests. However, I was still unable to see my Streamlit app!\nThis is when I discovered the critical limitation: Lambda functions and the Lambda Web Adapter, by design, operate using a request-response pattern with the HTTP protocol. While the Web Adapter does support HTTP request streaming, it does not support WebSockets. My Streamlit app was loading till the point where the page receives HTTP responses for loading the basic HTML page, JS and CSS files. The application was working as far as HTTP communication was concerned. However, as we discussed before, Streamlit relies heavily on WebSockets for its real-time interactivity. This is evident from the browser’s developer tools screenshot:\n\nYou can see that while the initial HTTP requests for resources like HTML, JS, and CSS files are successful, the crucial “stream” connection using WebSocket fails, hence the empty page. The frontend was trying to establish a WebSocket connection with the Lambda, but since the Lambda Function URL + Web Adapter only support HTTP/s requests and does not support WebSocket, the connection attempt failed, and the page was stuck.\nThis realization was a learning moment and made it clear to me that I needed a completely different approach.\n\n\n\n\n\n\nAWS Lambda Function URL and Lambda Web Adapter: HTTP-Only Support\n\n\n\n\n\nAWS Lambda Function URL and Lambda Web Adapter enable HTTP(S) access to Lambda functions:\n\nLambda Function URL: Creates a direct HTTPS endpoint for invoking Lambda functions, ideal for lightweight, event-driven HTTP-based applications.\nLambda Web Adapter: Bridges Lambda with web frameworks (e.g., Flask, Express.js), routing HTTP requests to the application.\n\nBoth support only HTTP(S) traffic, not protocols like WebSocket or raw TCP/UDP, making them suited for RESTful APIs, webhooks, and serverless web apps requiring HTTP communication."
  },
  {
    "objectID": "posts/2025-01-21-streamlit-deployment-aws.html#second-attempt-containerized-deployment-with-aws-app-runner",
    "href": "posts/2025-01-21-streamlit-deployment-aws.html#second-attempt-containerized-deployment-with-aws-app-runner",
    "title": "My Streamlit Deployment Journey on AWS : What Worked, What Didn’t",
    "section": "Second Attempt: Containerized Deployment with AWS App Runner",
    "text": "Second Attempt: Containerized Deployment with AWS App Runner\nAfter my unsuccessful attempt with AWS Lambda, I knew I needed a solution that handled persistent WebSockets. AWS App Runner seemed like a promising next step, as a managed container platform that would supposedly handle the underlying infrastructure for me. Since my Docker image was already on ECR, I thought switching to App Runner would be easy. With just a few clicks, I could run my containerized app, automatically managing the networking configurations, and access it through a public URL.\nI configured an App Runner service, pointing it to my container image and specifying the necessary port configurations. The service deployed successfully and provided a public URL. However, just like before, my browser showed a blank page!\nThe issue, as with Lambda, was the lack of WebSocket support. AWS App Runner, as highlighted in its documentation below, is designed for HTTP 1.0 and HTTP 1.1 applications and does not provide support for persistent WebSockets connections, and it can be seen in the screenshot below as well. Link to docs here.\n\nThe screenshot from my browser’s developer tools shows the same issue that I observed with Lambda, highlighting the fact that the initial resources were loading using HTTP protocol, but the websocket connection attempt fails:\n\nThis was another learning moment for me and helped me realize that a containerized service alone doesn’t guarantee compatibility with applications like Streamlit, which require WebSockets. It highlighted the importance of understanding the specific requirements of my application before choosing a deployment service.\n\n\n\n\n\n\nAWS App Runner: HTTP Support\n\n\n\n\n\nAWS App Runner provides support for HTTP 1.0 and HTTP 1.1 to handle web requests directed to container instances."
  },
  {
    "objectID": "posts/2025-01-21-streamlit-deployment-aws.html#third-attempt-the-unexpected-success-with-aws-lightsail",
    "href": "posts/2025-01-21-streamlit-deployment-aws.html#third-attempt-the-unexpected-success-with-aws-lightsail",
    "title": "My Streamlit Deployment Journey on AWS : What Worked, What Didn’t",
    "section": "Third Attempt: The Unexpected Success with AWS Lightsail",
    "text": "Third Attempt: The Unexpected Success with AWS Lightsail\nAfter hitting roadblocks with serverless and managed container solutions, I decided to explore AWS Lightsail, and I was drawn to Lightsail for its simplicity and intuitive interface. With a few clicks, you can provision a virtual server, and also it has predictable and budget-friendly pricing, making it an attractive option for various projects. Moreover, unlike App Runner, Lightsail’s container service provides a dedicated virtual server to run your container. And not only that it also gives a public URL to access your web app. Lightsail basically offers an alternative and simplified version of services such as instances, containers, databases, storage, and more to get your web application up and running in just a few clicks. This makes it an excellent choice for rapid prototyping, proof-of-concept projects, and developers seeking a hassle-free experience.\nI was initially skeptical because the official Lightsail documentation clearly states that its public endpoints only support HTTPS traffic and do not support TCP or UDP, which also meant it does not provide WebSockets. Here is the image of the documentation. Link to docs here\n\nHowever, I had also come across blog posts where others had successfully deployed WebSocket applications on Lightsail. I decided to give it a try. I’ll also link one such post here Link to the blog post. Since my container image was already in ECR, deploying it to Lightsail was straightforward.\nI configured my Lightsail container service, provided my ECR image URI, and set up the correct port mappings. You may use the linked post to find all the required steps Link to the blog post. The entire process was remarkably simple and I was very hopeful, this time.\n\n\n\n\n\n\n\n\n\n\nWhen I checked the public URL, my Streamlit app was working! The app was not showing a blank page anymore, and the “Hello From Streamlit” message was properly rendered. I was surprised to see that it was working, and also curious to see how it worked since Lightsail does not directly provide WebSockets support. I looked at the browser’s developer tools and confirmed that the WebSocket connection was indeed established successfully!\n\nYou can see from the image above that the initial HTTP requests are completed to load the basic html and javascripts and then the application was able to successfully establish a stream connection using websockets.\nIt turned out that even though Lightsail’s public endpoint is HTTP based, it somehow transparently handles WebSocket connections for containerized applications. This experience was a clear indication that sometimes documentation might not capture the entire picture, and it is important to try things out."
  },
  {
    "objectID": "posts/2025-01-21-streamlit-deployment-aws.html#the-final-piece-deploying-streamlit-with-fargate-and-aws-cdk",
    "href": "posts/2025-01-21-streamlit-deployment-aws.html#the-final-piece-deploying-streamlit-with-fargate-and-aws-cdk",
    "title": "My Streamlit Deployment Journey on AWS : What Worked, What Didn’t",
    "section": "The Final Piece: Deploying Streamlit with Fargate and AWS CDK",
    "text": "The Final Piece: Deploying Streamlit with Fargate and AWS CDK\nWhile the success with Lightsail was encouraging, I still felt the need for a solution that gave me greater transparency and control over my infrastructure. Lightsail is fantastic for quick deployments and prototypes, but it still abstracts away certain complexities that I wanted to manage myself. I started researching alternative approaches, keeping my desire for rapid setup and customization in mind. This led me to a solution based on AWS Fargate, fronted by an Application Load Balancer, and using the AWS CDK for infrastructure as code.\nAs illustrated in the architectural diagram below, the client browser connects to an Application Load Balancer, which then routes the traffic to the Fargate container running the Streamlit application.\n\nAt first, the thought of setting up a full-fledged ECS cluster with Fargate and a load balancer seemed like an overwhelming task. The process involves provisioning various resources like VPC, security groups, load balancers, and more. However, I then discovered the aws_ecs_patterns module within AWS CDK, which provides a convenient way to create common application patterns using a single line of code. One such pre-built pattern is ApplicationLoadBalancedFargateService, which creates all the necessary resources and then deploys your container on it.\nThe beauty of this approach is that while it takes care of setting up all the underlying infrastructure, it also gives you complete control. Everything is defined and deployed through code using CDK. You can version control your infrastructure, enabling easy management and scalability. This combination of control, simplicity, and automation made this the ideal choice. My experience with the previous three AWS services gave me confidence in my understanding of the problem. Thus, armed with this knowledge and the power of AWS CDK, I decided to build the custom stack to deploy my application.\nI have used this small piece of code to create the entire stack.\n\n\n\nShow the code\n\nstreamlit_fargate_stack.py\n\nvpc = ec2.Vpc(\n    self,\n    \"StreamlitFargateVpc\",\n    max_azs=2,  # Min=2. Default is all AZs in the region\n)\n\n# Create an ECS Cluster\ncluster = ecs.Cluster(self, \"StreamlitFargateCluster\", vpc=vpc)\n\n# Define a Fargate Task Definition and Service\nfargate_service = ecs_patterns.ApplicationLoadBalancedFargateService(\n    self,\n    \"StreamlitFargateService\",\n    cluster=cluster,\n    task_image_options={\n        \"image\": image,\n        \"container_port\": 8501,  # Streamlit default port=8501. Should be adjusted based on Dockerfile\n    },\n    public_load_balancer=True,\n)\n\n\n\nWhile the code looks quite simple, a lot is happening behind the scenes. If you check the CloudFormation console, you will see the resources created.\n\nAs you can see from the image, the code has created 41 resources, including VPC, subnets, route tables, security groups, Elastic IPs, NAT Gateways, Internet Gateways, and IAM roles. This is a complex stack that was created with a very small piece of code.\nAfter deploying the stack using CDK, I was able to access my Streamlit application through a public URL. And this time the WebSockets were working perfectly!\n\nAs you can see, the app is working as expected, the connection has been established and the “Hello From Streamlit” message is shown on the page. This was my final and successful approach in deploying the Streamlit application.\n\n\n\n\n\n\nWant Full Stack Control?\n\n\n\n\n\nIf you want more control and flexibility, you may built the entire stack on your own without relying on the aws_ecs_patterns module. If you want that, check out this post from the AWS Blog: Build and deploy a UI for your generative AI applications with AWS and Python. The GitHub repo associated with this post builds the complete stack using individual components."
  },
  {
    "objectID": "posts/2025-01-21-streamlit-deployment-aws.html#a-practical-look-at-pricing-lightsail-vs.-fargate",
    "href": "posts/2025-01-21-streamlit-deployment-aws.html#a-practical-look-at-pricing-lightsail-vs.-fargate",
    "title": "My Streamlit Deployment Journey on AWS : What Worked, What Didn’t",
    "section": "A Practical Look at Pricing: Lightsail vs. Fargate",
    "text": "A Practical Look at Pricing: Lightsail vs. Fargate\nWhile the custom Fargate solution provides the control and transparency I was looking for, it’s important to consider the cost implications. Let’s compare it with the simplicity and pricing of Lightsail to see how they stack up.\nLightsail offers a very straightforward and transparent pricing structure. For my initial Streamlit deployment, I used the container service with the Nano option, which has a fixed cost of 7 USD per month. You can see its pricing structure in the screenshot below.\n\nThis makes it a very appealing choice for quick prototypes and projects that need a budget-friendly setup.\nNow let’s take a look at the costs associated with the custom Fargate solution. Although Fargate itself is relatively inexpensive, the overall cost can become significantly higher due to other resources like NAT Gateways, Load Balancers, and Elastic IPs that get created along with it. In my setup, the Fargate cost was almost negligible, less than 1 USD. The rest of the cost was distributed in these other services.\nThe most expensive component in this solution is the NAT Gateways. Since my setup involved deploying the app across two availability zones (aws_ecs_patterns enforces high availability), the solution also created two NAT gateways, one for each AZ. The minimum cost of each NAT Gateway is around 32 USD per month, totaling around 64 USD per month for this setup.\n\nAs you can see from the image, NAT Gateway charges by hour, and it costs $0.045 per hour. If we assume a minimum usage of 30 days a month, it amounts to $32 per month for a single NAT gateway. This is 4 times more expensive than the entire cost of the Lightsail container service.\nAnother significant cost factor is the Application Load Balancer (ALB). With minimum usage, it roughly costs 16 USD per month, which also charges hourly.\n\nLooking at the image, it can be seen that the cost of the ALB is $0.0225 per ALB-hour. For minimum usage it roughly translates to around $16 USD a month.\nLastly, two Elastic IPs (EIPs) are created by the stack (one for each NAT gateway). The cost for these two EIPs is around 7.30 USD per month.\nWhen you sum it up, while the actual cost of running the Fargate container itself is barely around 1 USD per month, the cost of NAT gateways, Load Balancer, and EIPs adds up significantly. For this simple setup, I estimate that the cost of this custom Fargate solution would easily amount to around 90 USD per month. This is much higher than the 7 USD per month that I estimated with the Lightsail container service. It’s important to note that these costs might vary based on usage, but even with minimal activity, the custom Fargate solution will remain significantly more expensive than the Lightsail option.\nWhile the custom Fargate setup does provide more control and transparency, the cost implications are something that you should keep in mind for running your prototypes."
  },
  {
    "objectID": "posts/2025-01-21-streamlit-deployment-aws.html#lessons-learned-and-looking-ahead",
    "href": "posts/2025-01-21-streamlit-deployment-aws.html#lessons-learned-and-looking-ahead",
    "title": "My Streamlit Deployment Journey on AWS : What Worked, What Didn’t",
    "section": "Lessons Learned and Looking Ahead",
    "text": "Lessons Learned and Looking Ahead\nMy journey to deploy a Streamlit application on AWS was filled with unexpected challenges, and valuable learning moments. I started with a simple assumption but quickly learned the critical role of WebSockets for Streamlit applications, and the limitations of AWS Lambda and App Runner, which only support HTTP requests. Though AWS Lightsail provided a surprisingly simple solution for deploying the app, I decided to build my own solution with AWS CDK and Fargate for more control.\nThis journey highlighted the importance of understanding your application requirements, and the costs associated with them. While a custom Fargate deployment provides the most control, it also came with hidden costs that made it far more expensive than the simpler Lightsail setup. The most important takeaway for me is that there is not a one size fits all solution, and each deployment strategy comes with its own limitations, and it is critical to understand what we are doing.\nUltimately, the right approach depends on your specific needs. I hope that by sharing my experiences, you can avoid some of the pitfalls I encountered. Thank you for joining me on this journey!"
  },
  {
    "objectID": "posts/2025-01-29-pure-python-web-server.html",
    "href": "posts/2025-01-29-pure-python-web-server.html",
    "title": "Building a Web Server From Scratch in Pure Python",
    "section": "",
    "text": "image source: https://www.artbreeder.com/image/ea82e4713d9d1468ae4c68676e3c"
  },
  {
    "objectID": "posts/2025-01-29-pure-python-web-server.html#introduction",
    "href": "posts/2025-01-29-pure-python-web-server.html#introduction",
    "title": "Building a Web Server From Scratch in Pure Python",
    "section": "Introduction",
    "text": "Introduction\nHave you ever wondered what’s happening under the hood when you access a website? I certainly have! That curiosity led me down a rabbit hole – building a web server from scratch using nothing but pure Python. Now, before you get too excited, let me be clear: this isn’t about creating the next Nginx. My goal is much simpler, and hopefully more insightful: to understand the fundamental concepts of networking and concurrency that power the web.\nThink of this as an educational adventure. We’re going to ditch the fancy frameworks and get our hands dirty with the raw building blocks. We’ll be exploring different ways to construct a basic web server, starting with Python’s built-in http.server and then diving deeper into sockets, threading, selectors, and finally, the asynchronous magic of asyncio.\nAlong the way, we’ll benchmark each server using Apache Benchmark (ab) on an AWS t2.micro EC2 instance. EC2 machine is though entirely optional, and you can run these benchmarks on your local machine as well. I used it to give us a consistent way to compare performance and replicate. So, join me as we embark on this journey. It’s all about learning, experimenting, and maybe, just maybe, gaining a newfound appreciation for what goes into serving up those cat 🐈 videos you love.\n\nEnvironment Details\nAll code examples provided with this post were tested with Python 3.9.20.\n\n\n\n\n\n\nCode Samples\n\n\n\nAll the code examples used in this post can be found on the GitHub repo 2025-01-29-pure-python-web-server"
  },
  {
    "objectID": "posts/2025-01-29-pure-python-web-server.html#attempt-1---the-simplicity-of-http.server",
    "href": "posts/2025-01-29-pure-python-web-server.html#attempt-1---the-simplicity-of-http.server",
    "title": "Building a Web Server From Scratch in Pure Python",
    "section": "Attempt 1 - The Simplicity of http.server",
    "text": "Attempt 1 - The Simplicity of http.server\nTo start our journey, I wanted to establish a really simple baseline. Python’s http.server module is perfect for this. It’s like the ‘Hello, World!’ of web servers – incredibly easy to set up. Let’s take a look at the code.\n\n\n\nhttpd/server.py\n\nimport http.server\nimport time\nfrom http import HTTPStatus\n\nPORT = 8000\n\nclass MyHandler(http.server.BaseHTTPRequestHandler):\n    def do_GET(self):\n        # Simulate a database call or some processing\n        time.sleep(0.1)\n\n        self.send_response(HTTPStatus.OK)\n        self.send_header(\"Content-type\", \"text/html\")\n        self.end_headers()\n        response_html = \"\"\"\n            &lt;html&gt;\n                &lt;head&gt;\n                    &lt;title&gt;My Basic Server&lt;/title&gt;\n                &lt;/head&gt;\n                &lt;body&gt;\n                    &lt;h1&gt;Hello from my basic server&lt;/h1&gt;\n                &lt;/body&gt;\n            &lt;/html&gt;\n        \"\"\"\n        self.wfile.write(response_html.encode())\n\nif __name__ == \"__main__\":\n    with http.server.HTTPServer((\"\", PORT), MyHandler) as httpd:\n        print(f\"Serving on port {PORT}\")\n        httpd.serve_forever()\n\n\nAs you can see, it’s remarkably short.\n\nWe import http.server, define a handler class MyHandler that inherits from BaseHTTPRequestHandler, and override the do_GET method.\nThis method is called whenever the server receives a GET request.\nInside, I’ve added time.sleep(0.1) to mimic a slow database call or some processing – because real-world servers aren’t instant.\nThen, we construct a simple HTML response and send it back.\n\nTo run this, just type python server.py in your terminal. You should see “Serving on port 8000”. To test it, open your browser or use curl http://localhost:8000. You should see the “Hello from my basic server” message.\nNow, let’s see how it performs under a bit of load. I used Apache Benchmark with this command:\n\nab -n 1000 -c 10 http://localhost:8000/.\n\nThis sends 1000 requests with a concurrency of 10. Here are the results I got.\nServer Software:        BaseHTTP/0.6\nServer Hostname:        3.90.155.197\nServer Port:            8000\n\nDocument Path:          /\nDocument Length:        249 bytes\n\nConcurrency Level:      10\nTime taken for tests:   114.367 seconds\nComplete requests:      1000\nFailed requests:        0\nTotal transferred:      366000 bytes\nHTML transferred:       249000 bytes\nRequests per second:    8.74 [#/sec] (mean)\nTime per request:       1143.666 [ms] (mean)\nTime per request:       114.367 [ms] (mean, across all concurrent requests)\nTransfer rate:          3.13 [Kbytes/sec] received\nLooking at these numbers, especially the Requests per second: 8.74, it’s clear that while http.server is incredibly easy to use, it’s not exactly a performance beast. This is because it’s a very basic, single-threaded server. When time.sleep(0.1) is running for one request, the entire server is essentially waiting. This is what we call blocking I/O. One operation blocks everything else.\nThis simple server gives us a starting point. It works, but it’s clearly not designed for handling many requests concurrently. In the next step, we’ll dive into raw sockets to understand things at a lower level and see if we can improve performance"
  },
  {
    "objectID": "posts/2025-01-29-pure-python-web-server.html#attempt-2---web-server-with-raw-sockets",
    "href": "posts/2025-01-29-pure-python-web-server.html#attempt-2---web-server-with-raw-sockets",
    "title": "Building a Web Server From Scratch in Pure Python",
    "section": "Attempt 2 - Web Server with Raw Sockets",
    "text": "Attempt 2 - Web Server with Raw Sockets\nOkay, http.server was easy, but it felt a bit like magic, right? To really understand what’s going on, I decided to ditch the convenience and build a server using raw sockets. This means we’re going to interact directly with the network, handling connections and HTTP protocol details ourselves. Let’s look at the code:\n\n\n\nsockets/server.py\n\nimport socket\nimport time\n\ndef handle_request(conn, addr):\n    try:\n        request_data = conn.recv(1024).decode()\n        if request_data:\n            # Simulate a database call or some processing\n            time.sleep(0.1)  # 100 milliseconds delay\n            response_html = \"\"\"\n            &lt;html&gt;\n                &lt;head&gt;\n                    &lt;title&gt;My Basic Server&lt;/title&gt;\n                &lt;/head&gt;\n                &lt;body&gt;\n                    &lt;h1&gt;Hello from my basic server&lt;/h1&gt;\n                &lt;/body&gt;\n            &lt;/html&gt;\n        \"\"\"\n            response = \"HTTP/1.1 200 OK\\r\\n\"\n            response += \"Content-Type: text/html\\r\\n\"\n            response += f\"Content-Length: {len(response_html)}\\r\\n\"\n            response += \"\\r\\n\"\n            response += response_html\n            conn.sendall(response.encode())\n        else:\n            print(f\"Client {addr} sent no data\")\n    except Exception as e:\n        print(f\"Error handling client {addr}: {e}\")\n    finally:\n        conn.close()\n\n\nif __name__ == \"__main__\":\n    HOST = \"\"  # Listen on all available interfaces\n    PORT = 8000\n    with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:\n        s.bind((HOST, PORT))\n        s.listen()\n        print(f\"Listening on port {PORT}\")\n        while True:\n            conn, addr = s.accept()\n            with conn:\n                handle_request(conn, addr)\n\n\nThis code is a bit more involved, but still pretty straightforward.\n\nWe start by importing the socket module.\nThe socket.socket() function is used to create a new socket object in Python. It takes two main arguments: socket.socket(family, type). I used socket.socket(socket.AF_INET, socket.SOCK_STREAM) where\n\nsocket.AF_INET specifies the address family, meaning the socket will use IPv4. If you wanted an IPv6 socket, you would use socket.AF_INET6\nsocket.SOCK_STREAM specifies the socket type, meaning it will be a TCP (stream-based) socket. If you wanted a UDP socket, you would use socket.SOCK_DGRAM\n\nThen, s.bind((HOST, PORT)) binds the socket to listen on all available interfaces (HOST = ““) and port 8000.\ns.listen() puts the socket into listening mode, ready to accept incoming connections.\nThe while True: loop is the heart of our server. s.accept() waits (blocks) for a new connection and, when one arrives it returns\n\na new socket object (conn) for sending/receiving data with that client\naddr contains the client’s (IP address, port) tuple.\n\nWe then call handle_request(conn, addr) to process this connection.\nInside handle_request\n\nconn.recv(1024) attempts to receive up to 1024 bytes of data from the client – this is where we get the HTTP request. If the client sends less than 1024 bytes, it reads whatever is available. If the request is longer than 1024 bytes, only the first part is read (you may need a loop for large requests).\n.decode() we decode it i.e. convert the received raw bytes into a string using UTF-8 encoding, and check if there’s any data. Data sent over sockets is binary, so it needs decoding.\n\nJust like before, time.sleep(0.1) simulates processing work.\nThen comes the part where we manually construct the HTTP response. We need to include the status line (HTTP/1.1 200 OK), headers like Content-Type, Content-Length, and the HTML body, all separated by as per HTTP protocol. Finally, conn.sendall(response.encode()) sends the encoded (or binary) response back to the client, and conn.close() closes the connection.\n\n\n\n\n\n\n\nOptimal Buffer Size (recv(N))\n\n\n\n\n\nThe optimal size for recv() depends on several factors, such as the expected request size, network performance, and memory efficiency. Here’s how you can determine the best size:\n\n\n\n\n\n\n\nBuffer Size (recv(N))\nUse Case\n\n\n\n\n1024 (1 KB)\nWorks well for small HTTP requests (GET requests, simple headers).\n\n\n2048 (2 KB)\nGood for typical HTTP requests with longer headers.\n\n\n4096 (4 KB)\nOften used as a standard buffer size for web servers.\n\n\n8192 (8 KB)\nSuitable for handling larger requests efficiently.\n\n\n16384+ (16 KB or more)\nUsed for high-performance servers or large payloads (e.g., file uploads, API requests).\n\n\n\nChoosing the Optimal Size\n\nFor a simple web server: 4096 (4 KB) or 8192 (8 KB) is a good choice because:\n\nMost HTTP request headers are under 8 KB.\nThis balances efficiency and memory usage.\n\nFor handling large requests (e.g., POST with form data or JSON):\n\nUse 8192 (8 KB) or more.\nImplement a loop to dynamically read the entire request.\n\n\n\n\n\nLet’s benchmark it with the same ab command, and here are the results:\nConcurrency Level:      10\nTime taken for tests:   100.414 seconds\nComplete requests:      1000\nFailed requests:        0\nTotal transferred:      314000 bytes\nHTML transferred:       249000 bytes\nRequests per second:    9.96 [#/sec] (mean)\nTime per request:       1004.143 [ms] (mean)\nTime per request:       100.414 [ms] (mean, across all concurrent requests)\nTransfer rate:          3.05 [Kbytes/sec] received\nLooking at Requests per second: 9.96, the performance is actually slightly better than http.server (8.74 req/sec), but still in the same ballpark. It’s not a significant improvement. Why? Because we are still using blocking sockets and a single process. Just like before, time.sleep(0.1) in handle_request blocks the entire server from handling other requests while it’s waiting. We are still processing requests sequentially, one after another.\nBuilding with raw sockets gives us more control and a deeper understanding, but in terms of concurrency and performance, this version is not fundamentally different from http.server. In the next step, we’ll introduce threads to handle multiple requests concurrently and hopefully see a real jump in performance."
  },
  {
    "objectID": "posts/2025-01-29-pure-python-web-server.html#attempt-3---threading-to-the-rescue",
    "href": "posts/2025-01-29-pure-python-web-server.html#attempt-3---threading-to-the-rescue",
    "title": "Building a Web Server From Scratch in Pure Python",
    "section": "Attempt 3 - Threading to the Rescue",
    "text": "Attempt 3 - Threading to the Rescue\nThe single-threaded nature of our previous servers is clearly the bottleneck. To handle multiple requests concurrently, the classic solution is threading. Let’s see how threading can boost our server’s performance. Here’s the code:\n\n\n\nthreading/server.py\n\nimport socket\nimport threading\nimport time\n\ndef handle_request(conn, addr):\n    try:\n        request_data = conn.recv(1024).decode()\n        if request_data:\n            # Simulate a database call or some processing\n            time.sleep(0.1)  # 100 milliseconds delay\n            response_html = \"\"\"\n            &lt;html&gt;\n                &lt;head&gt;\n                    &lt;title&gt;My Basic Server&lt;/title&gt;\n                &lt;/head&gt;\n                &lt;body&gt;\n                    &lt;h1&gt;Hello from my basic server&lt;/h1&gt;\n                &lt;/body&gt;\n            &lt;/html&gt;\n        \"\"\"\n            response = \"HTTP/1.1 200 OK\\r\\n\"\n            response += \"Content-Type: text/html\\r\\n\"\n            response += f\"Content-Length: {len(response_html)}\\r\\n\"\n            response += \"\\r\\n\"\n            response += response_html\n            conn.sendall(response.encode())\n        else:\n            print(f\"Client {addr} sent no data\")\n    except Exception as e:\n        print(f\"Error handling client {addr}: {e}\")\n    finally:\n        conn.close()\n\n\ndef threaded_server():\n    HOST = \"\"\n    PORT = 8000\n    with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:\n        s.bind((HOST, PORT))\n        s.listen()\n        print(f\"Listening on port {PORT}\")\n        while True:\n            conn, addr = s.accept()\n            thread = threading.Thread(target=handle_request, args=(conn, addr))\n            thread.start()\n\n\nif __name__ == \"__main__\":\n    threaded_server()\n\n\nThe core handle_request function remains the same as in sockets/server.py. The key change is in the threaded_server() function. Inside the while True: loop, after accepting a connection with conn, addr = s.accept(), instead of directly calling handle_request, we now create a new thread:\n\nthread = threading.Thread(target=handle_request, args=(conn, addr)).\n\nWe pass the handle_request function as the target for the thread, and the connection object “conn” and address “addr” as arguments.\nThen, thread.start() starts the new thread, which will execute handle_request concurrently.\n\n\nThis means that when a new connection comes in, the main thread quickly accepts it and offloads the actual request handling to a separate thread. The main thread then immediately goes back to listening for new connections. This allows us to handle multiple requests seemingly at the same time.\nNow, let’s benchmark it, and here are the results:\nConcurrency Level:      10\nTime taken for tests:   11.085 seconds\nComplete requests:      1000\nFailed requests:        0\nTotal transferred:      314000 bytes\nHTML transferred:       249000 bytes\nRequests per second:    90.21 [#/sec] (mean)\nTime per request:       110.852 [ms] (mean)\nTime per request:       11.085 [ms] (mean, across all concurrent requests)\nTransfer rate:          27.66 [Kbytes/sec] received\nWow! Look at the Requests per second: 90.21. That’s a huge jump compared to our previous servers (around 9 req/sec)! Threading has made a massive difference. The time per request has also dropped significantly. This is because now, while one thread is waiting for time.sleep(0.1) to finish, other threads can continue processing other requests concurrently. We are no longer blocking the entire server on a single request.\nThreading is a simple way to achieve concurrency in Python and is very effective for I/O-bound tasks like web servers, where the server spends a lot of time waiting for network operations or, in our case, our simulated database call. However, it’s important to remember that Python’s Global Interpreter Lock (GIL) can limit the effectiveness of threads for CPU-bound tasks. Also, there’s overhead associated with creating and managing threads.\nDespite these limitations, for our simple I/O-bound web server, threading provides a dramatic performance improvement. In the next sections, we’ll explore asynchronous I/O using selectors and asyncio to see if we can achieve even better concurrency and efficiency."
  },
  {
    "objectID": "posts/2025-01-29-pure-python-web-server.html#attempt-4---selectors-with-blocking-time-simulation",
    "href": "posts/2025-01-29-pure-python-web-server.html#attempt-4---selectors-with-blocking-time-simulation",
    "title": "Building a Web Server From Scratch in Pure Python",
    "section": "Attempt 4 - Selectors with Blocking Time Simulation",
    "text": "Attempt 4 - Selectors with Blocking Time Simulation\nThreading significantly improved concurrency, but there’s another approach: asynchronous I/O. Instead of threads, asynchronous I/O allows a single thread to handle multiple connections by using non-blocking sockets and event notifications. Let’s explore this with Python’s selectors module. We’ll start with server_blocking.py, which introduces selectors but still uses a blocking time.sleep to simulate work – this is intentional to highlight the structure of a selector-based server, even with a blocking operation.\nHere’s the code:\n\n\n\nselectors/server_blocking.py\n\nimport socket\nimport selectors\nimport time\n\nselector = selectors.DefaultSelector()\n\n\ndef send_response(conn, addr, response):\n    \"\"\"Send the response when the socket is ready for writing.\"\"\"\n    try:\n        conn.sendall(response.encode())\n    except Exception as e:\n        print(f\"Error sending response to {addr}: {e}\")\n    finally:\n        selector.unregister(conn)\n        conn.close()\n\n\ndef handle_request(conn, addr):\n    try:\n        request_data = conn.recv(1024).decode()\n        if request_data:\n            response_html = \"\"\"\n            &lt;html&gt;\n                &lt;head&gt;\n                    &lt;title&gt;My Basic Server&lt;/title&gt;\n                &lt;/head&gt;\n                &lt;body&gt;\n                    &lt;h1&gt;Hello from my basic server&lt;/h1&gt;\n                &lt;/body&gt;\n            &lt;/html&gt;\n        \"\"\"\n            response = \"HTTP/1.1 200 OK\\r\\n\"\n            response += \"Content-Type: text/html\\r\\n\"\n            response += f\"Content-Length: {len(response_html)}\\r\\n\"\n            response += \"\\r\\n\"\n            response += response_html\n\n            time.sleep(0.1)  # &lt;-- Ideally, replace this with a non-blocking timer\n            selector.modify(\n                conn,\n                selectors.EVENT_WRITE,\n                lambda conn: send_response(conn, addr, response),\n            )\n        else:\n            print(f\"Client {addr} sent no data\")\n            selector.unregister(conn)\n            conn.close()\n    except Exception as e:\n        print(f\"Error handling client {addr}: {e}\")\n        selector.unregister(conn)\n        conn.close()\n\n\ndef accept_connection(sock):\n    conn, addr = sock.accept()\n    conn.setblocking(False)  # Set the connection to non-blocking\n    selector.register(\n        conn, selectors.EVENT_READ, lambda conn: handle_request(conn, addr)\n    )\n\n\ndef asynchronous_server():\n    HOST = \"\"\n    PORT = 8000\n    with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as sock:\n        sock.bind((HOST, PORT))\n        sock.listen()\n        sock.setblocking(False)  # Set the main socket to non-blocking\n\n        selector.register(sock, selectors.EVENT_READ, accept_connection)\n\n        print(f\"Listening on port {PORT}\")\n\n        while True:\n            events = selector.select()  # This function returns all the events\n            for key, _ in events:\n                callback = key.data  # This is the function we registered earlier\n                callback(key.fileobj)  # Execute callback with the socket\n\n\nif __name__ == \"__main__\":\n    asynchronous_server()\n\n\nThis code introduces several new concepts. Let’s unpack them before moving forward.\n\n\n\n\n\n\nUnpack Selectors — High-level I/O multiplexing\n\n\n\n\n\n\nWhat Are Selectors and Why Do We Need Them?\nSelectors are a way to efficiently manage multiple sockets at the same time without blocking the program. Instead of waiting for one socket to send or receive data before moving to the next, a selector watches multiple sockets at once and tells the program when each socket is ready. This makes it possible to handle thousands of connections in a single thread, saving system resources and improving performance.\nWe need selectors because traditional blocking sockets make the server wait for each client one at a time, which is slow. Instead of creating a separate thread for each connection (which is expensive), selectors allow us to handle all connections efficiently in an event-driven manner.\n\n\nHow Are Selectors Different from Sockets?\nA socket is just an endpoint for sending and receiving data over a network, like a phone line for communication. Normally, a server listens for connections and then handles each socket one at a time (blocking) or spawns a thread for each socket (multi-threading).\nImagine a basic blocking server that handles one client at a time.\nconn, addr = sock.accept()  # Blocks until a connection is received\ndata = conn.recv(1024)  # Blocks until data is received\nconn.sendall(response)  # Blocks until data is sent\nconn.close()\nEach step blocks the execution, meaning the server can’t handle other clients until the current one is fully processed. This becomes a huge bottleneck!\nA selector, on the other hand, is a tool that monitors multiple sockets at once. Instead of blocking or creating threads, it checks all registered sockets and only acts on the ones that are ready. Selectors provide a non-blocking, event-driven approach. Instead of waiting for each client, the server registers multiple sockets with a selector and processes them only when they are ready. This makes it much more efficient, especially when dealing with a large number of clients.\nThink of it like this:\n\nBasic sockets: You call each person on the phone one by one.\nSockets + threads: You hire an assistant for each phone call.\nSelectors: You put all calls on hold and switch between them only when they need attention.\n\n\n\nHow Do Selectors Compare with Sockets + Threads?\nUsing sockets with threads, the server creates a new thread for each client connection. This works well for a small number of clients, but as the number grows, CPU and memory usage skyrocket due to context switching and thread management. If thousands of clients connect, the system slows down or crashes because threads take too much memory.\nSelectors solve this by handling all connections in a single thread. Instead of creating a new thread per client, it waits for any socket to be ready and processes it immediately. This allows a single-threaded server to handle tens of thousands of connections efficiently, using far less memory and CPU.\nFor Comparison\n\n\n\n\n\n\n\n\n\nFeature\nSockets (Blocking)\nSockets + Threads\nSelectors\n\n\n\n\nConcurrency\nLow (one at a time)\nMedium (one thread per client)\nHigh (handles many clients in one thread)\n\n\nCPU Usage\nLow (but slow)\nHigh (many threads)\nLow (single-threaded, event-driven)\n\n\nMemory Usage\nLow\nHigh (each thread takes memory)\nVery Low\n\n\nScalability\nPoor\nMedium (limited by threads)\nExcellent (handles thousands of clients)\n\n\nUse Case\nSmall servers\nModerate workload\nHigh-performance servers (e.g., Nginx, chat apps)\n\n\n\n\n\n\n\nAfter unpacking Selectors and their importance, let’s move back to our code again.\n\nFirst, we import selectors and create a selector object using selectors.DefaultSelector().\n\nWhat is selectors.DefaultSelector()? It provides a high-level abstraction for I/O multiplexing, meaning it allows monitoring multiple sockets (or file descriptors) for events like:\n\nRead readiness (EVENT_READ) → Data is available to read\nWrite readiness (EVENT_WRITE) → The socket is ready to send data\n\nselectors.DefaultSelector() automatically picks the best available system-dependent selector mechanism. This ensures optimal performance depending on the operating system:\n\n\n\n\nOS\nSelector Used\n\n\n\n\nWindows\nSelectSelector (based on select())\n\n\nLinux\nEpollSelector (based on epoll())\n\n\nmacOS\nKqueueSelector (based on kqueue())\n\n\n\n\n\nIn asynchronous_server(), we create a socket, bind, and listen, just like before. Crucially, we set both the listening socket (sock.setblocking(False)) and the connection socket (conn.setblocking(False) in accept_connection()) to non-blocking mode. This means that operations like sock.accept() and conn.recv() will return immediately, even if there’s no data or connection ready.\nWe register the listening socket with the selector: selector.register(sock, selectors.EVENT_READ, accept_connection). This tells the selector to monitor “sock” for read events (new connections) and call the accept_connection function when a connection is ready. Similarly, in accept_connection, we register each new connection socket (conn) with the selector to monitor for EVENT_READ and call handle_request when data is ready to be read from that connection.\n\n\n\n\n\n\n\nWhy Do We Register Twice for selectors.EVENT_READ?\n\n\n\n\n\n1st Registration: The Listening Socket (sock)\nselector.register(sock, selectors.EVENT_READ, accept_connection)\n\nThis registers the main server socket (sock) with the selector.\nThe event type is selectors.EVENT_READ, meaning the selector will monitor when a new client connection is ready to be accepted.\nWhen a new connection arrives, the accept_connection function is called.\n\n2nd Registration: Client Connection (conn)\nselector.register(\n    conn, selectors.EVENT_READ, lambda conn: handle_request(conn, addr)\n)\n\nThis registers the newly accepted client socket (conn).\nAgain, the event type is selectors.EVENT_READ, meaning the selector will monitor when the client sends data (HTTP request).\nWhen data arrives, the handle_request function is called.\n\nWhy Is This Necessary? Each socket has a different role:\n\nThe server socket (sock) listens for new connections.\nThe client socket (conn) listens for incoming data from the client.\n\nIf we only registered the server socket, we wouldn’t be able to read incoming HTTP requests from clients. Likewise, if we didn’t register the client socket, we wouldn’t know when a client sends data.\n\n\n\n\nThe while True: loop in asynchronous_server is the event loop.\n\nevents = selector.select() waits (but importantly, not blocking the entire thread) until one or more registered sockets are ready.\nIt returns a list of events. For each event, callback = key.data retrieves the callback function we registered (e.g., accept_connection or handle_request), and callback(key.fileobj) executes that function, passing the socket object as an argument.\n\nNow, look at handle_request. After receiving the request and preparing the response, we still have time.sleep(0.1). And then, instead of sending the response directly, we register the connection conn with the selector for EVENT_WRITE and associate it with the send_response callback: selector.modify(...). The intention here is to send the response when the socket is ready for writing… however, because of time.sleep(0.1) being before selector.modify, we are still blocking during the sleep.\n\nLet’s benchmark it.\nConcurrency Level:      10\nTime taken for tests:   100.484 seconds\nComplete requests:      1000\nFailed requests:        0\nTotal transferred:      314000 bytes\nHTML transferred:       249000 bytes\nRequests per second:    9.95 [#/sec] (mean)\nTime per request:       1004.840 [ms] (mean)\nTime per request:       100.484 [ms] (mean, across all concurrent requests)\nTransfer rate:          3.05 [Kbytes/sec] received\nThe Requests per second: 9.95 is similar to our raw sockets server, and even slightly worse than threaded server. This is not surprising. Even though we’ve introduced selectors and non-blocking sockets, the time.sleep(0.1) in handle_request is still blocking the event loop. While selector.select() itself is non-blocking and efficient for handling multiple connections, our simulated work is still synchronous and serializing request processing.\nThis selectors/server_blocking.py example, as written, doesn’t give us the performance benefits of asynchronous I/O because of the blocking time.sleep. However, it’s a crucial stepping stone. It demonstrates the structure of a selector-based event loop, registering sockets and callbacks. In the next iteration, we’ll try to replace the blocking time.sleep with a truly non-blocking delay mechanism to unlock the real power of asynchronous I/O with selectors."
  },
  {
    "objectID": "posts/2025-01-29-pure-python-web-server.html#attempt-5---selectors-with-non-blocking-timer-simulation",
    "href": "posts/2025-01-29-pure-python-web-server.html#attempt-5---selectors-with-non-blocking-timer-simulation",
    "title": "Building a Web Server From Scratch in Pure Python",
    "section": "Attempt 5 - Selectors with Non-Blocking Timer Simulation",
    "text": "Attempt 5 - Selectors with Non-Blocking Timer Simulation\nIn the previous section, we saw the structure of a selector-based server, but the blocking time.sleep negated any performance gains. To truly leverage asynchronous I/O, we need to replace that blocking delay with a non-blocking mechanism. In this attempt, I’ve used threading.Timer to simulate a non-blocking delay in conjunction with selectors. It’s still not pure asynchronous I/O in the ideal sense, as threading.Timer uses threads behind the scenes, but it’s a step closer and demonstrates the concept.\nHere’s the code:\n\n\n\nselectors/server_nonblocking.py\n\nimport socket\nimport selectors\nimport threading\n\nselector = selectors.DefaultSelector()\n\n\ndef send_response(conn, addr, response):\n    \"\"\"Send the response when the timer expires.\"\"\"\n    try:\n        conn.sendall(response.encode())\n    except Exception as e:\n        print(f\"Error sending response to {addr}: {e}\")\n    finally:\n        selector.unregister(conn)\n        conn.close()\n\n\ndef handle_request(conn, addr):\n    try:\n        request_data = conn.recv(1024).decode()\n        if request_data:\n            response_html = \"\"\"\n            &lt;html&gt;\n                &lt;head&gt;\n                    &lt;title&gt;My Basic Server&lt;/title&gt;\n                &lt;/head&gt;\n                &lt;body&gt;\n                    &lt;h1&gt;Hello from my basic server&lt;/h1&gt;\n                &lt;/body&gt;\n            &lt;/html&gt;\n        \"\"\"\n            response = \"HTTP/1.1 200 OK\\r\\n\"\n            response += \"Content-Type: text/html\\r\\n\"\n            response += f\"Content-Length: {len(response_html)}\\r\\n\"\n            response += \"\\r\\n\"\n            response += response_html\n\n            # Use threading.Timer to call send_response after a delay\n            timer = threading.Timer(0.1, send_response, args=(conn, addr, response))\n            timer.start()\n\n        else:\n            print(f\"Client {addr} sent no data\")\n            selector.unregister(conn)\n            conn.close()\n    except Exception as e:\n        print(f\"Error handling client {addr}: {e}\")\n        selector.unregister(conn)\n        conn.close()\n\n\ndef accept_connection(sock):\n    conn, addr = sock.accept()\n    conn.setblocking(False)  # Set the connection to non-blocking\n    selector.register(\n        conn, selectors.EVENT_READ, lambda conn: handle_request(conn, addr)\n    )\n\n\ndef asynchronous_server():\n    HOST = \"\"\n    PORT = 8000\n    with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as sock:\n        sock.bind((HOST, PORT))\n        sock.listen()\n        sock.setblocking(False)  # Set the main socket to non-blocking\n\n        selector.register(sock, selectors.EVENT_READ, accept_connection)\n\n        print(f\"Listening on port {PORT}\")\n\n        while True:\n            events = selector.select()  # This function returns all the events\n            for key, _ in events:\n                callback = key.data  # This is the function we registered earlier\n                callback(key.fileobj)  # Execute callback with the socket\n\n\nif __name__ == \"__main__\":\n    asynchronous_server()\n\n\nThe accept_connection and asynchronous_server functions are the same as in selectors/server_blocking.py. The key change is again in handle_request. Instead of time.sleep(0.1) and then registering for EVENT_WRITE, we now use threading.Timer:\n\n\n\n\n\n\nWhat is threading.Timer?\n\n\n\n\n\nthreading.Timer is part of Python’s threading module and it allows you to run a function after a specified delay. It works in the following way:\n\nDelay: You give it a time delay in seconds.\nFunction: You specify the function that should be executed when the time delay is over.\nArguments: You can pass arguments to that function.\n\nThis creates a new thread that waits for the specified delay and then runs the given function. It’s important to note that this is happening in the background, so the rest of the code can keep running.\ntimer.start() line starts the timer. Without calling start(), the timer won’t actually run. The timer runs in the background, which means it doesn’t block the main code from executing. The rest of the program continues to run while waiting for the timer to expire. Once the timer expires, the given function is called automatically, and the response is sent back to the client.\n\n\n\ntimer = threading.Timer(0.1, send_response, args=(conn, addr, response))\ntimer.start()\nThis creates a Timer that will call send_response(conn, addr, response) after 0.1 seconds, but importantly, it does this in a separate thread. The handle_request function itself returns immediately after starting the timer. This means the main event loop in asynchronous_server() is no longer blocked during the simulated delay. It can continue to process other events, like handling new connections or reading data from other sockets.\nLet’s benchmark it.\nConcurrency Level:      10\nTime taken for tests:   11.073 seconds\nComplete requests:      1000\nFailed requests:        0\nTotal transferred:      314000 bytes\nHTML transferred:       249000 bytes\nRequests per second:    90.31 [#/sec] (mean)\nTime per request:       110.731 [ms] (mean)\nTime per request:       11.073 [ms] (mean, across all concurrent requests)\nTransfer rate:          27.69 [Kbytes/sec] received\nThe Requests per second: 90.31 is very similar to our threaded server (90.21 req/sec)! This is a significant improvement over the blocking selector version and the basic socket servers. By using threading.Timer, we’ve effectively offloaded the delay to a separate thread, allowing the main selector event loop to remain non-blocked and handle other connections concurrently.\nWhile this approach works and shows improved concurrency, it’s crucial to understand that it’s still a hybrid approach, not pure asynchronous I/O. We’re using threads (via threading.Timer) to achieve non-blocking behavior. True asynchronous I/O aims to avoid threads altogether for concurrency, relying solely on event loops and non-blocking operations within a single thread.\nIn the next and final step, we’ll explore asyncio, Python’s built-in library for true asynchronous programming, to see how we can achieve non-blocking I/O and concurrency in a more elegant and efficient way, without relying on threads for the simulated delay."
  },
  {
    "objectID": "posts/2025-01-29-pure-python-web-server.html#attempt-6---asyncio-true-non-blocking-io",
    "href": "posts/2025-01-29-pure-python-web-server.html#attempt-6---asyncio-true-non-blocking-io",
    "title": "Building a Web Server From Scratch in Pure Python",
    "section": "Attempt 6 - asyncio True Non-Blocking I/O",
    "text": "Attempt 6 - asyncio True Non-Blocking I/O\nFinally, we arrive at asyncio, Python’s built-in library for asynchronous programming. asyncio provides a framework for writing single-threaded concurrent code using coroutines, allowing for true non-blocking I/O without the complexities of threads for concurrency in I/O-bound operations.\n\n\n\n\n\n\nWhat is a coroutine?\n\n\n\n\n\nA coroutine in Python is a special type of function that can be paused and resumed during execution, making it useful for asynchronous programming. Coroutines allow Python to handle non-blocking operations efficiently, such as network requests, file I/O, or database queries, without needing multiple threads.\nHow is a Coroutine Different from a Regular Function?\n\nDefined with async def: Unlike normal functions (def), coroutines use async def.\nUses await to pause execution: Coroutines can pause at await statements, allowing other coroutines to run in the meantime.\nNeeds to be explicitly scheduled: Calling a coroutine doesn’t execute it immediately; instead, it returns a coroutine object that must be awaited or run using an event loop.\n\nExample of a Coroutine\nimport asyncio\n\nasync def say_hello():\n    print(\"Hello!\")\n    await asyncio.sleep(2)  # Simulates a non-blocking delay\n    print(\"World!\")\n\n# Running the coroutine\nasyncio.run(say_hello())\n\n\n\nLet’s examine the code.\n\n\n\nasyncio/server.py\n\nimport asyncio\n\nasync def send_response(writer, response):\n    \"\"\"Send the response when the timer expires.\"\"\"\n    try:\n        writer.write(response.encode())\n        await writer.drain()  # Ensure data is sent\n    except Exception as e:\n        print(f\"Error sending response: {e}\")\n    finally:\n        writer.close()\n        await writer.wait_closed()  # Wait for the writer to close\n\n\nasync def handle_request(reader, writer):\n    addr = writer.get_extra_info(\"peername\")\n    try:\n        request_data = await reader.read(1024)  # Asynchronously read data\n        request_data = request_data.decode()\n\n        if request_data:\n            response_html = \"\"\"\n            &lt;html&gt;\n                &lt;head&gt;\n                    &lt;title&gt;My Basic Server&lt;/title&gt;\n                &lt;/head&gt;\n                &lt;body&gt;\n                    &lt;h1&gt;Hello from my basic server&lt;/h1&gt;\n                &lt;/body&gt;\n            &lt;/html&gt;\n        \"\"\"\n            response = \"HTTP/1.1 200 OK\\r\\n\"\n            response += \"Content-Type: text/html\\r\\n\"\n            response += f\"Content-Length: {len(response_html)}\\r\\n\"\n            response += \"\\r\\n\"\n            response += response_html\n\n            # Use asyncio.sleep for non-blocking delay\n            await asyncio.sleep(0.1)\n            asyncio.create_task(\n                send_response(writer, response)\n            )  # Create a task to send response asynchronously\n\n        else:\n            print(f\"Client {addr} sent no data\")\n            writer.close()\n            await writer.wait_closed()\n\n    except Exception as e:\n        print(f\"Error handling client {addr}: {e}\")\n        writer.close()\n        await writer.wait_closed()\n\n\nasync def main():\n    HOST = \"\"\n    PORT = 8000\n\n    async def accept_connection(reader, writer):\n        await handle_request(reader, writer)\n\n    server = await asyncio.start_server(accept_connection, HOST, PORT)\n    addr = server.sockets[0].getsockname()\n    print(f\"Serving on {addr}\")\n\n    async with server:\n        await server.serve_forever()\n\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n\n\nThis code looks quite different from the previous versions, leveraging async and await keywords.\n\n\n\n\n\n\nNote\n\n\n\n\n\n\nThe send_response and handle_request functions are now defined as async def, making them coroutines.\nasyncio.start_server(accept_connection, HOST, PORT) starts the asynchronous server.\n\nasyncio.start_server() expects a callback function that follows this signature:\n\nasync def callback(reader: asyncio.StreamReader, writer: asyncio.StreamWriter):\nOur accept_connection function matches this format, where\n\nreader: A StreamReader object to read data from the client.\nwriter: A StreamWriter object to send data to the client.\n\n\n\n\n\n\n\n\nInside handle_request, reader and writer are asyncio.StreamReader and asyncio.StreamWriter objects, providing asynchronous read and write operations.\nrequest_data = await reader.read(1024) asynchronously reads data from the client. The await keyword is crucial here. It’s where the magic of non-blocking I/O happens. When await reader.read(1024) is encountered, the handle_request coroutine pauses execution, yielding control back to the asyncio event loop. The event loop can then proceed to handle other tasks, like processing other connections. When data is available to be read on this connection, the event loop will resume the handle_request coroutine right after the await line. This is true non-blocking I/O within a single thread.\nSimilarly, await asyncio.sleep(0.1) provides a non-blocking delay. Instead of pausing the entire thread, it pauses only the current coroutine, allowing the event loop to continue processing other tasks.\nasyncio.create_task(send_response(writer, response)) creates an asyncio.Task to run send_response concurrently. This means that sending the response happens in the background, without blocking the handle_request coroutine from processing further requests (though in our simple example, handle_request is essentially done after this).\n\n\n\n\n\n\n\nTask vs Coroutine?\n\n\n\n\n\nCoroutine This is an asynchronous function (a function defined with async def) that can pause its execution using await, allowing other tasks to run while it’s paused.\nTask\n\nA task is created from a coroutine using asyncio.create_task(coroutine()).\nIt is responsible for actually running the coroutine in the event loop.\nYou can manage the Task (e.g., cancel it, wait for it to finish, etc.).\nOnce a task is created, it begins running immediately and will be completed when the coroutine finishes its work.\n\nWhy have we not created any other Task? What is special about send_response() There’s no need to create additional tasks for other operations because:\n\nThey’re non-blocking (they let the event loop run other tasks while waiting).\nThe response sending (via send_response) is the only operation that benefits from being run in the background while the server handles other clients.\n\nOkay, so could we use Blocking Sleep as well but assign it to a separate Task? Yes, we can do that as well. I have used non-blocking sleep in this example. But similar to blocking send_response() you can define a Task for a blocking sleep coroutine.\n#| filename: \"asyncio/server_blocking.py\"\ndef blocking_sleep():\n    \"\"\"This simulates a blocking sleep.\"\"\"\n    time.sleep(1)  # This is a blocking sleep\n\nasync def blocking_sleep_task():\n    \"\"\"Run blocking sleep in a separate task.\"\"\"\n    # This simulates a blocking operation\n    loop = asyncio.get_event_loop()  # Get the current event loop\n    await loop.run_in_executor(None, blocking_sleep)  # Run the blocking function in a separate thread\n\n# Use blocking sleep in a separate task\nasyncio.create_task(blocking_sleep_task())\n\n\n\n\nThe main() function sets up the server and starts the asyncio event loop using asyncio.run(main()).\n\nLet’s bencharmark it.\nConcurrency Level:      10\nTime taken for tests:   12.061 seconds\nComplete requests:      1000\nFailed requests:        0\nTotal transferred:      314000 bytes\nHTML transferred:       249000 bytes\nRequests per second:    82.91 [#/sec] (mean)\nTime per request:       120.612 [ms] (mean)\nTime per request:       12.061 [ms] (mean, across all concurrent requests)\nTransfer rate:          25.42 [Kbytes/sec] received\nThe Requests per second: 82.91 is slightly lower than the threaded and timer-selector versions in this particular benchmark run, but still significantly better than the initial blocking servers. In many scenarios, asyncio can outperform threading for I/O-bound tasks due to lower overhead and more efficient concurrency management. However, the exact performance can vary depending on the workload and specific system conditions.\n“asyncio” represents a more modern and efficient approach to concurrency for I/O-intensive applications in Python. It allows us to write highly concurrent code within a single thread, avoiding many of the complexities and overheads associated with threads. It’s the foundation for many modern Python web frameworks and asynchronous libraries."
  },
  {
    "objectID": "posts/2025-01-29-pure-python-web-server.html#conclusion",
    "href": "posts/2025-01-29-pure-python-web-server.html#conclusion",
    "title": "Building a Web Server From Scratch in Pure Python",
    "section": "Conclusion",
    "text": "Conclusion\nWe’ve come a long way, starting from a super simple http.server to exploring threading, selectors, and finally, asyncio. Let’s take a moment to look back at the performance of each server implementation. Here’s a table summarizing the ‘Requests per second’ we observed with Apache Benchmark:\n\n\n\nServer Implementation\nRequests per Second (approx.)\n\n\n\n\nhttp.server\n8.74\n\n\nRaw Sockets\n9.96\n\n\nThreading\n90.21\n\n\nSelectors (Blocking)\n9.95\n\n\nSelectors (Non Blocking)\n90.31\n\n\nasyncio\n82.91\n\n\n\nAs you can clearly see, threading and the selector-timer hybrid approaches provided a dramatic performance boost compared to the basic single-threaded servers. asyncio, while in this specific benchmark run showing slightly lower RPS than the threaded versions, still demonstrated a significant improvement over the blocking approaches and represents a more robust and scalable architecture for I/O-bound applications in the long run.\nIt’s important to remember that these are very basic, toy servers. They lack many features of production-ready web servers, such as robust HTTP parsing, proper error handling, security considerations, and more. They are meant for educational purposes – to illustrate the core concepts of networking and concurrency.\nBuilding these basic servers from scratch was a learning journey for me. I hope it has been for you too! It really demystifies what’s happening behind the scenes and gives a deeper appreciation for the evolution of concurrency approaches in Python and the power of asynchronous I/O."
  },
  {
    "objectID": "posts/2025-02-15-diffusion-model-mnist-part2.html",
    "href": "posts/2025-02-15-diffusion-model-mnist-part2.html",
    "title": "From Noise to Numbers: A Hands-On Guide to Generating MNIST Digits with Diffusion Models (Part 2)",
    "section": "",
    "text": "image source: https://www.artbreeder.com/image/6b4df6c697078f0e2cda42348ec6"
  },
  {
    "objectID": "posts/2025-02-15-diffusion-model-mnist-part2.html#introduction",
    "href": "posts/2025-02-15-diffusion-model-mnist-part2.html#introduction",
    "title": "From Noise to Numbers: A Hands-On Guide to Generating MNIST Digits with Diffusion Models (Part 2)",
    "section": "Introduction",
    "text": "Introduction\nWelcome back to Part 2 of our journey into diffusion models! In the first part, we successfully built a basic Convolutional UNet from scratch and trained it to directly predict denoised MNIST digits. We saw that it could indeed remove some noise, but the results were still a bit blurry, and it wasn’t quite the “diffusion model magic” we were hoping for.\nOne of the key limitations we hinted at was the simplicity of our BasicUNet architecture. For this second part, we’re going to address that and we’ll be upgrading our UNet architecture to something more powerful and feature-rich.\nTo do this, we’ll be leveraging the fantastic diffusers library from Hugging Face. diffusers is a widely adopted toolkit in the world of diffusion models, providing pre-built and optimized components that can significantly simplify our development process and boost performance.\nIn this part, we’ll replace our BasicUNet with a UNet2DModel from diffusers. We’ll keep the core task the same – direct image prediction – but with a more advanced UNet under the hood. This will allow us to see firsthand how architectural improvements can impact the quality of our denoising results, setting the stage for even more exciting explorations in future parts! Let’s dive in!\n\nCredits\nThis post is inspired by the Hugging Face Diffusion Course\n\n\nEnvironment Details\nYou can access and run this Jupyter Notebook from the GitHub repository on this link 2025-02-15-diffusion-model-mnist-part2.ipynb\nRun the following cell to install the required packages.\n\nThis notebook can be run with Google Colab T4 GPU runtime.\nI have also tested this notebook with AWS SageMaker Jupyter Notebook running on instance “ml.g5.xlarge” and image “SageMaker Distribution 2.3.0”.\n\n\n%%capture\n!pip install datasets[vision]\n!pip install diffusers\n!pip install watermark\n!pip install torchinfo\n!pip install matplotlib\n\nWaterMark is an IPython magic extension for printing date and time stamps, version numbers, and hardware information. Let’s load this extension and print the environment details.\n\n%load_ext watermark\n\n\n%watermark -v -m -p torch,torchvision,datasets,diffusers,matplotlib,watermark,torchinfo\n\nPython implementation: CPython\nPython version       : 3.11.11\nIPython version      : 7.34.0\n\ntorch      : 2.5.1+cu124\ntorchvision: 0.20.1+cu124\ndatasets   : 3.3.0\ndiffusers  : 0.32.2\nmatplotlib : 3.10.0\nwatermark  : 2.5.0\ntorchinfo  : 1.8.0\n\nCompiler    : GCC 11.4.0\nOS          : Linux\nRelease     : 6.1.85+\nMachine     : x86_64\nProcessor   : x86_64\nCPU cores   : 2\nArchitecture: 64bit"
  },
  {
    "objectID": "posts/2025-02-15-diffusion-model-mnist-part2.html#diving-into-diffusers-and-unet2dmodel",
    "href": "posts/2025-02-15-diffusion-model-mnist-part2.html#diving-into-diffusers-and-unet2dmodel",
    "title": "From Noise to Numbers: A Hands-On Guide to Generating MNIST Digits with Diffusion Models (Part 2)",
    "section": "Diving into diffusers and UNet2DModel",
    "text": "Diving into diffusers and UNet2DModel\nSo, what exactly is this diffusers library we’re so excited about? Think of diffusers as a comprehensive, community-driven library in PyTorch specifically designed for working with diffusion models. It’s maintained by Hugging Face, the same team behind the popular Transformers library, so you know it’s built with quality and ease of use in mind.\nWhy are we using diffusers now? Several reasons! First, it provides well-tested and optimized implementations of various diffusion model components, saving us from writing everything from scratch. Second, it’s a vibrant ecosystem, constantly evolving with the latest research and techniques in diffusion models. By using diffusers, we’re standing on the shoulders of giants!\nFor Part 2, the star of the show is the UNet2DModel class from diffusers. This is a more sophisticated UNet architecture compared to our BasicUNet. It’s like upgrading from a standard bicycle to a mountain bike – both are bikes, but the mountain bike is built for more challenging terrain and better performance.\nWhat makes UNet2DModel more advanced? Let’s look at some key architectural improvements under the hood:\n\nResNet Blocks: Instead of simple convolutional layers, UNet2DModel utilizes ResNet blocks within its downsampling and upsampling paths. ResNet blocks are known for making it easier to train deeper networks, which can capture more complex features in images. Think of them as more efficient and powerful building blocks for our UNet.\nAttention Mechanisms: UNet2DModel incorporates attention mechanisms, specifically “Attention Blocks,” in its architecture. Attention is a powerful concept in deep learning that allows the model to focus on the most relevant parts of the input when processing information. In image generation, attention can help the model selectively focus on different regions of the image, potentially leading to finer details and more coherent structures.\nGroup Normalization: Instead of Batch Normalization, UNet2DModel uses Group Normalization. Group Normalization is often favored in generative models, especially when working with smaller batch sizes, as it tends to be more stable and perform better in those scenarios.\nTimestep Embedding: Even though we are still doing direct image prediction in this part, UNet2DModel is designed with diffusion models in mind. It includes a TimestepEmbedding layer, which is a standard component in diffusion models to handle the timestep information (which we’ll explore in later parts!). For now, we’ll just be passing in a timestep of 0, but this layer is there, ready for when we move to true diffusion.\n\nThese architectural enhancements in UNet2DModel give it a greater capacity to learn and potentially denoise images more effectively than our BasicUNet. Let’s see if it lives up to the hype!"
  },
  {
    "objectID": "posts/2025-02-15-diffusion-model-mnist-part2.html#data-preparation-and-preprocessing-for-mnist",
    "href": "posts/2025-02-15-diffusion-model-mnist-part2.html#data-preparation-and-preprocessing-for-mnist",
    "title": "From Noise to Numbers: A Hands-On Guide to Generating MNIST Digits with Diffusion Models (Part 2)",
    "section": "Data Preparation and Preprocessing for MNIST",
    "text": "Data Preparation and Preprocessing for MNIST\nAs we are building upon the foundations laid in Part 1, we will reuse the same data preparation and preprocessing steps for the MNIST dataset. For a more in-depth explanation of these steps, please refer back to the first part of this guide. Here, we will quickly outline the process to ensure our data is ready for training our enhanced UNet.\nFirst, we load the MNIST dataset using the datasets library:\n\nfrom datasets import load_dataset\ndataset = load_dataset(\"mnist\")\nprint(dataset)\n\n/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:104: UserWarning: \nError while fetching `HF_TOKEN` secret value from your vault: 'Requesting secret HF_TOKEN timed out. Secrets can only be fetched when running from the Colab UI.'.\nYou are not authenticated with the Hugging Face Hub in this notebook.\nIf the error persists, please let us know by opening an issue on GitHub (https://github.com/huggingface/huggingface_hub/issues/new).\n  warnings.warn(\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDatasetDict({\n    train: Dataset({\n        features: ['image', 'label'],\n        num_rows: 60000\n    })\n    test: Dataset({\n        features: ['image', 'label'],\n        num_rows: 10000\n    })\n})\n\n\nThis code snippet downloads and loads the MNIST dataset. As we saw in Part 1, this dataset is provided as a DatasetDict with ‘train’ and ‘test’ splits, each containing ‘image’ and ‘label’ features.\nNext, we define our preprocessing pipeline using torchvision.transforms:\n\nimport torch\nfrom torchvision import transforms\n\nimage_size = 32  # Define the target image size\n\npreprocess = transforms.Compose([\n    transforms.Resize((image_size, image_size)),\n    transforms.ToTensor(),\n])\n\nThis preprocess pipeline consists of two transformations:\n\ntransforms.Resize((image_size, image_size)): Resizes each image to a fixed size of 32x32 pixels. This ensures consistent input dimensions for our UNet model.\ntransforms.ToTensor(): Converts the images to PyTorch tensors and scales the pixel values to the range [0, 1]. This normalization is crucial for training deep learning models effectively.\n\nTo apply this preprocessing to the dataset efficiently, we define a transform function and set it for our dataset:\n\n# Define the transform function\ndef transform(examples):\n    examples = [preprocess(image) for image in examples[\"image\"]]\n    return {\"images\": examples}\n\n# Apply the transform to the dataset\ndataset.set_transform(transform)\n\nThis transform function applies our preprocess pipeline to each image in the dataset on-the-fly, meaning preprocessing happens only when the data is accessed, saving memory and keeping our dataset efficient.\nFinally, we need the noise corruption function that we introduced in Part 1. This function will be used to add controlled noise to our clean MNIST images during training, simulating the forward diffusion process:\n\n# Definition of the noise corruption function\ndef corrupt(x, noise, amount):\n    amount = amount.view(-1, 1, 1, 1)  # make sure it's broadcastable\n    return (\n        x * (1 - amount) + noise * amount\n    )  # equivalent to x.lerp(noise, amount)\n\nThis corrupt function takes a clean image (x), random noise (noise), and a noise amount (ranging from 0 to 1) as input. It then blends the clean image with the noise based on the amount, effectively creating a noisy version of the image. The higher the amount, the more noise is added.\nWith the MNIST dataset loaded, preprocessed, and the noise corruption function defined, we are now fully prepared to train our enhanced UNet architecture from the diffusers library! Let’s move on to explore the UNet2DModel."
  },
  {
    "objectID": "posts/2025-02-15-diffusion-model-mnist-part2.html#model-2-implementing-unet2dmodel",
    "href": "posts/2025-02-15-diffusion-model-mnist-part2.html#model-2-implementing-unet2dmodel",
    "title": "From Noise to Numbers: A Hands-On Guide to Generating MNIST Digits with Diffusion Models (Part 2)",
    "section": "Model 2: Implementing UNet2DModel",
    "text": "Model 2: Implementing UNet2DModel\nNow, let’s see how to put the diffusers UNet2DModel into action for our MNIST digit denoising task. Here’s the code snippet we’ll use to instantiate the model:\n\nfrom diffusers import UNet2DModel\n\nmodel = UNet2DModel(\n    sample_size=32,\n    in_channels=1,\n    out_channels=1,\n    layers_per_block=2,\n    block_out_channels=(32, 64, 64),\n    down_block_types=(\n        \"DownBlock2D\",\n        \"AttnDownBlock2D\",\n        \"AttnDownBlock2D\",\n    ),\n    up_block_types=(\n        \"AttnUpBlock2D\",\n        \"AttnUpBlock2D\",\n        \"UpBlock2D\",\n    ),\n)\n\nThe cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n\n\n\n\n\n\nprint(model)\n\nUNet2DModel(\n  (conv_in): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (time_proj): Timesteps()\n  (time_embedding): TimestepEmbedding(\n    (linear_1): Linear(in_features=32, out_features=128, bias=True)\n    (act): SiLU()\n    (linear_2): Linear(in_features=128, out_features=128, bias=True)\n  )\n  (down_blocks): ModuleList(\n    (0): DownBlock2D(\n      (resnets): ModuleList(\n        (0-1): 2 x ResnetBlock2D(\n          (norm1): GroupNorm(32, 32, eps=1e-05, affine=True)\n          (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (time_emb_proj): Linear(in_features=128, out_features=32, bias=True)\n          (norm2): GroupNorm(32, 32, eps=1e-05, affine=True)\n          (dropout): Dropout(p=0.0, inplace=False)\n          (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (nonlinearity): SiLU()\n        )\n      )\n      (downsamplers): ModuleList(\n        (0): Downsample2D(\n          (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n        )\n      )\n    )\n    (1): AttnDownBlock2D(\n      (attentions): ModuleList(\n        (0-1): 2 x Attention(\n          (group_norm): GroupNorm(32, 64, eps=1e-05, affine=True)\n          (to_q): Linear(in_features=64, out_features=64, bias=True)\n          (to_k): Linear(in_features=64, out_features=64, bias=True)\n          (to_v): Linear(in_features=64, out_features=64, bias=True)\n          (to_out): ModuleList(\n            (0): Linear(in_features=64, out_features=64, bias=True)\n            (1): Dropout(p=0.0, inplace=False)\n          )\n        )\n      )\n      (resnets): ModuleList(\n        (0): ResnetBlock2D(\n          (norm1): GroupNorm(32, 32, eps=1e-05, affine=True)\n          (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (time_emb_proj): Linear(in_features=128, out_features=64, bias=True)\n          (norm2): GroupNorm(32, 64, eps=1e-05, affine=True)\n          (dropout): Dropout(p=0.0, inplace=False)\n          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (nonlinearity): SiLU()\n          (conv_shortcut): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1))\n        )\n        (1): ResnetBlock2D(\n          (norm1): GroupNorm(32, 64, eps=1e-05, affine=True)\n          (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (time_emb_proj): Linear(in_features=128, out_features=64, bias=True)\n          (norm2): GroupNorm(32, 64, eps=1e-05, affine=True)\n          (dropout): Dropout(p=0.0, inplace=False)\n          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (nonlinearity): SiLU()\n        )\n      )\n      (downsamplers): ModuleList(\n        (0): Downsample2D(\n          (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n        )\n      )\n    )\n    (2): AttnDownBlock2D(\n      (attentions): ModuleList(\n        (0-1): 2 x Attention(\n          (group_norm): GroupNorm(32, 64, eps=1e-05, affine=True)\n          (to_q): Linear(in_features=64, out_features=64, bias=True)\n          (to_k): Linear(in_features=64, out_features=64, bias=True)\n          (to_v): Linear(in_features=64, out_features=64, bias=True)\n          (to_out): ModuleList(\n            (0): Linear(in_features=64, out_features=64, bias=True)\n            (1): Dropout(p=0.0, inplace=False)\n          )\n        )\n      )\n      (resnets): ModuleList(\n        (0-1): 2 x ResnetBlock2D(\n          (norm1): GroupNorm(32, 64, eps=1e-05, affine=True)\n          (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (time_emb_proj): Linear(in_features=128, out_features=64, bias=True)\n          (norm2): GroupNorm(32, 64, eps=1e-05, affine=True)\n          (dropout): Dropout(p=0.0, inplace=False)\n          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (nonlinearity): SiLU()\n        )\n      )\n    )\n  )\n  (up_blocks): ModuleList(\n    (0): AttnUpBlock2D(\n      (attentions): ModuleList(\n        (0-2): 3 x Attention(\n          (group_norm): GroupNorm(32, 64, eps=1e-05, affine=True)\n          (to_q): Linear(in_features=64, out_features=64, bias=True)\n          (to_k): Linear(in_features=64, out_features=64, bias=True)\n          (to_v): Linear(in_features=64, out_features=64, bias=True)\n          (to_out): ModuleList(\n            (0): Linear(in_features=64, out_features=64, bias=True)\n            (1): Dropout(p=0.0, inplace=False)\n          )\n        )\n      )\n      (resnets): ModuleList(\n        (0-2): 3 x ResnetBlock2D(\n          (norm1): GroupNorm(32, 128, eps=1e-05, affine=True)\n          (conv1): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (time_emb_proj): Linear(in_features=128, out_features=64, bias=True)\n          (norm2): GroupNorm(32, 64, eps=1e-05, affine=True)\n          (dropout): Dropout(p=0.0, inplace=False)\n          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (nonlinearity): SiLU()\n          (conv_shortcut): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n        )\n      )\n      (upsamplers): ModuleList(\n        (0): Upsample2D(\n          (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        )\n      )\n    )\n    (1): AttnUpBlock2D(\n      (attentions): ModuleList(\n        (0-2): 3 x Attention(\n          (group_norm): GroupNorm(32, 64, eps=1e-05, affine=True)\n          (to_q): Linear(in_features=64, out_features=64, bias=True)\n          (to_k): Linear(in_features=64, out_features=64, bias=True)\n          (to_v): Linear(in_features=64, out_features=64, bias=True)\n          (to_out): ModuleList(\n            (0): Linear(in_features=64, out_features=64, bias=True)\n            (1): Dropout(p=0.0, inplace=False)\n          )\n        )\n      )\n      (resnets): ModuleList(\n        (0-1): 2 x ResnetBlock2D(\n          (norm1): GroupNorm(32, 128, eps=1e-05, affine=True)\n          (conv1): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (time_emb_proj): Linear(in_features=128, out_features=64, bias=True)\n          (norm2): GroupNorm(32, 64, eps=1e-05, affine=True)\n          (dropout): Dropout(p=0.0, inplace=False)\n          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (nonlinearity): SiLU()\n          (conv_shortcut): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n        )\n        (2): ResnetBlock2D(\n          (norm1): GroupNorm(32, 96, eps=1e-05, affine=True)\n          (conv1): Conv2d(96, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (time_emb_proj): Linear(in_features=128, out_features=64, bias=True)\n          (norm2): GroupNorm(32, 64, eps=1e-05, affine=True)\n          (dropout): Dropout(p=0.0, inplace=False)\n          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (nonlinearity): SiLU()\n          (conv_shortcut): Conv2d(96, 64, kernel_size=(1, 1), stride=(1, 1))\n        )\n      )\n      (upsamplers): ModuleList(\n        (0): Upsample2D(\n          (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        )\n      )\n    )\n    (2): UpBlock2D(\n      (resnets): ModuleList(\n        (0): ResnetBlock2D(\n          (norm1): GroupNorm(32, 96, eps=1e-05, affine=True)\n          (conv1): Conv2d(96, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (time_emb_proj): Linear(in_features=128, out_features=32, bias=True)\n          (norm2): GroupNorm(32, 32, eps=1e-05, affine=True)\n          (dropout): Dropout(p=0.0, inplace=False)\n          (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (nonlinearity): SiLU()\n          (conv_shortcut): Conv2d(96, 32, kernel_size=(1, 1), stride=(1, 1))\n        )\n        (1-2): 2 x ResnetBlock2D(\n          (norm1): GroupNorm(32, 64, eps=1e-05, affine=True)\n          (conv1): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (time_emb_proj): Linear(in_features=128, out_features=32, bias=True)\n          (norm2): GroupNorm(32, 32, eps=1e-05, affine=True)\n          (dropout): Dropout(p=0.0, inplace=False)\n          (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (nonlinearity): SiLU()\n          (conv_shortcut): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))\n        )\n      )\n    )\n  )\n  (mid_block): UNetMidBlock2D(\n    (attentions): ModuleList(\n      (0): Attention(\n        (group_norm): GroupNorm(32, 64, eps=1e-05, affine=True)\n        (to_q): Linear(in_features=64, out_features=64, bias=True)\n        (to_k): Linear(in_features=64, out_features=64, bias=True)\n        (to_v): Linear(in_features=64, out_features=64, bias=True)\n        (to_out): ModuleList(\n          (0): Linear(in_features=64, out_features=64, bias=True)\n          (1): Dropout(p=0.0, inplace=False)\n        )\n      )\n    )\n    (resnets): ModuleList(\n      (0-1): 2 x ResnetBlock2D(\n        (norm1): GroupNorm(32, 64, eps=1e-05, affine=True)\n        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        (time_emb_proj): Linear(in_features=128, out_features=64, bias=True)\n        (norm2): GroupNorm(32, 64, eps=1e-05, affine=True)\n        (dropout): Dropout(p=0.0, inplace=False)\n        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        (nonlinearity): SiLU()\n      )\n    )\n  )\n  (conv_norm_out): GroupNorm(32, 32, eps=1e-05, affine=True)\n  (conv_act): SiLU()\n  (conv_out): Conv2d(32, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n)\n\n\n\nfrom torchinfo import summary\n\nsummary(model)\n\n======================================================================\nLayer (type:depth-idx)                        Param #\n======================================================================\nUNet2DModel                                   --\n├─Conv2d: 1-1                                 320\n├─Timesteps: 1-2                              --\n├─TimestepEmbedding: 1-3                      --\n│    └─Linear: 2-1                            4,224\n│    └─SiLU: 2-2                              --\n│    └─Linear: 2-3                            16,512\n├─ModuleList: 1-4                             --\n│    └─DownBlock2D: 2-4                       --\n│    │    └─ModuleList: 3-1                   45,504\n│    │    └─ModuleList: 3-2                   9,248\n│    └─AttnDownBlock2D: 2-5                   --\n│    │    └─ModuleList: 3-3                   33,536\n│    │    └─ModuleList: 3-4                   148,352\n│    │    └─ModuleList: 3-5                   36,928\n│    └─AttnDownBlock2D: 2-6                   --\n│    │    └─ModuleList: 3-6                   33,536\n│    │    └─ModuleList: 3-7                   164,736\n├─ModuleList: 1-5                             --\n│    └─AttnUpBlock2D: 2-7                     --\n│    │    └─ModuleList: 3-8                   50,304\n│    │    └─ModuleList: 3-9                   382,848\n│    │    └─ModuleList: 3-10                  36,928\n│    └─AttnUpBlock2D: 2-8                     --\n│    │    └─ModuleList: 3-11                  50,304\n│    │    └─ModuleList: 3-12                  362,304\n│    │    └─ModuleList: 3-13                  36,928\n│    └─UpBlock2D: 2-9                         --\n│    │    └─ModuleList: 3-14                  112,640\n├─UNetMidBlock2D: 1-6                         --\n│    └─ModuleList: 2-10                       --\n│    │    └─Attention: 3-15                   16,768\n│    └─ModuleList: 2-11                       --\n│    │    └─ResnetBlock2D: 3-16               82,368\n│    │    └─ResnetBlock2D: 3-17               82,368\n├─GroupNorm: 1-7                              64\n├─SiLU: 1-8                                   --\n├─Conv2d: 1-9                                 289\n======================================================================\nTotal params: 1,707,009\nTrainable params: 1,707,009\nNon-trainable params: 0\n======================================================================\n\n\n\n\n\n\n\n\nNote\n\n\n\nOur model based on diffuser library UNet2DModel has 5x more trainable parameters as compared to Basic Convolutional UNet Model from Part 1.\n\n\nLet’s break down the parameters we’ve used when creating our UNet2DModel instance:\n\nsample_size=32: This specifies the size of the input images. We’re still working with 32x32 MNIST images after preprocessing, so we set this to 32.\nin_channels=1: MNIST images are grayscale, meaning they have a single color channel. Therefore, in_channels is set to 1.\nout_channels=1: We want our UNet to output denoised grayscale images, so out_channels is also 1.\nlayers_per_block=2: This parameter controls the number of ResNet layers within each UNet block (both downsampling and upsampling blocks). We’ve chosen 2, meaning each block will have two ResNet layers. Increasing this would make the model deeper and potentially more powerful, but also increase training time.\nblock_out_channels=(32, 64, 64): This is a crucial parameter that defines the number of output channels for each block in the downsampling path.\n\nThe first value, 32, corresponds to the output channels of the initial downsampling block.\nThe second value, 64, is for the next downsampling block, and so on.\nWe’ve chosen (32, 64, 64), which is “roughly matching our basic unet example” as we noted in the code comments. This is a deliberate choice to keep the model size somewhat comparable to our BasicUNet while still benefiting from the architectural improvements of UNet2DModel.\n\ndown_block_types=(\"DownBlock2D\", \"AttnDownBlock2D\", \"AttnDownBlock2D\"): This list specifies the type of downsampling blocks to use in the encoder path.\n\n\"DownBlock2D\": A standard ResNet downsampling block.\n\"AttnDownBlock2D\": A ResNet downsampling block with added attention mechanisms.\nWe’re using a mix of standard and attention-based downsampling blocks to leverage the benefits of attention in capturing important image features.\n\nup_block_types=(\"AttnUpBlock2D\", \"AttnUpBlock2D\", \"UpBlock2D\"): Similarly, this list defines the types of upsampling blocks in the decoder path, mirroring the downsampling path and also incorporating attention blocks in the upsampling process.\n\nBy carefully configuring these parameters, we’ve created a UNet2DModel tailored for our MNIST denoising task, leveraging the power of diffusers and incorporating more advanced architectural components compared to our BasicUNet. The print(model) output (or summary(model)) will show the detailed architecture and confirm the parameter settings we’ve defined. You’ll likely notice a significantly larger number of parameters compared to BasicUNet, hinting at the increased capacity of this enhanced model.\n\nTraining the Enhanced UNet\nWith our UNet2DModel defined, the next step is to train it! The training process for this enhanced UNet will be remarkably similar to what we did in Part 1 with our BasicUNet. This is intentional! By keeping the training process consistent, we can isolate the impact of the architectural changes we’ve made by switching to UNet2DModel.\nWe will still be using:\n\nDirect Image Prediction: Our model will still be trained to directly predict the denoised version of a noisy MNIST image in a single forward pass.\nMean Squared Error (MSE) Loss: We’ll continue to use MSE loss (F.mse_loss) to measure the difference between the predicted denoised image and the clean target image.\nAdam Optimizer: We’ll stick with the Adam optimizer (torch.optim.Adam) to update the model’s weights during training.\n\nHere’s a snippet of the training loop code. You’ll notice it’s almost identical to the training loop from Part 1:\n\nimport torch.nn.functional as F\nfrom torch.optim import Adam\nimport matplotlib.pyplot as plt\n\n# --- Setup (Device, Model, Optimizer, Loss History, Hyperparameters) ---\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = model.to(device) # Our UNet2DModel from diffusers\noptimizer = Adam(model.parameters(), lr=1e-3) # Same learning rate as Part 1\nlosses = []\nnum_epochs = 5 # Same number of epochs as Part 1\nbatch_size = 128 # Same batch size as Part 1\n\ntrain_dataloader = torch.utils.data.DataLoader(\n    dataset[\"train\"], batch_size=batch_size, shuffle=True\n)\n\nprint(device)\n\ncuda\n\n\n\n# --- Training Loop ---\nfor epoch in range(num_epochs):\n    for batch in train_dataloader:\n        clean_images = batch[\"images\"].to(device)\n        noise = torch.rand_like(clean_images).to(device)\n        noise_amount = torch.randn(clean_images.shape[0]).to(device)\n        noisy_images = corrupt(clean_images, noise, noise_amount) # Same corrupt function\n\n        predicted_images = model(noisy_images, 0).sample # Still passing timestep 0\n\n        loss = F.mse_loss(predicted_images, clean_images)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        losses.append(loss.item())\n\n    avg_loss = sum(losses[-len(train_dataloader):]) / len(train_dataloader)\n    print(f\"Epoch {epoch+1}/{num_epochs} - Average Loss: {avg_loss:.4f}\")\n\nEpoch 1/5 - Average Loss: 0.0130\nEpoch 2/5 - Average Loss: 0.0073\nEpoch 3/5 - Average Loss: 0.0062\nEpoch 4/5 - Average Loss: 0.0058\nEpoch 5/5 - Average Loss: 0.0055\n\n\nAs you can see, the core training logic remains the same. We load batches, generate noise, corrupt images, feed them to the UNet2DModel (still with a timestep of 0), calculate MSE loss, and update the weights using Adam. We’ve also kept the hyperparameters (learning rate, batch size, number of epochs) consistent with Part 1 for a direct comparison.\nAfter running this training code, we obtain the following loss curve:\n\n# --- Plotting Loss Curve ---\nplt.figure(figsize=(8, 4))\nplt.plot(losses, label=\"Training Loss\")\nplt.title(\"Training Loss Curve (UNet2DModel - Direct Prediction)\") # Updated title\nplt.xlabel(\"Iteration\")\nplt.ylabel(\"MSE Loss\")\nplt.legend()\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\nNow that our enhanced UNet is trained, let’s see how it performs in denoising MNIST digits!\n\n\nInference and Results: Did diffusers UNet Improve Things?\nNow comes the crucial question: Did upgrading to UNet2DModel actually improve our denoising performance compared to the BasicUNet from Part 1? To find out, we need to evaluate our trained UNet2DModel on unseen MNIST test data.\nWe’ll follow a similar inference process as in Part 1. We’ll load a batch of test images, generate noisy versions of them using the same corrupt function, feed these noisy images into our trained UNet2DModel, and then visualize the results.\nHere’s the code for inference and visualization:\n\n# --- Inference with Model 1 and Visualization ---\n# --- Prepare test dataset and dataloader ---\ntest_dataset = load_dataset(\"mnist\", split=\"test\") # Load MNIST test split dataset\ntest_dataset.set_transform(transform)             # Apply preprocessing transform to test dataset\ntest_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=8, shuffle=True) # Create DataLoader for test dataset\n\n# --- Get a batch of test images ---\nbatch = next(iter(test_dataloader))\nclean_images = batch[\"images\"].to(device) # Load clean test images to the device\n\n# --- Generate new random noise for inference ---\nnoise = torch.rand_like(clean_images).to(device) # Generate random noise tensor\nnoise_amount = torch.randn(clean_images.shape[0]).to(device) # Generate noise amount tensor\nnoisy_images = corrupt(clean_images, noise, noise_amount) # Create noisy test images by corruption\n\n# --- Perform inference (get denoised images from Model 1) ---\nmodel.eval()  # Set model to evaluation mode for inference\nwith torch.no_grad():  # Disable gradient calculation during inference\n    denoised_images = model(noisy_images, 0) # Get denoised images from model\n\n# --- Move tensors to CPU and convert to NumPy for visualization ---\nnoisy_images_np = noisy_images.cpu().numpy() # Move noisy images to CPU and convert to NumPy\ndenoised_images_np = denoised_images[\"sample\"].cpu().numpy() # Move denoised images to CPU and convert to NumPy\nclean_images_np = clean_images.cpu().numpy() # Move clean images to CPU and convert to NumPy\n\n# --- Plotting the results: Original, Noisy, Denoised ---\nnum_images = 6  # Set number of images to visualize\nplt.figure(figsize=(12, 6)) # Initialize matplotlib figure for plotting\n\nfor i in range(num_images): # Loop through number of images to plot\n    # --- Plot Original (Clean) Images ---\n    plt.subplot(3, num_images, i + 1) # Create subplot for original images (top row)\n    plt.imshow(clean_images_np[i].squeeze(), cmap='Greys') # Display original clean image\n    plt.title(\"Original\") # Set title for original image subplot\n    plt.axis('off') # Hide axes for cleaner image display\n\n    # --- Plot Noisy Images ---\n    plt.subplot(3, num_images, i + num_images + 1) # Create subplot for noisy images (middle row)\n    plt.imshow(noisy_images_np[i].squeeze(), cmap='Greys') # Display noisy image input\n    plt.title(\"Noisy\") # Set title for noisy image subplot\n    plt.axis('off') # Hide axes\n\n    # --- Plot Denoised Images ---\n    plt.subplot(3, num_images, i + 2 * num_images + 1) # Create subplot for denoised images (bottom row)\n    plt.imshow(denoised_images_np[i].squeeze(), cmap='Greys') # Display denoised output image\n    plt.title(\"Denoised\") # Set title for denoised image subplot\n    plt.axis('off') # Hide axes\n\nplt.tight_layout() # Adjust subplot layout for better spacing\nplt.show() # Show the complete plot\n\n\n\n\n\n\n\n\nNow, let’s analyze the results visually. Looking at the “Denoised (UNet2DModel Output)” row (bottom row), and comparing it to the “Noisy (Input)” row (middle row), we can observe that the UNet2DModel is indeed denoising the digits. The random noise is significantly reduced, and the digit shapes become much clearer.\nHowever, the key question is: Is it better than BasicUNet?\n\nSharpness: Are the denoised digits from UNet2DModel sharper and more well-defined compared to BasicUNet?\nBlurriness: Is there less residual blurriness in the UNet2DModel outputs?\nArtifacts/Noise: Are there fewer artifacts or less remaining noise in the background or around the digits generated by UNet2DModel?\nOverall Visual Quality: Subjectively, do the denoised digits from UNet2DModel look more like clean MNIST digits compared to BasicUNet?\n\nUpon visual inspection, the denoised digits from UNet2DModel appear to be noticeably sharper and more defined than those from BasicUNet in Part 1. The residual blurriness seems reduced, and the digits have a slightly cleaner appearance. While not perfectly restored, the UNet2DModel outputs do seem to represent a step up in visual quality compared to our baseline model.\nLet’s run inference one more time, but with a different approach.\nInstead of starting with an original MNIST digit and adding noise to it, we will feed the model pure noise as input and observe the results.\n\nimport torch\nimport matplotlib.pyplot as plt\n\n# Generate a noisy image (random noise)\nnoise_image = torch.rand(1, 32, 32)  # Example: Single-channel (grayscale) 32x32 noise image\n\n# Assume `model` is trained and available\nwith torch.no_grad():\n    denoised_image = model(noise_image.unsqueeze(0).to(device), 0)  # Add batch dimension & move to device\n    denoised_image = denoised_image[\"sample\"].squeeze(0).cpu().detach()  # Remove batch dim & move to CPU\n\n# Plot both images side by side\nfig, axs = plt.subplots(1, 2, figsize=(8, 4))\n\naxs[0].imshow(noise_image.squeeze(), cmap=\"Greys\")\naxs[0].set_title(\"Noisy Image\")\naxs[0].axis(\"off\")\n\naxs[1].imshow(denoised_image.squeeze(), cmap=\"Greys\")\naxs[1].set_title(\"Model Prediction (Denoised)\")\naxs[1].axis(\"off\")\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nIn summary, our initial visual assessment suggests that enhancing our UNet architecture by using UNet2DModel from diffusers has led to an improvement in denoising performance for direct image prediction. The more sophisticated architecture, seems to be better at capturing and removing noise while preserving digit details.\nHowever, let’s push this further and see if iterative refinement can further enhance the quality of images generated by our UNet2DModel!\n\n\nExploring Iterative Refinement (Again)\nAt the end of Part 1, we experimented with a simple iterative refinement approach to see if we could further improve the denoising results from our BasicUNet. We found that by repeatedly applying the model in steps, we could nudge the image towards a cleaner state. Now, let’s revisit this iterative refinement strategy, but this time using our enhanced UNet2DModel.\nThe idea is the same: instead of just denoising in a single shot, we’ll start with pure random noise and iteratively refine it over multiple steps using our trained UNet2DModel. The following code performs iterative refinement over n_steps (set to 5 in this example). In each step, we feed the current image x (initially random noise) into our UNet2DModel to get a prediction. We then blend this prediction with the current image x using a linear interpolation, gradually refining the image towards the model’s predictions. We also track the image at each step (step_history) and the model’s direct predictions (pred_output_history) for visualization.\nRunning this iterative refinement code with our UNet2DModel produces the following visualization:\n\nimport torchvision\nfrom matplotlib import pyplot as plt\n\nn_steps = 5\nx = torch.rand(8, 1, 32, 32).to(device)  # Start from random\nstep_history = [x.detach().cpu()]\npred_output_history = []\n\nfor i in range(n_steps):\n    # Predict denoise image\n    with torch.no_grad():\n        pred = model(x, 0).sample\n\n    # Store output for plotting\n    pred_output_history.append(pred.detach().cpu())\n\n    # Move slightly towards that direction\n    mix_factor = 1 / (n_steps - i)\n    x = x * (1 - mix_factor) + pred * mix_factor\n\n    # Store output for plotting\n    step_history.append(x.detach().cpu())\n\nfig, axs = plt.subplots(n_steps, 2, figsize=(9, 4), sharex=True)\naxs[0, 0].set_title(\"x (model input)\")\naxs[0, 1].set_title(\"model prediction\")\nfor i in range(n_steps):\n    axs[i, 0].imshow(\n        torchvision.utils.make_grid(step_history[i])[0].clip(0, 1), cmap=\"Greys\"\n    )\n    axs[i, 1].imshow(\n        torchvision.utils.make_grid(pred_output_history[i])[0].clip(0, 1),\n        cmap=\"Greys\",\n    )\n\n\n\n\n\n\n\n\nLet’s analyze the results. Looking at the left column, “x (Model Input),” which shows the image at each refinement step, we can observe how the initially random noise gradually transforms into recognizable digit-like structures over the 5 steps.\nNow, let’s compare these iteratively refined results to: Iterative Refinement with BasicUNet\n\n\n\n\n\n\nBasicUNet\n\n\n\n\n\n\n\nUNet2DModel\n\n\n\n\n\nWe can see a subtle but noticeable improvement in UNet2DModel image quality. This suggests that UNet2DModel iterative refinement can still be a valuable technique to squeeze out a bit more image quality for direct image prediction. However, it’s crucial to remember that this iterative process is still a simplification. It is not representative of true diffusion model sampling. We are essentially manually guiding the image towards a cleaner state by repeatedly using our direct prediction model, rather than implementing the probabilistic reverse diffusion process that is at the heart of true diffusion models.\nIn the next and final part of this series, we will finally move beyond direct image prediction and delve into the core principles of diffusion models: noise prediction and scheduled denoising. This will unlock the true power and flexibility of diffusion models for image generation!"
  },
  {
    "objectID": "posts/2025-02-15-diffusion-model-mnist-part2.html#discussion-and-key-takeaways",
    "href": "posts/2025-02-15-diffusion-model-mnist-part2.html#discussion-and-key-takeaways",
    "title": "From Noise to Numbers: A Hands-On Guide to Generating MNIST Digits with Diffusion Models (Part 2)",
    "section": "Discussion and Key Takeaways",
    "text": "Discussion and Key Takeaways\nIn Part 2, we took a significant step forward by enhancing our UNet architecture. By replacing our BasicUNet with the UNet2DModel from the diffusers library, we aimed to improve the quality of our MNIST digit denoising through direct image prediction.\nOur experiments and visual analysis suggest the following key takeaways:\n\nUNet2DModel Shows Improvement: Visually, the UNet2DModel does appear to denoise MNIST digits more effectively than our BasicUNet from Part 1.\nIterative Refinement Still Helps: Even with the enhanced UNet2DModel, we found that iterative refinement could still further improve the visual quality of the denoised digits, leading to slightly sharper and more well-formed digits. This reinforces the idea that even with a better model, iterative approaches can be a useful strategy to refine outputs in direct prediction scenarios.\ndiffusers is a Powerful Tool: This part highlighted the value of using libraries like diffusers. By leveraging UNet2DModel, we were able to quickly implement a more advanced UNet architecture without having to build it from scratch. diffusers provides a rich set of pre-built, optimized components that can significantly accelerate diffusion model development and experimentation.\nDirect Prediction is Limited: Despite the improvements with UNet2DModel, it’s crucial to remember that we are still working within the paradigm of direct image prediction. While we can achieve some level of denoising and even generate somewhat digit-like images from pure noise through iterative refinement, this approach is fundamentally different from how true diffusion models work. It lacks the probabilistic nature and controlled noise scheduling that are the hallmarks of diffusion models.\n\nNext Steps: Embracing True Diffusion!\nIn Part 3, we will finally move beyond direct image prediction and dive into the heart of diffusion models! We will transition to noise prediction – training our model to predict the noise in a noisy image, rather than directly predicting the clean image itself. We will also introduce scheduled denoising, a key technique that controls the denoising process over multiple steps, mimicking the reverse of the gradual noise addition process in diffusion models."
  },
  {
    "objectID": "posts/2025-07-28-the-horse-in-motion-ffmpeg-gotchas-part-1.html#introduction",
    "href": "posts/2025-07-28-the-horse-in-motion-ffmpeg-gotchas-part-1.html#introduction",
    "title": "The Horse in Motion and FFmpeg Gotchas - Part 1",
    "section": "Introduction",
    "text": "Introduction\nI recently worked on a project that involved images, videos, and audio—transforming and stitching them together into a cohesive experience. I started with MoviePy, but it turned out to be too slow for most of the heavy lifting. So I switched to FFmpeg and was immediately impressed by its speed. That said, working with FFmpeg came with its own set of challenges.\nThis post is the first in a series where I’ll walk through some of the gotchas I ran into while working with FFmpeg. The project itself involved taking real estate images and turning them into virtual property tours using generative AI. Each video also had embedded narration, giving the feel of a live guided tour. For the generative part, I used external services — RunwayML for generating video segments and OpenAI for voiceovers. FFmpeg was the glue that stitched everything together into a final polished video. The service was deployed as an API where users could upload property images, and I used AWS Lambda and Step Functions for code execution and orchestration.\nTo simulate some of the challenges I faced, I decided to recreate the famous “The Horse in Motion” video. Originally created by Eadweard Muybridge in 1878, it’s a landmark in the history of motion pictures and has been widely studied and referenced.\nIn this first part, we’ll create the video using Google Colab. We’ll start with still frames from the original motion study, convert them into a video, and overlay some text using FFmpeg’s drawtext filter. In upcoming posts, we’ll try running all of this within an AWS Lambda function—and work through the issues that pop up along the way."
  },
  {
    "objectID": "posts/2025-07-28-the-horse-in-motion-ffmpeg-gotchas-part-1.html#environment-details",
    "href": "posts/2025-07-28-the-horse-in-motion-ffmpeg-gotchas-part-1.html#environment-details",
    "title": "The Horse in Motion and FFmpeg Gotchas - Part 1",
    "section": "Environment Details",
    "text": "Environment Details\nYou can access and run this Jupyter Notebook from the GitHub repository on this link: 2025-07-28-the-horse-in-motion-ffmpeg-gotchas-part-1.ipynb\nThis notebook was created and tested in Google Colab. The great thing about Google Colab is that it comes with FFmpeg pre-installed, so you can run the code without any additional setup. You can run it in any Jupyter environment, but you might need to install FFmpeg first.\nLet’s start by checking the installed FFmpeg version.\n\n!ffmpeg -version\n\nffmpeg version 4.4.2-0ubuntu0.22.04.1 Copyright (c) 2000-2021 the FFmpeg developers\nbuilt with gcc 11 (Ubuntu 11.2.0-19ubuntu1)\nconfiguration: --prefix=/usr --extra-version=0ubuntu0.22.04.1 --toolchain=hardened --libdir=/usr/lib/x86_64-linux-gnu --incdir=/usr/include/x86_64-linux-gnu --arch=amd64 --enable-gpl --disable-stripping --enable-gnutls --enable-ladspa --enable-libaom --enable-libass --enable-libbluray --enable-libbs2b --enable-libcaca --enable-libcdio --enable-libcodec2 --enable-libdav1d --enable-libflite --enable-libfontconfig --enable-libfreetype --enable-libfribidi --enable-libgme --enable-libgsm --enable-libjack --enable-libmp3lame --enable-libmysofa --enable-libopenjpeg --enable-libopenmpt --enable-libopus --enable-libpulse --enable-librabbitmq --enable-librubberband --enable-libshine --enable-libsnappy --enable-libsoxr --enable-libspeex --enable-libsrt --enable-libssh --enable-libtheora --enable-libtwolame --enable-libvidstab --enable-libvorbis --enable-libvpx --enable-libwebp --enable-libx265 --enable-libxml2 --enable-libxvid --enable-libzimg --enable-libzmq --enable-libzvbi --enable-lv2 --enable-omx --enable-openal --enable-opencl --enable-opengl --enable-sdl2 --enable-pocketsphinx --enable-librsvg --enable-libmfx --enable-libdc1394 --enable-libdrm --enable-libiec61883 --enable-chromaprint --enable-frei0r --enable-libx264 --enable-shared\nlibavutil      56. 70.100 / 56. 70.100\nlibavcodec     58.134.100 / 58.134.100\nlibavformat    58. 76.100 / 58. 76.100\nlibavdevice    58. 13.100 / 58. 13.100\nlibavfilter     7.110.100 /  7.110.100\nlibswscale      5.  9.100 /  5.  9.100\nlibswresample   3.  9.100 /  3.  9.100\nlibpostproc    55.  9.100 / 55.  9.100\n\n\nWe’ll also install and use the watermark library. It’s an IPython magic extension that’s incredibly useful for printing environment details like Python versions, library versions, and hardware information. It’s a great practice for reproducibility.\n\n%%capture\n!pip install watermark\n\n\n%load_ext watermark\n%watermark -v -m -p watermark,matplotlib,PIL\n\nPython implementation: CPython\nPython version       : 3.11.13\nIPython version      : 7.34.0\n\nwatermark : 2.5.0\nmatplotlib: 3.10.0\nPIL       : 11.3.0\n\nCompiler    : GCC 11.4.0\nOS          : Linux\nRelease     : 6.1.123+\nMachine     : x86_64\nProcessor   : x86_64\nCPU cores   : 2\nArchitecture: 64bit"
  },
  {
    "objectID": "posts/2025-07-28-the-horse-in-motion-ffmpeg-gotchas-part-1.html#a-quick-intro-to-ffmpeg",
    "href": "posts/2025-07-28-the-horse-in-motion-ffmpeg-gotchas-part-1.html#a-quick-intro-to-ffmpeg",
    "title": "The Horse in Motion and FFmpeg Gotchas - Part 1",
    "section": "A Quick Intro to FFmpeg",
    "text": "A Quick Intro to FFmpeg\nBefore we dive in, what exactly is FFmpeg ?\nThink of FFmpeg as the Swiss Army knife for multimedia. It’s not a single program you click on, but a powerful command-line framework that can decode, encode, transcode, mux, demux, stream, filter, and play virtually any media format created. It’s the silent engine behind many famous applications like VLC, Plex, and even YouTube, which uses it for video processing.\nIts power comes from its command-line nature. You string together a series of flags and options to tell it exactly what you want to do. This can feel intimidating at first, but once you grasp the basic pattern, you unlock a world of possibilities for automating video and audio manipulation."
  },
  {
    "objectID": "posts/2025-07-28-the-horse-in-motion-ffmpeg-gotchas-part-1.html#download-the-video-frames",
    "href": "posts/2025-07-28-the-horse-in-motion-ffmpeg-gotchas-part-1.html#download-the-video-frames",
    "title": "The Horse in Motion and FFmpeg Gotchas - Part 1",
    "section": "Download the video frames",
    "text": "Download the video frames\nFirst, we need the raw materials: the individual frames of “The Horse in Motion.” I’ve hosted them on a public GitHub repository. The following Python code will create a directory called video_frames and download the 15 images into it.\n\nimport os\nimport requests\n\n# Create the output directory if it doesn't exist\noutput_dir = \"video_frames\"\nos.makedirs(output_dir, exist_ok=True)\n\n# Base URL for the raw images\nbase_url = \"https://raw.githubusercontent.com/hassaanbinaslam/myblog/5c15e72dde03112c5c8dea177bfed7c835aca399/posts/images/2025-07-28-the-horse-in-motion-ffmpeg-gotchas-part-1/video_frames\"\n\n# Download each frame\nfor i in range(1, 16):\n    frame_number = str(i).zfill(2)  # Pad with zero if needed (e.g., 01, 02)\n    image_url = f\"{base_url}/frame{frame_number}.png\"\n    response = requests.get(image_url)\n    if response.status_code == 200:\n        with open(os.path.join(output_dir, f\"frame{frame_number}.png\"), \"wb\") as f:\n            f.write(response.content)\n        print(f\"Downloaded frame{frame_number}.png\")\n    else:\n        print(f\"Failed to download frame{frame_number}.png\")\n\nprint(\"\\nListing downloaded files:\")\n!ls video_frames\n\nDownloaded frame01.png\nDownloaded frame02.png\nDownloaded frame03.png\nDownloaded frame04.png\nDownloaded frame05.png\nDownloaded frame06.png\nDownloaded frame07.png\nDownloaded frame08.png\nDownloaded frame09.png\nDownloaded frame10.png\nDownloaded frame11.png\nDownloaded frame12.png\nDownloaded frame13.png\nDownloaded frame14.png\nDownloaded frame15.png\n\nListing downloaded files:\nframe01.png  frame04.png  frame07.png  frame10.png  frame13.png\nframe02.png  frame05.png  frame08.png  frame11.png  frame14.png\nframe03.png  frame06.png  frame09.png  frame12.png  frame15.png\n\n\nBefore we stitch them into a video, let’s use matplotlib and Pillow to have a look at the frames we just downloaded. This helps confirm everything looks correct.\n\nimport os\nimport matplotlib.pyplot as plt\nfrom PIL import Image\n\n# Get the list of image files\nimage_files = sorted([os.path.join(\"video_frames\", f) for f in os.listdir(\"video_frames\") if f.endswith(\".png\")])\n\n# Create a grid of subplots\nfig, axes = plt.subplots(3, 5, figsize=(15, 9))\naxes = axes.flatten()\n\n# Display each image\nfor i, (img_path, ax) in enumerate(zip(image_files, axes)):\n    img = Image.open(img_path)\n    ax.imshow(img)\n    ax.set_title(f\"Frame {i+1}\")\n    ax.axis(\"off\")\n\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "posts/2025-07-28-the-horse-in-motion-ffmpeg-gotchas-part-1.html#creating-the-video-with-ffmpeg",
    "href": "posts/2025-07-28-the-horse-in-motion-ffmpeg-gotchas-part-1.html#creating-the-video-with-ffmpeg",
    "title": "The Horse in Motion and FFmpeg Gotchas - Part 1",
    "section": "Creating the Video with FFmpeg",
    "text": "Creating the Video with FFmpeg\nNow for the main event. We’ll use a single FFmpeg command to take our 15 separate png images and turn them into a looping mp4 video file with text overlaid.\n\n!ffmpeg \\\n-stream_loop -1 -framerate 1.5 -i video_frames/frame%02d.png \\\n-vf \"drawtext=fontfile=/usr/share/fonts/truetype/liberation/LiberationSans-Regular.ttf:text='The Horse in Motion and FFmpeg Gotchas':fontcolor=white:fontsize=14:box=1:boxcolor=black@0.8:boxborderw=5:x=(w-text_w)/2:y=(h-text_h)/2:enable='between(t,0,10)'\" \\\n-c:v libx264 -r 30 -pix_fmt yuv420p -t 40 \\\noutput_with_text.mp4\n\n\nffmpeg version 4.4.2-0ubuntu0.22.04.1 Copyright (c) 2000-2021 the FFmpeg developers\n\n  built with gcc 11 (Ubuntu 11.2.0-19ubuntu1)\n\n  configuration: --prefix=/usr --extra-version=0ubuntu0.22.04.1 --toolchain=hardened --libdir=/usr/lib/x86_64-linux-gnu --incdir=/usr/include/x86_64-linux-gnu --arch=amd64 --enable-gpl --disable-stripping --enable-gnutls --enable-ladspa --enable-libaom --enable-libass --enable-libbluray --enable-libbs2b --enable-libcaca --enable-libcdio --enable-libcodec2 --enable-libdav1d --enable-libflite --enable-libfontconfig --enable-libfreetype --enable-libfribidi --enable-libgme --enable-libgsm --enable-libjack --enable-libmp3lame --enable-libmysofa --enable-libopenjpeg --enable-libopenmpt --enable-libopus --enable-libpulse --enable-librabbitmq --enable-librubberband --enable-libshine --enable-libsnappy --enable-libsoxr --enable-libspeex --enable-libsrt --enable-libssh --enable-libtheora --enable-libtwolame --enable-libvidstab --enable-libvorbis --enable-libvpx --enable-libwebp --enable-libx265 --enable-libxml2 --enable-libxvid --enable-libzimg --enable-libzmq --enable-libzvbi --enable-lv2 --enable-omx --enable-openal --enable-opencl --enable-opengl --enable-sdl2 --enable-pocketsphinx --enable-librsvg --enable-libmfx --enable-libdc1394 --enable-libdrm --enable-libiec61883 --enable-chromaprint --enable-frei0r --enable-libx264 --enable-shared\n\n  libavutil      56. 70.100 / 56. 70.100\n\n  libavcodec     58.134.100 / 58.134.100\n\n  libavformat    58. 76.100 / 58. 76.100\n\n  libavdevice    58. 13.100 / 58. 13.100\n\n  libavfilter     7.110.100 /  7.110.100\n\n  libswscale      5.  9.100 /  5.  9.100\n\n  libswresample   3.  9.100 /  3.  9.100\n\n  libpostproc    55.  9.100 / 55.  9.100\n\nInput #0, image2, from 'video_frames/frame%02d.png':\n\n  Duration: 00:00:10.00, start: 0.000000, bitrate: N/A\n\n  Stream #0:0: Video: png, rgba(pc), 300x200, 1.50 fps, 1.50 tbr, 1.50 tbn, 1.50 tbc\n\nStream mapping:\n\n  Stream #0:0 -&gt; #0:0 (png (native) -&gt; h264 (libx264))\n\nPress [q] to stop, [?] for help\n\n[libx264 @ 0x575690e3ff40] using cpu capabilities: MMX2 SSE2Fast SSSE3 SSE4.2 AVX FMA3 BMI2 AVX2\n\n[libx264 @ 0x575690e3ff40] profile High, level 1.3, 4:2:0, 8-bit\n\n[libx264 @ 0x575690e3ff40] 264 - core 163 r3060 5db6aa6 - H.264/MPEG-4 AVC codec - Copyleft 2003-2021 - http://www.videolan.org/x264.html - options: cabac=1 ref=3 deblock=1:0:0 analyse=0x3:0x113 me=hex subme=7 psy=1 psy_rd=1.00:0.00 mixed_ref=1 me_range=16 chroma_me=1 trellis=1 8x8dct=1 cqm=0 deadzone=21,11 fast_pskip=1 chroma_qp_offset=-2 threads=3 lookahead_threads=1 sliced_threads=0 nr=0 decimate=1 interlaced=0 bluray_compat=0 constrained_intra=0 bframes=3 b_pyramid=2 b_adapt=1 b_bias=0 direct=1 weightb=1 open_gop=0 weightp=2 keyint=250 keyint_min=25 scenecut=40 intra_refresh=0 rc_lookahead=40 rc=crf mbtree=1 crf=23.0 qcomp=0.60 qpmin=0 qpmax=69 qpstep=4 ip_ratio=1.40 aq=1:1.00\n\nOutput #0, mp4, to 'output_with_text.mp4':\n\n  Metadata:\n\n    encoder         : Lavf58.76.100\n\n  Stream #0:0: Video: h264 (avc1 / 0x31637661), yuv420p(tv, progressive), 300x200, q=2-31, 30 fps, 15360 tbn\n\n    Metadata:\n\n      encoder         : Lavc58.134.100 libx264\n\n    Side data:\n\n      cpb: bitrate max/min/avg: 0/0/0 buffer size: 0 vbv_delay: N/A\n\nMore than 1000 frames duplicated\n\nframe= 1199 fps=495 q=-1.0 Lsize=     571kB time=00:00:39.86 bitrate= 117.3kbits/s dup=1139 drop=0 speed=16.5x    \n\nvideo:556kB audio:0kB subtitle:0kB other streams:0kB global headers:0kB muxing overhead: 2.672978%\n\n[libx264 @ 0x575690e3ff40] frame I:7     Avg QP:17.65  size: 11831\n\n[libx264 @ 0x575690e3ff40] frame P:300   Avg QP:19.27  size:  1576\n\n[libx264 @ 0x575690e3ff40] frame B:892   Avg QP:23.90  size:    15\n\n[libx264 @ 0x575690e3ff40] consecutive B-frames:  0.7%  0.0%  1.3% 98.1%\n\n[libx264 @ 0x575690e3ff40] mb I  I16..4:  8.3% 47.7% 44.0%\n\n[libx264 @ 0x575690e3ff40] mb P  I16..4:  0.7%  7.4%  4.7%  P16..4:  2.8%  2.4%  1.4%  0.0%  0.0%    skip:80.6%\n\n[libx264 @ 0x575690e3ff40] mb B  I16..4:  0.0%  0.0%  0.0%  B16..8:  0.4%  0.0%  0.0%  direct: 0.0%  skip:99.6%  L0:22.5% L1:77.3% BI: 0.2%\n\n[libx264 @ 0x575690e3ff40] 8x8 transform intra:55.9% inter:47.1%\n\n[libx264 @ 0x575690e3ff40] coded y,uvDC,uvAC intra: 92.8% 0.0% 0.0% inter: 1.2% 0.0% 0.0%\n\n[libx264 @ 0x575690e3ff40] i16 v,h,dc,p:  5% 33% 20% 42%\n\n[libx264 @ 0x575690e3ff40] i8 v,h,dc,ddl,ddr,vr,hd,vl,hu: 12% 24% 28%  5%  5%  4%  6%  4% 11%\n\n[libx264 @ 0x575690e3ff40] i4 v,h,dc,ddl,ddr,vr,hd,vl,hu: 21% 20% 14%  7%  9%  6%  9%  5%  9%\n\n[libx264 @ 0x575690e3ff40] i8c dc,h,v,p: 100%  0%  0%  0%\n\n[libx264 @ 0x575690e3ff40] Weighted P-Frames: Y:9.7% UV:0.0%\n\n[libx264 @ 0x575690e3ff40] ref P L0: 60.5% 23.8% 14.2%  1.5%  0.1%\n\n[libx264 @ 0x575690e3ff40] ref B L0: 92.2%  7.8%\n\n[libx264 @ 0x575690e3ff40] ref B L1: 84.0% 16.0%\n\n[libx264 @ 0x575690e3ff40] kb/s:113.87\n\n\n\n\nLet’s view the output video.\nVideo link: output_with_text.mp4\n\n\nUnderstanding the FFmpeg Command\nThat command looks complicated, so let’s break it down flag by flag. This is the key to understanding FFmpeg.\n\n!ffmpeg: The ! tells our Jupyter Notebook to execute this as a shell command. ffmpeg is the program we’re running.\n-stream_loop -1: This is an input option that tells FFmpeg to loop the input stream (-1 means loop indefinitely). Since we only have 15 frames, this allows us to create a video longer than just a few seconds.\n-framerate 1.5: This sets the input framerate. It tells FFmpeg to treat our sequence of images as a video playing at a very slow 1.5 frames per second, giving it that classic, choppy, old-timey feel.\n-i video_frames/frame%02d.png: This is our input file. The %02d is a pattern that tells FFmpeg to look for image files named frame01.png, frame02.png, frame03.png, and so on.\n-vf \"...\": This stands for Video Filter. It’s where we apply transformations to the video. We’re using the drawtext filter here to burn text onto the video frames. We’ll look at this more closely in a moment.\n-c:v libx264: This sets the video codec. -c:v means “codec for video,” and libx264 is a very popular, high-quality, and widely compatible H.264 video encoder.\n-r 30: This sets the output framerate to 30 frames per second. This is a common gotcha! We read the input at 1.5 fps but write the output at 30 fps. FFmpeg handles this by duplicating frames to fill in the gaps, resulting in a smooth video file that plays correctly everywhere.\n-pix_fmt yuv420p: This sets the pixel format. Don’t worry too much about the details, but yuv420p is the most common format for web video and ensures our MP4 is compatible with the widest range of players and browsers.\n-t 40: This sets the total duration of the output video to 40 seconds.\noutput_with_text.mp4: This is simply the name of our output file."
  },
  {
    "objectID": "posts/2025-07-28-the-horse-in-motion-ffmpeg-gotchas-part-1.html#inspecting-the-output-with-ffprobe",
    "href": "posts/2025-07-28-the-horse-in-motion-ffmpeg-gotchas-part-1.html#inspecting-the-output-with-ffprobe",
    "title": "The Horse in Motion and FFmpeg Gotchas - Part 1",
    "section": "Inspecting the Output with ffprobe",
    "text": "Inspecting the Output with ffprobe\nWe’ve created our video, but how do we know it worked correctly? Did FFmpeg actually apply the settings we asked for? This is where ffprobe, FFmpeg’s companion tool for media analysis, comes in.\nIf ffmpeg is the factory that builds the car, ffprobe is the inspector who reads the spec sheet to make sure it was built correctly. It doesn’t change the file; it just tells you everything about it.\nLet’s run it on our new video to inspect its properties. We’ll ask for the video stream information in a clean, readable format.\n\n!ffprobe -v quiet -print_format default=noprint_wrappers=1 -show_format -show_streams /content/output_with_text.mp4\n\nindex=0\ncodec_name=h264\ncodec_long_name=H.264 / AVC / MPEG-4 AVC / MPEG-4 part 10\nprofile=High\ncodec_type=video\ncodec_tag_string=avc1\ncodec_tag=0x31637661\nwidth=300\nheight=200\ncoded_width=300\ncoded_height=200\nclosed_captions=0\nhas_b_frames=2\nsample_aspect_ratio=N/A\ndisplay_aspect_ratio=N/A\npix_fmt=yuv420p\nlevel=13\ncolor_range=unknown\ncolor_space=unknown\ncolor_transfer=unknown\ncolor_primaries=unknown\nchroma_location=left\nfield_order=unknown\nrefs=1\nis_avc=true\nnal_length_size=4\nid=N/A\nr_frame_rate=30/1\navg_frame_rate=30/1\ntime_base=1/15360\nstart_pts=0\nstart_time=0.000000\nduration_ts=613888\nduration=39.966667\nbit_rate=114005\nmax_bit_rate=N/A\nbits_per_raw_sample=8\nnb_frames=1199\nnb_read_frames=N/A\nnb_read_packets=N/A\nDISPOSITION:default=1\nDISPOSITION:dub=0\nDISPOSITION:original=0\nDISPOSITION:comment=0\nDISPOSITION:lyrics=0\nDISPOSITION:karaoke=0\nDISPOSITION:forced=0\nDISPOSITION:hearing_impaired=0\nDISPOSITION:visual_impaired=0\nDISPOSITION:clean_effects=0\nDISPOSITION:attached_pic=0\nDISPOSITION:timed_thumbnails=0\nTAG:language=und\nTAG:handler_name=VideoHandler\nTAG:vendor_id=[0][0][0][0]\nfilename=/content/output_with_text.mp4\nnb_streams=1\nnb_programs=0\nformat_name=mov,mp4,m4a,3gp,3g2,mj2\nformat_long_name=QuickTime / MOV\nstart_time=0.000000\nduration=39.967000\nsize=584776\nbit_rate=117051\nprobe_score=100\nTAG:major_brand=isom\nTAG:minor_version=512\nTAG:compatible_brands=isomiso2avc1mp41\nTAG:encoder=Lavf58.76.100\n\n\nLet’s break down the ffprobe command and see how the output confirms our ffmpeg settings:\n\n-v quiet: This tells ffprobe to be quiet and not print its usual banner and logging info.\n-print_format default=noprint_wrappers=1: This formats the output as simple key=value lines, which is much easier to read than the default JSON.\n-show_streams: This is the core instruction, telling ffprobe to show information about the media streams (in this case, our single video stream).\n\nNow, let’s look at the output. We can see our ffmpeg flags reflected perfectly:\n\ncodec_name=h264: Confirms we used the libx264 encoder.\nwidth=300, height=200: Confirms the resolution is correct.\npix_fmt=yuv420p: Confirms our pixel format for compatibility.\navg_frame_rate=30/1: Confirms our output framerate is 30 fps, just as we set with -r 30.\nduration=39.966667: Confirms the video duration is approximately 40 seconds, matching our -t 40 flag.\n\nUsing ffprobe is a crucial step in any FFmpeg workflow. It gives you confidence that your command worked as expected and is the first tool you should reach for when a video isn’t behaving correctly."
  },
  {
    "objectID": "posts/2025-07-28-the-horse-in-motion-ffmpeg-gotchas-part-1.html#a-word-on-codecs",
    "href": "posts/2025-07-28-the-horse-in-motion-ffmpeg-gotchas-part-1.html#a-word-on-codecs",
    "title": "The Horse in Motion and FFmpeg Gotchas - Part 1",
    "section": "A Word on Codecs",
    "text": "A Word on Codecs\nWe used -c:v libx264, but what is a “codec”? The word stands for Coder-Decoder. Think of it like a language for video.\n\nAn encoder compresses and writes the video file in a specific language (like H.264).\nA decoder reads the file and plays it back, speaking the same language.\n\nYour FFmpeg build comes with a set of built-in codecs. Not every build has every codec, which can be a huge gotcha in different environments. You can see all the codecs your current FFmpeg build supports with this command:\n!ffmpeg -codecs\nThe output is long, but you’ll see lines like this:\nDEV.LS h264          H.264 / AVC / MPEG-4 AVC / MPEG-4 part 10 (decoders: h264 h264_qsv h264_cuvid ) (encoders: libx264 libx264rgb h264_nvenc h264_omx h264_qsv )\nThe letters at the beginning tell you what FFmpeg can do with this format. D means decoding is supported, E means encoding is supported, and V means it’s a video codec. Our libx264 is listed as an available encoder.\n\n!ffmpeg -codecs\n\nffmpeg version 4.4.2-0ubuntu0.22.04.1 Copyright (c) 2000-2021 the FFmpeg developers\n  built with gcc 11 (Ubuntu 11.2.0-19ubuntu1)\n  configuration: --prefix=/usr --extra-version=0ubuntu0.22.04.1 --toolchain=hardened --libdir=/usr/lib/x86_64-linux-gnu --incdir=/usr/include/x86_64-linux-gnu --arch=amd64 --enable-gpl --disable-stripping --enable-gnutls --enable-ladspa --enable-libaom --enable-libass --enable-libbluray --enable-libbs2b --enable-libcaca --enable-libcdio --enable-libcodec2 --enable-libdav1d --enable-libflite --enable-libfontconfig --enable-libfreetype --enable-libfribidi --enable-libgme --enable-libgsm --enable-libjack --enable-libmp3lame --enable-libmysofa --enable-libopenjpeg --enable-libopenmpt --enable-libopus --enable-libpulse --enable-librabbitmq --enable-librubberband --enable-libshine --enable-libsnappy --enable-libsoxr --enable-libspeex --enable-libsrt --enable-libssh --enable-libtheora --enable-libtwolame --enable-libvidstab --enable-libvorbis --enable-libvpx --enable-libwebp --enable-libx265 --enable-libxml2 --enable-libxvid --enable-libzimg --enable-libzmq --enable-libzvbi --enable-lv2 --enable-omx --enable-openal --enable-opencl --enable-opengl --enable-sdl2 --enable-pocketsphinx --enable-librsvg --enable-libmfx --enable-libdc1394 --enable-libdrm --enable-libiec61883 --enable-chromaprint --enable-frei0r --enable-libx264 --enable-shared\n  libavutil      56. 70.100 / 56. 70.100\n  libavcodec     58.134.100 / 58.134.100\n  libavformat    58. 76.100 / 58. 76.100\n  libavdevice    58. 13.100 / 58. 13.100\n  libavfilter     7.110.100 /  7.110.100\n  libswscale      5.  9.100 /  5.  9.100\n  libswresample   3.  9.100 /  3.  9.100\n  libpostproc    55.  9.100 / 55.  9.100\nCodecs:\n D..... = Decoding supported\n .E.... = Encoding supported\n ..V... = Video codec\n ..A... = Audio codec\n ..S... = Subtitle codec\n ...I.. = Intra frame-only codec\n ....L. = Lossy compression\n .....S = Lossless compression\n -------\n D.VI.S 012v                 Uncompressed 4:2:2 10-bit\n D.V.L. 4xm                  4X Movie\n D.VI.S 8bps                 QuickTime 8BPS video\n .EVIL. a64_multi            Multicolor charset for Commodore 64 (encoders: a64multi )\n .EVIL. a64_multi5           Multicolor charset for Commodore 64, extended with 5th color (colram) (encoders: a64multi5 )\n D.V..S aasc                 Autodesk RLE\n D.V.L. agm                  Amuse Graphics Movie\n D.VIL. aic                  Apple Intermediate Codec\n DEVI.S alias_pix            Alias/Wavefront PIX image\n DEVIL. amv                  AMV Video\n D.V.L. anm                  Deluxe Paint Animation\n D.V.L. ansi                 ASCII/ANSI art\n DEV..S apng                 APNG (Animated Portable Network Graphics) image\n D.V.L. arbc                 Gryphon's Anim Compressor\n D.V.L. argo                 Argonaut Games Video\n DEVIL. asv1                 ASUS V1\n DEVIL. asv2                 ASUS V2\n D.VIL. aura                 Auravision AURA\n D.VIL. aura2                Auravision Aura 2\n DEV.L. av1                  Alliance for Open Media AV1 (decoders: libdav1d libaom-av1 av1 av1_cuvid av1_qsv ) (encoders: libaom-av1 )\n D.V... avrn                 Avid AVI Codec\n DEVI.S avrp                 Avid 1:1 10-bit RGB Packer\n D.V.L. avs                  AVS (Audio Video Standard) video\n ..V.L. avs2                 AVS2-P2/IEEE1857.4\n ..V.L. avs3                 AVS3-P2/IEEE1857.10\n DEVI.S avui                 Avid Meridien Uncompressed\n DEVI.S ayuv                 Uncompressed packed MS 4:4:4:4\n D.V.L. bethsoftvid          Bethesda VID video\n D.V.L. bfi                  Brute Force & Ignorance\n D.V.L. binkvideo            Bink video\n D.VI.. bintext              Binary text\n D.VI.S bitpacked            Bitpacked\n DEVI.S bmp                  BMP (Windows and OS/2 bitmap)\n D.V..S bmv_video            Discworld II BMV video\n D.VI.S brender_pix          BRender PIX image\n D.V.L. c93                  Interplay C93\n D.V.L. cavs                 Chinese AVS (Audio Video Standard) (AVS1-P2, JiZhun profile)\n D.V.L. cdgraphics           CD Graphics video\n D.V..S cdtoons              CDToons video\n D.VIL. cdxl                 Commodore CDXL video\n DEV.L. cfhd                 GoPro CineForm HD\n DEV.L. cinepak              Cinepak\n D.V.L. clearvideo           Iterated Systems ClearVideo\n DEVIL. cljr                 Cirrus Logic AccuPak\n D.VI.S cllc                 Canopus Lossless Codec\n D.V.L. cmv                  Electronic Arts CMV video (decoders: eacmv )\n D.V... cpia                 CPiA video format\n D.VILS cri                  Cintel RAW\n D.V..S cscd                 CamStudio (decoders: camstudio )\n D.VIL. cyuv                 Creative YUV (CYUV)\n ..V.LS daala                Daala\n D.VILS dds                  DirectDraw Surface image decoder\n D.V.L. dfa                  Chronomaster DFA\n DEV.LS dirac                Dirac (encoders: vc2 )\n DEVIL. dnxhd                VC3/DNxHD\n DEVI.S dpx                  DPX (Digital Picture Exchange) image\n D.V.L. dsicinvideo          Delphine Software International CIN video\n DEVIL. dvvideo              DV (Digital Video)\n D.V..S dxa                  Feeble Files/ScummVM DXA\n D.VI.S dxtory               Dxtory\n D.VIL. dxv                  Resolume DXV\n D.V.L. escape124            Escape 124\n D.V.L. escape130            Escape 130\n DEVILS exr                  OpenEXR image\n DEV..S ffv1                 FFmpeg video codec #1\n DEVI.S ffvhuff              Huffyuv FFmpeg variant\n D.V.L. fic                  Mirillis FIC\n DEVI.S fits                 FITS (Flexible Image Transport System)\n DEV..S flashsv              Flash Screen Video v1\n DEV.L. flashsv2             Flash Screen Video v2\n D.V..S flic                 Autodesk Animator Flic video\n DEV.L. flv1                 FLV / Sorenson Spark / Sorenson H.263 (Flash Video) (decoders: flv ) (encoders: flv )\n D.V..S fmvc                 FM Screen Capture Codec\n D.VI.S fraps                Fraps\n D.VI.S frwu                 Forward Uncompressed\n D.V.L. g2m                  Go2Meeting\n D.V.L. gdv                  Gremlin Digital Video\n DEV..S gif                  CompuServe GIF (Graphics Interchange Format)\n DEV.L. h261                 H.261\n DEV.L. h263                 H.263 / H.263-1996, H.263+ / H.263-1998 / H.263 version 2 (decoders: h263 h263_v4l2m2m ) (encoders: h263 h263_v4l2m2m )\n D.V.L. h263i                Intel H.263\n DEV.L. h263p                H.263+ / H.263-1998 / H.263 version 2\n DEV.LS h264                 H.264 / AVC / MPEG-4 AVC / MPEG-4 part 10 (decoders: h264 h264_v4l2m2m h264_qsv h264_cuvid ) (encoders: libx264 libx264rgb h264_nvenc h264_omx h264_qsv h264_v4l2m2m h264_vaapi nvenc nvenc_h264 )\n DEVIL. hap                  Vidvox Hap\n DEV.L. hevc                 H.265 / HEVC (High Efficiency Video Coding) (decoders: hevc hevc_qsv hevc_v4l2m2m hevc_cuvid ) (encoders: libx265 nvenc_hevc hevc_nvenc hevc_qsv hevc_v4l2m2m hevc_vaapi )\n D.V.L. hnm4video            HNM 4 video\n D.VIL. hq_hqa               Canopus HQ/HQA\n D.VIL. hqx                  Canopus HQX\n DEVI.S huffyuv              HuffYUV\n D.VI.S hymt                 HuffYUV MT\n D.V.L. idcin                id Quake II CIN video (decoders: idcinvideo )\n D.VI.. idf                  iCEDraw text\n D.V.L. iff_ilbm             IFF ACBM/ANIM/DEEP/ILBM/PBM/RGB8/RGBN (decoders: iff )\n D.V.L. imm4                 Infinity IMM4\n D.V.L. imm5                 Infinity IMM5\n D.V.L. indeo2               Intel Indeo 2\n D.V.L. indeo3               Intel Indeo 3\n D.V.L. indeo4               Intel Indeo Video Interactive 4\n D.V.L. indeo5               Intel Indeo Video Interactive 5\n D.V.L. interplayvideo       Interplay MVE video\n D.VIL. ipu                  IPU Video\n DEVILS jpeg2000             JPEG 2000 (decoders: jpeg2000 libopenjpeg ) (encoders: jpeg2000 libopenjpeg )\n DEVILS jpegls               JPEG-LS\n D.VIL. jv                   Bitmap Brothers JV video\n D.V.L. kgv1                 Kega Game Video\n D.V.L. kmvc                 Karl Morton's video codec\n D.VI.S lagarith             Lagarith lossless\n .EVI.S ljpeg                Lossless JPEG\n D.VI.S loco                 LOCO\n D.V.L. lscr                 LEAD Screen Capture\n D.VI.S m101                 Matrox Uncompressed SD\n D.V.L. mad                  Electronic Arts Madcow Video (decoders: eamad )\n DEVI.S magicyuv             MagicYUV video\n D.VIL. mdec                 Sony PlayStation MDEC (Motion DECoder)\n D.V.L. mimic                Mimic\n DEVIL. mjpeg                Motion JPEG (decoders: mjpeg mjpeg_cuvid mjpeg_qsv ) (encoders: mjpeg mjpeg_qsv mjpeg_vaapi )\n D.VIL. mjpegb               Apple MJPEG-B\n D.V.L. mmvideo              American Laser Games MM Video\n D.V.L. mobiclip             MobiClip Video\n D.V.L. motionpixels         Motion Pixels video\n DEV.L. mpeg1video           MPEG-1 video (decoders: mpeg1video mpeg1_v4l2m2m mpeg1_cuvid )\n DEV.L. mpeg2video           MPEG-2 video (decoders: mpeg2video mpegvideo mpeg2_v4l2m2m mpeg2_qsv mpeg2_cuvid ) (encoders: mpeg2video mpeg2_qsv mpeg2_vaapi )\n DEV.L. mpeg4                MPEG-4 part 2 (decoders: mpeg4 mpeg4_v4l2m2m mpeg4_cuvid ) (encoders: mpeg4 libxvid mpeg4_omx mpeg4_v4l2m2m )\n D.V.L. msa1                 MS ATC Screen\n D.VI.S mscc                 Mandsoft Screen Capture Codec\n D.V.L. msmpeg4v1            MPEG-4 part 2 Microsoft variant version 1\n DEV.L. msmpeg4v2            MPEG-4 part 2 Microsoft variant version 2\n DEV.L. msmpeg4v3            MPEG-4 part 2 Microsoft variant version 3 (decoders: msmpeg4 ) (encoders: msmpeg4 )\n D.VI.S msp2                 Microsoft Paint (MSP) version 2\n D.V..S msrle                Microsoft RLE\n D.V.L. mss1                 MS Screen 1\n D.VIL. mss2                 MS Windows Media Video V9 Screen\n DEV.L. msvideo1             Microsoft Video 1\n D.VI.S mszh                 LCL (LossLess Codec Library) MSZH\n D.V.L. mts2                 MS Expression Encoder Screen\n D.V.L. mv30                 MidiVid 3.0\n D.VIL. mvc1                 Silicon Graphics Motion Video Compressor 1\n D.VIL. mvc2                 Silicon Graphics Motion Video Compressor 2\n D.V.L. mvdv                 MidiVid VQ\n D.VIL. mvha                 MidiVid Archive Codec\n D.V..S mwsc                 MatchWare Screen Capture Codec\n D.V.L. mxpeg                Mobotix MxPEG video\n D.VIL. notchlc              NotchLC\n D.V.L. nuv                  NuppelVideo/RTJPEG\n D.V.L. paf_video            Amazing Studio Packed Animation File Video\n DEVI.S pam                  PAM (Portable AnyMap) image\n DEVI.S pbm                  PBM (Portable BitMap) image\n DEVI.S pcx                  PC Paintbrush PCX image\n DEVI.S pfm                  PFM (Portable FloatMap) image\n DEVI.S pgm                  PGM (Portable GrayMap) image\n DEVI.S pgmyuv               PGMYUV (Portable GrayMap YUV) image\n D.VI.S pgx                  PGX (JPEG2000 Test Format)\n D.V.L. photocd              Kodak Photo CD\n D.VIL. pictor               Pictor/PC Paint\n D.VIL. pixlet               Apple Pixlet\n DEV..S png                  PNG (Portable Network Graphics) image\n DEVI.S ppm                  PPM (Portable PixelMap) image\n DEVIL. prores               Apple ProRes (iCodec Pro) (encoders: prores prores_aw prores_ks )\n D.VIL. prosumer             Brooktree ProSumer Video\n D.VI.S psd                  Photoshop PSD file\n D.VIL. ptx                  V.Flash PTX image\n D.VI.S qdraw                Apple QuickDraw\n D.V.L. qpeg                 Q-team QPEG\n DEV..S qtrle                QuickTime Animation (RLE) video\n DEVI.S r10k                 AJA Kona 10-bit RGB Codec\n DEVI.S r210                 Uncompressed RGB 10-bit\n D.V.L. rasc                 RemotelyAnywhere Screen Capture\n DEVI.S rawvideo             raw video\n D.VIL. rl2                  RL2 video\n DEV.L. roq                  id RoQ video (decoders: roqvideo ) (encoders: roqvideo )\n DEV.L. rpza                 QuickTime video (RPZA)\n D.V..S rscc                 innoHeim/Rsupport Screen Capture Codec\n DEV.L. rv10                 RealVideo 1.0\n DEV.L. rv20                 RealVideo 2.0\n D.V.L. rv30                 RealVideo 3.0\n D.V.L. rv40                 RealVideo 4.0\n D.V.L. sanm                 LucasArts SANM/SMUSH video\n D.V.LS scpr                 ScreenPressor\n D.V..S screenpresso         Screenpresso\n D.V.L. sga                  Digital Pictures SGA Video\n DEVI.S sgi                  SGI image\n D.VI.S sgirle               SGI RLE 8-bit\n D.VI.S sheervideo           BitJazz SheerVideo\n D.V.L. simbiosis_imx        Simbiosis Interactive IMX Video\n D.V.L. smackvideo           Smacker video (decoders: smackvid )\n D.V.L. smc                  QuickTime Graphics (SMC)\n D.VIL. smvjpeg              Sigmatel Motion Video\n DEV.LS snow                 Snow\n D.VIL. sp5x                 Sunplus JPEG (SP5X)\n DEVIL. speedhq              NewTek SpeedHQ\n D.VI.S srgc                 Screen Recorder Gold Codec\n DEVI.S sunrast              Sun Rasterfile image\n D.V..S svg                  Scalable Vector Graphics (decoders: librsvg )\n DEV.L. svq1                 Sorenson Vector Quantizer 1 / Sorenson Video 1 / SVQ1\n D.V.L. svq3                 Sorenson Vector Quantizer 3 / Sorenson Video 3 / SVQ3\n DEVI.S targa                Truevision Targa image\n D.VI.S targa_y216           Pinnacle TARGA CineWave YUV16\n D.V.L. tdsc                 TDSC\n D.V.L. tgq                  Electronic Arts TGQ video (decoders: eatgq )\n D.V.L. tgv                  Electronic Arts TGV video (decoders: eatgv )\n DEV.L. theora               Theora (encoders: libtheora )\n D.VIL. thp                  Nintendo Gamecube THP video\n D.V.L. tiertexseqvideo      Tiertex Limited SEQ video\n DEVI.S tiff                 TIFF image\n D.VIL. tmv                  8088flex TMV\n D.V.L. tqi                  Electronic Arts TQI video (decoders: eatqi )\n D.V.L. truemotion1          Duck TrueMotion 1.0\n D.V.L. truemotion2          Duck TrueMotion 2.0\n D.VIL. truemotion2rt        Duck TrueMotion 2.0 Real Time\n D.V..S tscc                 TechSmith Screen Capture Codec (decoders: camtasia )\n D.V.L. tscc2                TechSmith Screen Codec 2\n D.VIL. txd                  Renderware TXD (TeXture Dictionary) image\n D.V.L. ulti                 IBM UltiMotion (decoders: ultimotion )\n DEVI.S utvideo              Ut Video\n DEVI.S v210                 Uncompressed 4:2:2 10-bit\n D.VI.S v210x                Uncompressed 4:2:2 10-bit\n DEVI.S v308                 Uncompressed packed 4:4:4\n DEVI.S v408                 Uncompressed packed QT 4:4:4:4\n DEVI.S v410                 Uncompressed 4:4:4 10-bit\n D.V.L. vb                   Beam Software VB\n D.VI.S vble                 VBLE Lossless Codec\n D.V.L. vc1                  SMPTE VC-1 (decoders: vc1 vc1_qsv vc1_v4l2m2m vc1_cuvid )\n D.V.L. vc1image             Windows Media Video 9 Image v2\n D.VIL. vcr1                 ATI VCR1\n D.VIL. vixl                 Miro VideoXL (decoders: xl )\n D.V.L. vmdvideo             Sierra VMD video\n D.V..S vmnc                 VMware Screen Codec / VMware Video\n D.V.L. vp3                  On2 VP3\n D.V.L. vp4                  On2 VP4\n D.V.L. vp5                  On2 VP5\n D.V.L. vp6                  On2 VP6\n D.V.L. vp6a                 On2 VP6 (Flash version, with alpha channel)\n D.V.L. vp6f                 On2 VP6 (Flash version)\n D.V.L. vp7                  On2 VP7\n DEV.L. vp8                  On2 VP8 (decoders: vp8 vp8_v4l2m2m libvpx vp8_cuvid vp8_qsv ) (encoders: libvpx vp8_v4l2m2m vp8_vaapi )\n DEV.L. vp9                  Google VP9 (decoders: vp9 vp9_v4l2m2m libvpx-vp9 vp9_cuvid vp9_qsv ) (encoders: libvpx-vp9 vp9_vaapi vp9_qsv )\n ..V.L. vvc                  H.266 / VVC (Versatile Video Coding)\n D.V..S wcmv                 WinCAM Motion Video\n DEVILS webp                 WebP (encoders: libwebp_anim libwebp )\n DEV.L. wmv1                 Windows Media Video 7\n DEV.L. wmv2                 Windows Media Video 8\n D.V.L. wmv3                 Windows Media Video 9\n D.V.L. wmv3image            Windows Media Video 9 Image\n D.VIL. wnv1                 Winnov WNV1\n DEV..S wrapped_avframe      AVFrame to AVPacket passthrough\n D.V.L. ws_vqa               Westwood Studios VQA (Vector Quantized Animation) video (decoders: vqavideo )\n D.V.L. xan_wc3              Wing Commander III / Xan\n D.V.L. xan_wc4              Wing Commander IV / Xxan\n D.VI.. xbin                 eXtended BINary text\n DEVI.S xbm                  XBM (X BitMap) image\n DEVIL. xface                X-face image\n D.VI.S xpm                  XPM (X PixMap) image\n DEVI.S xwd                  XWD (X Window Dump) image\n DEVI.S y41p                 Uncompressed YUV 4:1:1 12-bit\n D.VI.S ylc                  YUY2 Lossless Codec\n D.V.L. yop                  Psygnosis YOP Video\n DEVI.S yuv4                 Uncompressed packed 4:2:0\n D.V..S zerocodec            ZeroCodec Lossless Video\n DEVI.S zlib                 LCL (LossLess Codec Library) ZLIB\n DEV..S zmbv                 Zip Motion Blocks Video\n ..AIL. 4gv                  4GV (Fourth Generation Vocoder)\n D.AIL. 8svx_exp             8SVX exponential\n D.AIL. 8svx_fib             8SVX fibonacci\n DEAIL. aac                  AAC (Advanced Audio Coding) (decoders: aac aac_fixed )\n D.AIL. aac_latm             AAC LATM (Advanced Audio Coding LATM syntax)\n DEAIL. ac3                  ATSC A/52A (AC-3) (decoders: ac3 ac3_fixed ) (encoders: ac3 ac3_fixed )\n D.AIL. acelp.kelvin         Sipro ACELP.KELVIN\n D.AIL. adpcm_4xm            ADPCM 4X Movie\n DEAIL. adpcm_adx            SEGA CRI ADX ADPCM\n D.AIL. adpcm_afc            ADPCM Nintendo Gamecube AFC\n D.AIL. adpcm_agm            ADPCM AmuseGraphics Movie AGM\n D.AIL. adpcm_aica           ADPCM Yamaha AICA\n DEAIL. adpcm_argo           ADPCM Argonaut Games\n D.AIL. adpcm_ct             ADPCM Creative Technology\n D.AIL. adpcm_dtk            ADPCM Nintendo Gamecube DTK\n D.AIL. adpcm_ea             ADPCM Electronic Arts\n D.AIL. adpcm_ea_maxis_xa    ADPCM Electronic Arts Maxis CDROM XA\n D.AIL. adpcm_ea_r1          ADPCM Electronic Arts R1\n D.AIL. adpcm_ea_r2          ADPCM Electronic Arts R2\n D.AIL. adpcm_ea_r3          ADPCM Electronic Arts R3\n D.AIL. adpcm_ea_xas         ADPCM Electronic Arts XAS\n DEAIL. adpcm_g722           G.722 ADPCM (decoders: g722 ) (encoders: g722 )\n DEAIL. adpcm_g726           G.726 ADPCM (decoders: g726 ) (encoders: g726 )\n DEAIL. adpcm_g726le         G.726 ADPCM little-endian (decoders: g726le ) (encoders: g726le )\n DEAIL. adpcm_ima_alp        ADPCM IMA High Voltage Software ALP\n DEAIL. adpcm_ima_amv        ADPCM IMA AMV\n D.AIL. adpcm_ima_apc        ADPCM IMA CRYO APC\n DEAIL. adpcm_ima_apm        ADPCM IMA Ubisoft APM\n D.AIL. adpcm_ima_cunning    ADPCM IMA Cunning Developments\n D.AIL. adpcm_ima_dat4       ADPCM IMA Eurocom DAT4\n D.AIL. adpcm_ima_dk3        ADPCM IMA Duck DK3\n D.AIL. adpcm_ima_dk4        ADPCM IMA Duck DK4\n D.AIL. adpcm_ima_ea_eacs    ADPCM IMA Electronic Arts EACS\n D.AIL. adpcm_ima_ea_sead    ADPCM IMA Electronic Arts SEAD\n D.AIL. adpcm_ima_iss        ADPCM IMA Funcom ISS\n D.AIL. adpcm_ima_moflex     ADPCM IMA MobiClip MOFLEX\n D.AIL. adpcm_ima_mtf        ADPCM IMA Capcom's MT Framework\n D.AIL. adpcm_ima_oki        ADPCM IMA Dialogic OKI\n DEAIL. adpcm_ima_qt         ADPCM IMA QuickTime\n D.AIL. adpcm_ima_rad        ADPCM IMA Radical\n D.AIL. adpcm_ima_smjpeg     ADPCM IMA Loki SDL MJPEG\n DEAIL. adpcm_ima_ssi        ADPCM IMA Simon & Schuster Interactive\n DEAIL. adpcm_ima_wav        ADPCM IMA WAV\n D.AIL. adpcm_ima_ws         ADPCM IMA Westwood\n DEAIL. adpcm_ms             ADPCM Microsoft\n D.AIL. adpcm_mtaf           ADPCM MTAF\n D.AIL. adpcm_psx            ADPCM Playstation\n D.AIL. adpcm_sbpro_2        ADPCM Sound Blaster Pro 2-bit\n D.AIL. adpcm_sbpro_3        ADPCM Sound Blaster Pro 2.6-bit\n D.AIL. adpcm_sbpro_4        ADPCM Sound Blaster Pro 4-bit\n DEAIL. adpcm_swf            ADPCM Shockwave Flash\n D.AIL. adpcm_thp            ADPCM Nintendo THP\n D.AIL. adpcm_thp_le         ADPCM Nintendo THP (Little-Endian)\n D.AIL. adpcm_vima           LucasArts VIMA audio\n D.AIL. adpcm_xa             ADPCM CDROM XA\n DEAIL. adpcm_yamaha         ADPCM Yamaha\n D.AIL. adpcm_zork           ADPCM Zork\n DEAI.S alac                 ALAC (Apple Lossless Audio Codec)\n D.AIL. amr_nb               AMR-NB (Adaptive Multi-Rate NarrowBand) (decoders: amrnb )\n D.AIL. amr_wb               AMR-WB (Adaptive Multi-Rate WideBand) (decoders: amrwb )\n D.AI.S ape                  Monkey's Audio\n DEAIL. aptx                 aptX (Audio Processing Technology for Bluetooth)\n DEAIL. aptx_hd              aptX HD (Audio Processing Technology for Bluetooth)\n D.AIL. atrac1               ATRAC1 (Adaptive TRansform Acoustic Coding)\n D.AIL. atrac3               ATRAC3 (Adaptive TRansform Acoustic Coding 3)\n D.AI.S atrac3al             ATRAC3 AL (Adaptive TRansform Acoustic Coding 3 Advanced Lossless)\n D.AIL. atrac3p              ATRAC3+ (Adaptive TRansform Acoustic Coding 3+) (decoders: atrac3plus )\n D.AI.S atrac3pal            ATRAC3+ AL (Adaptive TRansform Acoustic Coding 3+ Advanced Lossless) (decoders: atrac3plusal )\n D.AIL. atrac9               ATRAC9 (Adaptive TRansform Acoustic Coding 9)\n D.AIL. avc                  On2 Audio for Video Codec (decoders: on2avc )\n D.AIL. binkaudio_dct        Bink Audio (DCT)\n D.AIL. binkaudio_rdft       Bink Audio (RDFT)\n D.AIL. bmv_audio            Discworld II BMV audio\n ..AIL. celt                 Constrained Energy Lapped Transform (CELT)\n DEAIL. codec2               codec2 (very low bitrate speech codec) (decoders: libcodec2 ) (encoders: libcodec2 )\n DEAIL. comfortnoise         RFC 3389 Comfort Noise\n D.AIL. cook                 Cook / Cooker / Gecko (RealAudio G2)\n D.AIL. derf_dpcm            DPCM Xilam DERF\n D.AIL. dolby_e              Dolby E\n D.AIL. dsd_lsbf             DSD (Direct Stream Digital), least significant bit first\n D.AIL. dsd_lsbf_planar      DSD (Direct Stream Digital), least significant bit first, planar\n D.AIL. dsd_msbf             DSD (Direct Stream Digital), most significant bit first\n D.AIL. dsd_msbf_planar      DSD (Direct Stream Digital), most significant bit first, planar\n D.AIL. dsicinaudio          Delphine Software International CIN audio\n D.AIL. dss_sp               Digital Speech Standard - Standard Play mode (DSS SP)\n D.AI.S dst                  DST (Direct Stream Transfer)\n DEAILS dts                  DCA (DTS Coherent Acoustics) (decoders: dca ) (encoders: dca )\n D.AIL. dvaudio              DV audio\n DEAIL. eac3                 ATSC A/52B (AC-3, E-AC-3)\n D.AIL. evrc                 EVRC (Enhanced Variable Rate Codec)\n D.AIL. fastaudio            MobiClip FastAudio\n DEAI.S flac                 FLAC (Free Lossless Audio Codec)\n DEAIL. g723_1               G.723.1\n D.AIL. g729                 G.729\n D.AIL. gremlin_dpcm         DPCM Gremlin\n DEAIL. gsm                  GSM (decoders: gsm libgsm ) (encoders: libgsm )\n DEAIL. gsm_ms               GSM Microsoft variant (decoders: gsm_ms libgsm_ms ) (encoders: libgsm_ms )\n D.AIL. hca                  CRI HCA\n D.AIL. hcom                 HCOM Audio\n D.AIL. iac                  IAC (Indeo Audio Coder)\n D.AIL. ilbc                 iLBC (Internet Low Bitrate Codec)\n D.AIL. imc                  IMC (Intel Music Coder)\n D.AIL. interplay_dpcm       DPCM Interplay\n D.AIL. interplayacm         Interplay ACM\n D.AIL. mace3                MACE (Macintosh Audio Compression/Expansion) 3:1\n D.AIL. mace6                MACE (Macintosh Audio Compression/Expansion) 6:1\n D.AIL. metasound            Voxware MetaSound\n DEAI.S mlp                  MLP (Meridian Lossless Packing)\n D.AIL. mp1                  MP1 (MPEG audio layer 1) (decoders: mp1 mp1float )\n DEAIL. mp2                  MP2 (MPEG audio layer 2) (decoders: mp2 mp2float ) (encoders: mp2 mp2fixed libtwolame )\n DEAIL. mp3                  MP3 (MPEG audio layer 3) (decoders: mp3float mp3 ) (encoders: libmp3lame libshine )\n D.AIL. mp3adu               ADU (Application Data Unit) MP3 (MPEG audio layer 3) (decoders: mp3adufloat mp3adu )\n D.AIL. mp3on4               MP3onMP4 (decoders: mp3on4float mp3on4 )\n D.AI.S mp4als               MPEG-4 Audio Lossless Coding (ALS) (decoders: als )\n ..A.L. mpegh_3d_audio       MPEG-H 3D Audio\n D.AIL. musepack7            Musepack SV7 (decoders: mpc7 )\n D.AIL. musepack8            Musepack SV8 (decoders: mpc8 )\n DEAIL. nellymoser           Nellymoser Asao\n DEAIL. opus                 Opus (Opus Interactive Audio Codec) (decoders: opus libopus ) (encoders: opus libopus )\n D.AIL. paf_audio            Amazing Studio Packed Animation File Audio\n DEAIL. pcm_alaw             PCM A-law / G.711 A-law\n D.AI.S pcm_bluray           PCM signed 16|20|24-bit big-endian for Blu-ray media\n DEAI.S pcm_dvd              PCM signed 20|24-bit big-endian\n D.AI.S pcm_f16le            PCM 16.8 floating point little-endian\n D.AI.S pcm_f24le            PCM 24.0 floating point little-endian\n DEAI.S pcm_f32be            PCM 32-bit floating point big-endian\n DEAI.S pcm_f32le            PCM 32-bit floating point little-endian\n DEAI.S pcm_f64be            PCM 64-bit floating point big-endian\n DEAI.S pcm_f64le            PCM 64-bit floating point little-endian\n D.AI.S pcm_lxf              PCM signed 20-bit little-endian planar\n DEAIL. pcm_mulaw            PCM mu-law / G.711 mu-law\n DEAI.S pcm_s16be            PCM signed 16-bit big-endian\n DEAI.S pcm_s16be_planar     PCM signed 16-bit big-endian planar\n DEAI.S pcm_s16le            PCM signed 16-bit little-endian\n DEAI.S pcm_s16le_planar     PCM signed 16-bit little-endian planar\n DEAI.S pcm_s24be            PCM signed 24-bit big-endian\n DEAI.S pcm_s24daud          PCM D-Cinema audio signed 24-bit\n DEAI.S pcm_s24le            PCM signed 24-bit little-endian\n DEAI.S pcm_s24le_planar     PCM signed 24-bit little-endian planar\n DEAI.S pcm_s32be            PCM signed 32-bit big-endian\n DEAI.S pcm_s32le            PCM signed 32-bit little-endian\n DEAI.S pcm_s32le_planar     PCM signed 32-bit little-endian planar\n DEAI.S pcm_s64be            PCM signed 64-bit big-endian\n DEAI.S pcm_s64le            PCM signed 64-bit little-endian\n DEAI.S pcm_s8               PCM signed 8-bit\n DEAI.S pcm_s8_planar        PCM signed 8-bit planar\n D.AI.S pcm_sga              PCM SGA\n DEAI.S pcm_u16be            PCM unsigned 16-bit big-endian\n DEAI.S pcm_u16le            PCM unsigned 16-bit little-endian\n DEAI.S pcm_u24be            PCM unsigned 24-bit big-endian\n DEAI.S pcm_u24le            PCM unsigned 24-bit little-endian\n DEAI.S pcm_u32be            PCM unsigned 32-bit big-endian\n DEAI.S pcm_u32le            PCM unsigned 32-bit little-endian\n DEAI.S pcm_u8               PCM unsigned 8-bit\n DEAIL. pcm_vidc             PCM Archimedes VIDC\n D.AIL. qcelp                QCELP / PureVoice\n D.AIL. qdm2                 QDesign Music Codec 2\n D.AIL. qdmc                 QDesign Music\n DEAIL. ra_144               RealAudio 1.0 (14.4K) (decoders: real_144 ) (encoders: real_144 )\n D.AIL. ra_288               RealAudio 2.0 (28.8K) (decoders: real_288 )\n D.AI.S ralf                 RealAudio Lossless\n DEAIL. roq_dpcm             DPCM id RoQ\n DEAI.S s302m                SMPTE 302M\n DEAIL. sbc                  SBC (low-complexity subband codec)\n D.AIL. sdx2_dpcm            DPCM Squareroot-Delta-Exact\n D.AI.S shorten              Shorten\n D.AIL. sipr                 RealAudio SIPR / ACELP.NET\n D.AIL. siren                Siren\n D.AIL. smackaudio           Smacker audio (decoders: smackaud )\n ..AIL. smv                  SMV (Selectable Mode Vocoder)\n D.AIL. sol_dpcm             DPCM Sol\n DEAI.. sonic                Sonic\n .EAI.. sonicls              Sonic lossless\n DEAIL. speex                Speex (decoders: libspeex ) (encoders: libspeex )\n D.A..S tak                  TAK (Tom's lossless Audio Kompressor)\n DEA..S truehd               TrueHD\n D.AIL. truespeech           DSP Group TrueSpeech\n DEAI.S tta                  TTA (True Audio)\n D.AIL. twinvq               VQF TwinVQ\n D.AIL. vmdaudio             Sierra VMD audio\n DEAIL. vorbis               Vorbis (decoders: vorbis libvorbis ) (encoders: vorbis libvorbis )\n D.AI.. wavesynth            Wave synthesis pseudo-codec\n DEAILS wavpack              WavPack\n D.AIL. westwood_snd1        Westwood Audio (SND1) (decoders: ws_snd1 )\n D.AI.S wmalossless          Windows Media Audio Lossless\n D.AIL. wmapro               Windows Media Audio 9 Professional\n DEAIL. wmav1                Windows Media Audio 1\n DEAIL. wmav2                Windows Media Audio 2\n D.AIL. wmavoice             Windows Media Audio Voice\n D.AIL. xan_dpcm             DPCM Xan\n D.AIL. xma1                 Xbox Media Audio 1\n D.AIL. xma2                 Xbox Media Audio 2\n ..D... bin_data             binary data\n ..D... dvd_nav_packet       DVD Nav packet\n ..D... epg                  Electronic Program Guide\n ..D... klv                  SMPTE 336M Key-Length-Value (KLV) metadata\n ..D... mpegts               raw MPEG-TS stream\n ..D... otf                  OpenType font\n ..D... scte_35              SCTE 35 Message Queue\n ..D... timed_id3            timed ID3 metadata\n ..D... ttf                  TrueType font\n ..S... arib_caption         ARIB STD-B24 caption\n DES... ass                  ASS (Advanced SSA) subtitle (decoders: ssa ass ) (encoders: ssa ass )\n DES... dvb_subtitle         DVB subtitles (decoders: dvbsub ) (encoders: dvbsub )\n D.S... dvb_teletext         DVB teletext (decoders: libzvbi_teletextdec )\n DES... dvd_subtitle         DVD subtitles (decoders: dvdsub ) (encoders: dvdsub )\n D.S... eia_608              EIA-608 closed captions (decoders: cc_dec )\n D.S... hdmv_pgs_subtitle    HDMV Presentation Graphic Stream subtitles (decoders: pgssub )\n ..S... hdmv_text_subtitle   HDMV Text subtitle\n D.S... jacosub              JACOsub subtitle\n D.S... microdvd             MicroDVD subtitle\n DES... mov_text             MOV text\n D.S... mpl2                 MPL2 subtitle\n D.S... pjs                  PJS (Phoenix Japanimation Society) subtitle\n D.S... realtext             RealText subtitle\n D.S... sami                 SAMI subtitle\n ..S... srt                  SubRip subtitle with embedded timing\n ..S... ssa                  SSA (SubStation Alpha) subtitle\n D.S... stl                  Spruce subtitle format\n DES... subrip               SubRip subtitle (decoders: srt subrip ) (encoders: srt subrip )\n D.S... subviewer            SubViewer subtitle\n D.S... subviewer1           SubViewer v1 subtitle\n DES... text                 raw UTF-8 text\n .ES... ttml                 Timed Text Markup Language\n D.S... vplayer              VPlayer subtitle\n DES... webvtt               WebVTT subtitle\n DES... xsub                 XSUB"
  },
  {
    "objectID": "posts/2025-07-28-the-horse-in-motion-ffmpeg-gotchas-part-1.html#the-power-of-filters-and-build-configuration",
    "href": "posts/2025-07-28-the-horse-in-motion-ffmpeg-gotchas-part-1.html#the-power-of-filters-and-build-configuration",
    "title": "The Horse in Motion and FFmpeg Gotchas - Part 1",
    "section": "The Power of Filters and Build Configuration",
    "text": "The Power of Filters and Build Configuration\nThe -vf (video filter) flag is where much of FFmpeg’s magic happens. We used drawtext, but there are filters for scaling, cropping, rotating, color correction, and hundreds of other tasks.\nA critical gotcha with filters is that they often depend on external libraries that must be included when FFmpeg is compiled. The drawtext filter, for instance, needs a font rendering library called freetype.\nHow can you know if your build supports it? You can check the build configuration:\nffmpeg -buildconf\nIn the output, you’ll see a long list of --enable- flags. If you see --enable-libfreetype, you know the drawtext filter should work. This becomes extremely important when you’re building FFmpeg for a minimal environment like Docker or AWS Lambda, where you might not have these libraries by default.\n\n!ffmpeg -buildconf\n\nffmpeg version 4.4.2-0ubuntu0.22.04.1 Copyright (c) 2000-2021 the FFmpeg developers\n  built with gcc 11 (Ubuntu 11.2.0-19ubuntu1)\n  configuration: --prefix=/usr --extra-version=0ubuntu0.22.04.1 --toolchain=hardened --libdir=/usr/lib/x86_64-linux-gnu --incdir=/usr/include/x86_64-linux-gnu --arch=amd64 --enable-gpl --disable-stripping --enable-gnutls --enable-ladspa --enable-libaom --enable-libass --enable-libbluray --enable-libbs2b --enable-libcaca --enable-libcdio --enable-libcodec2 --enable-libdav1d --enable-libflite --enable-libfontconfig --enable-libfreetype --enable-libfribidi --enable-libgme --enable-libgsm --enable-libjack --enable-libmp3lame --enable-libmysofa --enable-libopenjpeg --enable-libopenmpt --enable-libopus --enable-libpulse --enable-librabbitmq --enable-librubberband --enable-libshine --enable-libsnappy --enable-libsoxr --enable-libspeex --enable-libsrt --enable-libssh --enable-libtheora --enable-libtwolame --enable-libvidstab --enable-libvorbis --enable-libvpx --enable-libwebp --enable-libx265 --enable-libxml2 --enable-libxvid --enable-libzimg --enable-libzmq --enable-libzvbi --enable-lv2 --enable-omx --enable-openal --enable-opencl --enable-opengl --enable-sdl2 --enable-pocketsphinx --enable-librsvg --enable-libmfx --enable-libdc1394 --enable-libdrm --enable-libiec61883 --enable-chromaprint --enable-frei0r --enable-libx264 --enable-shared\n  libavutil      56. 70.100 / 56. 70.100\n  libavcodec     58.134.100 / 58.134.100\n  libavformat    58. 76.100 / 58. 76.100\n  libavdevice    58. 13.100 / 58. 13.100\n  libavfilter     7.110.100 /  7.110.100\n  libswscale      5.  9.100 /  5.  9.100\n  libswresample   3.  9.100 /  3.  9.100\n  libpostproc    55.  9.100 / 55.  9.100\n\n  configuration:\n    --prefix=/usr\n    --extra-version=0ubuntu0.22.04.1\n    --toolchain=hardened\n    --libdir=/usr/lib/x86_64-linux-gnu\n    --incdir=/usr/include/x86_64-linux-gnu\n    --arch=amd64\n    --enable-gpl\n    --disable-stripping\n    --enable-gnutls\n    --enable-ladspa\n    --enable-libaom\n    --enable-libass\n    --enable-libbluray\n    --enable-libbs2b\n    --enable-libcaca\n    --enable-libcdio\n    --enable-libcodec2\n    --enable-libdav1d\n    --enable-libflite\n    --enable-libfontconfig\n    --enable-libfreetype\n    --enable-libfribidi\n    --enable-libgme\n    --enable-libgsm\n    --enable-libjack\n    --enable-libmp3lame\n    --enable-libmysofa\n    --enable-libopenjpeg\n    --enable-libopenmpt\n    --enable-libopus\n    --enable-libpulse\n    --enable-librabbitmq\n    --enable-librubberband\n    --enable-libshine\n    --enable-libsnappy\n    --enable-libsoxr\n    --enable-libspeex\n    --enable-libsrt\n    --enable-libssh\n    --enable-libtheora\n    --enable-libtwolame\n    --enable-libvidstab\n    --enable-libvorbis\n    --enable-libvpx\n    --enable-libwebp\n    --enable-libx265\n    --enable-libxml2\n    --enable-libxvid\n    --enable-libzimg\n    --enable-libzmq\n    --enable-libzvbi\n    --enable-lv2\n    --enable-omx\n    --enable-openal\n    --enable-opencl\n    --enable-opengl\n    --enable-sdl2\n    --enable-pocketsphinx\n    --enable-librsvg\n    --enable-libmfx\n    --enable-libdc1394\n    --enable-libdrm\n    --enable-libiec61883\n    --enable-chromaprint\n    --enable-frei0r\n    --enable-libx264\n    --enable-shared"
  },
  {
    "objectID": "posts/2025-07-28-the-horse-in-motion-ffmpeg-gotchas-part-1.html#conclusion-and-next-steps",
    "href": "posts/2025-07-28-the-horse-in-motion-ffmpeg-gotchas-part-1.html#conclusion-and-next-steps",
    "title": "The Horse in Motion and FFmpeg Gotchas - Part 1",
    "section": "Conclusion and Next Steps",
    "text": "Conclusion and Next Steps\nIn this post, we successfully took a sequence of still images and, with a single command, turned them into a looping, annotated video. We’ve demystified some of the most common FFmpeg flags and explored the core concepts of codecs and filters.\nEverything worked smoothly here in Google Colab, which provides a feature-rich FFmpeg build out of the box. But what happens when we move to a more constrained, production-like environment?\nIn Part 2, we’ll take this exact process and try to run it inside an AWS Lambda function. We’ll quickly run into new gotchas related to missing libraries, incorrect file paths, and the challenge of packaging the FFmpeg binary itself. Stay tuned!"
  },
  {
    "objectID": "posts/2025-08-03-the-horse-in-motion-ffmpeg-gotchas-part-3.html#introduction-the-final-frontier",
    "href": "posts/2025-08-03-the-horse-in-motion-ffmpeg-gotchas-part-3.html#introduction-the-final-frontier",
    "title": "The Horse in Motion and FFmpeg Gotchas - Part 3 Building FFmpeg From Scratch",
    "section": "Introduction: The Final Frontier",
    "text": "Introduction: The Final Frontier\nIn Part 1, we mastered the basics of FFmpeg in a friendly Colab environment. In Part 2, we took our code to the cloud, navigating the world of static builds and Lambda Layers to run our process on AWS. We found a pre-built binary that worked, but it wasn’t perfect. We had to compromise, giving up the high-quality libx264 encoder to get the drawtext filter we needed.\nWhat if you can’t compromise? What if you need a specific combination of features that no public build offers? Or what if the available builds are massive—packed with dozens of codecs and filters you’ll never use—making your Lambda deployment package unnecessarily large?\nWelcome to Part 3, where we take the ultimate step: we build FFmpeg ourselves.\nLet me start with a serious word of caution. I do not recommend this lightly. Building FFmpeg from source is a journey into a complex world of libraries, dependencies, and cryptic configuration flags. It took me seven or eight attempts to find the right combination of library versions and compile options. You will likely wander into strange dependency rabbit holes and have to start over. Before you go down this path, exhaust all options for pre-built binaries.\nBut if you must, this guide is for you. By compiling it ourselves, we can create a lean, mean, purpose-built FFmpeg binary that has exactly what we need—libx264 and drawtext—and nothing we don’t. Our final binary will be significantly smaller and perfectly tailored for our AWS Lambda function.\nLet’s roll up our sleeves and build the ultimate video processing tool."
  },
  {
    "objectID": "posts/2025-08-03-the-horse-in-motion-ffmpeg-gotchas-part-3.html#the-workshop-setting-up-our-build-environment",
    "href": "posts/2025-08-03-the-horse-in-motion-ffmpeg-gotchas-part-3.html#the-workshop-setting-up-our-build-environment",
    "title": "The Horse in Motion and FFmpeg Gotchas - Part 3 Building FFmpeg From Scratch",
    "section": "The Workshop: Setting Up Our Build Environment",
    "text": "The Workshop: Setting Up Our Build Environment\nWe can’t compile a project as complex as FFmpeg on a small machine or in a standard Lambda environment. We need a proper workshop. For this task, I used an AWS EC2 instance with the following specifications:\n\nInstance Type: t3.large (I initially tried a t3.micro and repeatedly ran into out-of-memory errors during compilation. Save yourself the headache and start with more power).\nAMI (Amazon Machine Image): Amazon Linux 2023 (kernel 6.1).\n\nOnce your EC2 instance is running and you’ve connected to it via SSH, the first step is to prepare the system for compilation.\n\n\n\nEC2 instance up and running\n\n\nFirst, switch to the root user and update the system packages.\nsudo -i\nyum update -y\n\n\n\nYum update\n\n\nNext, we’ll install a suite of essential development tools and libraries.\nyum groupinstall \"Development Tools\" -y\nyum install -y nasm glibc-static expat-static gperf zlib-static cmake\n\nDevelopment Tools: This is a group of packages that includes essentials like gcc, make, and other compilers.\nnasm: An assembler required by x264.\n-static libraries (glibc-static, etc.): These are crucial for building our self-contained, static binary.\ncmake: A build tool used by some of the libraries we need, like Brotli.\n\n\n\n\n\n\n\nDevelopment tools installed\n\n\n\n\n\n\n\nEssential libraries installed\n\n\n\n\n\nNow, let’s create a dedicated directory to keep all our source code organized.\nmkdir -p ~/ffmpeg_sources\ncd ~/ffmpeg_sources\nFinally, we need to tell the system’s package configuration tool (pkg-config) where to find the custom libraries we are about to build. We’ll install them in /usr/local, so we need to add that path.\nexport PKG_CONFIG_PATH=\"/usr/local/lib/pkgconfig:/usr/lib64/pkgconfig\"\n\n\n\nEnvironment setup complete\n\n\nThis environment variable ensures that when we compile one library, it can find the other libraries it depends on. Our workshop is now ready. It’s time to start forging the components."
  },
  {
    "objectID": "posts/2025-08-03-the-horse-in-motion-ffmpeg-gotchas-part-3.html#the-dependency-gauntlet-compiling-the-libraries",
    "href": "posts/2025-08-03-the-horse-in-motion-ffmpeg-gotchas-part-3.html#the-dependency-gauntlet-compiling-the-libraries",
    "title": "The Horse in Motion and FFmpeg Gotchas - Part 3 Building FFmpeg From Scratch",
    "section": "The Dependency Gauntlet: Compiling the Libraries",
    "text": "The Dependency Gauntlet: Compiling the Libraries\nHere comes the most challenging part. FFmpeg relies on external libraries for many of its features. To get a static build with libx264 and drawtext, we need to first compile static versions of all their dependencies. The order matters! We’ll build them from the ground up.\nFor each library, the pattern will be the same: download the source code, configure the build, compile it, and install it.\n\n1. The Encoder: x264\nThis is the high-quality H.264 video encoder we had to give up in Part 2. Let’s make it our first priority.\ngit clone --branch stable --depth 1 https://code.videolan.org/videolan/x264.git\ncd x264\n./configure --prefix=/usr/local --enable-static --disable-shared --enable-pic\nmake -j$(nproc)\nmake install\ncd /root/ffmpeg_sources/\n\n--prefix=/usr/local: Tells the build to install into our custom directory.\n--enable-static --disable-shared: This is the key. We instruct it to build only the static library (.a file) and not a shared one (.so file).\n--enable-pic: Generates Position-Independent Code, which is required for linking into a static library.\nmake -j$(nproc): Compiles the code using all available CPU cores to speed things up.\n\n\n\n\nx264 installed\n\n\n\n\n2. The Font Rendering Stack\nTo enable the drawtext filter, we need a whole stack of font-related libraries. We must build them in order of dependency.\nA. Compression Libraries: brotli and bzip2\nThese are low-level dependencies for the font libraries.\n# Brotli\nwget https://github.com/google/brotli/archive/refs/tags/v1.1.0.tar.gz -O brotli-1.1.0.tar.gz\ntar -xzf brotli-1.1.0.tar.gz\ncd brotli-1.1.0/\nmkdir out && cd out\ncmake -DCMAKE_INSTALL_PREFIX=/usr/local -DBUILD_SHARED_LIBS=OFF ..\nmake -j$(nproc)\nmake install\ncd /root/ffmpeg_sources/\n\n# bzip2\nwget https://sourceware.org/pub/bzip2/bzip2-1.0.8.tar.gz\ntar -xzf bzip2-1.0.8.tar.gz\ncd bzip2-1.0.8/\nmake -j$(nproc)\nmake install PREFIX=/usr/local\ncd /root/ffmpeg_sources/\n\nNote: Brotli uses cmake for its build, where -DBUILD_SHARED_LIBS=OFF achieves our static-only goal. bzip2 has a unique makefile, requiring us to specify the install location with PREFIX=/usr/local.\n\n\n\n\n\n\n\nBrotli installed\n\n\n\n\n\n\n\nbzip2 installed\n\n\n\n\n\nB. Image Format: libpng\nFreeType, the font engine, needs this to handle certain font formats.\nwget https://download.sourceforge.net/libpng/libpng-1.6.40.tar.gz\ntar -xzf libpng-1.6.40.tar.gz\ncd libpng-1.6.40/\n./configure --prefix=/usr/local --enable-static --disable-shared\nmake -j$(nproc)\nmake install\ncd /root/ffmpeg_sources/\n\n\n\nlibpng installed\n\n\nC. The Font Engine: FreeType\nThis is the core library that actually renders font glyphs from font files.\nwget https://download.savannah.gnu.org/releases/freetype/freetype-2.13.2.tar.gz\ntar -xzf freetype-2.13.2.tar.gz\ncd freetype-2.13.2/\n./configure --prefix=/usr/local --enable-static --disable-shared\nmake -j$(nproc)\nmake install\ncd /root/ffmpeg_sources/\n\n\n\n\n\n\nFreetype2 configure output\n\n\n\n\n\n\n\nFreetype2 installed\n\n\n\n\n\nD. Text Layout Libraries: FriBidi and HarfBuzz\nThese libraries handle complex text layout, like right-to-left languages and shaping glyphs correctly.\n# FriBidi\nwget https://github.com/fribidi/fribidi/releases/download/v1.0.13/fribidi-1.0.13.tar.xz\ntar -xf fribidi-1.0.13.tar.xz\ncd fribidi-1.0.13/\n./configure --prefix=/usr/local --enable-static --disable-shared\nmake -j$(nproc)\nmake install\ncd /root/ffmpeg_sources/\n\n# HarfBuzz\nwget https://github.com/harfbuzz/harfbuzz/releases/download/8.2.1/harfbuzz-8.2.1.tar.xz\ntar -xf harfbuzz-8.2.1.tar.xz\ncd harfbuzz-8.2.1/\nLIBS=\"-lbrotlidec -lbrotlicommon\" ./configure --prefix=/usr/local --enable-static --disable-shared --with-freetype=yes\nmake -j$(nproc)\nmake install\ncd /root/ffmpeg_sources/\n\nNote: The LIBS=\"...\" part for HarfBuzz is a crucial fix, explicitly telling it how to link with the Brotli libraries we just built.\n\n\n\n\n\n\n\nFribidi installed\n\n\n\n\n\n\n\nHarfBuzz installed\n\n\n\n\n\nE. Font Management: Fontconfig\nThis is the high-level library that finds and manages the fonts on a system. It’s the final piece of our font stack.\nwget https://www.freedesktop.org/software/fontconfig/release/fontconfig-2.14.2.tar.xz\ntar -xf fontconfig-2.14.2.tar.xz\ncd fontconfig-2.14.2/\nLIBS=\"-lbrotlidec -lbrotlicommon\" ./configure --prefix=/usr/local --enable-static --disable-shared\nmake -j$(nproc)\nmake install\ncd /root/ffmpeg_sources/\n\n\n\nFontconfig2 installed\n\n\nPhew! The dependency gauntlet is complete. All the components are forged and installed in /usr/local. Now for the main event."
  },
  {
    "objectID": "posts/2025-08-03-the-horse-in-motion-ffmpeg-gotchas-part-3.html#the-main-event-compiling-our-custom-ffmpeg",
    "href": "posts/2025-08-03-the-horse-in-motion-ffmpeg-gotchas-part-3.html#the-main-event-compiling-our-custom-ffmpeg",
    "title": "The Horse in Motion and FFmpeg Gotchas - Part 3 Building FFmpeg From Scratch",
    "section": "The Main Event: Compiling Our Custom FFmpeg",
    "text": "The Main Event: Compiling Our Custom FFmpeg\nWith all our dependencies in place, we can finally configure and compile FFmpeg itself.\nFirst, let’s clone a recent, stable version of the FFmpeg source code.\ngit clone --depth 1 --branch n7.1 https://github.com/FFmpeg/FFmpeg.git\ncd FFmpeg\nNow, we construct the master command. This ./configure script tells FFmpeg exactly how we want it to be built. Every flag matters.\n./configure \\\n  --prefix=\"/usr/local\" \\\n  --pkg-config-flags=\"--static\" \\\n  --extra-cflags=\"-I/usr/local/include\" \\\n  --extra-ldflags=\"-L/usr/local/lib -static\" \\\n  --extra-libs=\"-lpthread -lm\" \\\n  --bindir=\"/output\" \\\n  --enable-gpl \\\n  --enable-static \\\n  --disable-shared \\\n  --enable-libx264 \\\n  --enable-libfreetype \\\n  --enable-libfontconfig \\\n  --enable-libfribidi \\\n  --enable-libharfbuzz\nLet’s break this down:\n\n--prefix=\"/usr/local\" & --bindir=\"/output\": Defines where to install the libraries (/usr/local) and the final executable (/output).\n--pkg-config-flags=\"--static\": The secret sauce. This tells pkg-config to provide the flags needed for static linking against all our dependencies.\n--extra-cflags=\"-I/...\" & --extra-ldflags=\"-L/...\": These explicitly tell the compiler and linker where to find the header files (.h) and library files (.a) in our custom install directory. The -static flag in ldflags reinforces our goal.\n--enable-gpl & --enable-libx264: libx264 is licensed under the GPL, so we must enable GPL compatibility to use it.\n--enable-static --disable-shared: The core instruction to build a single, self-contained binary.\n--enable-lib...: This is where we flip the switch for each external library we painstakingly compiled. We are explicitly enabling libx264 and the entire font stack.\n\n\n\n\nffmpeg configure output\n\n\nWith the configuration set, it’s time to build and install.\nmake -j$(nproc)\nmake install\n\n\n\n\n\n\nffmpeg make output\n\n\n\n\n\n\n\nffmpeg install\n\n\n\n\n\nAfter a few minutes, if all went well, our custom-built ffmpeg binary will be waiting for us in the /output directory.\n\n\n\nffmpeg output folder"
  },
  {
    "objectID": "posts/2025-08-03-the-horse-in-motion-ffmpeg-gotchas-part-3.html#the-moment-of-truth-verification",
    "href": "posts/2025-08-03-the-horse-in-motion-ffmpeg-gotchas-part-3.html#the-moment-of-truth-verification",
    "title": "The Horse in Motion and FFmpeg Gotchas - Part 3 Building FFmpeg From Scratch",
    "section": "The Moment of Truth: Verification",
    "text": "The Moment of Truth: Verification\nDid it actually work? Let’s run a series of checks to verify our new binary.\nFirst, check its dependencies with ldd. A statically linked executable should have no dynamic dependencies.\nldd /output/ffmpeg\nThe expected output should be:\n\n\n\nffmpeg no dynamic dependencies verification\n\n\nThis is a perfect result. It means our binary is completely self-contained and portable—ideal for a Lambda Layer.\n\n\n\n\n\n\nThis is important\n\n\n\n\n\nIn one of my failed attempts I did not enable the static build flags correctly, and the output showed dynamic dependencies. If you see any lines with =&gt;, it means your binary is not fully static. You must go back and ensure all libraries are built with the --enable-static --disable-shared flags. Builds that have dynamic dependencies will not work in a Lambda Layer, as they require the underlying libraries to be present in the Lambda environment.\n\n\n\nffmpeg dynamic dependencies\n\n\n\n\n\nNext, check the version and configuration.\n/output/ffmpeg -version\nThe output should show our custom configuration flags right at the top, confirming our build options were respected. You should see --enable-gpl, --enable-libx264, --enable-libfreetype, etc.\n\n\n\nffmpeg version\n\n\nNow, let’s confirm both of our target features are present.\n# Check for the drawtext filter\n/output/ffmpeg -filters | grep drawtext\n\n# Check for the libx264 codec\n/output/ffmpeg -codecs | grep libx264\nBoth of these commands should return a result, proving that we have successfully built a single binary with both features enabled.\n\n\n\n\n\n\nffmpeg drawtext\n\n\n\n\n\n\n\nffmpeg x264\n\n\n\n\n\nFinally, let’s check the size.\nls -lh /output/ffmpeg\nYou’ll likely see a file size around 25-30 MB. Compare this to the 80-100+ MB of the general-purpose static builds we downloaded in Part 2. Our custom build is significantly smaller, which means faster Lambda cold starts and smaller deployment packages.\n\n\n\nffmpeg size"
  },
  {
    "objectID": "posts/2025-08-03-the-horse-in-motion-ffmpeg-gotchas-part-3.html#putting-our-custom-binary-to-work-the-final-deployment",
    "href": "posts/2025-08-03-the-horse-in-motion-ffmpeg-gotchas-part-3.html#putting-our-custom-binary-to-work-the-final-deployment",
    "title": "The Horse in Motion and FFmpeg Gotchas - Part 3 Building FFmpeg From Scratch",
    "section": "Putting Our Custom Binary to Work: The Final Deployment",
    "text": "Putting Our Custom Binary to Work: The Final Deployment\nWe’ve successfully compiled and verified our bespoke ffmpeg binary on our EC2 instance. Now for the final, crucial step: packaging it as a Lambda Layer and deploying it to our serverless application. This will be very similar to the process in Part 2, but with our new, superior binary.\n\nStep 1: Package the New Lambda Layer\nWe’ll package it into a new Lambda Layer zip file. It’s good practice to give it a new name to avoid confusion with the layer from Part 2.\n# Create the directory structure for our custom layer\nmkdir -p custom-ffmpeg-layer/bin\n\n# Copy our newly downloaded ffmpeg binary into it\ncp ./ffmpeg custom-ffmpeg-layer/bin/\n\n# Create the zip archive\ncd custom-ffmpeg-layer\nzip -r ../custom-ffmpeg-layer.zip .\ncd ..\nYou should now have a custom-ffmpeg-layer.zip file by now. Let’s check the layer size as well.\nls -lh custom-ffmpeg-layer.zip\n\n\n\nffmpeg custom lambda layer\n\n\nYou’ll likely see a file size around 10-15 MB. Compare this to the 80-100+ MB of the general-purpose static builds we downloaded in Part 2. Our custom build is significantly smaller, which means faster Lambda cold starts and smaller deployment packages.\n\n\nStep 2: Update the SAM Template\nNext, we’ll edit our template.yaml to point to this new layer. The only change required is updating the ContentUri of our FFmpegLayer resource.\n# In template.yaml\n\n...\n  # Define the Lambda Layer\n  FFmpegLayer:\n    Type: AWS::Serverless::LayerVersion\n    Properties:\n      LayerName: custom-ffmpeg-layer\n      Description: Custom FFmpeg build with libx264 and drawtext\n      ContentUri: layers/custom-ffmpeg-layer.zip # Points to our local zip file\n      CompatibleRuntimes:\n        - python3.12\n        - python3.13\n...\n\n\nStep 3: Update the Lambda Function Code (The Payoff!)\nThis is the moment we’ve been working towards. We can now update our src/app.py to use the high-quality libx264 encoder. This is the one-line change that makes this entire effort worthwhile.\n# In src/app.py\n...\n        # When a layer is used, its contents are available in the /opt directory.\n        # Our FFmpeg binary is therefore at /opt/bin/ffmpeg.\n        ffmpeg_cmd = [\n            \"/opt/bin/ffmpeg\",\n            \"-stream_loop\",\n            \"-1\",\n            \"-framerate\",\n            \"1.5\",\n            \"-i\",\n            input_path,\n            \"-vf\",\n            f\"drawtext=fontfile={font_file}:text='The Horse in Motion and FFmpeg Gotchas Part 3':fontcolor=white:fontsize=13:box=1:boxcolor=black@0.8:boxborderw=5:x=(w-text_w)/2:y=(h-text_h)/2:enable='between(t,0,10)'\",\n            \"-c:v\",\n            \"libx264\",  # THE PAYOFF: We can now use the superior encoder!\n            \"-r\",\n            \"30\",\n            \"-pix_fmt\",\n            \"yuv420p\",\n            \"-t\",\n            \"40\",\n            output_path,\n        ]\n\n...\nNo other changes to the Python code are necessary.\n\n\nStep 4: Deploy and Test\nWith our new layer and updated code, we’re ready for the final deployment. Run the standard SAM commands from your project’s root directory:\n# Build the application with the new layer\nsam build\n\n# Deploy the changes to AWS\nsam deploy\nAfter the deployment completes, invoke your Lambda function one last time. Check the CloudWatch logs, and you should see the FFmpeg command execute successfully. Then, head over to your S3 bucket. You will find a new horse-in-motion.mp4 file, created by your very own, custom-built, highly-optimized FFmpeg binary, running live in the cloud!\nHere is final video output: \n\n\n\n\n\n\nCode checkpoint\n\n\n\nAll the code for this post till this point is available in the Github repo horse-in-motion-ffmpeg-gotchas-part-2 (3e9eb)"
  },
  {
    "objectID": "posts/2025-08-03-the-horse-in-motion-ffmpeg-gotchas-part-3.html#conclusion-from-user-to-builder",
    "href": "posts/2025-08-03-the-horse-in-motion-ffmpeg-gotchas-part-3.html#conclusion-from-user-to-builder",
    "title": "The Horse in Motion and FFmpeg Gotchas - Part 3 Building FFmpeg From Scratch",
    "section": "Conclusion: From User to Builder",
    "text": "Conclusion: From User to Builder\nOur serverless journey is complete. We’ve gone from a user of FFmpeg to a builder, crafting a tool perfectly suited to our needs. While this path is fraught with challenges, it provides the ultimate level of control, optimization, and capability.\nBy successfully compiling and deploying our own FFmpeg binary, we’ve achieved the best of all worlds:\n\nTargeted Functionality: We have a build with both the high-quality libx264 encoder and the essential drawtext filter.\nOptimized Size: Our final binary is significantly smaller than general-purpose public builds, leading to faster Lambda deployments and cold starts.\nUltimate Control: We are no longer at the mercy of third-party build configurations. If a new library or feature is needed, we now have the blueprint to build it ourselves.\n\nThis three-part series has shown that while FFmpeg can be intimidating, breaking it down step-by-step—from basic commands to cloud deployment and finally to building it from source—can demystify this incredibly powerful tool. You are now equipped to handle almost any multimedia processing challenge that comes your way."
  },
  {
    "objectID": "posts/2025-08-27-squeezenet-implementation.html#introduction",
    "href": "posts/2025-08-27-squeezenet-implementation.html#introduction",
    "title": "Building SqueezeNet from Scratch with PyTorch: A Hands-On Implementation",
    "section": "Introduction",
    "text": "Introduction\nIn our previous post, we took a deep dive into the theory behind the landmark SqueezeNet paper. We deconstructed the “why”—the motivations and the brilliant design strategies that allowed researchers to achieve AlexNet-level performance with a model over 500 times smaller.\nNow, it’s time to get our hands dirty and build the “how.”\nReading a paper is one thing, but true understanding often comes from implementation. The goal of this post is to translate the concepts from the SqueezeNet paper into a complete, working PyTorch model. We’ll walk through the code step-by-step, build the architecture from the ground up, train it on a real dataset, and analyze the results to see if we can replicate the incredible efficiency that made SqueezeNet famous.\n\n\n\n\n\n\nTip\n\n\n\nThe complete Jupyter Notebook for this post is available on GitHub. You can view it, download it, and run the code yourself at the following link:\n2025-08-27-squeezenet-implementation.ipynb\nWhere to run this notebook?\nI ran this notebook on an AWS SageMaker Notebook Instance with the instance type ml.g4dn.xlarge and platform identifier Amazon Linux 2, JupyterLab 4. The g4dn.xlarge instance belongs to the GPU family and comes with 4 vCPUs, 16 GiB of memory, and 1 NVIDIA T4 Tensor Core GPU.\nAlternatively, you can use Google Colab, which provides free access to the same type of GPU and is an excellent platform for running Jupyter notebooks. I tested the notebook on Colab with the free-tier T4 GPU, and it worked seamlessly. The training loop takes around 1.5 hours to complete on Colab.\nIn my experience, training completes faster on the g4dn.xlarge instance compared to Colab.\n\n\n\nOur Training Ground: The CIFAR-10 Dataset\nThe original SqueezeNet was trained on the massive ImageNet dataset, which contains over a million images across 1000 categories. Training on ImageNet from scratch requires immense computational resources and time.\nFor this post, we’ll use the popular CIFAR-10 dataset. It consists of 60,000 32x32 color images in 10 classes, such as ‘plane’, ‘car’, ‘bird’, and ‘dog’. CIFAR-10 is a fantastic learning tool because it’s complex enough to be a meaningful challenge, but small enough that we can train our network in a reasonable amount of time on a single GPU. It’s the perfect environment to validate our SqueezeNet implementation.\n\n\nOur Roadmap for this Post\nWe will follow the logical flow of a machine learning project, from data preparation to final analysis. Here’s what we’ll cover:\n\nSetting the Stage: We’ll begin by setting up our environment, importing the necessary libraries, and creating our data loaders with powerful augmentations to help our model generalize.\nBuilding the Fire Module: We’ll implement the heart of SqueezeNet, the Fire module, as a custom nn.Module in PyTorch.\nAssembling the Full SqueezeNet: Using our Fire module as a building block, we’ll construct the complete SqueezeNet macroarchitecture.\nTraining and Analysis: We’ll write a complete training loop, train the model on CIFAR-10, and visualize its performance with detailed plots.\nThe Final Verdict: Finally, we’ll calculate our trained model’s parameter count and file size to see how it stacks up against the paper’s promise of extreme efficiency.\n\nLet’s dive in and start writing some code!"
  },
  {
    "objectID": "posts/2025-08-27-squeezenet-implementation.html#setting-the-stage---data-augmentation-and-optimization",
    "href": "posts/2025-08-27-squeezenet-implementation.html#setting-the-stage---data-augmentation-and-optimization",
    "title": "Building SqueezeNet from Scratch with PyTorch: A Hands-On Implementation",
    "section": "Setting the Stage - Data, Augmentation, and Optimization",
    "text": "Setting the Stage - Data, Augmentation, and Optimization\nBefore we can build our network, we need to prepare our data. A robust and efficient data pipeline is the foundation of any successful deep learning project. In this section, we’ll handle three key steps: setting up our environment, defining our data transformations (including augmentation), and creating optimized data loaders to feed the GPU.\n\nStep 1: Environment and Imports\nFirst, we’ll import all the necessary libraries. We’re using PyTorch for building and training the model, Matplotlib for plotting, and a few other helpful utilities. For reproducibility, it’s great practice to log the versions of our key libraries using watermark. This helps ensure that others (or our future selves!) can recreate the same environment.\n\n%%capture\n!pip install watermark\n\n\n%load_ext watermark\n\n\nimport torch\nimport torchvision\nimport torchvision.transforms as transforms\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.init as init\nimport os\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nfrom tqdm import tqdm\n\n%matplotlib inline\n\n\n%watermark -v -m -p torch,torchvision,matplotlib,watermark,tqdm\n\nPython implementation: CPython\nPython version       : 3.10.18\nIPython version      : 8.37.0\n\ntorch      : 2.6.0+cu124\ntorchvision: 0.21.0+cu124\nmatplotlib : 3.10.5\nwatermark  : 2.5.0\ntqdm       : 4.67.1\n\nCompiler    : GCC 13.3.0\nOS          : Linux\nRelease     : 5.10.240-238.959.amzn2.x86_64\nMachine     : x86_64\nProcessor   : x86_64\nCPU cores   : 4\nArchitecture: 64bit\n\n\n\n\n\nStep 2: Data Augmentation and Transformation\nRaw image data is rarely fed directly into a neural network. We need to process it first. We’ll define two separate transformation pipelines: one for our training data and one for our validation/testing data.\nFor Training Data:\nOur train_transform is a sequence of operations designed to prepare the data and also to augment it. Data augmentation is a powerful technique to artificially increase the diversity of your training set and prevent the model from overfitting.\n# For training: resize, augment, convert to tensor, normalize\ntrain_transform = transforms.Compose([\n    transforms.Resize(224),\n    transforms.RandomHorizontalFlip(),\n    transforms.RandomRotation(10),\n    transforms.ToTensor(),\n    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n])\nLet’s break down what each step does:\n\ntransforms.Resize(224): The original SqueezeNet was designed for 224x224 pixel images from ImageNet. We resize our smaller 32x32 CIFAR-10 images to match this expected input size.\ntransforms.RandomHorizontalFlip(): This randomly flips half of the images horizontally. This teaches the model that a car facing left is still a car, making it more robust.\ntransforms.RandomRotation(10): This randomly rotates images by a small amount (up to 10 degrees). This helps the model become invariant to slight changes in orientation.\ntransforms.ToTensor(): This converts the image from a PIL Image format into a PyTorch tensor, the fundamental data structure used by the framework.\ntransforms.Normalize(...): This standardizes the pixel values of the tensor. By normalizing them to have a mean of 0.5 and a standard deviation of 0.5, we ensure the data is in a range that’s ideal for the network to learn from, which can lead to faster and more stable training.\n\nFor Validation/Testing Data:\nOur test_transform is simpler. We perform the necessary resizing, conversion to a tensor, and normalization, but we do not use augmentation. We want to evaluate our model on the clean, original test data, not modified versions of it.\n# For validation/testing: just resize, convert to tensor, normalize\ntest_transform = transforms.Compose([\n    transforms.Resize(224),\n    transforms.ToTensor(),\n    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n])\n\n\nStep 3: Creating Optimized DataLoaders\nNow that we have our transformation pipelines, we’ll create PyTorch DataLoaders. A DataLoader is a powerful utility that automatically handles batching our data, shuffling it, and loading it in parallel.\nWe’ll pay attention to a few key parameters to make our data pipeline as fast as possible:\n\nbatch_size = 128: We’ll process the data in batches of 128 images. This is more efficient for the GPU than processing images one by one.\nnum_workers: This tells the DataLoader to use multiple subprocesses to load data in the background. A good starting point is the number of CPU cores you have. This prevents the CPU from becoming a bottleneck while the GPU waits for data.\npin_memory=True: This is a great optimization. It tells PyTorch to place the data tensors in a special “pinned” memory region, which allows for much faster, non-blocking data transfer from the CPU to the GPU.\n\n\n# --- Data Loading and Augmentation ---\n# For training: resize, augment, convert to tensor, normalize\ntrain_transform = transforms.Compose([\n    transforms.Resize(224),\n    transforms.RandomHorizontalFlip(),\n    transforms.RandomRotation(10),\n    transforms.ToTensor(),\n    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n])\n\n# For validation/testing: just resize, convert to tensor, normalize\ntest_transform = transforms.Compose([\n    transforms.Resize(224),\n    transforms.ToTensor(),\n    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n])\n\n# --- Data Loader Optimizations ---\n# Increased batch size for better GPU utilization\nbatch_size = 128\n# Use more workers to load data in parallel\n# Use os.cpu_count() for a good starting point, but you may need to tune this\nnum_workers = min(os.cpu_count(), 8)\nprint(\"num_workers: \", num_workers)\n\ntrainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n                                        download=True, transform=train_transform)\n# Enabled pin_memory=True for faster CPU to GPU data transfer\ntrainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n                                          shuffle=True, num_workers=num_workers, pin_memory=True)\n\ntestset = torchvision.datasets.CIFAR10(root='./data', train=False,\n                                       download=True, transform=test_transform)\ntestloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n                                         shuffle=False, num_workers=num_workers, pin_memory=True)\n\nclasses = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n\nnum_workers:  4\n\n\n\n\nStep 4: Final Sanity Check - Let’s See the Data!\nBefore we move on to building our model, it’s always a good idea to perform a quick sanity check to make sure our data pipeline is working as expected. Let’s grab one batch of images from our trainloader and display them with their correct labels.\n\n# --- Get a batch of training data ---\n# Create an iterator from the dataloader\ndataiter = iter(trainloader)\n# Get the first batch of images and labels\nimages, labels = next(dataiter)\n\n# --- Define a function to display an image ---\n# This function will un-normalize and display a single image tensor\ndef imshow(img):\n    img = img / 2 + 0.5     # Un-normalize the image\n    npimg = img.numpy()\n    plt.imshow(np.transpose(npimg, (1, 2, 0))) # Convert from Tensor image\n\n# --- Create a grid of images and display them ---\n# Number of images to display\nnum_images_to_show = 10\n\n# Create a figure with a grid of subplots\n# We'll use 2 rows and 5 columns to display 10 images\nfig, axes = plt.subplots(nrows=2, ncols=5, figsize=(15, 6))\n\n# Flatten the axes array to make it easy to loop over\naxes = axes.flatten()\n\nfor i in range(num_images_to_show):\n    ax = axes[i]\n\n    # Detach the image tensor, move to CPU, and un-normalize\n    img = images[i].cpu()\n    img = img / 2 + 0.5  # Un-normalize\n    npimg = img.numpy()\n\n    # Display the image\n    ax.imshow(np.transpose(npimg, (1, 2, 0)))\n\n    # Set the title of the subplot to the corresponding class label\n    ax.set_title(classes[labels[i]])\n\n    # Remove the x and y axis ticks for a cleaner look\n    ax.set_xticks([])\n    ax.set_yticks([])\n\n# Add a title for the entire figure and adjust layout\nplt.suptitle('Sample Training Images with Labels', fontsize=16)\nplt.tight_layout(rect=[0, 0, 1, 0.96]) # Adjust layout to make room for suptitle\nplt.show()"
  },
  {
    "objectID": "posts/2025-08-27-squeezenet-implementation.html#implementing-the-fire-module-in-pytorch",
    "href": "posts/2025-08-27-squeezenet-implementation.html#implementing-the-fire-module-in-pytorch",
    "title": "Building SqueezeNet from Scratch with PyTorch: A Hands-On Implementation",
    "section": "Implementing the Fire Module in PyTorch",
    "text": "Implementing the Fire Module in PyTorch\nWith our data pipeline ready, it’s time to build the engine of SqueezeNet: the Fire module. As we learned in our theory deep-dive, this compact and efficient module is the key to SqueezeNet’s small size and high performance.\nWe’ll implement the Fire module as a custom class that inherits from PyTorch’s nn.Module. This allows us to define its constituent layers in the __init__ method and specify how data flows through them in the forward method.\n\nThe Code: Fire Module\nHere is the complete PyTorch implementation of the Fire module:\n\nclass Fire(nn.Module):\n    def __init__(self, inplanes: int, squeeze_planes: int, expand1x1_planes: int, expand3x3_planes: int) -&gt; None:\n        super().__init__()\n        self.inplanes = inplanes\n        \n        # Squeeze Layer\n        self.squeeze = nn.Conv2d(inplanes, squeeze_planes, kernel_size=1)\n        self.squeeze_activation = nn.ReLU(inplace=True)\n        \n        # Expand Layer - 1x1 branch\n        self.expand1x1 = nn.Conv2d(squeeze_planes, expand1x1_planes, kernel_size=1)\n        self.expand1x1_activation = nn.ReLU(inplace=True)\n\n        # Expand Layer - 3x3 branch\n        self.expand3x3 = nn.Conv2d(squeeze_planes, expand3x3_planes, kernel_size=3, padding=1)\n        self.expand3x3_activation = nn.ReLU(inplace=True)\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        # Squeeze the input\n        x = self.squeeze_activation(self.squeeze(x))\n        \n        # Pass through both expand branches and concatenate the results\n        return torch.cat(\n            [self.expand1x1_activation(self.expand1x1(x)), self.expand3x3_activation(self.expand3x3(x))], 1\n        )\n\n\nCode Breakdown\nLet’s walk through this code to see how it perfectly mirrors the paper’s design.\nThe __init__ Method (The Blueprint):\nThis is where we define all the layers our module will use.\n\nself.squeeze: This is our bottleneck layer, a simple nn.Conv2d with a kernel_size=1. Its purpose is to take the input (inplanes) and reduce its channel dimension down to squeeze_planes.\nself.expand1x1: This is the 1x1 branch of the expand layer. It takes the squeezed feature map and applies expand1x1_planes number of 1x1 filters.\nself.expand3x3: This is the 3x3 branch. It also takes the squeezed feature map and applies expand3x3_planes number of 3x3 filters.\n\nCrucial Detail: Notice the padding=1 argument. A 3x3 convolution would normally shrink the feature map’s height and width. This padding ensures the output of the 3x3 branch has the exact same spatial dimensions as the output of the 1x1 branch, which is essential for the next step.\n\nReLU(inplace=True): We use the standard ReLU activation function after each convolution. inplace=True is a small memory optimization.\n\nThe forward Method (The Assembly Line):\nThis method defines how data flows through the layers we just created.\n\nFirst, the input tensor x is passed through the squeeze convolution and its activation function. This creates our thin, bottlenecked feature map.\nNext, this single squeezed tensor is fed into both the expand1x1 and expand3x3 branches in parallel.\nFinally, the magic happens with torch.cat([...], 1). We take the outputs of the two expand branches and concatenate them along dimension 1 (the channel dimension). This stacks the feature maps from both branches together, creating the final, wider output of the Fire module.\n\nThis clean and modular implementation gives us the reusable building block we need to construct the full SqueezeNet architecture."
  },
  {
    "objectID": "posts/2025-08-27-squeezenet-implementation.html#assembling-the-full-squeezenet-model",
    "href": "posts/2025-08-27-squeezenet-implementation.html#assembling-the-full-squeezenet-model",
    "title": "Building SqueezeNet from Scratch with PyTorch: A Hands-On Implementation",
    "section": "Assembling the Full SqueezeNet Model",
    "text": "Assembling the Full SqueezeNet Model\nNow that we have our Fire module “LEGO brick,” we can stack them together to build the complete SqueezeNet macroarchitecture, just as described in Table 1 of the paper.\n\nclass SqueezeNet(nn.Module):\n    def __init__(self, num_classes: int = 10, dropout: float = 0.5) -&gt; None:\n        super().__init__()\n        self.num_classes = num_classes\n\n        # Feature extractor, following the paper's architecture\n        self.features = nn.Sequential(\n            nn.Conv2d(3, 96, kernel_size=7, stride=2),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n            Fire(96, 16, 64, 64),\n            Fire(128, 16, 64, 64),\n            Fire(128, 32, 128, 128),\n            nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n            Fire(256, 32, 128, 128),\n            Fire(256, 48, 192, 192),\n            Fire(384, 48, 192, 192),\n            Fire(384, 64, 256, 256),\n            nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n            Fire(512, 64, 256, 256),\n        )\n\n        # Final convolution is initialized differently from the rest\n        final_conv = nn.Conv2d(512, self.num_classes, kernel_size=1)\n\n        # Classifier head\n        self.classifier = nn.Sequential(\n            nn.Dropout(p=dropout), final_conv, nn.ReLU(inplace=True), nn.AdaptiveAvgPool2d((1, 1))\n        )\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                if m is final_conv:\n                    init.normal_(m.weight, mean=0.0, std=0.01)\n                else:\n                    init.kaiming_uniform_(m.weight)\n                if m.bias is not None:\n                    init.constant_(m.bias, 0)\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        x = self.features(x)\n        x = self.classifier(x)\n        return torch.flatten(x, 1)\n\n\nCode Breakdown\n\nself.features: We use the convenient nn.Sequential to define the main body of the network. You can literally trace the layers in this code and match them to the architecture in the paper’s Table 1. Notice the MaxPool2d layers are spaced out, perfectly implementing Strategy #3 (Late Downsampling). The numbers inside each Fire(...) call correspond exactly to the inplanes, squeeze_planes, and expand_planes for that stage of the network.\nself.classifier: This is our modern, efficient classifier.\n\nnn.Dropout: A standard regularization technique to prevent overfitting.\nfinal_conv: A simple nn.Conv2d with a 1x1 kernel that maps the 512 channels from the feature extractor down to the final num_classes (10 for CIFAR-10).\nnn.AdaptiveAvgPool2d((1, 1)): This is PyTorch’s implementation of Global Average Pooling. It takes the feature maps from the final convolution, no matter their height and width, and averages them down to a 1x1 size. This is what allows us to avoid the bulky, parameter-heavy Linear layers used in older models.\n\nforward method: The data flow is beautifully simple. The input x goes through the features block, then the classifier block. The final torch.flatten(x, 1) converts the output from a (batch_size, num_classes, 1, 1) tensor into the standard (batch_size, num_classes) shape expected by the loss function.\n\nWith just these two classes, we have a complete implementation of the entire SqueezeNet architecture. Now, let’s train it!"
  },
  {
    "objectID": "posts/2025-08-27-squeezenet-implementation.html#training-squeezenet-and-analyzing-the-results",
    "href": "posts/2025-08-27-squeezenet-implementation.html#training-squeezenet-and-analyzing-the-results",
    "title": "Building SqueezeNet from Scratch with PyTorch: A Hands-On Implementation",
    "section": "Training SqueezeNet and Analyzing the Results",
    "text": "Training SqueezeNet and Analyzing the Results\nWe have our data pipeline and our model architecture. Now, let’s put them together and train our SqueezeNet model on the CIFAR-10 dataset. We’ll set up a robust training loop and then analyze the output to understand how our model learned over time.\n\nStep 1: The Training Setup\nBefore we start the main loop, we need to define a few key components for training:\n\nDevice: We’ll automatically detect if a CUDA-enabled GPU is available and use it; otherwise, we’ll fall back to the CPU. Training on a GPU is dramatically faster.\nOptimizer: We’ll use AdamW, a popular and effective optimizer that often converges faster than standard SGD.\nLoss Function: Since this is a multi-class classification problem, nn.CrossEntropyLoss is the standard choice.\nLearning Rate Scheduler: We’ll use ReduceLROnPlateau. This is a smart scheduler that monitors the validation loss. If the loss stops improving for a set number of epochs (a “patience” of 2 in our case), it will automatically reduce the learning rate. This helps the model fine-tune its weights in the later stages of training.\nEarly Stopping: To save time and prevent overfitting, we’ll implement early stopping. We’ll monitor the validation loss at the end of each epoch and save a copy of the model only when it achieves a new “best” (lowest) validation loss.\n\n\n# --- Model, Optimizer, and Training Setup ---\ndevice = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using device: {device}\")\n\nnet = SqueezeNet() # initialize your model\nnet.to(device)\n\n# --- Compile the model (PyTorch 2.0+ feature) ---\n# This is a massive speedup!\nif torch.__version__[0] == '2':\n    print(\"PyTorch 2.0+ detected. Compiling the model...\")\n    net = torch.compile(net)\n\nUsing device: cuda:0\nPyTorch 2.0+ detected. Compiling the model...\n\n\n\ncriterion = nn.CrossEntropyLoss()\n# Using a potentially faster-converging optimizer\noptimizer = optim.AdamW(net.parameters(), lr=0.001, weight_decay=1e-4)\n\nnum_epochs = 40\n\n\n# --- DEFINE THE ReduceLROnPlateau SCHEDULER ---\n# This scheduler monitors the validation loss.\n# 'patience=2': Wait for 2 epochs of no improvement before reducing LR.\n# 'factor=0.1': Reduce LR by a factor of 10 (e.g., 0.001 -&gt; 0.0001).\n# 'mode=min': It's monitoring a value that should be minimized (loss).\nscheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=2)\n\n\n# --- EARLY STOPPING SETUP ---\n# Path to save the best model\nSAVE_PATH = 'best_model.pth'\n# Initialize variables to track the best performance\nbest_val_loss = float('inf')\nbest_epoch = 0\n\n\n# --- History Tracking ---\nhistory = {\n    'train_loss': [],\n    'val_loss': [],\n    'train_acc': [],\n    'val_acc': [],\n    'learning_rate': [] # Let's also track the learning rate to plot it!\n}\n\n\n\nStep 2: The Training Loop and Output\nOur training loop runs for a total of 40 epochs. In each epoch, it performs a full pass over the training data to update the model’s weights, followed by a full pass over the validation data to check its performance on unseen images.\n\nprint(\"Starting Training...\")\nfor epoch in range(num_epochs):\n    # --- Training Phase ---\n    net.train()\n    \n    train_loss = 0.0\n    correct_train = 0\n    total_train = 0\n    current_lr = optimizer.param_groups[0]['lr']\n    train_bar = tqdm(trainloader, desc=f\"Epoch {epoch+1}/{num_epochs} [Training] LR: {current_lr:.6f}\")\n\n    for data in train_bar:\n        inputs, labels = data[0].to(device, non_blocking=True), data[1].to(device, non_blocking=True)\n        optimizer.zero_grad(set_to_none=True)\n        outputs = net(inputs)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        train_loss += loss.item()\n        _, predicted = torch.max(outputs.data, 1)\n        total_train += labels.size(0)\n        correct_train += (predicted == labels).sum().item()\n        train_bar.set_postfix(loss=loss.item())\n\n    avg_train_loss = train_loss / len(trainloader)\n    train_accuracy = 100 * correct_train / total_train\n    history['train_loss'].append(avg_train_loss)\n    history['train_acc'].append(train_accuracy)\n    history['learning_rate'].append(current_lr)\n\n    # --- Validation Phase ---\n    net.eval()\n    val_loss = 0.0\n    correct_val = 0\n    total_val = 0\n    val_bar = tqdm(testloader, desc=f\"Epoch {epoch+1}/{num_epochs} [Validation]\")\n    with torch.no_grad():\n        for data in val_bar:\n            inputs, labels = data[0].to(device, non_blocking=True), data[1].to(device, non_blocking=True)\n            outputs = net(inputs)\n            loss = criterion(outputs, labels)\n            val_loss += loss.item()\n            _, predicted = torch.max(outputs.data, 1)\n            total_val += labels.size(0)\n            correct_val += (predicted == labels).sum().item()\n            val_bar.set_postfix(loss=loss.item())\n\n    avg_val_loss = val_loss / len(testloader)\n    val_accuracy = 100 * correct_val / total_val\n    history['val_loss'].append(avg_val_loss)\n    history['val_acc'].append(val_accuracy)\n\n    # --- EARLY STOPPING LOGIC ---\n    # Check if the current validation loss is the best we've seen so far.\n    if avg_val_loss &lt; best_val_loss:\n        best_val_loss = avg_val_loss\n        best_epoch = epoch + 1\n        # Save the model's state dictionary\n        torch.save(net._orig_mod.state_dict(), SAVE_PATH)\n        print(f\"\\n---&gt; New best model saved at epoch {best_epoch} with validation loss: {best_val_loss:.4f}\\n\")\n\n    scheduler.step(avg_val_loss)\n\n    print(f\"\\nEpoch {epoch+1}/{num_epochs} Summary:\")\n    print(f\"  Train Loss: {avg_train_loss:.4f} | Train Acc: {train_accuracy:.2f}%\")\n    print(f\"  Valid Loss: {avg_val_loss:.4f} | Valid Acc: {val_accuracy:.2f}%\")\n    print(f\"  Current LR: {optimizer.param_groups[0]['lr']:.6f}\\n\")\n\nprint('Finished Training')\nprint(f\"The best model was saved from epoch {best_epoch} at path: {SAVE_PATH}\")\n\nStarting Training...\n\n\nEpoch 1/40 [Training] LR: 0.001000: 100%|██████████| 391/391 [01:47&lt;00:00,  3.64it/s, loss=1.93]\nEpoch 1/40 [Validation]: 100%|██████████| 79/79 [00:10&lt;00:00,  7.34it/s, loss=1.75]\n\n\n\n---&gt; New best model saved at epoch 1 with validation loss: 1.8390\n\n\nEpoch 1/40 Summary:\n  Train Loss: 2.0931 | Train Acc: 21.73%\n  Valid Loss: 1.8390 | Valid Acc: 31.86%\n  Current LR: 0.001000\n\n\n\nEpoch 2/40 [Training] LR: 0.001000: 100%|██████████| 391/391 [01:39&lt;00:00,  3.94it/s, loss=1.6] \nEpoch 2/40 [Validation]: 100%|██████████| 79/79 [00:08&lt;00:00,  8.80it/s, loss=1.72]\n\n\n\n---&gt; New best model saved at epoch 2 with validation loss: 1.6319\n\n\nEpoch 2/40 Summary:\n  Train Loss: 1.7570 | Train Acc: 36.83%\n  Valid Loss: 1.6319 | Valid Acc: 41.62%\n  Current LR: 0.001000\n\n\n\nEpoch 3/40 [Training] LR: 0.001000: 100%|██████████| 391/391 [01:39&lt;00:00,  3.93it/s, loss=1.65]\nEpoch 3/40 [Validation]: 100%|██████████| 79/79 [00:09&lt;00:00,  8.70it/s, loss=1.76]\n\n\n\n---&gt; New best model saved at epoch 3 with validation loss: 1.5504\n\n\nEpoch 3/40 Summary:\n  Train Loss: 1.5792 | Train Acc: 45.08%\n  Valid Loss: 1.5504 | Valid Acc: 46.14%\n  Current LR: 0.001000\n\n\n\nEpoch 4/40 [Training] LR: 0.001000: 100%|██████████| 391/391 [01:39&lt;00:00,  3.93it/s, loss=1.31] \nEpoch 4/40 [Validation]: 100%|██████████| 79/79 [00:08&lt;00:00,  8.81it/s, loss=1.59] \n\n\n\n---&gt; New best model saved at epoch 4 with validation loss: 1.1688\n\n\nEpoch 4/40 Summary:\n  Train Loss: 1.3565 | Train Acc: 52.19%\n  Valid Loss: 1.1688 | Valid Acc: 58.67%\n  Current LR: 0.001000\n\n\n\nEpoch 5/40 [Training] LR: 0.001000: 100%|██████████| 391/391 [01:39&lt;00:00,  3.92it/s, loss=1.18] \nEpoch 5/40 [Validation]: 100%|██████████| 79/79 [00:08&lt;00:00,  8.78it/s, loss=1.56] \n\n\n\n---&gt; New best model saved at epoch 5 with validation loss: 1.1274\n\n\nEpoch 5/40 Summary:\n  Train Loss: 1.1526 | Train Acc: 58.94%\n  Valid Loss: 1.1274 | Valid Acc: 61.63%\n  Current LR: 0.001000\n\n\n\nEpoch 6/40 [Training] LR: 0.001000: 100%|██████████| 391/391 [01:39&lt;00:00,  3.92it/s, loss=1.02] \nEpoch 6/40 [Validation]: 100%|██████████| 79/79 [00:08&lt;00:00,  8.79it/s, loss=1.23] \n\n\n\n---&gt; New best model saved at epoch 6 with validation loss: 0.9898\n\n\nEpoch 6/40 Summary:\n  Train Loss: 1.0576 | Train Acc: 62.63%\n  Valid Loss: 0.9898 | Valid Acc: 65.36%\n  Current LR: 0.001000\n\n\n\nEpoch 7/40 [Training] LR: 0.001000: 100%|██████████| 391/391 [01:39&lt;00:00,  3.93it/s, loss=1.11] \nEpoch 7/40 [Validation]: 100%|██████████| 79/79 [00:09&lt;00:00,  8.71it/s, loss=1.07] \n\n\n\n---&gt; New best model saved at epoch 7 with validation loss: 0.9031\n\n\nEpoch 7/40 Summary:\n  Train Loss: 0.9776 | Train Acc: 65.39%\n  Valid Loss: 0.9031 | Valid Acc: 68.49%\n  Current LR: 0.001000\n\n\n\nEpoch 8/40 [Training] LR: 0.001000: 100%|██████████| 391/391 [01:39&lt;00:00,  3.93it/s, loss=0.938]\nEpoch 8/40 [Validation]: 100%|██████████| 79/79 [00:09&lt;00:00,  8.74it/s, loss=1.17] \n\n\n\n---&gt; New best model saved at epoch 8 with validation loss: 0.8737\n\n\nEpoch 8/40 Summary:\n  Train Loss: 0.9200 | Train Acc: 67.50%\n  Valid Loss: 0.8737 | Valid Acc: 69.74%\n  Current LR: 0.001000\n\n\n\nEpoch 9/40 [Training] LR: 0.001000: 100%|██████████| 391/391 [01:39&lt;00:00,  3.93it/s, loss=1.02] \nEpoch 9/40 [Validation]: 100%|██████████| 79/79 [00:09&lt;00:00,  8.65it/s, loss=1.17] \n\n\n\n---&gt; New best model saved at epoch 9 with validation loss: 0.8723\n\n\nEpoch 9/40 Summary:\n  Train Loss: 0.8787 | Train Acc: 69.02%\n  Valid Loss: 0.8723 | Valid Acc: 69.43%\n  Current LR: 0.001000\n\n\n\nEpoch 10/40 [Training] LR: 0.001000: 100%|██████████| 391/391 [01:39&lt;00:00,  3.92it/s, loss=0.826]\nEpoch 10/40 [Validation]: 100%|██████████| 79/79 [00:09&lt;00:00,  8.60it/s, loss=1.01] \n\n\n\nEpoch 10/40 Summary:\n  Train Loss: 0.8413 | Train Acc: 70.54%\n  Valid Loss: 0.9084 | Valid Acc: 68.65%\n  Current LR: 0.001000\n\n\n\nEpoch 11/40 [Training] LR: 0.001000: 100%|██████████| 391/391 [01:39&lt;00:00,  3.92it/s, loss=0.689]\nEpoch 11/40 [Validation]: 100%|██████████| 79/79 [00:09&lt;00:00,  8.58it/s, loss=0.941]\n\n\n\n---&gt; New best model saved at epoch 11 with validation loss: 0.8162\n\n\nEpoch 11/40 Summary:\n  Train Loss: 0.8002 | Train Acc: 71.84%\n  Valid Loss: 0.8162 | Valid Acc: 71.49%\n  Current LR: 0.001000\n\n\n\nEpoch 12/40 [Training] LR: 0.001000: 100%|██████████| 391/391 [01:39&lt;00:00,  3.93it/s, loss=0.683]\nEpoch 12/40 [Validation]: 100%|██████████| 79/79 [00:09&lt;00:00,  8.68it/s, loss=1.09] \n\n\n\n---&gt; New best model saved at epoch 12 with validation loss: 0.7833\n\n\nEpoch 12/40 Summary:\n  Train Loss: 0.7779 | Train Acc: 72.71%\n  Valid Loss: 0.7833 | Valid Acc: 72.92%\n  Current LR: 0.001000\n\n\n\nEpoch 13/40 [Training] LR: 0.001000: 100%|██████████| 391/391 [01:39&lt;00:00,  3.92it/s, loss=0.838]\nEpoch 13/40 [Validation]: 100%|██████████| 79/79 [00:09&lt;00:00,  8.73it/s, loss=0.779]\n\n\n\nEpoch 13/40 Summary:\n  Train Loss: 0.7426 | Train Acc: 73.95%\n  Valid Loss: 0.7906 | Valid Acc: 73.53%\n  Current LR: 0.001000\n\n\n\nEpoch 14/40 [Training] LR: 0.001000: 100%|██████████| 391/391 [01:39&lt;00:00,  3.93it/s, loss=0.911]\nEpoch 14/40 [Validation]: 100%|██████████| 79/79 [00:09&lt;00:00,  8.64it/s, loss=0.651]\n\n\n\n---&gt; New best model saved at epoch 14 with validation loss: 0.7424\n\n\nEpoch 14/40 Summary:\n  Train Loss: 0.7237 | Train Acc: 74.84%\n  Valid Loss: 0.7424 | Valid Acc: 74.53%\n  Current LR: 0.001000\n\n\n\nEpoch 15/40 [Training] LR: 0.001000: 100%|██████████| 391/391 [01:39&lt;00:00,  3.93it/s, loss=0.681]\nEpoch 15/40 [Validation]: 100%|██████████| 79/79 [00:09&lt;00:00,  8.68it/s, loss=0.989]\n\n\n\n---&gt; New best model saved at epoch 15 with validation loss: 0.6920\n\n\nEpoch 15/40 Summary:\n  Train Loss: 0.7021 | Train Acc: 75.52%\n  Valid Loss: 0.6920 | Valid Acc: 76.06%\n  Current LR: 0.001000\n\n\n\nEpoch 16/40 [Training] LR: 0.001000: 100%|██████████| 391/391 [01:39&lt;00:00,  3.93it/s, loss=0.608]\nEpoch 16/40 [Validation]: 100%|██████████| 79/79 [00:09&lt;00:00,  8.68it/s, loss=0.713]\n\n\n\n---&gt; New best model saved at epoch 16 with validation loss: 0.6897\n\n\nEpoch 16/40 Summary:\n  Train Loss: 0.6817 | Train Acc: 76.48%\n  Valid Loss: 0.6897 | Valid Acc: 76.15%\n  Current LR: 0.001000\n\n\n\nEpoch 17/40 [Training] LR: 0.001000: 100%|██████████| 391/391 [01:39&lt;00:00,  3.94it/s, loss=0.516]\nEpoch 17/40 [Validation]: 100%|██████████| 79/79 [00:09&lt;00:00,  8.66it/s, loss=0.491]\n\n\n\n---&gt; New best model saved at epoch 17 with validation loss: 0.6855\n\n\nEpoch 17/40 Summary:\n  Train Loss: 0.6645 | Train Acc: 76.98%\n  Valid Loss: 0.6855 | Valid Acc: 76.53%\n  Current LR: 0.001000\n\n\n\nEpoch 18/40 [Training] LR: 0.001000: 100%|██████████| 391/391 [01:39&lt;00:00,  3.93it/s, loss=0.684]\nEpoch 18/40 [Validation]: 100%|██████████| 79/79 [00:09&lt;00:00,  8.63it/s, loss=0.584]\n\n\n\nEpoch 18/40 Summary:\n  Train Loss: 0.6430 | Train Acc: 77.51%\n  Valid Loss: 0.6882 | Valid Acc: 76.71%\n  Current LR: 0.001000\n\n\n\nEpoch 19/40 [Training] LR: 0.001000: 100%|██████████| 391/391 [01:39&lt;00:00,  3.94it/s, loss=0.686]\nEpoch 19/40 [Validation]: 100%|██████████| 79/79 [00:09&lt;00:00,  8.51it/s, loss=0.602]\n\n\n\n---&gt; New best model saved at epoch 19 with validation loss: 0.6528\n\n\nEpoch 19/40 Summary:\n  Train Loss: 0.6260 | Train Acc: 78.59%\n  Valid Loss: 0.6528 | Valid Acc: 77.44%\n  Current LR: 0.001000\n\n\n\nEpoch 20/40 [Training] LR: 0.001000: 100%|██████████| 391/391 [01:39&lt;00:00,  3.93it/s, loss=0.597]\nEpoch 20/40 [Validation]: 100%|██████████| 79/79 [00:09&lt;00:00,  8.63it/s, loss=0.985]\n\n\n\nEpoch 20/40 Summary:\n  Train Loss: 0.6112 | Train Acc: 78.84%\n  Valid Loss: 0.6556 | Valid Acc: 78.34%\n  Current LR: 0.001000\n\n\n\nEpoch 21/40 [Training] LR: 0.001000: 100%|██████████| 391/391 [01:39&lt;00:00,  3.93it/s, loss=0.619]\nEpoch 21/40 [Validation]: 100%|██████████| 79/79 [00:09&lt;00:00,  8.66it/s, loss=0.469]\n\n\n\n---&gt; New best model saved at epoch 21 with validation loss: 0.6311\n\n\nEpoch 21/40 Summary:\n  Train Loss: 0.6036 | Train Acc: 79.25%\n  Valid Loss: 0.6311 | Valid Acc: 77.80%\n  Current LR: 0.001000\n\n\n\nEpoch 22/40 [Training] LR: 0.001000: 100%|██████████| 391/391 [01:39&lt;00:00,  3.93it/s, loss=0.489]\nEpoch 22/40 [Validation]: 100%|██████████| 79/79 [00:09&lt;00:00,  8.60it/s, loss=0.933]\n\n\n\n---&gt; New best model saved at epoch 22 with validation loss: 0.6292\n\n\nEpoch 22/40 Summary:\n  Train Loss: 0.5879 | Train Acc: 79.67%\n  Valid Loss: 0.6292 | Valid Acc: 78.66%\n  Current LR: 0.001000\n\n\n\nEpoch 23/40 [Training] LR: 0.001000: 100%|██████████| 391/391 [01:39&lt;00:00,  3.93it/s, loss=0.591]\nEpoch 23/40 [Validation]: 100%|██████████| 79/79 [00:09&lt;00:00,  8.60it/s, loss=0.809]\n\n\n\nEpoch 23/40 Summary:\n  Train Loss: 0.5719 | Train Acc: 80.13%\n  Valid Loss: 0.6631 | Valid Acc: 78.27%\n  Current LR: 0.001000\n\n\n\nEpoch 24/40 [Training] LR: 0.001000: 100%|██████████| 391/391 [01:39&lt;00:00,  3.93it/s, loss=0.582]\nEpoch 24/40 [Validation]: 100%|██████████| 79/79 [00:09&lt;00:00,  8.59it/s, loss=0.523]\n\n\n\nEpoch 24/40 Summary:\n  Train Loss: 0.5611 | Train Acc: 80.51%\n  Valid Loss: 0.6358 | Valid Acc: 78.58%\n  Current LR: 0.001000\n\n\n\nEpoch 25/40 [Training] LR: 0.001000: 100%|██████████| 391/391 [01:39&lt;00:00,  3.93it/s, loss=0.651]\nEpoch 25/40 [Validation]: 100%|██████████| 79/79 [00:09&lt;00:00,  8.69it/s, loss=0.522]\n\n\n\n---&gt; New best model saved at epoch 25 with validation loss: 0.6080\n\n\nEpoch 25/40 Summary:\n  Train Loss: 0.5564 | Train Acc: 80.69%\n  Valid Loss: 0.6080 | Valid Acc: 79.01%\n  Current LR: 0.001000\n\n\n\nEpoch 26/40 [Training] LR: 0.001000: 100%|██████████| 391/391 [01:39&lt;00:00,  3.93it/s, loss=0.42] \nEpoch 26/40 [Validation]: 100%|██████████| 79/79 [00:09&lt;00:00,  8.62it/s, loss=0.701]\n\n\n\nEpoch 26/40 Summary:\n  Train Loss: 0.5376 | Train Acc: 81.27%\n  Valid Loss: 0.6286 | Valid Acc: 79.54%\n  Current LR: 0.001000\n\n\n\nEpoch 27/40 [Training] LR: 0.001000: 100%|██████████| 391/391 [01:39&lt;00:00,  3.93it/s, loss=0.586]\nEpoch 27/40 [Validation]: 100%|██████████| 79/79 [00:09&lt;00:00,  8.60it/s, loss=0.558]\n\n\n\nEpoch 27/40 Summary:\n  Train Loss: 0.5353 | Train Acc: 81.31%\n  Valid Loss: 0.6098 | Valid Acc: 79.64%\n  Current LR: 0.001000\n\n\n\nEpoch 28/40 [Training] LR: 0.001000: 100%|██████████| 391/391 [01:39&lt;00:00,  3.93it/s, loss=0.319]\nEpoch 28/40 [Validation]: 100%|██████████| 79/79 [00:09&lt;00:00,  8.44it/s, loss=0.325]\n\n\n\n---&gt; New best model saved at epoch 28 with validation loss: 0.5744\n\n\nEpoch 28/40 Summary:\n  Train Loss: 0.5299 | Train Acc: 81.55%\n  Valid Loss: 0.5744 | Valid Acc: 80.60%\n  Current LR: 0.001000\n\n\n\nEpoch 29/40 [Training] LR: 0.001000: 100%|██████████| 391/391 [01:39&lt;00:00,  3.94it/s, loss=0.504]\nEpoch 29/40 [Validation]: 100%|██████████| 79/79 [00:09&lt;00:00,  8.65it/s, loss=0.484]\n\n\n\nEpoch 29/40 Summary:\n  Train Loss: 0.5223 | Train Acc: 81.90%\n  Valid Loss: 0.5836 | Valid Acc: 80.31%\n  Current LR: 0.001000\n\n\n\nEpoch 30/40 [Training] LR: 0.001000: 100%|██████████| 391/391 [01:39&lt;00:00,  3.93it/s, loss=0.315]\nEpoch 30/40 [Validation]: 100%|██████████| 79/79 [00:09&lt;00:00,  8.58it/s, loss=0.412]\n\n\n\n---&gt; New best model saved at epoch 30 with validation loss: 0.5589\n\n\nEpoch 30/40 Summary:\n  Train Loss: 0.5080 | Train Acc: 82.46%\n  Valid Loss: 0.5589 | Valid Acc: 80.98%\n  Current LR: 0.001000\n\n\n\nEpoch 31/40 [Training] LR: 0.001000: 100%|██████████| 391/391 [01:39&lt;00:00,  3.92it/s, loss=0.469]\nEpoch 31/40 [Validation]: 100%|██████████| 79/79 [00:09&lt;00:00,  8.59it/s, loss=0.582]\n\n\n\nEpoch 31/40 Summary:\n  Train Loss: 0.5047 | Train Acc: 82.40%\n  Valid Loss: 0.5764 | Valid Acc: 80.59%\n  Current LR: 0.001000\n\n\n\nEpoch 32/40 [Training] LR: 0.001000: 100%|██████████| 391/391 [01:39&lt;00:00,  3.92it/s, loss=0.304]\nEpoch 32/40 [Validation]: 100%|██████████| 79/79 [00:10&lt;00:00,  7.80it/s, loss=0.51] \n\n\n\nEpoch 32/40 Summary:\n  Train Loss: 0.4902 | Train Acc: 83.11%\n  Valid Loss: 0.5664 | Valid Acc: 81.12%\n  Current LR: 0.001000\n\n\n\nEpoch 33/40 [Training] LR: 0.001000: 100%|██████████| 391/391 [01:39&lt;00:00,  3.92it/s, loss=0.494]\nEpoch 33/40 [Validation]: 100%|██████████| 79/79 [00:09&lt;00:00,  8.72it/s, loss=0.756]\n\n\n\nEpoch 33/40 Summary:\n  Train Loss: 0.4846 | Train Acc: 83.10%\n  Valid Loss: 0.5599 | Valid Acc: 81.43%\n  Current LR: 0.000100\n\n\n\nEpoch 34/40 [Training] LR: 0.000100: 100%|██████████| 391/391 [01:39&lt;00:00,  3.93it/s, loss=0.434]\nEpoch 34/40 [Validation]: 100%|██████████| 79/79 [00:09&lt;00:00,  8.73it/s, loss=0.296]\n\n\n\n---&gt; New best model saved at epoch 34 with validation loss: 0.4800\n\n\nEpoch 34/40 Summary:\n  Train Loss: 0.3871 | Train Acc: 86.55%\n  Valid Loss: 0.4800 | Valid Acc: 83.42%\n  Current LR: 0.000100\n\n\n\nEpoch 35/40 [Training] LR: 0.000100: 100%|██████████| 391/391 [01:39&lt;00:00,  3.93it/s, loss=0.488]\nEpoch 35/40 [Validation]: 100%|██████████| 79/79 [00:09&lt;00:00,  8.58it/s, loss=0.267]\n\n\n\nEpoch 35/40 Summary:\n  Train Loss: 0.3628 | Train Acc: 87.45%\n  Valid Loss: 0.4835 | Valid Acc: 83.83%\n  Current LR: 0.000100\n\n\n\nEpoch 36/40 [Training] LR: 0.000100: 100%|██████████| 391/391 [01:39&lt;00:00,  3.93it/s, loss=0.363]\nEpoch 36/40 [Validation]: 100%|██████████| 79/79 [00:09&lt;00:00,  8.60it/s, loss=0.309]\n\n\n\n---&gt; New best model saved at epoch 36 with validation loss: 0.4772\n\n\nEpoch 36/40 Summary:\n  Train Loss: 0.3519 | Train Acc: 87.70%\n  Valid Loss: 0.4772 | Valid Acc: 84.23%\n  Current LR: 0.000100\n\n\n\nEpoch 37/40 [Training] LR: 0.000100: 100%|██████████| 391/391 [01:39&lt;00:00,  3.92it/s, loss=0.473]\nEpoch 37/40 [Validation]: 100%|██████████| 79/79 [00:10&lt;00:00,  7.84it/s, loss=0.342]\n\n\n\nEpoch 37/40 Summary:\n  Train Loss: 0.3442 | Train Acc: 87.98%\n  Valid Loss: 0.4817 | Valid Acc: 83.88%\n  Current LR: 0.000100\n\n\n\nEpoch 38/40 [Training] LR: 0.000100: 100%|██████████| 391/391 [01:39&lt;00:00,  3.92it/s, loss=0.585]\nEpoch 38/40 [Validation]: 100%|██████████| 79/79 [00:09&lt;00:00,  8.63it/s, loss=0.274]\n\n\n\nEpoch 38/40 Summary:\n  Train Loss: 0.3441 | Train Acc: 88.03%\n  Valid Loss: 0.4802 | Valid Acc: 84.05%\n  Current LR: 0.000100\n\n\n\nEpoch 39/40 [Training] LR: 0.000100: 100%|██████████| 391/391 [01:39&lt;00:00,  3.92it/s, loss=0.342]\nEpoch 39/40 [Validation]: 100%|██████████| 79/79 [00:09&lt;00:00,  8.64it/s, loss=0.271]\n\n\n\nEpoch 39/40 Summary:\n  Train Loss: 0.3385 | Train Acc: 88.23%\n  Valid Loss: 0.4807 | Valid Acc: 84.42%\n  Current LR: 0.000010\n\n\n\nEpoch 40/40 [Training] LR: 0.000010: 100%|██████████| 391/391 [01:39&lt;00:00,  3.93it/s, loss=0.388]\nEpoch 40/40 [Validation]: 100%|██████████| 79/79 [00:09&lt;00:00,  8.45it/s, loss=0.247]\n\n\n\n---&gt; New best model saved at epoch 40 with validation loss: 0.4746\n\n\nEpoch 40/40 Summary:\n  Train Loss: 0.3228 | Train Acc: 88.80%\n  Valid Loss: 0.4746 | Valid Acc: 84.46%\n  Current LR: 0.000010\n\nFinished Training\nThe best model was saved from epoch 40 at path: best_model.pth\n\n\n\n\n\nAs we can see from the logs, the model steadily improves. The training loss decreases, and both training and validation accuracy increase, which is exactly what we want to see.\n\n\nStep 3: Visualizing the Training Progress\nWhile logs are useful, plots give us a much more intuitive understanding of the training dynamics. Let’s visualize the loss, accuracy, and learning rate over all 40 epochs.\n\n# --- Modified Plotting Function for Per-Epoch LR ---\ndef plot_metrics_and_lr(history):\n    fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(22, 6))\n    epochs = range(1, len(history['train_loss']) + 1)\n\n    # Plot Loss\n    ax1.plot(epochs, history['train_loss'], 'bo-', label='Training Loss')\n    ax1.plot(epochs, history['val_loss'], 'ro-', label='Validation Loss')\n    ax1.set_title('Loss Over Epochs')\n    ax1.set_xlabel('Epoch')\n    ax1.set_ylabel('Loss')\n    ax1.legend()\n    ax1.grid(True)\n\n    # Plot Accuracy\n    ax2.plot(epochs, history['train_acc'], 'bo-', label='Training Accuracy')\n    ax2.plot(epochs, history['val_acc'], 'ro-', label='Validation Accuracy')\n    ax2.set_title('Accuracy Over Epochs')\n    ax2.set_xlabel('Epoch')\n    ax2.set_ylabel('Accuracy (%)')\n    ax2.legend()\n    ax2.grid(True)\n\n    # Plot Learning Rate (now per epoch)\n    ax3.plot(epochs, history['learning_rate'], 'go-')\n    ax3.set_title('Learning Rate Over Epochs')\n    ax3.set_xlabel('Epoch')\n    ax3.set_ylabel('Learning Rate')\n    ax3.grid(True)\n\n    fig.suptitle('Model Training Progress', fontsize=20)\n    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n    plt.show()\n\n# --- Call the updated plotting function ---\nplot_metrics_and_lr(history)\n\n\n\n\n\n\n\n\nInterpreting the Plots:\n\nLoss Over Epochs (Left Plot): This is the classic sign of a healthy training process. Both the training loss (blue) and validation loss (red) decrease steadily and start to converge. Importantly, the validation loss doesn’t start to climb, which would be a sign of overfitting.\nAccuracy Over Epochs (Middle Plot): This plot mirrors the loss plot. Both training and validation accuracy climb consistently, reaching a final validation accuracy of over 84%. The gap between the two curves is small, again indicating that our model is generalizing well.\nLearning Rate Over Epochs (Right Plot): This plot shows our ReduceLROnPlateau scheduler in action. The learning rate stays constant for the first ~32 epochs, and then, as the validation loss plateaus, the scheduler drops the LR twice to allow for more precise fine-tuning.\n\nThese plots confirm that our SqueezeNet implementation not only works but learns very effectively on the CIFAR-10 dataset.\n\n# 1. Create a new instance of your model architecture\nfinal_model = SqueezeNet()\n\n# 2. Load the saved state dictionary\nfinal_model.load_state_dict(torch.load(SAVE_PATH))\n\n# 3. Move the model to the correct device and set to evaluation mode\nfinal_model.to(device)\nfinal_model.eval()\n\n# Now `final_model` is ready for inference and contains the weights\n# from the epoch with the lowest validation loss.\nprint(\"Successfully loaded the best model for inference!\")\n\nSuccessfully loaded the best model for inference!\n\n\n\n\nStep 5: The Final Report Card - Per-Class Accuracy\nA single accuracy number can sometimes be misleading. Let’s break down the performance for each of the 10 classes in CIFAR-10 to get a more nuanced view.\n\ndef visualize_model_predictions(model, dataloader, num_images=10):\n    was_training = model.training\n    model.eval()\n    images_so_far = 0\n    fig = plt.figure(figsize=(15, 7))\n\n    with torch.no_grad():\n        # Get one batch of test images\n        dataiter = iter(dataloader)\n        images, labels = next(dataiter)\n\n        # Move data to the same device as the model\n        images = images.to(device)\n        labels = labels.to(device)\n\n        # Get model predictions\n        outputs = model(images)\n        # Convert outputs to probabilities and then to class predictions\n        _, preds = torch.max(outputs, 1)\n\n        # Plot the first `num_images` from the batch\n        for i in range(num_images):\n            ax = plt.subplot(2, num_images//2, i+1)\n            ax.axis('off')\n\n            # Get the predicted and true labels\n            pred_label = classes[preds[i]]\n            true_label = classes[labels[i]]\n\n            # Set title color based on correctness\n            title_color = 'green' if pred_label == true_label else 'red'\n            ax.set_title(f'Pred: {pred_label}\\nTrue: {true_label}', color=title_color)\n\n            # Un-normalize and display the image\n            img = images[i].cpu().numpy().transpose((1, 2, 0))\n            mean = np.array([0.5, 0.5, 0.5])\n            std = np.array([0.5, 0.5, 0.5])\n            img = std * img + mean\n            img = np.clip(img, 0, 1) # Ensure pixel values are valid\n            plt.imshow(img)\n\n        model.train(mode=was_training)\n        plt.tight_layout()\n        plt.show()\n\n# --- Run the Visualization ---\n# Use the testloader to get unseen images\nvisualize_model_predictions(final_model, testloader)\n\n\n\n\n\n\n\n\n\n# Dictionary to store correct predictions for each class\ncorrect_pred = {classname: 0 for classname in classes}\n# Dictionary to store total count of each class\ntotal_pred = {classname: 0 for classname in classes}\n\n# --- Iterate Over the Entire Test Dataset ---\nwith torch.no_grad(): # No need to calculate gradients during evaluation\n    for data in tqdm(testloader, desc=\"Calculating class accuracies\"):\n        images, labels = data[0].to(device), data[1].to(device)\n        outputs = final_model(images)\n        _, predictions = torch.max(outputs, 1)\n\n        # Collect the correct predictions for each class\n        for label, prediction in zip(labels, predictions):\n            if label == prediction:\n                correct_pred[classes[label]] += 1\n            total_pred[classes[label]] += 1\n\nCalculating class accuracies: 100%|██████████| 79/79 [00:11&lt;00:00,  7.09it/s]\n\n\n\n# --- Print Accuracies and Prepare Data for Plotting ---\nclass_accuracies = []\nprint(\"\\n\" + \"=\"*30)\nprint(\"PER-CLASS ACCURACY REPORT\")\nprint(\"=\"*30)\nfor classname, correct_count in correct_pred.items():\n    total_count = total_pred[classname]\n    accuracy = (100 * float(correct_count) / total_count) if total_count &gt; 0 else 0\n    class_accuracies.append(accuracy)\n    print(f'Accuracy for class: {classname:5s} is {accuracy:.1f} %')\nprint(\"=\"*30)\n\n\n==============================\nPER-CLASS ACCURACY REPORT\n==============================\nAccuracy for class: plane is 89.1 %\nAccuracy for class: car   is 92.4 %\nAccuracy for class: bird  is 76.8 %\nAccuracy for class: cat   is 71.4 %\nAccuracy for class: deer  is 82.6 %\nAccuracy for class: dog   is 76.5 %\nAccuracy for class: frog  is 85.9 %\nAccuracy for class: horse is 86.9 %\nAccuracy for class: ship  is 90.8 %\nAccuracy for class: truck is 92.2 %\n==============================\n\n\n\n# --- Create and Display the Bar Chart ---\nplt.style.use('seaborn-v0_8-whitegrid') # A nice-looking style\nfig, ax = plt.subplots(figsize=(12, 7))\n\n# Create bars\nbars = ax.bar(classes, class_accuracies, color=plt.cm.viridis(np.linspace(0.1, 0.9, len(classes))))\n\n# Add labels and title\nax.set_ylabel('Accuracy (%)', fontsize=12)\nax.set_title('Per-Class Model Accuracy', fontsize=16)\nax.set_xticklabels(classes, rotation=45, ha=\"right\")\nax.set_ylim([0, 100]) # Set y-axis from 0 to 100\n\n# Add the accuracy value on top of each bar\nfor bar in bars:\n    yval = bar.get_height()\n    ax.text(bar.get_x() + bar.get_width()/2.0, yval + 1, f'{yval:.1f}%', ha='center', va='bottom')\n\nplt.tight_layout()\nplt.show()\n\n/tmp/ipykernel_28907/1062035383.py:11: UserWarning: set_ticklabels() should only be used with a fixed number of ticks, i.e. after set_ticks() or using a FixedLocator.\n  ax.set_xticklabels(classes, rotation=45, ha=\"right\")\n\n\n\n\n\n\n\n\n\nThis gives us a much clearer picture. We can see that the model is excellent at identifying ‘car’, ‘ship’, and ‘truck’ (all over 90% accuracy), but finds ‘cat’ a bit more challenging (around 71% accuracy). This is a common result on CIFAR-10 and provides valuable insight into the model’s specific strengths and weaknesses.\nWith training complete and performance thoroughly analyzed, there’s only one thing left to do: verify if our model lives up to the “Squeeze” promise of efficiency."
  },
  {
    "objectID": "posts/2025-08-27-squeezenet-implementation.html#the-verdict---did-we-fulfill-the-squeeze-promise",
    "href": "posts/2025-08-27-squeezenet-implementation.html#the-verdict---did-we-fulfill-the-squeeze-promise",
    "title": "Building SqueezeNet from Scratch with PyTorch: A Hands-On Implementation",
    "section": "The Verdict - Did We Fulfill the “Squeeze” Promise?",
    "text": "The Verdict - Did We Fulfill the “Squeeze” Promise?\nWe’ve built it, we’ve trained it, and we’ve proven that our SqueezeNet implementation is an effective classifier for the CIFAR-10 dataset, achieving over 84% validation accuracy.\nBut we started this journey with one primary goal in mind, inspired by the SqueezeNet paper: to create a model that is not just accurate, but radically efficient. It’s time for the moment of truth. Let’s measure our final model and see how it stacks up.\n\nMeasuring the Model: Parameters and File Size\nThere are two key metrics for model efficiency:\n\nNumber of Trainable Parameters: This is an intrinsic measure of the model’s complexity. Fewer parameters generally mean less computation and a lower risk of overfitting.\nFile Size on Disk: This is a practical measure of the model’s storage footprint. This is what matters for over-the-air updates and fitting a model into the limited memory of an edge device.\n\nWe can easily calculate both of these for our saved PyTorch model.\n\n# --- Calculate the Number of Trainable Parameters ---\n\n# Use a function to make it reusable and clean\ndef count_parameters(model):\n    \"\"\"Counts the number of trainable parameters in a model.\"\"\"\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n\nnum_params = count_parameters(final_model)\nprint(f\"Number of Trainable Parameters: {num_params:,}\") # {:,} adds commas for readability\n\n# --- Calculate the File Size on Disk ---\n\n# The size of the saved state_dict file is what we want\n# os.path.getsize returns the size in bytes\nfile_size_bytes = os.path.getsize(SAVE_PATH)\n# Convert bytes to megabytes (MB)\nfile_size_mb = file_size_bytes / (1024 * 1024)\n\nprint(f\"Model File Size on Disk: {file_size_mb:.2f} MB\")\n\nNumber of Trainable Parameters: 740,554\nModel File Size on Disk: 2.84 MB\n\n\nLet’s pause and appreciate these numbers. Our model, which achieved a very respectable 84% accuracy on a non-trivial dataset, has:\n\nFewer than 750,000 parameters.\nA total file size of less than 3 megabytes.\n\nTo put this in perspective, the classic VGG-16 model has 138 million parameters and a file size of over 500 MB. Our SqueezeNet implementation achieves strong performance while being orders of magnitude more efficient. This is a direct and powerful validation of the design principles laid out in the original paper."
  },
  {
    "objectID": "posts/2025-08-27-squeezenet-implementation.html#pushing-the-limits---an-experiment-with-batch-size",
    "href": "posts/2025-08-27-squeezenet-implementation.html#pushing-the-limits---an-experiment-with-batch-size",
    "title": "Building SqueezeNet from Scratch with PyTorch: A Hands-On Implementation",
    "section": "Pushing the Limits - An Experiment with Batch Size",
    "text": "Pushing the Limits - An Experiment with Batch Size\nAfter achieving these great baseline results, I was curious: could we push the training even further? A common technique to speed up training and potentially improve model performance is to use the largest batch size that your GPU memory can handle. My baseline model was trained with a batch size of 128. I decided to see what would happen if I increased it.\nThe Experiment:\nI was using a T4 GPU on Google Colab. My first attempt was to push the batch size to 512. Unfortunately, this was too ambitious—the session ran out of GPU memory and the kernel crashed. This is a very common and practical limitation that developers face.\nMy second attempt was to use a more moderate batch size of 384. This time, the training completed successfully. Here are the results from that run:\n\n\n\n\n\n\n\n\n\n\nAnalysis of the Results:\nInterestingly, while the training was faster per epoch, the final model’s performance was slightly lower than the baseline:\nThe Lesson: While larger batch sizes can speed up training, they don’t always guarantee a better final model. There’s a complex interplay between batch size, learning rate, and model convergence, and sometimes a smaller batch size can act as a form of regularization, helping the model find a more generalizable solution. It highlights that finding the optimal hyperparameters often requires experimentation and iteration."
  },
  {
    "objectID": "posts/2025-08-27-squeezenet-implementation.html#conclusion-from-theory-to-reality",
    "href": "posts/2025-08-27-squeezenet-implementation.html#conclusion-from-theory-to-reality",
    "title": "Building SqueezeNet from Scratch with PyTorch: A Hands-On Implementation",
    "section": "Conclusion: From Theory to Reality",
    "text": "Conclusion: From Theory to Reality\nThis journey from paper to code has been incredibly rewarding. We’ve seen firsthand how SqueezeNet’s elegant design principles translate into a real, working PyTorch model that is both compact and powerful. The follow-up experiment further reminds us that deep learning is an empirical science, where pushing the limits can reveal fascinating and practical trade-offs.\nI encourage you to take this code, experiment with it yourself, and continue exploring the vast and exciting design space of efficient neural networks."
  },
  {
    "objectID": "posts/2025-09-02-mobilenetv1-reading-notes.html#introduction",
    "href": "posts/2025-09-02-mobilenetv1-reading-notes.html#introduction",
    "title": "MobileNetV1 Explained: A Deep Dive into Lightweight Neural Networks",
    "section": "Introduction",
    "text": "Introduction\nEver wonder how your smartphone pulls off those magical AI tricks? Portrait mode that blurs the background perfectly, real-time language translation through your camera, or those fun filters that pop up on your face? These features require a tremendous amount of computation, yet your phone doesn’t get scorching hot or run out of battery in five minutes. How is that possible?\nThe secret lies in designing neural networks that are not just accurate, but also incredibly efficient. They need to be small enough to fit in your phone’s memory and fast enough to run in real-time. This is a huge challenge, and for a long time, the worlds of top-tier accuracy and on-device performance were far apart.\nIn 2017, a team at Google published a landmark paper called “MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications.” This paper didn’t just introduce a single new model; it presented a brilliant and practical blueprint for building a whole family of lightweight, fast, and powerful models designed specifically for the constraints of mobile and embedded devices.\n\n\n\nPaper Title\n\n\nIn this post, we’ll take a guided tour through the MobileNets paper, breaking it down page-by-page. By the end, you’ll understand the simple yet powerful ideas behind its design, why it was so revolutionary, and how its core components have become fundamental building blocks for modern AI.\nLet’s start with the abstract."
  },
  {
    "objectID": "posts/2025-09-02-mobilenetv1-reading-notes.html#the-abstract-mobilenets-in-a-nutshell",
    "href": "posts/2025-09-02-mobilenetv1-reading-notes.html#the-abstract-mobilenets-in-a-nutshell",
    "title": "MobileNetV1 Explained: A Deep Dive into Lightweight Neural Networks",
    "section": "The Abstract: MobileNets in a Nutshell",
    "text": "The Abstract: MobileNets in a Nutshell\nA paper’s abstract is its elevator pitch. In just a few sentences, it tells you the problem, the proposed solution, the results, and why it matters. The MobileNets abstract is a perfect example, laying out the entire story concisely.\nHere is the abstract from the paper:\n\n\n\nPaper Abstract\n\n\nLet’s unpack the key promises made in this short paragraph.\n\nThe Goal: To create “efficient models… for mobile and embedded vision applications.” This is the core mission statement.\nThe Secret Sauce: The key technical idea is an operation called depthwise separable convolutions. This is the magic ingredient that allows them to build networks that are both “light weight” and “deep” (which is crucial for accuracy).\nThe Killer Feature: They introduce two simple “control knobs” (hyper-parameters) that let any developer easily trade off speed for accuracy. This is incredibly practical. It means you can choose the perfect model for your specific needs—whether you need maximum speed for a live video filter or higher accuracy for an offline photo-sorting app.\nThe Proof: They don’t just propose an idea; they prove it works. They show strong results on the standard ImageNet benchmark and demonstrate that MobileNets are versatile enough to be used for a wide variety of tasks, from object detection to face analysis.\n\nIn essence, the abstract promises a practical, powerful, and proven recipe for building efficient AI. Now, let’s dive into the paper to see how they deliver on these promises."
  },
  {
    "objectID": "posts/2025-09-02-mobilenetv1-reading-notes.html#setting-the-stage-for-a-new-kind-of-network",
    "href": "posts/2025-09-02-mobilenetv1-reading-notes.html#setting-the-stage-for-a-new-kind-of-network",
    "title": "MobileNetV1 Explained: A Deep Dive into Lightweight Neural Networks",
    "section": "Setting the Stage for a New Kind of Network",
    "text": "Setting the Stage for a New Kind of Network\nThe introduction of a paper provides context. It frames the problem and outlines the argument that the rest of the paper will support. On page 1, the authors walk us through the state of computer vision at the time to show us exactly where their contribution fits in.\n\n\n\n\n\n\nIntroduction\n\n\n\n\n\n\n\n\n\nThe “Bigger is Better” Problem\n\n(Page 1, Introduction, Para 1): “Convolutional neural networks have become ubiquitous in computer vision ever since AlexNet [19] popularized deep convolutional neural networks… The general trend has been to make deeper and more complicated networks in order to achieve higher accuracy… However, these advances to improve accuracy are not necessarily making networks more efficient with respect to size and speed.”\n\nThe authors start by acknowledging a clear trend in AI research: the race for accuracy. Ever since the breakthrough of AlexNet in 2012, the dominant philosophy was that to get better results, you needed to build bigger, deeper, and more complex models.\nThis “bigger is better” approach worked wonders for winning competitions, but it created a major problem for real-world applications. Models like VGG or ResNet were incredibly powerful, but they were also massive and slow. They required powerful servers with expensive GPUs to run. This was a dead-end for applications that needed to run on a device in your hand—like robotics, self-driving cars, and augmented reality.\nThis is the gap that MobileNets was designed to fill. How can we get the benefits of deep learning without the crushing computational cost?\n\n\nA Glimpse into Prior Work\nBefore diving into their own solution, the authors give a quick tour of existing approaches to this problem on page 1. They group the methods into two main categories:\n\nModel Compression: This involves taking a large, pre-trained, powerful model and trying to shrink it. Techniques include pruning (removing unnecessary connections), quantization (using fewer bits to store numbers), and hashing. Think of this as taking a large file and zipping it.\nTraining Small Networks Directly: This involves designing an architecture that is efficient and small from the very beginning.\n\nThe authors place MobileNets firmly in the second category. They make a crucial distinction: while many previous efforts focused on making models small in terms of file size, MobileNets are primarily designed to be fast in terms of latency (the time it takes to make a prediction). This focus on speed is what makes them so practical.\nThe key takeaway from this section is that the core idea of MobileNets—the depthwise separable convolution—was not a brand-new invention. It had appeared before, for instance, in the early layers of Google’s own Inception models. However, the revolutionary step the MobileNet paper takes is to show that you can build an entire, high-performing network almost exclusively from this single, hyper-efficient building block.\nNow that the stage is set, let’s look at the architectural details that make MobileNets work."
  },
  {
    "objectID": "posts/2025-09-02-mobilenetv1-reading-notes.html#why-we-need-smarter-not-just-bigger-ai",
    "href": "posts/2025-09-02-mobilenetv1-reading-notes.html#why-we-need-smarter-not-just-bigger-ai",
    "title": "MobileNetV1 Explained: A Deep Dive into Lightweight Neural Networks",
    "section": "Why We Need Smarter, Not Just Bigger, AI",
    "text": "Why We Need Smarter, Not Just Bigger, AI\nThe introduction of a paper is crucial. It sets the stage, frames the problem, and tells you why you should care about the solution being proposed. On page 1, the authors of MobileNets walk us through the state of computer vision at the time, painting a clear picture of a field heading in a direction that was unsustainable for mobile applications.\n\n\n\n\n\n\nIntroduction\n\n\n\n\n\n\n\n\n\nThe “Accuracy Arms Race”\nThe first paragraph takes us back to the years following 2012, a time when deep learning was exploding in popularity thanks to a model called AlexNet. The dominant trend was clear: to get better results, you just had to build bigger and deeper networks. This led to an “arms race” where research labs competed to create massive, complex models that could inch out a new state-of-the-art score on academic benchmarks.\nBut the authors point out a critical flaw in this approach:\n\n(Page 1, Introduction, Para 1): “The general trend has been to make deeper and more complicated networks in order to achieve higher accuracy… However, these advances to improve accuracy are not necessarily making networks more efficient with respect to size and speed. In many real world applications such as robotics, self-driving car and augmented reality, the recognition tasks need to be carried out in a timely fashion on a computationally limited platform.”\n\nThis is the core problem MobileNets was designed to solve. While giant models running on powerful servers in the cloud are great, they are useless for tasks that need to happen right here, right now, on the device in your hand. Your phone, your drone, or the smart camera in your home is a “computationally limited platform.” It doesn’t have endless power or memory. The relentless pursuit of accuracy was leaving these real-world applications behind.\n\n\nThe Blueprint for a Solution\nSo, how do we break out of this “bigger is better” cycle? The paper’s introduction immediately lays out a clear, two-part plan.\n\n(Page 1, Introduction, Para 2): “This paper describes an efficient network architecture and a set of two hyper-parameters in order to build very small, low latency models that can be easily matched to the design requirements for mobile and embedded vision applications.”\n\nThis single sentence is the blueprint for the entire paper. The solution isn’t just one thing; it’s a combination of two powerful ideas:\n\nAn Efficient Architecture: They will propose a new way to build the fundamental layers of a neural network, making them inherently faster and smaller from the ground up.\nTwo Simple Hyper-parameters: They will provide two easy-to-use “control knobs” that allow anyone to customize the architecture. These knobs, which they will later call the width multiplier and the resolution multiplier, give developers the power to fine-tune the trade-off between speed, size, and accuracy for their specific needs.\n\nThe introduction does its job perfectly. It establishes a clear tension between the academic trend of building massive networks and the practical need for efficient on-device AI. It then promises a clear, elegant, and customizable solution to resolve that tension.\nWith the stage set, the paper next takes a look at the “Prior Work”"
  },
  {
    "objectID": "posts/2025-09-02-mobilenetv1-reading-notes.html#standing-on-the-shoulders-of-giants-prior-work",
    "href": "posts/2025-09-02-mobilenetv1-reading-notes.html#standing-on-the-shoulders-of-giants-prior-work",
    "title": "MobileNetV1 Explained: A Deep Dive into Lightweight Neural Networks",
    "section": "Standing on the Shoulders of Giants (Prior Work)",
    "text": "Standing on the Shoulders of Giants (Prior Work)\nGreat research rarely happens in a vacuum. Before diving into the technical details of their new architecture, the authors take a moment in Section 2 to survey the landscape of efficient neural network design. This shows they are aware of other approaches and helps us understand exactly where MobileNets fits in and what makes it different.\n\nTwo Paths to a Smaller Network\nThe paper begins by outlining the two main strategies that researchers were using to create smaller, more efficient models.\n\n\n\nPrior Work, Para 1\n\n\n\n(Page 1, Section 2, Para 1): “Many different approaches can be generally categorized into either compressing pretrained networks or training small networks directly. This paper proposes a class of network architectures that allows a model developer to specifically choose a small network that matches the resource restrictions (latency, size) for their application.”\n\nThis is a fantastic, high-level summary. Let’s break down these two paths:\n\nThe “Compression” Path: This strategy starts with a big, powerful, pre-trained model (like a VGG or ResNet). Then, it applies a series of clever tricks to shrink it down, like pruning away unimportant connections or using quantization to represent the weights with fewer bits. Think of this as taking a large, high-resolution photo and running it through a compression algorithm to get a smaller JPEG file. You lose some quality, but the file size is much smaller.\nThe “Efficient Design” Path: This strategy doesn’t start with a big model. Instead, it focuses on designing a network architecture that is small and fast from the very beginning. It’s about building with lighter, more efficient materials from the ground up, rather than trying to slim down a heavyweight construction later.\n\nThe authors clearly state that MobileNets belong to the second category. They aren’t a trick to compress big models; they are a fundamentally new recipe for building efficient models from scratch.\n\nA Focus on Speed, Not Just Size\nThis first paragraph ends with a crucial philosophical point that distinguishes MobileNets from much of the other research at the time.\n\n(Page 1, Section 2, Para 1): “MobileNets primarily focus on optimizing for latency but also yield small networks. Many papers on small networks focus only on size but do not consider speed.”\n\nThis is a subtle but incredibly important distinction. Latency is about how fast the model can make a prediction. Size is about how much storage space the model takes up. While the two are often related, they are not the same thing.\nA model could be small in file size but use computationally awkward operations that are slow to run on a mobile phone’s processor. The MobileNet team made a conscious decision to prioritize speed (low latency) above all else. Their goal was to design an architecture that was not just theoretically efficient, but one that would be blazingly fast on real hardware. As we’ll see, this focus on practical, real-world speed would guide many of their most important design decisions.\n\n\n\nThe Building Blocks of Efficiency\nThe authors now zoom in on the second category of research—designing efficient networks from scratch—and acknowledge the key ideas and contemporary papers that influenced their work. This paragraph is a whirlwind tour of the concepts that were “in the air” at the time.\n\n\n\nPrior Work, Para 2\n\n\n\n(Page 1, Section 2, Para 2): “MobileNets are built primarily from depthwise separable convolutions initially introduced in [26] and subsequently used in Inception models [13] to reduce the computation in the first few layers…”\n\nThis first sentence is the most important. The authors give credit where credit is due, stating that the core component of MobileNets—the depthwise separable convolution—was not their invention. We’ll dive deep into what this operation is in the next section, but for now, the key takeaway is that this efficient building block had been seen before. It was even used in a limited capacity in Google’s own famous Inception models. The big innovation of MobileNets was not in inventing this block, but in recognizing its full potential and building an entire architecture out of it.\nThe paragraph then lists several other influential approaches to efficient network design:\n\nFactorized Convolutions (Flattened Networks, Factorized Networks): This is the general principle that depthwise separable convolutions are based on. “Factorizing” just means breaking down one big, expensive mathematical operation into several smaller, cheaper ones that approximate the original. These other papers explored different ways of doing this, proving that the core idea was powerful.\nThe Xception Network: This is a crucial paper that came out shortly before MobileNets. It took the ideas from the Inception family and pushed them to their logical extreme, showing that depthwise separable convolutions could be scaled up to build state-of-the-art models, not just small ones. This provided strong evidence that this was a powerful and general-purpose building block.\nSqueezeNet: This was another very famous small network. SqueezeNet’s approach was different; it cleverly used 1x1 convolutions in a “bottleneck” structure to “squeeze” the number of channels down, do some processing, and then expand them back up. This was another very effective way to reduce the number of parameters and computations.\n\nBy mentioning all these different papers, the authors show that they are part of a vibrant research community that was actively trying to solve the problem of network efficiency. While SqueezeNet used bottlenecks and Inception used complex multi-path modules, the MobileNet authors placed their bet on a single, elegant idea: what happens if we build an entire deep network using nothing but the simplest and most efficient building block we can find—the depthwise separable convolution?\n\n\nTricks of the Trade: Shrinking, Squeezing, and Distilling\nTo round out their review of prior work, the authors briefly touch on the other major philosophy for creating small models: taking a large, pre-trained network and applying clever techniques to shrink it.\n\n\n\n\n\n\n\n\n\n\n\n“A different approach for obtaining small networks is shrinking, factorizing or compressing pretrained networks. Compression based on product quantization [36], hashing [2], and pruning, vector quantization and Huffman coding [5] have been proposed in the literature.”\n\nThis sounds like a lot of jargon, but the core idea is simple. These are all different methods for “compressing” the information stored in a network’s weights, much like you would compress a large file on your computer.\n\nPruning: Imagine a dense web of connections in the network. Pruning is like a gardener snipping away the weakest, least important connections, leaving behind a sparse but still effective network.\nQuantization: A typical neural network stores its numbers (weights) with high precision (e.g., 32-bit floating point). Quantization is the process of using fewer bits to store these numbers. This can dramatically reduce the model’s file size, often with little loss in accuracy.\nHuffman Coding: This is a classic data compression technique (used in things like JPEG and MP3 files) that can be applied to the network’s weights to make the final model file even smaller.\n\nThe authors also mention a fascinating and powerful technique that bridges the gap between the “large model” world and the “small model” world.\n\n“Another method for training small networks is distillation [9] which uses a larger network to teach a smaller network. It is complementary to our approach and is covered in some of our use cases in section 4.”\n\nKnowledge Distillation is a beautiful idea. Imagine you have a wise, experienced “teacher” model that is very large and accurate, and a small, nimble “student” model (like a MobileNet).\nInstead of training the student on a raw dataset, you have it learn by mimicking the teacher. The teacher provides “soft labels”—not just the final answer, but its confidence and nuances. The student learns to replicate the teacher’s “thought process,” effectively transferring the knowledge from the large model into its own compact form.\nThe authors astutely point out that this is complementary to their work. MobileNet is the perfect “student” architecture for distillation. They even foreshadow that they will use this exact technique in their experiments later in the paper.\nWith this survey complete, the stage is now perfectly set. We understand the problem (the need for efficient on-device AI), the two main schools of thought for solving it (compression vs. efficient design), and where MobileNets fits in. Now, it’s time to dive into the technical heart of the paper: the MobileNet architecture itself."
  },
  {
    "objectID": "posts/2025-09-02-mobilenetv1-reading-notes.html#mobilenet-architecture",
    "href": "posts/2025-09-02-mobilenetv1-reading-notes.html#mobilenet-architecture",
    "title": "MobileNetV1 Explained: A Deep Dive into Lightweight Neural Networks",
    "section": "MobileNet Architecture",
    "text": "MobileNet Architecture\nNow we get to the heart of the paper: the MobileNet architecture itself. In Section 3, the authors introduce the brilliant and efficient building block that makes the entire system work: the depthwise separable convolution.\nThis sounds complicated, but the core idea is beautifully simple. It’s about taking the standard, workhorse operation of computer vision—the convolution—and breaking it apart into two smaller, much faster steps.\nFirst, the paper gives us a quick roadmap for the section.\n\n(Page 2, Section 3, Intro): “In this section we first describe the core layers that MobileNet is built on which are depthwise separable filters. We then describe the MobileNet network structure and conclude with descriptions of the two model shrinking hyper-parameters width multiplier and resolution multiplier.”\n\nThey’ll start with the fundamental building block, then show how it’s assembled into a full network, and finally explain the “control knobs” used to customize it.\n\n\nThe Two Jobs of a Standard Convolution\nBefore we can understand the MobileNet way, we need to understand what a standard convolution does. A standard convolution is a powerhouse, but it’s trying to do two very different jobs at the same time:\n\nIt filters spatially: It scans over an image to find spatial patterns, like edges, corners, textures, or shapes.\nIt combines channels: It mixes information from the input channels to create new, meaningful features in the output channels.\n\nThe MobileNet paper argues that forcing one single operation to do both of these jobs at once is inefficient. The key insight is to “separate” these responsibilities.\n\n\nThe MobileNet Way: Divide and Conquer\nThis brings us to the core concept of the paper, explained in the first paragraph of Section 3.1.\n\n(Page 2, Section 3.1, Para 1): “The MobileNet model is based on depthwise separable convolutions which is a form of factorized convolutions which factorize a standard convolution into a depthwise convolution and a 1 × 1 convolution called a pointwise convolution… A standard convolution both filters and combines inputs into a new set of outputs in one step. The depthwise separable convolution splits this into two layers, a separate layer for filtering and a separate layer for combining.”\n\nThis is the entire trick in a nutshell. Instead of one big, expensive layer, MobileNet uses two small, cheap layers:\n\nThe Depthwise Convolution (The “Filtering” Step): This first layer handles only the spatial filtering. It glides a filter over the input, but it does so for each input channel independently. It doesn’t mix information between channels at all. If the input has 64 channels, this step produces 64 filtered channels, keeping them all separate.\nThe Pointwise Convolution (The “Combining” Step): This second layer handles the channel mixing. It uses a brilliantly simple and fast 1x1 convolution. This tiny filter looks at a single pixel and intelligently combines the values from all the channels produced by the depthwise step to create a new, rich feature.\n\nThis “factorization”—splitting one big job into two specialized smaller jobs—is the key. The paper states the outcome in no uncertain terms:\n\n(Page 2, Section 3.1, Para 1): “This factorization has the effect of drastically reducing computation and model size.”\n\nThis is the secret sauce. By separating the concerns of filtering and combining, MobileNets achieve a massive reduction in the number of calculations and parameters needed. As we’ll see in the next section, this isn’t just a small improvement; it makes the operation about 8 to 9 times more efficient than a standard convolution, with almost no loss in accuracy. This is the breakthrough that makes fast, powerful on-device AI possible.\n\n\nThe Math of the Bottleneck: Why Standard Convolutions Are So Expensive\nTo truly appreciate the elegance of the MobileNet solution, we first need to understand the problem in more detail. Why, exactly, is a standard convolution so computationally expensive? The paper now dives into the math to give us a clear answer.\n\nThe authors start by defining the pieces involved in a standard convolution layer.\n\n(Page 2, Section 3.1, Para 2-4): A standard convolutional layer takes as input a D_F × D_F × M feature map F and produces a D_G × D_G × N feature map G… The standard convolutional layer is parameterized by convolution kernel K of size D_K × D_K × M × N…\n\nLet’s quickly translate this math-speak:\n\nInput Feature Map (F): This is the input data for the layer. It has a Height (D_F), a Width (D_F), and a number of Channels (M). For the very first layer of a network, this would be your image, and M would be 3 (for Red, Green, and Blue).\nOutput Feature Map (G): This is what the layer produces. It has a Height and Width and a new number of Channels (N).\nKernel (K): This is the filter that slides over the input. Its size is D_K × D_K. A common size is 3x3.\n\n\n\nThe Cost Formula That Changes Everything\nNow we get to the most important equation in this section, which calculates the total computational cost of a single standard convolution layer.\n\n(Page 2, Equation 2): Computational Cost = D_K · D_K · M · N · D_F · D_F\n\nThis formula looks intimidating, but it’s the key to everything. It tells us that the total number of multiplication operations is the product of:\n\nD_K · D_K: The size of our filter (e.g., 3x3 = 9).\nM: The number of channels in our input.\nN: The number of channels we want in our output.\nD_F · D_F: The size of our input feature map.\n\nThe crucial insight is that all these terms are multiplied together. Let’s look at the part of the formula that creates the bottleneck:\n... M · N ...\nThe cost is directly proportional to the number of input channels (M) multiplied by the number of output channels (N). In a deep neural network, these numbers can be very large (e.g., 256, 512, or even 1024). When you multiply two large numbers together, the result is huge. This M x N term is what causes the computational cost to explode.\nAs the paper states, this is exactly what MobileNet is designed to fix:\n\n(Page 2, Section 3.1, Para 5): “MobileNet models address each of these terms and their interactions. First it uses depthwise separable convolutions to break the interaction between the number of output channels and the size of the kernel.”\n\nThe phrase “break the interaction” is key. The genius of the depthwise separable convolution is that it restructures the operation so that M and N are no longer multiplied together in the most expensive part of the calculation.\nNow that we’ve seen the “before” picture—the math of the expensive standard convolution—we’re perfectly set up to see the “after” picture: the math that makes MobileNets so incredibly efficient.\n\n\nThe Math of Efficiency, Step 1: The Depthwise Filter\nNow that we understand why standard convolutions are so costly, we can finally appreciate the genius of the MobileNet approach. The authors now present the math for their two-step “divide and conquer” strategy, and the savings become immediately obvious.\n\nLet’s look at the first step, the depthwise convolution, which handles the spatial filtering.\n\n(Page 3, Section 3.1, Para 1): “Depthwise separable convolution are made up of two layers: depthwise convolutions and pointwise convolutions. We use depthwise convolutions to apply a single filter per each input channel (input depth)…”\n\nThis confirms what we discussed earlier. The first step’s job is to filter each channel on its own, without mixing them. If you have an input with 64 channels, this step will use 64 separate filters, one for each channel.\n\nThe New, Cheaper Cost Formula\nThe paper then presents the computational cost for just this depthwise filtering step.\n\n(Page 3, Equation 4): Depthwise Cost = D_K · D_K · M · D_F · D_F\n\nLet’s compare this to the original, expensive cost formula from the standard convolution:\n\nOriginal Cost: D_K · D_K · M · N · D_F · D_F\nNew Depthwise Cost: D_K · D_K · M · D_F · D_F\n\nThe difference is immediately clear. The N term is gone!\nWhy? Because in this step, we are no longer trying to create N new output channels. We are simply filtering the existing M input channels, so we only need M filters.\nThe impact of removing N (the number of output channels, which can be a large number like 512) is massive. This single change makes the depthwise filtering step N times cheaper than a full standard convolution.\nThis is a huge first step, but it’s not the whole story. As the paper points out, this layer is efficient but incomplete. It only filters the input channels; it doesn’t combine them to create new, more complex features. That’s the job of the second step: the pointwise convolution. But already, we can see how “breaking the interaction” has led to a dramatic reduction in computational cost.\n\n\n\nThe Math of Efficiency, Step 2: The Pointwise Combination and the Final Payoff\nWe’ve seen that the first step, the depthwise convolution, is incredibly efficient. But as the paper notes, it’s an incomplete solution.\n\n\n(Page 3, Section 3.1, Para 2): “However it only filters input channels, it does not combine them to create new features. So an additional layer that computes a linear combination of the output of depthwise convolution via a 1 × 1 convolution is needed in order to generate these new features.”\n\nThis is where the second part of the block comes in: the pointwise convolution. Its job is to take the independently filtered channels from the first step and intelligently mix them together to create a new set of meaningful features. This layer uses a brilliantly simple 1x1 convolution to accomplish this.\n\nThe Total Cost of Efficiency\nThe paper now presents the total computational cost of the complete two-step depthwise separable convolution. It’s simply the cost of the depthwise step plus the cost of the pointwise step.\n\n(Page 3, Equation 5): Total Cost = (D_K · D_K · M · D_F · D_F) + (M · N · D_F · D_F)\n\n\nPart 1 (Depthwise Cost): The first term is the cost of filtering, which we already saw is very cheap.\nPart 2 (Pointwise Cost): The second term is the cost of combining. It’s the cost of a standard convolution, but with the filter size D_K set to 1, making it highly efficient.\n\n\n\n\n\n\n\nTip\n\n\n\nA Quick Note: Where does the pointwise cost (M·N·D_F·D_F) come from?\nA pointwise convolution is just a special case of a standard convolution where the kernel size is 1x1.\nRemember the original cost formula for a standard convolution?\nStandard Cost = D_K · D_K · M · N · D_F · D_F\nIf we set our kernel size D_K = 1 for the pointwise step, the formula becomes:\nPointwise Cost = 1 · 1 · M · N · D_F · D_F\n…which simplifies to exactly M · N · D_F · D_F. It’s the same math, just applied to a tiny 1x1 filter, which is what makes it so fast.\n\n\nNow for the moment of truth. How does this new, two-part cost compare to the original, expensive cost of a standard convolution? The paper shows the ratio:\n\n(Page 3, The Ratio Equation): (New Cost) / (Old Cost) = 1/N + 1/D_K²\n\nThis simple, elegant formula is the punchline of the entire architectural design. It tells you exactly how much more efficient the MobileNet block is. Let’s plug in some typical numbers:\n\nMobileNets almost always use 3x3 filters, so D_K = 3, which means D_K² = 9.\nThe number of output channels N is usually a large number, like 128, 256, or 512. This makes the 1/N term very, very small (close to zero).\n\nThis means the cost ratio is approximately 1/9.\nThe authors state this incredible result in plain English:\n\n(Page 3, Section 3.1, Final Para): “MobileNet uses 3 × 3 depthwise separable convolutions which uses between 8 to 9 times less computation than standard convolutions at only a small reduction in accuracy…”\n\nThis is the breakthrough. By cleverly splitting one operation into two, MobileNets achieve a nearly 9x reduction in computational cost. And as their experiments will show, they do this while sacrificing almost no accuracy. This is the fundamental trade-off that makes MobileNets so powerful and is the key reason they can run so effectively on devices with limited computational power."
  },
  {
    "objectID": "posts/2025-09-02-mobilenetv1-reading-notes.html#assembling-the-architecture-a-blueprint-for-efficiency",
    "href": "posts/2025-09-02-mobilenetv1-reading-notes.html#assembling-the-architecture-a-blueprint-for-efficiency",
    "title": "MobileNetV1 Explained: A Deep Dive into Lightweight Neural Networks",
    "section": "Assembling the Architecture: A Blueprint for Efficiency",
    "text": "Assembling the Architecture: A Blueprint for Efficiency\nHaving established the power of their core building block—the depthwise separable convolution—the authors now zoom out in Section 3.2 to show us how these blocks are stacked together to create the full, 28-layer MobileNet.\n\nThe design philosophy is one of simplicity and uniformity, using modern best practices to create a clean and effective structure.\n\n(Page 3, Section 3.2, Para 1): “The MobileNet structure is built on depthwise separable convolutions as mentioned in the previous section except for the first layer which is a full convolution… All layers are followed by a batchnorm and ReLU nonlinearity…”\n\nThis paragraph gives us a high-level overview of the network’s construction, which we can break down into a few key principles:\n\nUniformity is Key: The network is simply a deep stack of the depthwise separable blocks we just learned about. This clean, repeating pattern makes the architecture easy to understand, scale, and implement. The only exception is the very first layer, which uses a standard convolution. This is a common trick to quickly process the input image and expand its channel depth, creating a rich set of features for the more efficient blocks to work with.\nModern Ingredients: Every convolutional layer (both depthwise and pointwise) is followed by two standard and essential components of modern deep learning:\n\nBatchNorm (Batch Normalization): Think of this as a regulator for the data flowing through the network. It keeps the numbers in a healthy range, which dramatically stabilizes and speeds up the training process.\nReLU (Rectified Linear Unit): This is the network’s non-linearity. It’s a very simple operation (it just clips all negative values to zero) that allows the network to learn complex, non-linear patterns.\n\nEfficient Downsampling: As an image progresses through a network, its spatial dimensions (height and width) are gradually reduced. Older networks often used separate “pooling” layers for this. MobileNet uses a more modern and efficient technique: strided convolutions. By setting the stride to 2 in some of the depthwise layers, the network downsamples the feature map and learns new features at the same time, killing two birds with one stone.\nA Smart Finish: At the very end of the network, instead of flattening the final high-dimensional feature map (which would create a huge number of parameters), MobileNet uses global average pooling. This simple operation averages each channel down to a single value, creating a compact feature vector that is then fed to the final classifier. This is a highly efficient and effective way to finish the network.\n\nIn summary, the MobileNet architecture is an elegant and straightforward stack of its core efficient blocks, seasoned with all the right modern ingredients (BatchNorm, ReLU, strided convolutions, and global average pooling) to make it a robust and high-performing network.\n\nFrom Theory to Reality: Designing for Real Hardware\nHaving a low theoretical operation count is great, but it’s only half the story. To build a truly fast network, you have to consider the nuts and bolts of how the calculations are actually performed on a physical CPU or GPU. The MobileNet authors display their deep engineering expertise here, explaining how their design is not just mathematically efficient, but also perfectly tailored for modern hardware.\n\n\n\n\n\n\n\n\n\n\nAs the authors wisely state:\n\n(Page 3, Section 3.2, Para 2): “It is not enough to simply define networks in terms of a small number of Mult-Adds. It is also important to make sure these operations can be efficiently implementable.”\n\nThis is a golden rule of performance engineering. Some operations are just easier for computer chips to execute quickly. The genius of MobileNet is that its design concentrates the vast majority of its work into the single most hardware-friendly operation available: the 1x1 convolution.\n\nThe Superpower of the 1x1 Convolution\nWhy is a 1x1 convolution so special? Because it is mathematically equivalent to a GEMM (General Matrix Multiply) operation.\nWhile “GEMM” might sound like a fancy acronym, it’s just a highly optimized, standardized way of performing matrix multiplication. Matrix multiplication is the single most studied and optimized operation in all of scientific computing. Hardware vendors and software engineers have spent decades creating libraries (like Intel’s MKL and NVIDIA’s cuBLAS) that make this operation run at blistering speeds.\n\n(Page 4, Section 3.2, Para 1): “Our model structure puts nearly all of the computation into dense 1 × 1 convolutions. This can be implemented with highly optimized general matrix multiply (GEMM) functions.”\n\nThis is the key. While a standard 3x3 convolution can be turned into a GEMM operation, it requires a slow and memory-hungry preparation step called im2col. A 1x1 convolution, on the other hand, does not need this step. It can be mapped directly to a GEMM call, making it incredibly fast and memory-efficient.\nThe authors then deliver the knockout punch with some stunning statistics from their architecture (which they detail in Table 2).\n\n(Page 4, Section 3.2, Para 1): “MobileNet spends 95% of it’s computation time in 1 × 1 convolutions which also has 75% of the parameters…”\n\nThis is a masterful piece of engineering. They designed an architecture where:\n\nThe vast majority of the work (95% of the computation!) is concentrated in the most efficient, hardware-friendly operation possible (the 1x1 convolution).\nThe less-optimized part (the 3x3 depthwise convolution) is so cheap that it barely registers in the total runtime.\n\nThis is the difference between an architecture that is merely theoretically efficient and one that is practically fast. By understanding the underlying hardware, the MobileNet team designed a network that wasn’t just smart on paper—it was built for speed in the real world.\n\n\n\nThe Art of Training: Less is More for Small Models\nHaving a great architecture is one thing, but you still need to train it effectively. In this section, the authors share some fascinating insights they discovered about the best way to train their new, lightweight MobileNets. The key takeaway? When it comes to training small models, sometimes less is more.\n\n\nA Different Set of Rules for Small Models\nThe team started with a standard, powerful training setup used for giant models like Inception V3. But they quickly realized that this aggressive approach wasn’t right for their smaller MobileNets.\n\n(Page 4, Section 3.2, Para 2): “However, contrary to training large models we use less regularization and data augmentation techniques because small models have less trouble with overfitting.”\n\nThis is the central insight. Overfitting is a major concern for large models. They have so much capacity that they can easily “memorize” the training data instead of learning general patterns. To combat this, researchers use heavy regularization and data augmentation—techniques that make the training task harder to prevent the model from memorizing.\nBut small models are different. A MobileNet simply doesn’t have enough parameters to memorize the entire dataset. It is naturally more resistant to overfitting. Therefore, the heavy-handed techniques used for large models are not only unnecessary, they can actually be harmful. It’s like trying to train for a marathon by running with a 100-pound backpack—it’s overkill and can prevent you from learning to run properly.\nThe authors found that MobileNets train best with a “gentler” touch:\n\nLess Augmentation: They reduced the amount of image distortion used during training, such as limiting the size of small, random crops.\nNo Advanced Regularization: They removed complex regularization tricks like “label smoothing” and “side heads,” which are often needed to tame massive models.\nCareful Weight Decay: They discovered that applying a standard penalty on large weights (called weight decay or L2 regularization) to the tiny depthwise filters was a bad idea. These filters have very few parameters, and penalizing them too much prevented them from learning useful features.\n\nThis is a brilliant lesson in the art of machine learning. The best training recipe is not one-size-fits-all. A small, efficient model like MobileNet benefits from a more direct and less aggressive training strategy, allowing it to learn the essential patterns in the data without being held back by unnecessarily harsh regularization.\n\n\n\nThe Blueprint and the Receipt: A Look at the Numbers\nTheory is great, but seeing the actual architecture laid out provides a new level of clarity. On page 4, the paper presents two crucial tables. Table 1 is the blueprint for the standard MobileNet, showing its layer-by-layer construction. Table 2 is the receipt, detailing the computational cost and parameter distribution, and it perfectly illustrates why the MobileNet design is so brilliant.\n\nTable 1: The MobileNet Blueprint\nThis table is the complete, layer-by-layer specification for the baseline MobileNet. While it looks dense, it reveals a clean, logical, and repeating pattern.\n\nBy reading down the columns, we can trace the journey of an image through the network:\n\nThe Start: The network begins with a standard 3x3 convolution (Conv / s2), which takes the 224x224x3 input image and immediately halves its size to 112x112 while increasing the channels to 32.\nThe Core Block: The rest of the network is a repeating sequence of our new favorite block: a Conv dw (depthwise) followed by a Conv (which is always a 1x1 pointwise convolution). This pattern is the heart of MobileNet.\nShrinking and Growing: As we go deeper, the spatial dimensions get progressively smaller (224 -&gt; 112 -&gt; 56 -&gt; 28 -&gt; 14 -&gt; 7), while the number of channels (the depth) gets progressively larger (32 -&gt; 64 -&gt; 128 -&gt; … -&gt; 1024). This is a classic and effective design for CNNs.\nThe Finish: The network ends with a Avg Pool (Global Average Pooling) and a final FC (Fully Connected) layer for classification.\n\nThis blueprint shows a clean, deep, and highly structured architecture built almost entirely from a single, repeating, efficient block.\n\n\n\n\n\n\nTip\n\n\n\nYou might have noticed the s1 and s2 in the “Type / Stride” column. This is a shorthand that tells us something very important about how the layer operates: its stride.\n\ns1 means a stride of 1. The filter moves one pixel at a time. It slides from one position to the very next, examining every single location meticulously.\n\nEffect on Image Size: The output feature map will have the same height and width as the input (assuming a bit of padding is added around the edges). It preserves the spatial dimensions.\n\ns2 means a stride of 2. The filter takes a bigger step, moving two pixels at a time. It effectively skips every other pixel.\n\nEffect on Image Size: The output feature map will be roughly half the height and half the width of the input. This is a very efficient way to shrink or downsample the data.\n\n\nThe strategic use of s1 and s2 is a key part of the architecture’s intelligence:\n\nLayers with s1 are used when the network wants to learn more complex features at the current scale. For example, in the middle of the network (the 5x block), all the convolutions use s1 because the goal is to get much smarter about the 14x14 feature maps without shrinking them.\nLayers with s2 are used at transition points, when the network is ready to summarize the information it has learned and move to a coarser level of detail. This also has a massive benefit for efficiency: by halving the height and width, you quarter the number of pixels, which drastically reduces the computational cost for all the layers that come after it.\n\nHere’s a quick summary:\n\n\n\n\n\n\n\n\n\nNotation\nMeaning\nEffect on Image Size\nPurpose in MobileNet\n\n\n\n\ns1\nStride 1\nPreserves Size\nLearn richer features at the same scale.\n\n\ns2\nStride 2\nHalves Size\nDownsample and reduce computational cost.\n\n\n\nThis clever mix of s1 and s2 layers allows MobileNet to build up a deep understanding of an image while progressively and efficiently reducing its size.\n\n\n\n\nTable 2: The Receipt - Where the Money is Spent\nThis second table is the punchline for the entire architectural design. It answers the question: “In this new design, where do the computations and parameters actually live?” The results are stunning.\n\nLet’s break down this “receipt”:\n\nConv 1 × 1 (Pointwise): The Workhorse\n\nThis layer accounts for a staggering 94.86% of the computation and 74.59% of the parameters. This is the masterful engineering trick we discussed earlier. MobileNet is designed to spend almost all of its time and resources on the 1x1 convolution, which is the single operation that can be executed most efficiently on modern hardware (via GEMM).\n\nConv DW 3 × 3 (Depthwise): The “Almost Free” Filter\n\nIn contrast, the spatial filtering part of the block is incredibly cheap. It accounts for a tiny 3.06% of the computation and a negligible 1.06% of the parameters. This is the entire justification for separating the convolution: the expensive part is isolated into this almost-free operation.\n\nFully Connected: Heavy but Not Slow\n\nThe final classifier layer holds a significant chunk of the parameters (24.33%), but because it only runs once at the very end on a small vector, it consumes a trivial 0.18% of the computation.\n\n\nThese two tables, the blueprint and the receipt, provide the ultimate proof of concept. The MobileNet architecture isn’t just a clever idea; it’s a meticulously engineered system that successfully shifts the computational burden onto the most efficient operations possible, resulting in a network that is both powerful and incredibly fast."
  },
  {
    "objectID": "posts/2025-09-02-mobilenetv1-reading-notes.html#the-control-knobs-part-1-the-width-multiplier-for-thinner-models",
    "href": "posts/2025-09-02-mobilenetv1-reading-notes.html#the-control-knobs-part-1-the-width-multiplier-for-thinner-models",
    "title": "MobileNetV1 Explained: A Deep Dive into Lightweight Neural Networks",
    "section": "The Control Knobs, Part 1: The Width Multiplier for Thinner Models",
    "text": "The Control Knobs, Part 1: The Width Multiplier for Thinner Models\nWe’ve seen the brilliant architecture and the smart training strategy. But what truly makes MobileNets a game-changer for developers is its customizability. The paper now introduces the first of two simple yet powerful “control knobs” that allow you to create the perfect-sized model for any application.\nThis first knob is called the width multiplier, and its job is to make the network “thinner.”\n\n\n\n\n\n\n\n\n\n\n\nOne Knob to Rule Them All\nThe baseline MobileNet is already small and fast, but what if you need something even smaller and even faster for a particularly demanding task, like real-time augmented reality on a low-end phone?\n\n(Page 4, Section 3.3, Para 1): “In order to construct these smaller and less computationally expensive models we introduce a very simple parameter α called width multiplier. The role of the width multiplier α is to thin a network uniformly at each layer.”\n\nThe idea is incredibly simple and elegant. The width multiplier, represented by the Greek letter alpha (α), is a single number (between 0 and 1) that uniformly reduces the number of channels in every single layer of the network.\nFor example, if you choose α = 0.5:\n\nA layer that originally had 64 channels will now have 0.5 * 64 = 32 channels.\nA layer that had 128 channels will now have 0.5 * 128 = 64 channels.\n\nYou simply decide on a value for α, and the entire network is scaled down proportionally. This is a much more principled way to shrink a network than just randomly removing layers.\n\n\nThe Power of Quadratic Scaling\nHere’s where things get really interesting. When you make the network half as “wide,” you might expect it to become twice as efficient. But the effect is much more dramatic. The paper reveals a crucial mathematical insight:\n\n(Page 4, Section 3.3, Para 2): “Width multiplier has the effect of reducing computational cost and the number of parameters quadratically by roughly α².”\n\nThis is the punchline. The cost doesn’t scale linearly; it scales quadratically (by α²).\nLet’s see what that means in practice:\n\nIf you set α = 0.75, the cost is reduced by 0.75² ≈ 0.56. The model becomes almost twice as fast.\nIf you set α = 0.5, the cost is reduced by 0.5² = 0.25. The model becomes four times as fast and has four times fewer parameters.\nIf you set α = 0.25, the cost is reduced by 0.25² ≈ 0.06. The model becomes over 16 times as fast.\n\nThis quadratic scaling gives developers an incredibly powerful tool. With one simple number, you can generate a whole spectrum of models, from the full-sized, most accurate version (α = 1.0) down to tiny, lightning-fast versions, all while knowing that you’re shrinking the network in a smart, uniform way.\nIt’s important to note, as the paper points out, that you use the width multiplier to define a new, smaller architecture that must then be trained from scratch. But now, let’s look at the second control knob, which works in a completely different but equally powerful way."
  },
  {
    "objectID": "posts/2025-09-02-mobilenetv1-reading-notes.html#the-control-knobs-part-2-the-resolution-multiplier-for-faster-processing",
    "href": "posts/2025-09-02-mobilenetv1-reading-notes.html#the-control-knobs-part-2-the-resolution-multiplier-for-faster-processing",
    "title": "MobileNetV1 Explained: A Deep Dive into Lightweight Neural Networks",
    "section": "The Control Knobs, Part 2: The Resolution Multiplier for Faster Processing",
    "text": "The Control Knobs, Part 2: The Resolution Multiplier for Faster Processing\nThe width multiplier (α) gives us a powerful way to make a network thinner. But the MobileNet authors provide a second, complementary control knob that is even more intuitive: the resolution multiplier.\n\n\n\n\n\n\n\n\n\n\nThe idea is incredibly simple: if you want the network to run faster, just feed it a smaller image!\n\n(Page 4, Section 3.4, Para 1): “The second hyper-parameter to reduce the computational cost of a neural network is a resolution multiplier ρ. We apply this to the input image and the internal representation of every layer is subsequently reduced by the same multiplier.”\n\nThe resolution multiplier, represented by the Greek letter rho (ρ), isn’t a number you set directly. Instead, you “implicitly” set it by choosing the input resolution for your images.\n\nThe baseline resolution is 224x224 (ρ = 1.0).\nIf you choose to use 192x192 images, you’ve implicitly set ρ ≈ 0.857.\nIf you choose to use 160x160 images, you’ve implicitly set ρ ≈ 0.714.\n\nThis size reduction at the input then propagates through the entire network, making every single feature map smaller and faster to process.\n\nAnother Quadratic Win\nJust like the width multiplier, the resolution multiplier has a powerful quadratic effect on the computational cost.\n\n(Page 5, Section 3.4, Para 2): “Resolution multiplier has the effect of reducing computational cost by ρ².”\n\nThis is because the computation is proportional to the number of pixels in the feature maps, which is height × width. When you reduce both the height and width by a factor of ρ, the total number of pixels is reduced by ρ².\n\nExample: If you switch from 224x224 images to 160x160 (ρ ≈ 0.714), you reduce the computational cost by a factor of ρ² ≈ 0.51. You’ve cut the work the network has to do in half just by giving it a smaller picture to look at.\n\n\n\nThe Grand Finale: Putting It All Together\nThe paper provides a brilliant summary in Table 3, showing the compounding effect of all these efficiency innovations on a single, typical layer from the middle of the network. This table is the ultimate “before and after” picture.\n\nLet’s walk through it step-by-step, seeing how each modification slashes the cost:\n\nBaseline (Standard Convolution): We start with an old-school, standard 3x3 convolution. For this one layer, it costs a massive 462 Million Mult-Adds and requires 2.36 Million parameters.\nAdd Depthwise Separable Conv: We swap the standard convolution for the efficient MobileNet block. The cost plummets to 52.3M Mult-Adds and 0.27M parameters. That’s a nearly 9x reduction in computation, right off the bat!\nAdd Width Multiplier (α = 0.75): Now, we make the layer “thinner.” The cost drops again to 29.6M Mult-Adds, a further reduction of almost half, just as the α² rule predicted.\nAdd Resolution Multiplier (ρ = 0.714): Finally, we process a smaller feature map through this thinner layer. The cost is halved one more time, down to a mere 15.1M Mult-Adds.\n\nThe final result is staggering. A layer that would have cost 462 Million operations in a traditional CNN now costs just 15 Million in a scaled-down MobileNet. That’s a 30-fold reduction in computation.\nThis powerful combination—an efficient core block and two simple, intuitive control knobs—is what gives developers the unprecedented ability to design a network that perfectly fits the performance constraints of any device. With the theory now fully explained, the paper turns to the experiments to prove that these models aren’t just efficient, but also highly accurate."
  },
  {
    "objectID": "posts/2025-09-02-mobilenetv1-reading-notes.html#the-experiments-part-1-putting-the-theory-to-the-test",
    "href": "posts/2025-09-02-mobilenetv1-reading-notes.html#the-experiments-part-1-putting-the-theory-to-the-test",
    "title": "MobileNetV1 Explained: A Deep Dive into Lightweight Neural Networks",
    "section": "The Experiments, Part 1: Putting the Theory to the Test",
    "text": "The Experiments, Part 1: Putting the Theory to the Test\nThe theory behind MobileNets is elegant, the math is compelling, but the ultimate question is always: does it actually work? And how well does it work compared to other approaches? In Section 4, the paper shifts from design to rigorous experimentation, providing the hard data to justify its core ideas.\nThe first set of experiments in Section 4.1 is designed to answer two fundamental questions:\n\nIs the depthwise separable convolution really a good trade-off?\nIs making a network “thinner” truly better than making it “shallower”?\n\n\nJustifying the Secret Sauce: Is the Trade-Off Worth It?\nFirst, the authors need to prove that their core building block is a smart choice. Is the massive gain in efficiency worth the potential drop in accuracy? To find out, they compare two models: a “Conv MobileNet” built with expensive standard convolutions, and the real MobileNet built with their efficient blocks. The results in Table 4 are a knockout.\n\n\n\n\n\n\n\n\n\n\n\n(Page 5, Section 4.1, Para 1-2): “First we show results for MobileNet with depthwise separable convolutions compared to a model built with full convolutions. In Table 4 we see that using depthwise separable convolutions compared to full convolutions only reduces accuracy by 1% on ImageNet was saving tremendously on mult-adds and parameters.”\n\nLet’s break down that trade-off:\n\nThe Cost: They sacrificed a mere 1.1% in ImageNet accuracy (71.7% -&gt; 70.6%).\nThe Reward: In return, they got a network that was ~8.5 times faster (fewer Mult-Adds) and ~7 times smaller (fewer Parameters).\n\nThis is a phenomenal result. It’s like being offered a car that’s 99% as fast as a supercar but gets 8 times the gas mileage and costs 7 times less. It’s an overwhelmingly positive trade-off and the ultimate validation of the paper’s core premise.\n\nA Smarter Way to Shrink: Thinner vs. Shallower\nNext, the authors justify their choice of the “width multiplier.” If you have a fixed computational budget, what’s a better way to make a model smaller?\n\nMake it thinner by reducing the number of channels in every layer?\nOr make it shallower by removing entire layers from the network?\n\nTo answer this, they create two models with a nearly identical computational budget: a “thinner” MobileNet (using α = 0.75) and a “shallower” MobileNet (with 5 layers removed from the middle).\n\n\n(Page 5, Section 4.1, Para 3): “Table 5 shows that at similar computation and number of parameters, that making MobileNets thinner is 3% better than making them shallower.”\n\nThe results from Table 5 were decisive. For the same computational cost, the thinner model was 3.1% more accurate than the shallower one.\n\nThis is a crucial design lesson. It suggests that maintaining the network’s depth is vital for its performance. It’s better to have many “thin” layers than a few “fat” ones. This experiment provides the perfect justification for using the width multiplier as the primary, principled way to scale down the MobileNet architecture.\nThese two initial experiments are foundational. They prove that MobileNet’s core building block is a massive win and that the proposed method for scaling it (making it thinner) is the right approach."
  },
  {
    "objectID": "posts/2025-09-02-mobilenetv1-reading-notes.html#the-experiments-part-2-a-universe-of-efficient-models",
    "href": "posts/2025-09-02-mobilenetv1-reading-notes.html#the-experiments-part-2-a-universe-of-efficient-models",
    "title": "MobileNetV1 Explained: A Deep Dive into Lightweight Neural Networks",
    "section": "The Experiments, Part 2: A Universe of Efficient Models",
    "text": "The Experiments, Part 2: A Universe of Efficient Models\nHaving justified their core design choices, the authors now unleash the full power of their two “control knobs”—the width and resolution multipliers. This section demonstrates how these simple hyper-parameters create a rich ecosystem of 16 different models, allowing developers to find the perfect balance of speed, size, and accuracy for any conceivable task.\n\nMapping the Trade-Offs: The Power of Predictable Scaling\nFirst, the paper presents the results of applying each knob independently.\n\n\n\n\n\n\n\n\n\n\n\nTable 6 (The Width Multiplier α): This table shows what happens as you make the network progressively “thinner” by decreasing α from 1.0 down to 0.25. The key finding is that the accuracy “drops off smoothly.” This is fantastic news. It means the trade-off is graceful and predictable. There are no sudden, catastrophic drops in performance, allowing a developer to confidently tune the knob to meet their latency budget.\nTable 7 (The Resolution Multiplier ρ): This table shows the results of shrinking the input image size from 224x224 down to 128x128. The story is the same: accuracy “drops off smoothly across resolution.” Again, this provides a predictable and reliable way to gain speed.\n\nBut the real magic happens when you combine them. Figure 4 is the ultimate summary of the MobileNet family’s performance.\n\nThis single graph plots all 16 models, showing the relationship between their computational cost (Mult-Adds) and their accuracy. The key takeaway is the shape of the curve: on this plot with a logarithmic x-axis, the points form an almost perfectly straight line.\nThis “log-linear” relationship is a developer’s dream. It means that for every multiplicative decrease in speed (e.g., making the model 10x faster), you get a predictable, additive decrease in accuracy (e.g., losing 15% accuracy). This turns the art of choosing a model into a science. You can look at this chart and say, “My phone has a budget of 100 Million Mult-Adds; I can expect to get about 63% accuracy.”\nFigure 5 tells the other half of the story, plotting accuracy against model size (number of parameters).\n\nThis chart shows that model size is determined only by the width multiplier α, creating distinct vertical clusters. Within each size budget, you can then use the resolution multiplier to further trade latency for accuracy. Together, these two figures provide a complete map for navigating the MobileNet universe.\n\nThe Main Event: MobileNet vs. The World\nSo, how do these new, efficient models stack up against the famous architectures of the day? The paper provides a head-to-head comparison in Table 8 and Table 9.\nTable 8 compares the full-size baseline MobileNet to the heavyweights:\n\n\nvs. VGG16: MobileNet achieves nearly the same accuracy (70.6% vs. 71.5%) while being 32 times smaller and 27 times faster. This is a revolutionary leap in efficiency.\nvs. GoogLeNet: MobileNet is more accurate, smaller, and over 2.5 times faster. It’s a clean sweep.\n\nBut what about the smaller MobileNet variants? Table 9 compares a shrunk-down MobileNet (α=0.5, 160x160 resolution) to other models famous for their small size:\n\n\nvs. AlexNet: The small MobileNet is 3% more accurate while being 45 times smaller and 9.4 times faster.\nvs. SqueezeNet: SqueezeNet was a celebrated small model. Yet, the small MobileNet is nearly 3% more accurate at the same size, while being a staggering 22 times faster in terms of computation.\n\nThe conclusion from these tables is undeniable. The MobileNet family doesn’t just create a single good model; it establishes a new state-of-the-art across the entire spectrum of performance. Whether you need a full-size competitor to VGG or a tiny model to rival SqueezeNet, the MobileNet architecture provides a superior solution. It proved that efficiency and high performance were not mutually exclusive."
  },
  {
    "objectID": "posts/2025-09-02-mobilenetv1-reading-notes.html#beyond-classification-a-versatile-tool-for-the-real-world",
    "href": "posts/2025-09-02-mobilenetv1-reading-notes.html#beyond-classification-a-versatile-tool-for-the-real-world",
    "title": "MobileNetV1 Explained: A Deep Dive into Lightweight Neural Networks",
    "section": "Beyond Classification: A Versatile Tool for the Real World",
    "text": "Beyond Classification: A Versatile Tool for the Real World\nHaving firmly established MobileNet’s dominance on the standard ImageNet benchmark, the paper spends the remainder of the experiments section (Sections 4.3 through 4.7) demonstrating its incredible versatility. The goal is to prove that MobileNet isn’t just a one-trick pony for classification; it’s a powerful and efficient “backbone” that can be adapted to a wide variety of real-world computer vision tasks.\nThe authors put MobileNet to the test on a diverse set of challenges, and it excels in every single one. Here’s a quick summary of the highlights:\n\nFine-Grained Recognition (Stanford Dogs dataset): MobileNet proves it can handle the subtle and difficult task of distinguishing between 120 different breeds of dogs, achieving near state-of-the-art results with a fraction of the computational cost.\nLarge-Scale Geolocalization (PlaNet): When used as a drop-in replacement for the massive Inception V3 model in the PlaNet system (which determines where a photo was taken), MobileNet delivers nearly the same performance while being 4 times smaller and 10 times faster.\nFace Attributes: Using a powerful technique called “knowledge distillation,” the authors show they can train a tiny MobileNet to mimic a huge, complex in-house face attribute model. The result is a model that performs just as well as the original but is up to 100 times more efficient.\nObject Detection (COCO dataset): When integrated into modern object detection systems like SSD and Faster R-CNN, MobileNet achieves comparable results to much larger backbones like VGG and Inception V2, but with a dramatic reduction in complexity and speed.\nFace Embeddings (FaceNet): Finally, they show that a MobileNet-based model can be trained to generate high-quality facial recognition embeddings, rivaling the famous FaceNet model but in a package small enough for mobile deployment.\n\nThe message from these experiments is loud and clear: MobileNet is a universally effective feature extractor. Its efficiency does not come at the cost of its ability to learn powerful and generalizable representations of the visual world."
  },
  {
    "objectID": "posts/2025-09-02-mobilenetv1-reading-notes.html#conclusion-the-legacy-of-mobilenet",
    "href": "posts/2025-09-02-mobilenetv1-reading-notes.html#conclusion-the-legacy-of-mobilenet",
    "title": "MobileNetV1 Explained: A Deep Dive into Lightweight Neural Networks",
    "section": "Conclusion: The Legacy of MobileNet",
    "text": "Conclusion: The Legacy of MobileNet\nThe MobileNet paper is more than just a description of a clever architecture. It’s a masterclass in principled, practical, and impactful research. By starting from a simple yet powerful insight—that the two core jobs of a convolution can be separated—the authors created not just a single model, but an entire philosophy for designing efficient neural networks.\nLet’s recap the journey:\n\nThe Core Idea: They replaced the expensive, monolithic standard convolution with a lightweight, two-part alternative called the depthwise separable convolution, achieving a nearly 9x gain in efficiency with minimal loss in accuracy.\nSmart Design: They built a clean, deep architecture that was not only theoretically efficient but also meticulously engineered to run at maximum speed on real hardware by concentrating its workload on highly optimized 1x1 convolutions.\nUnprecedented Flexibility: They introduced two simple but powerful “control knobs”—the width and resolution multipliers—that allow any developer to easily generate a whole family of models, finding the perfect trade-off between speed, size, and accuracy for their specific needs.\nProven Performance: Through extensive experiments, they proved that MobileNets outperform their larger, more cumbersome predecessors and are a versatile tool that excels at a wide range of computer vision tasks, from object detection to facial recognition."
  }
]