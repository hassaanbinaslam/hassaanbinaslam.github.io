<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.2.269">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="dcterms.date" content="2022-11-09">
<meta name="description" content="This is a practice notebook to work with a dataset of 50,000 movie reviews from the Internet Movie Database (IMDB) and build an LSTM predictor to distinguish between positive and negative reviews.">

<title>Random Thoughts - Predicting the Sentiment of IMDB Movie Reviews using LSTM in PyTorch</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../images/favicon.ico" rel="icon">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../site_libs/bootstrap/bootstrap-dark.min.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>


<link rel="stylesheet" href="../styles.css">
<meta property="og:title" content="Random Thoughts - Predicting the Sentiment of IMDB Movie Reviews using LSTM in PyTorch">
<meta property="og:description" content="This is a practice notebook to work with a dataset of 50,000 movie reviews from the Internet Movie Database (IMDB) and build an LSTM predictor to distinguish between positive and negative reviews.">
<meta property="og:image" content="images/2022-11-09-pytorch-lstm-imdb-sentiment-prediction.jpeg">
<meta property="og:site-name" content="Random Thoughts">
<meta name="twitter:title" content="Random Thoughts - Predicting the Sentiment of IMDB Movie Reviews using LSTM in PyTorch">
<meta name="twitter:description" content="This is a practice notebook to work with a dataset of 50,000 movie reviews from the Internet Movie Database (IMDB) and build an LSTM predictor to distinguish between positive and negative reviews.">
<meta name="twitter:image" content="images/2022-11-09-pytorch-lstm-imdb-sentiment-prediction.jpeg">
<meta name="twitter:card" content="summary_large_image">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../index.html">
    <span class="navbar-title">Random Thoughts</span>
    </a>
  </div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../about.html">
 <span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://www.linkedin.com/in/hassaanbinaslam/"><i class="bi bi-linkedin" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
</ul>
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/hassaanbinaslam/"><i class="bi bi-github" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com/hassaanbinaslam"><i class="bi bi-twitter" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
</ul>
              <div class="quarto-toggle-container">
                  <a href="" class="quarto-color-scheme-toggle nav-link" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
              </div>
              <div id="quarto-search" class="" title="Search"></div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#credits" id="toc-credits" class="nav-link active" data-scroll-target="#credits">Credits</a></li>
  <li><a href="#environment" id="toc-environment" class="nav-link" data-scroll-target="#environment">Environment</a></li>
  <li><a href="#data-preparation" id="toc-data-preparation" class="nav-link" data-scroll-target="#data-preparation">Data Preparation</a>
  <ul class="collapse">
  <li><a href="#download-data" id="toc-download-data" class="nav-link" data-scroll-target="#download-data">Download data</a></li>
  <li><a href="#split-train-data-further-into-train-and-validation-set" id="toc-split-train-data-further-into-train-and-validation-set" class="nav-link" data-scroll-target="#split-train-data-further-into-train-and-validation-set">Split train data further into train and validation set</a></li>
  <li><a href="#how-does-this-data-look" id="toc-how-does-this-data-look" class="nav-link" data-scroll-target="#how-does-this-data-look">How does this data look?</a></li>
  <li><a href="#data-preprocessing-steps" id="toc-data-preprocessing-steps" class="nav-link" data-scroll-target="#data-preprocessing-steps">Data preprocessing steps</a></li>
  <li><a href="#preparing-data-dictionary" id="toc-preparing-data-dictionary" class="nav-link" data-scroll-target="#preparing-data-dictionary">Preparing data dictionary</a></li>
  </ul></li>
  <li><a href="#define-data-processing-pipelines" id="toc-define-data-processing-pipelines" class="nav-link" data-scroll-target="#define-data-processing-pipelines">Define data processing pipelines</a>
  <ul class="collapse">
  <li><a href="#sequence-padding" id="toc-sequence-padding" class="nav-link" data-scroll-target="#sequence-padding">Sequence padding</a></li>
  <li><a href="#sequence-packing" id="toc-sequence-packing" class="nav-link" data-scroll-target="#sequence-packing">Sequence packing</a></li>
  <li><a href="#run-data-preprocessing-pipelines-on-an-example-batch" id="toc-run-data-preprocessing-pipelines-on-an-example-batch" class="nav-link" data-scroll-target="#run-data-preprocessing-pipelines-on-an-example-batch">Run data preprocessing pipelines on an example batch</a></li>
  </ul></li>
  <li><a href="#batching-the-training-validation-and-test-dataset" id="toc-batching-the-training-validation-and-test-dataset" class="nav-link" data-scroll-target="#batching-the-training-validation-and-test-dataset">Batching the training, validation, and test dataset</a></li>
  <li><a href="#define-model-training-and-evaluation-pipelines" id="toc-define-model-training-and-evaluation-pipelines" class="nav-link" data-scroll-target="#define-model-training-and-evaluation-pipelines">Define model training and evaluation pipelines</a></li>
  <li><a href="#rnn-model-configuration-loss-function-and-optimizer" id="toc-rnn-model-configuration-loss-function-and-optimizer" class="nav-link" data-scroll-target="#rnn-model-configuration-loss-function-and-optimizer">RNN model configuration, loss function, and optimizer</a>
  <ul class="collapse">
  <li><a href="#define-model-loss-function-and-optimizer" id="toc-define-model-loss-function-and-optimizer" class="nav-link" data-scroll-target="#define-model-loss-function-and-optimizer">Define model loss function and optimizer</a></li>
  </ul></li>
  <li><a href="#model-training-and-evaluation" id="toc-model-training-and-evaluation" class="nav-link" data-scroll-target="#model-training-and-evaluation">Model training and evaluation</a>
  <ul class="collapse">
  <li><a href="#evaluate-sentiments-on-random-texts" id="toc-evaluate-sentiments-on-random-texts" class="nav-link" data-scroll-target="#evaluate-sentiments-on-random-texts">Evaluate sentiments on random texts</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Predicting the Sentiment of IMDB Movie Reviews using LSTM in PyTorch</h1>
  <div class="quarto-categories">
    <div class="quarto-category">pytorch</div>
    <div class="quarto-category">lstm</div>
  </div>
  </div>

<div>
  <div class="description">
    This is a practice notebook to work with a dataset of 50,000 movie reviews from the Internet Movie Database (IMDB) and build an LSTM predictor to distinguish between positive and negative reviews.
  </div>
</div>


<div class="quarto-title-meta">

    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">November 9, 2022</p>
    </div>
  </div>
  
    
  </div>
  

</header>

<p><img src="images/2022-11-09-pytorch-lstm-imdb-sentiment-prediction.jpeg" class="img-fluid"></p>
<section id="credits" class="level2">
<h2 class="anchored" data-anchor-id="credits">Credits</h2>
<p>This notebook takes inspiration and ideas from the following sources.</p>
<ul>
<li>“Machine learning with PyTorch and Scikit-Learn” by “Sebastian Raschka, Yuxi (Hayden) Liu, and Vahid Mirjalili”. You can get the book from its website: <a href="https://sebastianraschka.com/books/#machine-learning-with-pytorch-and-scikit-learn">Machine learning with PyTorch and Scikit-Learn</a>. In addition, the GitHub repository for this book has valuable notebooks: <a href="https://github.com/rasbt/machine-learning-book">github.com/rasbt/machine-learning-book</a>. Parts of the code you see in this notebook are taken from <a href="https://github.com/rasbt/machine-learning-book/blob/main/ch15/ch15_part2.ipynb">chapter 15</a> notebook of the same book.</li>
<li>“Intro to Deep Learning and Generative Models Course” lecture series from “Sebastian Raschka”. Course website: <a href="https://sebastianraschka.com/teaching/stat453-ss2021/">stat453-ss2021</a>. YouTube Link: <a href="https://www.youtube.com/playlist?list=PLTKMiZHVd_2KJtIXOW0zFhFfBaJJilH51">Intro to Deep Learning and Generative Models Course</a>. Lectures that are related to this post are <a href="https://youtu.be/k6fSgUaWUF8">L15.5 Long Short-Term Memory</a> and <a href="https://youtu.be/KgrdifrlDxg">L15.7 An RNN Sentiment Classifier in PyTorch</a></li>
</ul>
</section>
<section id="environment" class="level2">
<h2 class="anchored" data-anchor-id="environment">Environment</h2>
<p>This notebook is prepared with Google Colab. For “runtime type” choose hardware accelerator as “GPU”. It will take a long time to complete the training without any GPU.</p>
<p>This notebook also depends on the PyTorch library <a href="https://pytorch.org/text/stable/index.html">TorchText</a>. We will use this library to fetch IMDB review data. While using the <code>torchtext</code> latest version, I found more dependencies on other libraries like <code>torchdata</code>. Even after resolving them, it threw strange encoding errors while fetching IMDB data. So I have downgraded this library till the version I found working without external dependencies. Consequently, <code>torch</code> is also downgraded to a compatible version, but I did not find any issue while working with a lower version of PyTorch for this notebook. It is preferred to restart the runtime after the library installation is complete.</p>
<div class="cell" data-outputid="6a37f823-61e1-44cb-a21f-9f0aef76f1b4" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co">#collapse-output</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="op">!</span> pip install torchtext<span class="op">==</span><span class="fl">0.11.0</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/
Requirement already satisfied: torchtext==0.11.0 in /usr/local/lib/python3.7/dist-packages (0.11.0)
Requirement already satisfied: torch==1.10.0 in /usr/local/lib/python3.7/dist-packages (from torchtext==0.11.0) (1.10.0)
Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchtext==0.11.0) (1.21.6)
Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchtext==0.11.0) (2.23.0)
Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torchtext==0.11.0) (4.64.1)
Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.10.0-&gt;torchtext==0.11.0) (4.1.1)
Requirement already satisfied: certifi&gt;=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests-&gt;torchtext==0.11.0) (2022.9.24)
Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,&lt;1.26,&gt;=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests-&gt;torchtext==0.11.0) (1.24.3)
Requirement already satisfied: idna&lt;3,&gt;=2.5 in /usr/local/lib/python3.7/dist-packages (from requests-&gt;torchtext==0.11.0) (2.10)
Requirement already satisfied: chardet&lt;4,&gt;=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests-&gt;torchtext==0.11.0) (3.0.4)</code></pre>
</div>
</div>
<div class="cell" data-outputid="85394c24-663a-427b-e8c9-d2084d2f9735" data-execution_count="2">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> platform <span class="im">import</span> python_version</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy, matplotlib, pandas, torch, torchtext</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"python=="</span> <span class="op">+</span> python_version())</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"numpy=="</span> <span class="op">+</span> numpy.__version__)</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"torch=="</span> <span class="op">+</span> torch.__version__)</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"torchtext=="</span> <span class="op">+</span> torchtext.__version__)</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"matplotlib=="</span> <span class="op">+</span> matplotlib.__version__)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>python==3.7.15
numpy==1.21.6
torch==1.10.0+cu102
torchtext==0.11.0
matplotlib==3.2.2</code></pre>
</div>
</div>
</section>
<section id="data-preparation" class="level2">
<h2 class="anchored" data-anchor-id="data-preparation">Data Preparation</h2>
<section id="download-data" class="level3">
<h3 class="anchored" data-anchor-id="download-data">Download data</h3>
<p>Let’s download our movie review dataset. This dataset is also known as <a href="https://ai.stanford.edu/~amaas/data/sentiment/">Large Movie Review Dataset</a>, and can also be obtained in a compressed zip file from <a href="https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz">this link</a>. Using the <code>torchtext</code> library makes downloading, extracting, and reading files a lot easier. ‘torchtext.datasets’ comes with many more NLP related datasets, and a full list can be <a href="https://pytorch.org/text/stable/datasets.html">found here</a>.</p>
<div class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torchtext.datasets <span class="im">import</span> IMDB</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.utils.data.dataset <span class="im">import</span> random_split</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>torch.manual_seed(<span class="dv">1</span>)</span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>train_dataset_raw <span class="op">=</span> IMDB(split<span class="op">=</span><span class="st">"train"</span>)</span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>test_dataset_raw <span class="op">=</span> IMDB(split<span class="op">=</span><span class="st">"test"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Check the size of the downloaded data.</p>
<div class="cell" data-outputid="15d68147-95cf-458c-a25c-b9aeec98e462" data-execution_count="4">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Train dataset size: "</span>, <span class="bu">len</span>(train_dataset_raw))</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Test dataset size: "</span>, <span class="bu">len</span>(test_dataset_raw))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Train dataset size:  25000
Test dataset size:  25000</code></pre>
</div>
</div>
</section>
<section id="split-train-data-further-into-train-and-validation-set" class="level3">
<h3 class="anchored" data-anchor-id="split-train-data-further-into-train-and-validation-set">Split train data further into train and validation set</h3>
<p>Both train and test datasets have 25000 reviews. Therefore, we can split the training set further into the train and validation sets.</p>
<div class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a>train_set_size <span class="op">=</span> <span class="dv">20000</span></span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>valid_set_size <span class="op">=</span> <span class="dv">5000</span></span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>train_dataset, valid_dataset <span class="op">=</span> random_split(<span class="bu">list</span>(train_dataset_raw), [<span class="dv">20000</span>, <span class="dv">5000</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="how-does-this-data-look" class="level3">
<h3 class="anchored" data-anchor-id="how-does-this-data-look">How does this data look?</h3>
<p>The data we have is in the form of tuples. The first index has the sentiment label, and the second contains the review text. Let’s check the first element in our training dataset.</p>
<div class="cell" data-outputid="adb1305a-df0c-46a8-9af4-972b3c5277cc" data-execution_count="6">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a>train_dataset[<span class="dv">0</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="6">
<pre><code>('pos',
 'An extra is called upon to play a general in a movie about the Russian Revolution. However, he is not any ordinary extra. He is Serguis Alexander, former commanding general of the Russia armies who is now being forced to relive the same scene, which he suffered professional and personal tragedy in, to satisfy the director who was once a revolutionist in Russia and was humiliated by Alexander. It can now be the time for this broken man to finally "win" his penultimate battle. This is one powerful movie with meticulous direction by Von Sternberg, providing the greatest irony in Alexander\'s character in every way he can. Jannings deserved his Oscar for the role with a very moving performance playing the general at his peak and at his deepest valley. Powell lends a sinister support as the revenge minded director and Brent is perfect in her role with her face and movements showing so much expression as Jannings\' love. All around brilliance. Rating, 10.')</code></pre>
</div>
</div>
<p>Check the first index of the validation set.</p>
<div class="cell" data-outputid="96ea82d5-8662-4285-d7e3-9f516b181f27" data-execution_count="7">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>valid_dataset[<span class="dv">0</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="7">
<pre><code>('neg',
 'The Dereks did seem to struggle to find rolls for Bo after "10".&lt;br /&gt;&lt;br /&gt;I used to work for a marine park in the Florida Keys. One day, the script for "Ghosts Can\'t Do It" was circulating among the trainers in the "fish house" where food was prepared for the dolphins. There was one scene where a -dolphin- supposedly propositions Bo (or Bo the dolphin), asking to "go make eggs." Reading the script, we -lauuughed-...&lt;br /&gt;&lt;br /&gt;We did not end up doing any portion of this movie at our facility, although our dolphins -were- in "The Big Blue!"&lt;br /&gt;&lt;br /&gt;This must have been very close to the end of Anthony Quinn\'s life. I hope he had fun in this film, as it certainly didn\'t do anything for his legacy.')</code></pre>
</div>
</div>
</section>
<section id="data-preprocessing-steps" class="level3">
<h3 class="anchored" data-anchor-id="data-preprocessing-steps">Data preprocessing steps</h3>
<p>From these two reviews, we can deduce that * We have two labels. ‘pos’ for a positive and ‘neg’ for a negative review * From the second review (from valid_dataset), we also get that text may contain HTML tags, special characters, and emoticons besides normal English words. It will require some preprocessing to remove them for proper word tokenization. * Reviews can have varying text lengths. It will require some padding to make all review texts the same size.</p>
<p>Let’s take a simple text example and apply these steps to understand why these steps are essential in preprocessing. In the last step, we will create tokens from the preprocessed text.</p>
<div class="cell" data-outputid="740babb1-00b7-4066-caa2-5cc5fe47053e" data-execution_count="8">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a>example_text <span class="op">=</span> <span class="st">'''This is awesome movie &lt;br /&gt;&lt;br /&gt;. I loved it so much :-) I</span><span class="ch">\'</span><span class="st">m goona watch it again :)'''</span></span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>example_text</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="8">
<pre><code>"This is awesome movie &lt;br /&gt;&lt;br /&gt;. I loved it so much :-) I'm goona watch it again :)"</code></pre>
</div>
</div>
<div class="cell" data-outputid="4e5cedfc-ba91-4b55-8dcb-235861b72be5" data-execution_count="9">
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="co">##</span></span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a><span class="co"># step 1. remove HTML tags. they are not helpful in understanding the sentiments of a review</span></span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> re</span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a>text <span class="op">=</span> re.sub(<span class="st">'&lt;[^&gt;]*&gt;'</span>, <span class="st">''</span>, example_text)</span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a>text</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="9">
<pre><code>"This is awesome movie . I loved it so much :-) I'm goona watch it again :)"</code></pre>
</div>
</div>
<div class="cell" data-outputid="27e40280-07b0-431f-e150-1b9c2ea0cf82" data-execution_count="10">
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="co">##</span></span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a><span class="co"># step 2: use lowercase for all text to keep symmetry</span></span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a>text <span class="op">=</span> text.lower()</span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a>text</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="10">
<pre><code>"this is awesome movie . i loved it so much :-) i'm goona watch it again :)"</code></pre>
</div>
</div>
<div class="cell" data-outputid="33a74799-341c-4d4b-a28a-4dc230e02e4f" data-execution_count="11">
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="co">##</span></span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a><span class="co"># step 3: extract emoticons. keep them as they are important sentiment signals</span></span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a>emoticons <span class="op">=</span> re.findall(<span class="st">'(?::|;|=)(?:-)?(?:\)|\(|D|P)'</span>, text)</span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a>emoticons</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="11">
<pre><code>[':-)', ':)']</code></pre>
</div>
</div>
<div class="cell" data-outputid="31ee748e-ac8c-4681-ae89-28360309f786" data-execution_count="12">
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a><span class="co">##</span></span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a><span class="co"># step 4: remove punctuation marks</span></span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a>text <span class="op">=</span> re.sub(<span class="st">'[\W]+'</span>, <span class="st">' '</span>, text)</span>
<span id="cb21-4"><a href="#cb21-4" aria-hidden="true" tabindex="-1"></a>text</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="12">
<pre><code>'this is awesome movie i loved it so much i m goona watch it again '</code></pre>
</div>
</div>
<div class="cell" data-outputid="d67ff36b-d664-4e14-dea4-9a18b637e6d7" data-execution_count="13">
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a><span class="co">##</span></span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a><span class="co"># step 5: put back emoticons</span></span>
<span id="cb23-3"><a href="#cb23-3" aria-hidden="true" tabindex="-1"></a>text <span class="op">=</span> text <span class="op">+</span> <span class="st">' '</span>.join(emoticons).replace(<span class="st">'-'</span>, <span class="st">''</span>)</span>
<span id="cb23-4"><a href="#cb23-4" aria-hidden="true" tabindex="-1"></a>text</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="13">
<pre><code>'this is awesome movie i loved it so much i m goona watch it again :) :)'</code></pre>
</div>
</div>
<div class="cell" data-outputid="aba0338c-a7a8-44a8-fc64-7f9b4210e8be" data-execution_count="14">
<div class="sourceCode cell-code" id="cb25"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a><span class="co">##</span></span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a><span class="co"># step 6: generate word tokens</span></span>
<span id="cb25-3"><a href="#cb25-3" aria-hidden="true" tabindex="-1"></a>text <span class="op">=</span> text.split()</span>
<span id="cb25-4"><a href="#cb25-4" aria-hidden="true" tabindex="-1"></a>text</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="14">
<pre><code>['this',
 'is',
 'awesome',
 'movie',
 'i',
 'loved',
 'it',
 'so',
 'much',
 'i',
 'm',
 'goona',
 'watch',
 'it',
 'again',
 ':)',
 ':)']</code></pre>
</div>
</div>
<p>Let’s put all the preprocessing steps in a nice function and give it a name.</p>
<div class="cell" data-execution_count="15">
<div class="sourceCode cell-code" id="cb27"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> tokenizer(text):</span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a>    <span class="co"># step 1. remove HTML tags. they are not helpful in understanding the sentiments of a review</span></span>
<span id="cb27-3"><a href="#cb27-3" aria-hidden="true" tabindex="-1"></a>    <span class="co"># step 2: use lowercase for all text to keep symmetry</span></span>
<span id="cb27-4"><a href="#cb27-4" aria-hidden="true" tabindex="-1"></a>    <span class="co"># step 3: extract emoticons. keep them as they are important sentiment signals</span></span>
<span id="cb27-5"><a href="#cb27-5" aria-hidden="true" tabindex="-1"></a>    <span class="co"># step 4: remove punctuation marks</span></span>
<span id="cb27-6"><a href="#cb27-6" aria-hidden="true" tabindex="-1"></a>    <span class="co"># step 5: put back emoticons</span></span>
<span id="cb27-7"><a href="#cb27-7" aria-hidden="true" tabindex="-1"></a>    <span class="co"># step 6: generate word tokens</span></span>
<span id="cb27-8"><a href="#cb27-8" aria-hidden="true" tabindex="-1"></a>    text <span class="op">=</span> re.sub(<span class="st">"&lt;[^&gt;]*&gt;"</span>, <span class="st">""</span>, text)</span>
<span id="cb27-9"><a href="#cb27-9" aria-hidden="true" tabindex="-1"></a>    text <span class="op">=</span> text.lower()</span>
<span id="cb27-10"><a href="#cb27-10" aria-hidden="true" tabindex="-1"></a>    emoticons <span class="op">=</span> re.findall(<span class="st">"(?::|;|=)(?:-)?(?:\)|\(|D|P)"</span>, text)</span>
<span id="cb27-11"><a href="#cb27-11" aria-hidden="true" tabindex="-1"></a>    text <span class="op">=</span> re.sub(<span class="st">"[\W]+"</span>, <span class="st">" "</span>, text)</span>
<span id="cb27-12"><a href="#cb27-12" aria-hidden="true" tabindex="-1"></a>    text <span class="op">=</span> text <span class="op">+</span> <span class="st">" "</span>.join(emoticons).replace(<span class="st">"-"</span>, <span class="st">""</span>)</span>
<span id="cb27-13"><a href="#cb27-13" aria-hidden="true" tabindex="-1"></a>    tokenized <span class="op">=</span> text.split()</span>
<span id="cb27-14"><a href="#cb27-14" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> tokenized</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Apply <code>tokenizer</code> on the <code>example_text</code> to verify the output.</p>
<div class="cell" data-outputid="c57eb03d-680b-4c55-d3a1-dedfd4eb9ce8" data-execution_count="16">
<div class="sourceCode cell-code" id="cb28"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a>example_tokens <span class="op">=</span> tokenizer(example_text)</span>
<span id="cb28-2"><a href="#cb28-2" aria-hidden="true" tabindex="-1"></a>example_tokens</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="16">
<pre><code>['this',
 'is',
 'awesome',
 'movie',
 'i',
 'loved',
 'it',
 'so',
 'much',
 'i',
 'm',
 'goona',
 'watch',
 'it',
 'again',
 ':)',
 ':)']</code></pre>
</div>
</div>
</section>
<section id="preparing-data-dictionary" class="level3">
<h3 class="anchored" data-anchor-id="preparing-data-dictionary">Preparing data dictionary</h3>
<p>We are successful in creating word tokens from our <code>example_text</code>. But there is one more problem. Some of the tokens are repeating. If we can convert these tokens into a dictionary along with their frequency count, we can significantly reduce the generated token size from these reviews. Let’s do that.</p>
<div class="cell" data-outputid="aec65607-3078-4f6f-8139-5ff9c2951097" data-execution_count="17">
<div class="sourceCode cell-code" id="cb30"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> collections <span class="im">import</span> Counter</span>
<span id="cb30-2"><a href="#cb30-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-3"><a href="#cb30-3" aria-hidden="true" tabindex="-1"></a>token_counts <span class="op">=</span> Counter()</span>
<span id="cb30-4"><a href="#cb30-4" aria-hidden="true" tabindex="-1"></a>token_counts.update(example_tokens)</span>
<span id="cb30-5"><a href="#cb30-5" aria-hidden="true" tabindex="-1"></a>token_counts</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="17">
<pre><code>Counter({'this': 1,
         'is': 1,
         'awesome': 1,
         'movie': 1,
         'i': 2,
         'loved': 1,
         'it': 2,
         'so': 1,
         'much': 1,
         'm': 1,
         'goona': 1,
         'watch': 1,
         'again': 1,
         ':)': 2})</code></pre>
</div>
</div>
<p>Let’s sort the output to have the most common words at the top.</p>
<div class="cell" data-outputid="4b6c783a-7352-4024-d298-93d831f4bd83" data-execution_count="18">
<div class="sourceCode cell-code" id="cb32"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a>sorted_by_freq_tuples <span class="op">=</span> <span class="bu">sorted</span>(token_counts.items(), key<span class="op">=</span><span class="kw">lambda</span> x: x[<span class="dv">1</span>], reverse<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb32-2"><a href="#cb32-2" aria-hidden="true" tabindex="-1"></a>sorted_by_freq_tuples</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="18">
<pre><code>[('i', 2),
 ('it', 2),
 (':)', 2),
 ('this', 1),
 ('is', 1),
 ('awesome', 1),
 ('movie', 1),
 ('loved', 1),
 ('so', 1),
 ('much', 1),
 ('m', 1),
 ('goona', 1),
 ('watch', 1),
 ('again', 1)]</code></pre>
</div>
</div>
<p>It shows that in our example text, the top place is taken by pronouns (i and it) followed by the emoticon. Though our data is now correctly processed, it needs to be prepared to be fed to a model. Because [machine] models love math and work with numbers exclusively. To convert our dictionary of word tokens into integers, we can take help from <code>torchtext.vocab</code>. Its purpose in the official documentation is defined as <a href="https://pytorch.org/text/stable/vocab.html">link here</a></p>
<blockquote class="blockquote">
<p>Factory method for creating a vocab object which maps tokens to indices.</p>
</blockquote>
<blockquote class="blockquote">
<p>Note that the ordering in which key value pairs were inserted in the ordered_dict will be respected when building the vocab. Therefore if sorting by token frequency is important to the user, the ordered_dict should be created in a way to reflect this.</p>
</blockquote>
<p>It highlights three points: * It maps tokens to indices * It requires an ordered dictionary (<code>OrderedDict</code>) to work * Tokens in vocab at the starting indices reflect higher frequency</p>
<div class="cell" data-outputid="d97dfd64-f37e-4372-cd52-339466927997" data-execution_count="19">
<div class="sourceCode cell-code" id="cb34"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a><span class="co">##</span></span>
<span id="cb34-2"><a href="#cb34-2" aria-hidden="true" tabindex="-1"></a><span class="co"># step 1: convert our sorted list of tokens to OrderedDict</span></span>
<span id="cb34-3"><a href="#cb34-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> collections <span class="im">import</span> OrderedDict</span>
<span id="cb34-4"><a href="#cb34-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-5"><a href="#cb34-5" aria-hidden="true" tabindex="-1"></a>ordered_dict <span class="op">=</span> OrderedDict(sorted_by_freq_tuples)</span>
<span id="cb34-6"><a href="#cb34-6" aria-hidden="true" tabindex="-1"></a>ordered_dict</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="19">
<pre><code>OrderedDict([('i', 2),
             ('it', 2),
             (':)', 2),
             ('this', 1),
             ('is', 1),
             ('awesome', 1),
             ('movie', 1),
             ('loved', 1),
             ('so', 1),
             ('much', 1),
             ('m', 1),
             ('goona', 1),
             ('watch', 1),
             ('again', 1)])</code></pre>
</div>
</div>
<div class="cell" data-outputid="7ec70e90-8c13-4b00-a6cd-abed87943542" data-execution_count="20">
<div class="sourceCode cell-code" id="cb36"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb36-1"><a href="#cb36-1" aria-hidden="true" tabindex="-1"></a><span class="co">##</span></span>
<span id="cb36-2"><a href="#cb36-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Check the length of our dictionary</span></span>
<span id="cb36-3"><a href="#cb36-3" aria-hidden="true" tabindex="-1"></a><span class="bu">len</span>(ordered_dict)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="20">
<pre><code>14</code></pre>
</div>
</div>
<div class="cell" data-outputid="1c03856f-5e50-4a0d-9218-52929ae78b0a" data-execution_count="21">
<div class="sourceCode cell-code" id="cb38"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb38-1"><a href="#cb38-1" aria-hidden="true" tabindex="-1"></a><span class="co">##</span></span>
<span id="cb38-2"><a href="#cb38-2" aria-hidden="true" tabindex="-1"></a><span class="co"># step 2: convert the ordered dict to torchtext.vocab</span></span>
<span id="cb38-3"><a href="#cb38-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torchtext.vocab <span class="im">import</span> vocab</span>
<span id="cb38-4"><a href="#cb38-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-5"><a href="#cb38-5" aria-hidden="true" tabindex="-1"></a>vb <span class="op">=</span> vocab(ordered_dict)</span>
<span id="cb38-6"><a href="#cb38-6" aria-hidden="true" tabindex="-1"></a>vb.get_stoi()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="21">
<pre><code>{'goona': 11,
 'much': 9,
 'm': 10,
 'loved': 7,
 'watch': 12,
 'so': 8,
 'movie': 6,
 'it': 1,
 'again': 13,
 'this': 3,
 'i': 0,
 'awesome': 5,
 ':)': 2,
 'is': 4}</code></pre>
</div>
</div>
<p>This generated vocabulary shows that tokens with higher frequency (<code>i</code>, <code>it</code>) have been assigned lower indices (or integers). This vocabulary will act as a lookup table for us, and during training for each word token, we will find a corresponding index from this vocab and pass it to our model.</p>
<p>We have done many steps while processing our <code>example_text</code>. Let’s summarize them here before moving further</p>
<section id="summary-of-data-dictionary-preparation-steps" class="level4">
<h4 class="anchored" data-anchor-id="summary-of-data-dictionary-preparation-steps"><strong>Summary of data dictionary preparation steps</strong></h4>
<ol type="1">
<li>Generate tokens from text using the function <code>tokenizer</code></li>
<li>Find the frequency of tokens using Python <a href="https://docs.python.org/3/library/collections.html#collections.Counter">collections.Counter</a></li>
<li>Sort the tokens based on their frequency in descending order</li>
<li>Put the sorted tokens in Python <a href="https://docs.python.org/3/library/collections.html#collections.OrderedDict">collections.OrderedDict</a></li>
<li>Convert the tokens into integers using <a href="https://pytorch.org/text/stable/vocab.html">torchtext.vocab</a></li>
</ol>
<p>Let’s apply all these steps on our IMDB reviews training dataset.</p>
<div class="cell" data-outputid="48bd4363-f820-4a43-93dc-0bf56c35eb0a" data-execution_count="22">
<div class="sourceCode cell-code" id="cb40"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb40-1"><a href="#cb40-1" aria-hidden="true" tabindex="-1"></a><span class="co">##</span></span>
<span id="cb40-2"><a href="#cb40-2" aria-hidden="true" tabindex="-1"></a><span class="co"># step 1: convert reviews into tokens</span></span>
<span id="cb40-3"><a href="#cb40-3" aria-hidden="true" tabindex="-1"></a><span class="co"># step 2: find frequency of tokens</span></span>
<span id="cb40-4"><a href="#cb40-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-5"><a href="#cb40-5" aria-hidden="true" tabindex="-1"></a>token_counts <span class="op">=</span> Counter()</span>
<span id="cb40-6"><a href="#cb40-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-7"><a href="#cb40-7" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> label, line <span class="kw">in</span> train_dataset:</span>
<span id="cb40-8"><a href="#cb40-8" aria-hidden="true" tabindex="-1"></a>    tokens <span class="op">=</span> tokenizer(line)</span>
<span id="cb40-9"><a href="#cb40-9" aria-hidden="true" tabindex="-1"></a>    token_counts.update(tokens)</span>
<span id="cb40-10"><a href="#cb40-10" aria-hidden="true" tabindex="-1"></a> </span>
<span id="cb40-11"><a href="#cb40-11" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'IMDB vocab size:'</span>, <span class="bu">len</span>(token_counts))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>IMDB vocab size: 69023</code></pre>
</div>
</div>
<p>After tokenizing IMDB reviews, we find that there <code>69023</code> unique tokens.</p>
<div class="cell" data-outputid="2da48263-8c05-456e-9e30-5e595919a618" data-execution_count="23">
<div class="sourceCode cell-code" id="cb42"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb42-1"><a href="#cb42-1" aria-hidden="true" tabindex="-1"></a><span class="co">##</span></span>
<span id="cb42-2"><a href="#cb42-2" aria-hidden="true" tabindex="-1"></a><span class="co"># step 3: sort the token based on their frequency</span></span>
<span id="cb42-3"><a href="#cb42-3" aria-hidden="true" tabindex="-1"></a><span class="co"># step 4: put the sorted tokens in OrderedDict</span></span>
<span id="cb42-4"><a href="#cb42-4" aria-hidden="true" tabindex="-1"></a><span class="co"># step 5: convert token to integers using vocab object</span></span>
<span id="cb42-5"><a href="#cb42-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-6"><a href="#cb42-6" aria-hidden="true" tabindex="-1"></a>sorted_by_freq_tuples <span class="op">=</span> <span class="bu">sorted</span>(token_counts.items(), key<span class="op">=</span><span class="kw">lambda</span> x: x[<span class="dv">1</span>], reverse<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb42-7"><a href="#cb42-7" aria-hidden="true" tabindex="-1"></a>ordered_dict <span class="op">=</span> OrderedDict(sorted_by_freq_tuples)</span>
<span id="cb42-8"><a href="#cb42-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-9"><a href="#cb42-9" aria-hidden="true" tabindex="-1"></a>vb <span class="op">=</span> vocab(ordered_dict)</span>
<span id="cb42-10"><a href="#cb42-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-11"><a href="#cb42-11" aria-hidden="true" tabindex="-1"></a>vb.insert_token(<span class="st">"&lt;pad&gt;"</span>, <span class="dv">0</span>)  <span class="co"># special token for padding</span></span>
<span id="cb42-12"><a href="#cb42-12" aria-hidden="true" tabindex="-1"></a>vb.insert_token(<span class="st">"&lt;unk&gt;"</span>, <span class="dv">1</span>)  <span class="co"># special token for unknown words</span></span>
<span id="cb42-13"><a href="#cb42-13" aria-hidden="true" tabindex="-1"></a>vb.set_default_index(<span class="dv">1</span>)</span>
<span id="cb42-14"><a href="#cb42-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-15"><a href="#cb42-15" aria-hidden="true" tabindex="-1"></a><span class="co"># print some token indexes from vocab</span></span>
<span id="cb42-16"><a href="#cb42-16" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> token <span class="kw">in</span> [<span class="st">"this"</span>, <span class="st">"is"</span>, <span class="st">"an"</span>, <span class="st">"example"</span>]:</span>
<span id="cb42-17"><a href="#cb42-17" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(token, <span class="st">" --&gt; "</span>, vb[token])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>this  --&gt;  11
is  --&gt;  7
an  --&gt;  35
example  --&gt;  457</code></pre>
</div>
</div>
<p>We have added two extra tokens to our vocabulary. * “pad” for padding. This token will come in handy when we pad our reviews to make them of the same length * “unk” for unknown. This token will come in handy if we find any token in the validation or test set that was not part of the train set</p>
<p>Let’s also print the tokens present at the first ten indices of our vocab object.</p>
<div class="cell" data-outputid="8a12125d-d609-43ed-d83e-7e855b55bc33" data-execution_count="24">
<div class="sourceCode cell-code" id="cb44"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb44-1"><a href="#cb44-1" aria-hidden="true" tabindex="-1"></a>vb.get_itos()[:<span class="dv">10</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="24">
<pre><code>['&lt;pad&gt;', '&lt;unk&gt;', 'the', 'and', 'a', 'of', 'to', 'is', 'it', 'in']</code></pre>
</div>
</div>
<p>It shows that articles, prepositions, and pronouns are the most common words in the training dataset. So let’s also check the least common words.</p>
<div class="cell" data-outputid="b3edc3ee-3cd0-4a46-9044-16e65da37883" data-execution_count="25">
<div class="sourceCode cell-code" id="cb46"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb46-1"><a href="#cb46-1" aria-hidden="true" tabindex="-1"></a>vb.get_itos()[<span class="op">-</span><span class="dv">10</span>:]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="25">
<pre><code>['hairband',
 'ratt',
 'bettiefile',
 'queueing',
 'johansen',
 'hemmed',
 'jardine',
 'morland',
 'seriousuly',
 'fictive']</code></pre>
</div>
</div>
<p>The least common words seem to be people or place names or misspelled words like ‘queueing’ and ‘seriousuly’.</p>
</section>
</section>
</section>
<section id="define-data-processing-pipelines" class="level2">
<h2 class="anchored" data-anchor-id="define-data-processing-pipelines">Define data processing pipelines</h2>
<p>At this point, we have our tokenizer function and vocabulary lookup ready. For each review item from the dataset, we are supposed to perform the following preprocessing steps:</p>
<p><strong>For review text</strong> * Create tokens from the review text * Assign a unique integer to each token from the vocab lookup</p>
<p><strong>For review label</strong> * Assign 1 for <code>pos</code> and 0 for <code>neg</code> label</p>
<p>Let’s create two simple functions (inline lambda) for review text and label processing.</p>
<div class="cell" data-execution_count="26">
<div class="sourceCode cell-code" id="cb48"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb48-1"><a href="#cb48-1" aria-hidden="true" tabindex="-1"></a><span class="co">##</span></span>
<span id="cb48-2"><a href="#cb48-2" aria-hidden="true" tabindex="-1"></a><span class="co"># inline lambda functions for text and label precessing</span></span>
<span id="cb48-3"><a href="#cb48-3" aria-hidden="true" tabindex="-1"></a>text_pipeline <span class="op">=</span> <span class="kw">lambda</span> x: [vb[token] <span class="cf">for</span> token <span class="kw">in</span> tokenizer(x)]</span>
<span id="cb48-4"><a href="#cb48-4" aria-hidden="true" tabindex="-1"></a>label_pipeline <span class="op">=</span> <span class="kw">lambda</span> x: <span class="fl">1.0</span> <span class="cf">if</span> x <span class="op">==</span> <span class="st">"pos"</span> <span class="cf">else</span> <span class="fl">0.0</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-outputid="eed8b2b9-ec2e-4c77-b64f-0003ddc95b36" data-execution_count="27">
<div class="sourceCode cell-code" id="cb49"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb49-1"><a href="#cb49-1" aria-hidden="true" tabindex="-1"></a><span class="co">##</span></span>
<span id="cb49-2"><a href="#cb49-2" aria-hidden="true" tabindex="-1"></a><span class="co"># apply text_pipeline to example_text</span></span>
<span id="cb49-3"><a href="#cb49-3" aria-hidden="true" tabindex="-1"></a>text_pipeline(example_text)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="27">
<pre><code>[11, 7, 1166, 18, 10, 450, 8, 37, 74, 10, 142, 1, 104, 8, 174, 2287, 2287]</code></pre>
</div>
</div>
<p>Instead of processing a single review at a time, we always prefer to work with a batch of them during model training. For each review item in the batch, we will be doing the same preprocessing steps i.e.&nbsp;review text processing and label processing. For handling preprocessing steps at a batch level, we can create another higher-level function that applies preprocessing steps at a batch level.</p>
<div class="cell" data-outputid="5d3efdc2-19ae-4615-8db6-4e652d720822" data-execution_count="28">
<div class="sourceCode cell-code" id="cb51"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb51-1"><a href="#cb51-1" aria-hidden="true" tabindex="-1"></a><span class="co">##</span></span>
<span id="cb51-2"><a href="#cb51-2" aria-hidden="true" tabindex="-1"></a><span class="co"># setting device on GPU if available, else CPU</span></span>
<span id="cb51-3"><a href="#cb51-3" aria-hidden="true" tabindex="-1"></a>device <span class="op">=</span> torch.device(<span class="st">'cuda'</span> <span class="cf">if</span> torch.cuda.is_available() <span class="cf">else</span> <span class="st">'cpu'</span>)</span>
<span id="cb51-4"><a href="#cb51-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Using device:'</span>, device)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Using device: cuda</code></pre>
</div>
</div>
<div class="cell" data-execution_count="29">
<div class="sourceCode cell-code" id="cb53"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb53-1"><a href="#cb53-1" aria-hidden="true" tabindex="-1"></a><span class="co">##</span></span>
<span id="cb53-2"><a href="#cb53-2" aria-hidden="true" tabindex="-1"></a><span class="co"># a function to apply pre-processing steps at a batch level</span></span>
<span id="cb53-3"><a href="#cb53-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb53-4"><a href="#cb53-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-5"><a href="#cb53-5" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> collate_batch(batch):</span>
<span id="cb53-6"><a href="#cb53-6" aria-hidden="true" tabindex="-1"></a>    label_list, text_list, lengths <span class="op">=</span> [], [], []</span>
<span id="cb53-7"><a href="#cb53-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-8"><a href="#cb53-8" aria-hidden="true" tabindex="-1"></a>    <span class="co"># iterate over all reviews in a batch</span></span>
<span id="cb53-9"><a href="#cb53-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> _label, _text <span class="kw">in</span> batch:</span>
<span id="cb53-10"><a href="#cb53-10" aria-hidden="true" tabindex="-1"></a>        <span class="co"># label preprocessing</span></span>
<span id="cb53-11"><a href="#cb53-11" aria-hidden="true" tabindex="-1"></a>        label_list.append(label_pipeline(_label))</span>
<span id="cb53-12"><a href="#cb53-12" aria-hidden="true" tabindex="-1"></a>        <span class="co"># text preprocessing</span></span>
<span id="cb53-13"><a href="#cb53-13" aria-hidden="true" tabindex="-1"></a>        processed_text <span class="op">=</span> torch.tensor(text_pipeline(_text), dtype<span class="op">=</span>torch.int64)</span>
<span id="cb53-14"><a href="#cb53-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-15"><a href="#cb53-15" aria-hidden="true" tabindex="-1"></a>        <span class="co"># store the processed text in a list</span></span>
<span id="cb53-16"><a href="#cb53-16" aria-hidden="true" tabindex="-1"></a>        text_list.append(processed_text)</span>
<span id="cb53-17"><a href="#cb53-17" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb53-18"><a href="#cb53-18" aria-hidden="true" tabindex="-1"></a>        <span class="co"># store the length of processed text</span></span>
<span id="cb53-19"><a href="#cb53-19" aria-hidden="true" tabindex="-1"></a>        <span class="co"># this will come handy in future when we want to know the original size of a text (without padding)</span></span>
<span id="cb53-20"><a href="#cb53-20" aria-hidden="true" tabindex="-1"></a>        lengths.append(processed_text.size(<span class="dv">0</span>))</span>
<span id="cb53-21"><a href="#cb53-21" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb53-22"><a href="#cb53-22" aria-hidden="true" tabindex="-1"></a>    label_list <span class="op">=</span> torch.tensor(label_list)</span>
<span id="cb53-23"><a href="#cb53-23" aria-hidden="true" tabindex="-1"></a>    lengths <span class="op">=</span> torch.tensor(lengths)</span>
<span id="cb53-24"><a href="#cb53-24" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb53-25"><a href="#cb53-25" aria-hidden="true" tabindex="-1"></a>    <span class="co"># pad the processed reviews to make their lengths consistant</span></span>
<span id="cb53-26"><a href="#cb53-26" aria-hidden="true" tabindex="-1"></a>    padded_text_list <span class="op">=</span> nn.utils.rnn.pad_sequence(</span>
<span id="cb53-27"><a href="#cb53-27" aria-hidden="true" tabindex="-1"></a>        text_list, batch_first<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb53-28"><a href="#cb53-28" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb53-29"><a href="#cb53-29" aria-hidden="true" tabindex="-1"></a>    <span class="co"># return</span></span>
<span id="cb53-30"><a href="#cb53-30" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 1. a list of processed and padded review texts</span></span>
<span id="cb53-31"><a href="#cb53-31" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 2. a list of processed labels</span></span>
<span id="cb53-32"><a href="#cb53-32" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 3. a list of review text original lengths (before padding)</span></span>
<span id="cb53-33"><a href="#cb53-33" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> padded_text_list.to(device), label_list.to(device), lengths.to(device)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<section id="sequence-padding" class="level3">
<h3 class="anchored" data-anchor-id="sequence-padding">Sequence padding</h3>
<p>In the above <code>collate_batch</code> function, I added one extra padding step.</p>
<pre><code>added_text_list = nn.utils.rnn.pad_sequence(text_list, batch_first=True)</code></pre>
<p>We intend to make all review texts in a batch of the same length. For this, we take the maximum length of a text in a batch, all pad all the smaller text with extra dummy tokens (‘pad’) to make their sizes equal. Finally, with all the data in a batch of the same dimension, we convert it into a tensor matrix for faster processing.</p>
<p>To understand how PyTorch utility <code>nn.utils.rnn.pad_sequence</code> works, we can take a simple example of three tensors (a, b, c) of varying sizes (1, 3, 5).</p>
<div class="cell" data-outputid="6df8f266-52c7-41ce-eb2f-7535a775d08d" data-execution_count="30">
<div class="sourceCode cell-code" id="cb55"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb55-1"><a href="#cb55-1" aria-hidden="true" tabindex="-1"></a><span class="co">##</span></span>
<span id="cb55-2"><a href="#cb55-2" aria-hidden="true" tabindex="-1"></a><span class="co"># initialize three tensors of varying sizes</span></span>
<span id="cb55-3"><a href="#cb55-3" aria-hidden="true" tabindex="-1"></a>a <span class="op">=</span> torch.tensor([<span class="dv">1</span>])</span>
<span id="cb55-4"><a href="#cb55-4" aria-hidden="true" tabindex="-1"></a>b <span class="op">=</span> torch.tensor([<span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">4</span>])</span>
<span id="cb55-5"><a href="#cb55-5" aria-hidden="true" tabindex="-1"></a>c <span class="op">=</span> torch.tensor([<span class="dv">5</span>, <span class="dv">6</span>, <span class="dv">7</span>, <span class="dv">8</span>, <span class="dv">9</span>])</span>
<span id="cb55-6"><a href="#cb55-6" aria-hidden="true" tabindex="-1"></a>a, b, c</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="30">
<pre><code>(tensor([1]), tensor([2, 3, 4]), tensor([5, 6, 7, 8, 9]))</code></pre>
</div>
</div>
<p>Now let’s pad them to make sizes consistant.</p>
<div class="cell" data-outputid="f3e0a572-1acf-45cc-f869-f5d15bdb1a65" data-execution_count="31">
<div class="sourceCode cell-code" id="cb57"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb57-1"><a href="#cb57-1" aria-hidden="true" tabindex="-1"></a><span class="co">##</span></span>
<span id="cb57-2"><a href="#cb57-2" aria-hidden="true" tabindex="-1"></a><span class="co"># apply padding on tensors</span></span>
<span id="cb57-3"><a href="#cb57-3" aria-hidden="true" tabindex="-1"></a>pad_seq <span class="op">=</span> nn.utils.rnn.pad_sequence([a, b, c])</span>
<span id="cb57-4"><a href="#cb57-4" aria-hidden="true" tabindex="-1"></a>pad_seq</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="31">
<pre><code>tensor([[1, 2, 5],
        [0, 3, 6],
        [0, 4, 7],
        [0, 0, 8],
        [0, 0, 9]])</code></pre>
</div>
</div>
</section>
<section id="sequence-packing" class="level3">
<h3 class="anchored" data-anchor-id="sequence-packing">Sequence packing</h3>
<p>From the above output, we can see that after padding tensors of varying sizes, we can convert them into a single matrix for faster processing. But the drawback of this approach is that we can have many, many padded tokens in our matrix. They are not helping us in any way, instead of occupying a lot of machine memory. To avoid this, we can also squish these matrixes into a much condensed form called <code>packed padded sequences</code> using PyTorch utility <code>nn.utils.rnn.pack_padded_sequence</code>.</p>
<div class="cell" data-outputid="f45f9d61-3b4f-492c-cd4c-b5ba6f115f63" data-execution_count="32">
<div class="sourceCode cell-code" id="cb59"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb59-1"><a href="#cb59-1" aria-hidden="true" tabindex="-1"></a>pack_pad_seq <span class="op">=</span> nn.utils.rnn.pack_padded_sequence(</span>
<span id="cb59-2"><a href="#cb59-2" aria-hidden="true" tabindex="-1"></a>    pad_seq, [<span class="dv">1</span>, <span class="dv">3</span>, <span class="dv">5</span>], enforce_sorted<span class="op">=</span><span class="va">False</span>, batch_first<span class="op">=</span><span class="va">False</span></span>
<span id="cb59-3"><a href="#cb59-3" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb59-4"><a href="#cb59-4" aria-hidden="true" tabindex="-1"></a>pack_pad_seq.data</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="32">
<pre><code>tensor([5, 2, 1, 6, 3, 7, 4, 8, 9])</code></pre>
</div>
</div>
<p>Here the tensor still holds all the original tensor values (1 to 9) but is very condensed and has no extra padded token. So how does this tensor know which tokens belong to which token? For this, it stores some additional information. * batch sizes (or original tensor length) * tensor indices</p>
<p>We can move back and forth between the padded pack and unpacked sequences using this information.</p>
<div class="cell" data-outputid="4041dad4-e6f7-48c9-8153-bd4cf94fc075" data-execution_count="33">
<div class="sourceCode cell-code" id="cb61"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb61-1"><a href="#cb61-1" aria-hidden="true" tabindex="-1"></a>pack_pad_seq</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="33">
<pre><code>PackedSequence(data=tensor([5, 2, 1, 6, 3, 7, 4, 8, 9]), batch_sizes=tensor([3, 2, 2, 1, 1]), sorted_indices=tensor([2, 1, 0]), unsorted_indices=tensor([2, 1, 0]))</code></pre>
</div>
</div>
</section>
<section id="run-data-preprocessing-pipelines-on-an-example-batch" class="level3">
<h3 class="anchored" data-anchor-id="run-data-preprocessing-pipelines-on-an-example-batch">Run data preprocessing pipelines on an example batch</h3>
<p>Let’s load our data in the PyTorch DataLoader class and create a small batch of 4 reviews. Preprocess the entire set with <code>collate_batch</code> function.</p>
<div class="cell" data-outputid="7c1bb941-285f-4f23-8dae-3626c58c2b68" data-execution_count="34">
<div class="sourceCode cell-code" id="cb63"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb63-1"><a href="#cb63-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.utils.data <span class="im">import</span> DataLoader</span>
<span id="cb63-2"><a href="#cb63-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb63-3"><a href="#cb63-3" aria-hidden="true" tabindex="-1"></a>dataloader <span class="op">=</span> DataLoader(</span>
<span id="cb63-4"><a href="#cb63-4" aria-hidden="true" tabindex="-1"></a>    train_dataset, batch_size<span class="op">=</span><span class="dv">4</span>, shuffle<span class="op">=</span><span class="va">False</span>, collate_fn<span class="op">=</span>collate_batch</span>
<span id="cb63-5"><a href="#cb63-5" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb63-6"><a href="#cb63-6" aria-hidden="true" tabindex="-1"></a>text_batch, label_batch, length_batch <span class="op">=</span> <span class="bu">next</span>(<span class="bu">iter</span>(dataloader))</span>
<span id="cb63-7"><a href="#cb63-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb63-8"><a href="#cb63-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"text_batch.shape: "</span>, text_batch.shape)</span>
<span id="cb63-9"><a href="#cb63-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"label_batch: "</span>, label_batch)</span>
<span id="cb63-10"><a href="#cb63-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"length_batch: "</span>, length_batch)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>text_batch.shape:  torch.Size([4, 218])
label_batch:  tensor([1., 1., 1., 0.], device='cuda:0')
length_batch:  tensor([165,  86, 218, 145], device='cuda:0')</code></pre>
</div>
</div>
<ul>
<li><code>text_batch.shape: torch.Size([4, 218])</code> tells us that in this batch, there are four reviews (or their tokens) and all have the same length of 218</li>
<li><code>label_batch:  tensor([1., 1., 1., 0.])</code> tells us that the first three reviews are positive and the last is negative</li>
<li><code>length_batch:  tensor([165,  86, 218, 145])</code> tells us that before padding the original length of review tokens</li>
</ul>
<p>Let’s check what the first review in this batch looks like after preprocessing and padding.</p>
<div class="cell" data-outputid="5615d0a2-06d7-4d6d-cf90-8f2cc5710aae" data-execution_count="35">
<div class="sourceCode cell-code" id="cb65"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb65-1"><a href="#cb65-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(text_batch[<span class="dv">0</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>tensor([   35,  1739,     7,   449,   721,     6,   301,     4,   787,     9,
            4,    18,    44,     2,  1705,  2460,   186,    25,     7,    24,
          100,  1874,  1739,    25,     7, 34415,  3568,  1103,  7517,   787,
            5,     2,  4991, 12401,    36,     7,   148,   111,   939,     6,
        11598,     2,   172,   135,    62,    25,  3199,  1602,     3,   928,
         1500,     9,     6,  4601,     2,   155,    36,    14,   274,     4,
        42945,     9,  4991,     3,    14, 10296,    34,  3568,     8,    51,
          148,    30,     2,    58,    16,    11,  1893,   125,     6,   420,
         1214,    27, 14542,   940,    11,     7,    29,   951,    18,    17,
        15994,   459,    34,  2480, 15211,  3713,     2,   840,  3200,     9,
         3568,    13,   107,     9,   175,    94,    25,    51, 10297,  1796,
           27,   712,    16,     2,   220,    17,     4,    54,   722,   238,
          395,     2,   787,    32,    27,  5236,     3,    32,    27,  7252,
         5118,  2461,  6390,     4,  2873,  1495,    15,     2,  1054,  2874,
          155,     3,  7015,     7,   409,     9,    41,   220,    17,    41,
          390,     3,  3925,   807,    37,    74,  2858,    15, 10297,   115,
           31,   189,  3506,   667,   163,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0],
       device='cuda:0')</code></pre>
</div>
</div>
<p>To complete the picture, I have re-printed the original text of the first review and manually processed a part of it. You can verify that the tokens match.</p>
<div class="cell" data-outputid="5e7f0099-c07f-4a4b-e2c9-3297074308b8" data-execution_count="36">
<div class="sourceCode cell-code" id="cb67"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb67-1"><a href="#cb67-1" aria-hidden="true" tabindex="-1"></a><span class="co">##</span></span>
<span id="cb67-2"><a href="#cb67-2" aria-hidden="true" tabindex="-1"></a><span class="co"># first review</span></span>
<span id="cb67-3"><a href="#cb67-3" aria-hidden="true" tabindex="-1"></a>train_dataset[<span class="dv">0</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="36">
<pre><code>('pos',
 'An extra is called upon to play a general in a movie about the Russian Revolution. However, he is not any ordinary extra. He is Serguis Alexander, former commanding general of the Russia armies who is now being forced to relive the same scene, which he suffered professional and personal tragedy in, to satisfy the director who was once a revolutionist in Russia and was humiliated by Alexander. It can now be the time for this broken man to finally "win" his penultimate battle. This is one powerful movie with meticulous direction by Von Sternberg, providing the greatest irony in Alexander\'s character in every way he can. Jannings deserved his Oscar for the role with a very moving performance playing the general at his peak and at his deepest valley. Powell lends a sinister support as the revenge minded director and Brent is perfect in her role with her face and movements showing so much expression as Jannings\' love. All around brilliance. Rating, 10.')</code></pre>
</div>
</div>
<div class="cell" data-outputid="63e0d0b1-d478-4bab-959a-a1654903b0f9" data-execution_count="37">
<div class="sourceCode cell-code" id="cb69"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb69-1"><a href="#cb69-1" aria-hidden="true" tabindex="-1"></a><span class="co">##</span></span>
<span id="cb69-2"><a href="#cb69-2" aria-hidden="true" tabindex="-1"></a><span class="co"># manually preprocessing a part of review text</span></span>
<span id="cb69-3"><a href="#cb69-3" aria-hidden="true" tabindex="-1"></a><span class="co"># notice that the generated tokens match</span></span>
<span id="cb69-4"><a href="#cb69-4" aria-hidden="true" tabindex="-1"></a>text <span class="op">=</span> <span class="st">'An extra is called upon to play a general in a movie about the Russian Revolution'</span></span>
<span id="cb69-5"><a href="#cb69-5" aria-hidden="true" tabindex="-1"></a>[vb[token] <span class="cf">for</span> token <span class="kw">in</span> tokenizer(text)]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="37">
<pre><code>[35, 1739, 7, 449, 721, 6, 301, 4, 787, 9, 4, 18, 44, 2, 1705, 2460]</code></pre>
</div>
</div>
</section>
</section>
<section id="batching-the-training-validation-and-test-dataset" class="level2">
<h2 class="anchored" data-anchor-id="batching-the-training-validation-and-test-dataset">Batching the training, validation, and test dataset</h2>
<p>Let’s proceed on creating DataLoaders for train, valid, and test data with <code>batch_size = 32</code></p>
<div class="cell" data-execution_count="38">
<div class="sourceCode cell-code" id="cb71"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb71-1"><a href="#cb71-1" aria-hidden="true" tabindex="-1"></a>batch_size <span class="op">=</span> <span class="dv">32</span></span>
<span id="cb71-2"><a href="#cb71-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-3"><a href="#cb71-3" aria-hidden="true" tabindex="-1"></a>train_dl <span class="op">=</span> DataLoader(</span>
<span id="cb71-4"><a href="#cb71-4" aria-hidden="true" tabindex="-1"></a>    train_dataset, batch_size<span class="op">=</span>batch_size, shuffle<span class="op">=</span><span class="va">True</span>, collate_fn<span class="op">=</span>collate_batch</span>
<span id="cb71-5"><a href="#cb71-5" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb71-6"><a href="#cb71-6" aria-hidden="true" tabindex="-1"></a>valid_dl <span class="op">=</span> DataLoader(</span>
<span id="cb71-7"><a href="#cb71-7" aria-hidden="true" tabindex="-1"></a>    valid_dataset, batch_size<span class="op">=</span>batch_size, shuffle<span class="op">=</span><span class="va">False</span>, collate_fn<span class="op">=</span>collate_batch</span>
<span id="cb71-8"><a href="#cb71-8" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb71-9"><a href="#cb71-9" aria-hidden="true" tabindex="-1"></a>test_dl <span class="op">=</span> DataLoader(</span>
<span id="cb71-10"><a href="#cb71-10" aria-hidden="true" tabindex="-1"></a>    test_dataset_raw, batch_size<span class="op">=</span>batch_size, shuffle<span class="op">=</span><span class="va">False</span>, collate_fn<span class="op">=</span>collate_batch</span>
<span id="cb71-11"><a href="#cb71-11" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="define-model-training-and-evaluation-pipelines" class="level2">
<h2 class="anchored" data-anchor-id="define-model-training-and-evaluation-pipelines">Define model training and evaluation pipelines</h2>
<p>I have defined two simple functions to train and evaluate the model in this section.</p>
<div class="cell" data-execution_count="39">
<div class="sourceCode cell-code" id="cb72"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb72-1"><a href="#cb72-1" aria-hidden="true" tabindex="-1"></a><span class="co">##</span></span>
<span id="cb72-2"><a href="#cb72-2" aria-hidden="true" tabindex="-1"></a><span class="co"># model training pipeline</span></span>
<span id="cb72-3"><a href="#cb72-3" aria-hidden="true" tabindex="-1"></a><span class="co"># https://github.com/rasbt/machine-learning-book/blob/main/ch15/ch15_part2.ipynb</span></span>
<span id="cb72-4"><a href="#cb72-4" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> train(dataloader):</span>
<span id="cb72-5"><a href="#cb72-5" aria-hidden="true" tabindex="-1"></a>    model.train()</span>
<span id="cb72-6"><a href="#cb72-6" aria-hidden="true" tabindex="-1"></a>    total_acc, total_loss <span class="op">=</span> <span class="dv">0</span>, <span class="dv">0</span></span>
<span id="cb72-7"><a href="#cb72-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> text_batch, label_batch, lengths <span class="kw">in</span> dataloader:</span>
<span id="cb72-8"><a href="#cb72-8" aria-hidden="true" tabindex="-1"></a>        optimizer.zero_grad()</span>
<span id="cb72-9"><a href="#cb72-9" aria-hidden="true" tabindex="-1"></a>        pred <span class="op">=</span> model(text_batch, lengths)[:, <span class="dv">0</span>]</span>
<span id="cb72-10"><a href="#cb72-10" aria-hidden="true" tabindex="-1"></a>        loss <span class="op">=</span> loss_fn(pred, label_batch)</span>
<span id="cb72-11"><a href="#cb72-11" aria-hidden="true" tabindex="-1"></a>        loss.backward()</span>
<span id="cb72-12"><a href="#cb72-12" aria-hidden="true" tabindex="-1"></a>        optimizer.step()</span>
<span id="cb72-13"><a href="#cb72-13" aria-hidden="true" tabindex="-1"></a>        total_acc <span class="op">+=</span> ((pred <span class="op">&gt;=</span> <span class="fl">0.5</span>).<span class="bu">float</span>() <span class="op">==</span> label_batch).<span class="bu">float</span>().<span class="bu">sum</span>().item()</span>
<span id="cb72-14"><a href="#cb72-14" aria-hidden="true" tabindex="-1"></a>        total_loss <span class="op">+=</span> loss.item() <span class="op">*</span> label_batch.size(<span class="dv">0</span>)</span>
<span id="cb72-15"><a href="#cb72-15" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> total_acc <span class="op">/</span> <span class="bu">len</span>(dataloader.dataset), total_loss <span class="op">/</span> <span class="bu">len</span>(dataloader.dataset)</span>
<span id="cb72-16"><a href="#cb72-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-17"><a href="#cb72-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-18"><a href="#cb72-18" aria-hidden="true" tabindex="-1"></a><span class="co"># model evaluation pipeline</span></span>
<span id="cb72-19"><a href="#cb72-19" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> evaluate(dataloader):</span>
<span id="cb72-20"><a href="#cb72-20" aria-hidden="true" tabindex="-1"></a>    model.<span class="bu">eval</span>()</span>
<span id="cb72-21"><a href="#cb72-21" aria-hidden="true" tabindex="-1"></a>    total_acc, total_loss <span class="op">=</span> <span class="dv">0</span>, <span class="dv">0</span></span>
<span id="cb72-22"><a href="#cb72-22" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> torch.no_grad():</span>
<span id="cb72-23"><a href="#cb72-23" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> text_batch, label_batch, lengths <span class="kw">in</span> dataloader:</span>
<span id="cb72-24"><a href="#cb72-24" aria-hidden="true" tabindex="-1"></a>            pred <span class="op">=</span> model(text_batch, lengths)[:, <span class="dv">0</span>]</span>
<span id="cb72-25"><a href="#cb72-25" aria-hidden="true" tabindex="-1"></a>            loss <span class="op">=</span> loss_fn(pred, label_batch)</span>
<span id="cb72-26"><a href="#cb72-26" aria-hidden="true" tabindex="-1"></a>            total_acc <span class="op">+=</span> ((pred <span class="op">&gt;=</span> <span class="fl">0.5</span>).<span class="bu">float</span>() <span class="op">==</span> label_batch).<span class="bu">float</span>().<span class="bu">sum</span>().item()</span>
<span id="cb72-27"><a href="#cb72-27" aria-hidden="true" tabindex="-1"></a>            total_loss <span class="op">+=</span> loss.item() <span class="op">*</span> label_batch.size(<span class="dv">0</span>)</span>
<span id="cb72-28"><a href="#cb72-28" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> total_acc <span class="op">/</span> <span class="bu">len</span>(dataloader.dataset), total_loss <span class="op">/</span> <span class="bu">len</span>(dataloader.dataset)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="rnn-model-configuration-loss-function-and-optimizer" class="level2">
<h2 class="anchored" data-anchor-id="rnn-model-configuration-loss-function-and-optimizer">RNN model configuration, loss function, and optimizer</h2>
<p>We have seen the review text, which can be long sequences. We will use the LSTM layer for capturing the long-term dependencies. Our sentiment analysis model is composed of the following layers * Start with an <strong>Embedding layer</strong>. Placing the embedding layer is similar to one-hot-encoding, where each word token is converted to a separate feature (or vector or column). But this can lead to too many features (curse of dimensionality or dimensional explosion). To avoid this, we try to map tokens to fixed-size vectors (or columns). In such a feature matrix, different elements denote different tokens. Tokens that are closed are also placed together. Further, during training, we also learn and update the positioning of tokens. Similar tokens are placed into closer and closer locations. Such a matrix layer is termed an embedding layer. * After the embedding layer, there is the RNN layer (LSTM to be specific). * Then we have a fully connected layer followed by activation and another fully connected layer. * Finally, we have a logistic sigmoid layer for prediction</p>
<div class="cell" data-execution_count="40">
<div class="sourceCode cell-code" id="cb73"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb73-1"><a href="#cb73-1" aria-hidden="true" tabindex="-1"></a><span class="co">##</span></span>
<span id="cb73-2"><a href="#cb73-2" aria-hidden="true" tabindex="-1"></a><span class="co"># https://github.com/rasbt/machine-learning-book/blob/main/ch15/ch15_part2.ipynb</span></span>
<span id="cb73-3"><a href="#cb73-3" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> RNN(nn.Module):</span>
<span id="cb73-4"><a href="#cb73-4" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, vocab_size, embed_dim, rnn_hidden_size, fc_hidden_size):</span>
<span id="cb73-5"><a href="#cb73-5" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb73-6"><a href="#cb73-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.embedding <span class="op">=</span> nn.Embedding(vocab_size, embed_dim, padding_idx<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb73-7"><a href="#cb73-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.rnn <span class="op">=</span> nn.LSTM(embed_dim, rnn_hidden_size, batch_first<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb73-8"><a href="#cb73-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.fc1 <span class="op">=</span> nn.Linear(rnn_hidden_size, fc_hidden_size)</span>
<span id="cb73-9"><a href="#cb73-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.relu <span class="op">=</span> nn.ReLU()</span>
<span id="cb73-10"><a href="#cb73-10" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.fc2 <span class="op">=</span> nn.Linear(fc_hidden_size, <span class="dv">1</span>)</span>
<span id="cb73-11"><a href="#cb73-11" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.sigmoid <span class="op">=</span> nn.Sigmoid()</span>
<span id="cb73-12"><a href="#cb73-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-13"><a href="#cb73-13" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, text, lengths):</span>
<span id="cb73-14"><a href="#cb73-14" aria-hidden="true" tabindex="-1"></a>        out <span class="op">=</span> <span class="va">self</span>.embedding(text)</span>
<span id="cb73-15"><a href="#cb73-15" aria-hidden="true" tabindex="-1"></a>        out <span class="op">=</span> nn.utils.rnn.pack_padded_sequence(</span>
<span id="cb73-16"><a href="#cb73-16" aria-hidden="true" tabindex="-1"></a>            out, lengths.cpu().numpy(), enforce_sorted<span class="op">=</span><span class="va">False</span>, batch_first<span class="op">=</span><span class="va">True</span></span>
<span id="cb73-17"><a href="#cb73-17" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb73-18"><a href="#cb73-18" aria-hidden="true" tabindex="-1"></a>        out, (hidden, cell) <span class="op">=</span> <span class="va">self</span>.rnn(out)</span>
<span id="cb73-19"><a href="#cb73-19" aria-hidden="true" tabindex="-1"></a>        out <span class="op">=</span> hidden[<span class="op">-</span><span class="dv">1</span>, :, :]</span>
<span id="cb73-20"><a href="#cb73-20" aria-hidden="true" tabindex="-1"></a>        out <span class="op">=</span> <span class="va">self</span>.fc1(out)</span>
<span id="cb73-21"><a href="#cb73-21" aria-hidden="true" tabindex="-1"></a>        out <span class="op">=</span> <span class="va">self</span>.relu(out)</span>
<span id="cb73-22"><a href="#cb73-22" aria-hidden="true" tabindex="-1"></a>        out <span class="op">=</span> <span class="va">self</span>.fc2(out)</span>
<span id="cb73-23"><a href="#cb73-23" aria-hidden="true" tabindex="-1"></a>        out <span class="op">=</span> <span class="va">self</span>.sigmoid(out)</span>
<span id="cb73-24"><a href="#cb73-24" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> out</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="41">
<div class="sourceCode cell-code" id="cb74"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb74-1"><a href="#cb74-1" aria-hidden="true" tabindex="-1"></a>vocab_size <span class="op">=</span> <span class="bu">len</span>(vb)</span>
<span id="cb74-2"><a href="#cb74-2" aria-hidden="true" tabindex="-1"></a>embed_dim <span class="op">=</span> <span class="dv">20</span></span>
<span id="cb74-3"><a href="#cb74-3" aria-hidden="true" tabindex="-1"></a>rnn_hidden_size <span class="op">=</span> <span class="dv">64</span></span>
<span id="cb74-4"><a href="#cb74-4" aria-hidden="true" tabindex="-1"></a>fc_hidden_size <span class="op">=</span> <span class="dv">64</span></span>
<span id="cb74-5"><a href="#cb74-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-6"><a href="#cb74-6" aria-hidden="true" tabindex="-1"></a>torch.manual_seed(<span class="dv">1</span>)</span>
<span id="cb74-7"><a href="#cb74-7" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> RNN(vocab_size, embed_dim, rnn_hidden_size, fc_hidden_size)</span>
<span id="cb74-8"><a href="#cb74-8" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> model.to(device)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<section id="define-model-loss-function-and-optimizer" class="level3">
<h3 class="anchored" data-anchor-id="define-model-loss-function-and-optimizer">Define model loss function and optimizer</h3>
<p>For loss function (or criterion), I have used <a href="https://pytorch.org/docs/stable/generated/torch.nn.BCELoss.html">Binary Cross Entropy</a>, and for loss optimization, I have used <a href="https://pytorch.org/docs/stable/generated/torch.optim.Adam.html">Adam algorithm</a></p>
<div class="cell" data-execution_count="42">
<div class="sourceCode cell-code" id="cb75"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb75-1"><a href="#cb75-1" aria-hidden="true" tabindex="-1"></a>torch.manual_seed(<span class="dv">1</span>)</span>
<span id="cb75-2"><a href="#cb75-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb75-3"><a href="#cb75-3" aria-hidden="true" tabindex="-1"></a>loss_fn <span class="op">=</span> nn.BCELoss()</span>
<span id="cb75-4"><a href="#cb75-4" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> torch.optim.Adam(model.parameters(), lr<span class="op">=</span><span class="fl">0.001</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
</section>
<section id="model-training-and-evaluation" class="level2">
<h2 class="anchored" data-anchor-id="model-training-and-evaluation">Model training and evaluation</h2>
<p>Let’s run the pipeline for ten epochs and compare the training and validation accuracy.</p>
<div class="cell" data-outputid="3c53356b-77ab-49e2-92ff-5e8086d3b9c2" data-execution_count="43">
<div class="sourceCode cell-code" id="cb76"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb76-1"><a href="#cb76-1" aria-hidden="true" tabindex="-1"></a>num_epochs <span class="op">=</span> <span class="dv">10</span></span>
<span id="cb76-2"><a href="#cb76-2" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(num_epochs):</span>
<span id="cb76-3"><a href="#cb76-3" aria-hidden="true" tabindex="-1"></a>    acc_train, loss_train <span class="op">=</span> train(train_dl)</span>
<span id="cb76-4"><a href="#cb76-4" aria-hidden="true" tabindex="-1"></a>    acc_valid, loss_valid <span class="op">=</span> evaluate(valid_dl)</span>
<span id="cb76-5"><a href="#cb76-5" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(</span>
<span id="cb76-6"><a href="#cb76-6" aria-hidden="true" tabindex="-1"></a>        <span class="ss">f"Epoch </span><span class="sc">{</span>epoch<span class="sc">}</span><span class="ss"> train accuracy: </span><span class="sc">{</span>acc_train<span class="sc">:.4f}</span><span class="ss">; val accuracy: </span><span class="sc">{</span>acc_valid<span class="sc">:.4f}</span><span class="ss">"</span></span>
<span id="cb76-7"><a href="#cb76-7" aria-hidden="true" tabindex="-1"></a>    )</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 0 train accuracy: 0.6085; val accuracy: 0.6502
Epoch 1 train accuracy: 0.7206; val accuracy: 0.7462
Epoch 2 train accuracy: 0.7613; val accuracy: 0.6250
Epoch 3 train accuracy: 0.8235; val accuracy: 0.8232
Epoch 4 train accuracy: 0.8819; val accuracy: 0.8482
Epoch 5 train accuracy: 0.9132; val accuracy: 0.8526
Epoch 6 train accuracy: 0.9321; val accuracy: 0.8374
Epoch 7 train accuracy: 0.9504; val accuracy: 0.8502
Epoch 8 train accuracy: 0.9643; val accuracy: 0.8608
Epoch 9 train accuracy: 0.9747; val accuracy: 0.8636</code></pre>
</div>
</div>
<section id="evaluate-sentiments-on-random-texts" class="level3">
<h3 class="anchored" data-anchor-id="evaluate-sentiments-on-random-texts">Evaluate sentiments on random texts</h3>
<p>Let’s create another helper method to evaluate sentiments on random texts.</p>
<div class="cell" data-execution_count="44">
<div class="sourceCode cell-code" id="cb78"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb78-1"><a href="#cb78-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> classify_review(text):</span>
<span id="cb78-2"><a href="#cb78-2" aria-hidden="true" tabindex="-1"></a>    text_list, lengths <span class="op">=</span> [], []</span>
<span id="cb78-3"><a href="#cb78-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb78-4"><a href="#cb78-4" aria-hidden="true" tabindex="-1"></a>    <span class="co"># process review text with text_pipeline</span></span>
<span id="cb78-5"><a href="#cb78-5" aria-hidden="true" tabindex="-1"></a>    <span class="co"># note: "text_pipeline" has dependency on data vocabulary</span></span>
<span id="cb78-6"><a href="#cb78-6" aria-hidden="true" tabindex="-1"></a>    processed_text <span class="op">=</span> torch.tensor(text_pipeline(text), dtype<span class="op">=</span>torch.int64)</span>
<span id="cb78-7"><a href="#cb78-7" aria-hidden="true" tabindex="-1"></a>    text_list.append(processed_text)</span>
<span id="cb78-8"><a href="#cb78-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb78-9"><a href="#cb78-9" aria-hidden="true" tabindex="-1"></a>    <span class="co"># get processed review tokens length</span></span>
<span id="cb78-10"><a href="#cb78-10" aria-hidden="true" tabindex="-1"></a>    lengths.append(processed_text.size(<span class="dv">0</span>))</span>
<span id="cb78-11"><a href="#cb78-11" aria-hidden="true" tabindex="-1"></a>    lengths <span class="op">=</span> torch.tensor(lengths)</span>
<span id="cb78-12"><a href="#cb78-12" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb78-13"><a href="#cb78-13" aria-hidden="true" tabindex="-1"></a>    <span class="co"># change the dimensions from (torch.Size([8]), torch.Size([1, 8]))</span></span>
<span id="cb78-14"><a href="#cb78-14" aria-hidden="true" tabindex="-1"></a>    <span class="co"># nn.utils.rnn.pad_sequence(text_list, batch_first=True) does this too</span></span>
<span id="cb78-15"><a href="#cb78-15" aria-hidden="true" tabindex="-1"></a>    padded_text_list <span class="op">=</span> torch.unsqueeze(processed_text, <span class="dv">0</span>)</span>
<span id="cb78-16"><a href="#cb78-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb78-17"><a href="#cb78-17" aria-hidden="true" tabindex="-1"></a>    <span class="co"># move tensors to correct device</span></span>
<span id="cb78-18"><a href="#cb78-18" aria-hidden="true" tabindex="-1"></a>    padded_text_list <span class="op">=</span> padded_text_list.to(device)</span>
<span id="cb78-19"><a href="#cb78-19" aria-hidden="true" tabindex="-1"></a>    lengths <span class="op">=</span> lengths.to(device)</span>
<span id="cb78-20"><a href="#cb78-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb78-21"><a href="#cb78-21" aria-hidden="true" tabindex="-1"></a>    <span class="co"># get prediction</span></span>
<span id="cb78-22"><a href="#cb78-22" aria-hidden="true" tabindex="-1"></a>    model.<span class="bu">eval</span>()</span>
<span id="cb78-23"><a href="#cb78-23" aria-hidden="true" tabindex="-1"></a>    pred <span class="op">=</span> model(padded_text_list, lengths)</span>
<span id="cb78-24"><a href="#cb78-24" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"model pred: "</span>, pred)</span>
<span id="cb78-25"><a href="#cb78-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb78-26"><a href="#cb78-26" aria-hidden="true" tabindex="-1"></a>    <span class="co"># positive or negative review</span></span>
<span id="cb78-27"><a href="#cb78-27" aria-hidden="true" tabindex="-1"></a>    review_class <span class="op">=</span> <span class="st">'negative'</span> <span class="co"># else case</span></span>
<span id="cb78-28"><a href="#cb78-28" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> (pred<span class="op">&gt;=</span><span class="fl">0.5</span>) <span class="op">==</span> <span class="dv">1</span>:</span>
<span id="cb78-29"><a href="#cb78-29" aria-hidden="true" tabindex="-1"></a>        review_class <span class="op">=</span> <span class="st">"positive"</span></span>
<span id="cb78-30"><a href="#cb78-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb78-31"><a href="#cb78-31" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"review type: "</span>, review_class)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="45">
<div class="sourceCode cell-code" id="cb79"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb79-1"><a href="#cb79-1" aria-hidden="true" tabindex="-1"></a><span class="co">##</span></span>
<span id="cb79-2"><a href="#cb79-2" aria-hidden="true" tabindex="-1"></a><span class="co"># create two random texts with strong positive and negative sentiments</span></span>
<span id="cb79-3"><a href="#cb79-3" aria-hidden="true" tabindex="-1"></a>pos_review <span class="op">=</span> <span class="st">'i love this movie. it was so good.'</span></span>
<span id="cb79-4"><a href="#cb79-4" aria-hidden="true" tabindex="-1"></a>neg_review <span class="op">=</span> <span class="st">'slow and boring. waste of time.'</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-outputid="0f2c5b34-3ce2-4304-da0a-30855050c1dd" data-execution_count="46">
<div class="sourceCode cell-code" id="cb80"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb80-1"><a href="#cb80-1" aria-hidden="true" tabindex="-1"></a>classify_review(pos_review)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>model pred:  tensor([[0.9388]], device='cuda:0', grad_fn=&lt;SigmoidBackward0&gt;)
review type:  positive</code></pre>
</div>
</div>
<div class="cell" data-outputid="2dee82f4-b8fa-44f8-d5aa-ffddc54488d9" data-execution_count="47">
<div class="sourceCode cell-code" id="cb82"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb82-1"><a href="#cb82-1" aria-hidden="true" tabindex="-1"></a>classify_review(neg_review)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>model pred:  tensor([[0.0057]], device='cuda:0', grad_fn=&lt;SigmoidBackward0&gt;)
review type:  negative</code></pre>
</div>
</div>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  let localAlternateSentinel = 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>