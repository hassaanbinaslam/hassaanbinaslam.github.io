<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.40">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="dcterms.date" content="2022-11-07">
<meta name="description" content="This is a practice notebook to understand and build models for time series data. We will explore some popular neural network architectures including RNN, GRU, LSTM, and 1D CNN.">

<title>Build Temporal Models for Univariate Time series Data with RNN, GRU, LSTM, CNN using PyTorch – Random Thoughts</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script><script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../images/favicon.ico" rel="icon">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-549806ee2085284f45b00abea8c6df48.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap-84543be43ff612bda7a31c913735130b.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-D1ST9BH6HX"></script>

<script type="text/javascript">

window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-D1ST9BH6HX', { 'anonymize_ip': true});
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" integrity="sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==" crossorigin="anonymous"></script>

<script type="application/javascript">define('jquery', [],function() {return window.jQuery;})</script>


<link rel="stylesheet" href="../styles.css">
<meta property="og:title" content="Build Temporal Models for Univariate Time series Data with RNN, GRU, LSTM, CNN using PyTorch – Random Thoughts">
<meta property="og:description" content="This is a practice notebook to understand and build models for time series data. We will explore some popular neural network architectures including RNN, GRU, LSTM, and 1D CNN.">
<meta property="og:image" content="images/2022-11-07-timeseries-rnn-gru-lstm-cnn-pytorch.jpeg">
<meta property="og:site_name" content="Random Thoughts">
<meta name="twitter:title" content="Build Temporal Models for Univariate Time series Data with RNN, GRU, LSTM, CNN using PyTorch – Random Thoughts">
<meta name="twitter:description" content="This is a practice notebook to understand and build models for time series data. We will explore some popular neural network architectures including RNN, GRU, LSTM, and 1D CNN.">
<meta name="twitter:image" content="images/2022-11-07-timeseries-rnn-gru-lstm-cnn-pytorch.jpeg">
<meta name="twitter:card" content="summary_large_image">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../index.html">
    <span class="navbar-title">Random Thoughts</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../about.html"> 
<span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://www.linkedin.com/in/hassaanbinaslam/"> <i class="bi bi-linkedin" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/hassaanbinaslam/"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com/hassaanbinaslam"> <i class="bi bi-twitter" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#credits" id="toc-credits" class="nav-link active" data-scroll-target="#credits">Credits</a></li>
  <li><a href="#environment" id="toc-environment" class="nav-link" data-scroll-target="#environment">Environment</a></li>
  <li><a href="#introduction" id="toc-introduction" class="nav-link" data-scroll-target="#introduction">Introduction</a></li>
  <li><a href="#section-i" id="toc-section-i" class="nav-link" data-scroll-target="#section-i">Section I</a>
  <ul class="collapse">
  <li><a href="#data-generation" id="toc-data-generation" class="nav-link" data-scroll-target="#data-generation">Data generation</a></li>
  <li><a href="#data-preparation" id="toc-data-preparation" class="nav-link" data-scroll-target="#data-preparation">Data preparation</a></li>
  <li><a href="#load-generated-data-into-pytorch-dataset-and-dataloader-class" id="toc-load-generated-data-into-pytorch-dataset-and-dataloader-class" class="nav-link" data-scroll-target="#load-generated-data-into-pytorch-dataset-and-dataloader-class">Load generated data into PyTorch Dataset and DataLoader class</a></li>
  <li><a href="#define-a-class-to-implement-training-validation-and-mini-batch-processing-pipeline" id="toc-define-a-class-to-implement-training-validation-and-mini-batch-processing-pipeline" class="nav-link" data-scroll-target="#define-a-class-to-implement-training-validation-and-mini-batch-processing-pipeline">Define a class to implement training, validation, and mini-batch processing pipeline</a></li>
  <li><a href="#model-configuration-and-training" id="toc-model-configuration-and-training" class="nav-link" data-scroll-target="#model-configuration-and-training">Model configuration and training</a></li>
  </ul></li>
  <li><a href="#section-ii" id="toc-section-ii" class="nav-link" data-scroll-target="#section-ii">Section II</a>
  <ul class="collapse">
  <li><a href="#data-preparation-1" id="toc-data-preparation-1" class="nav-link" data-scroll-target="#data-preparation-1">Data preparation</a></li>
  <li><a href="#recurrent-neural-network-rnn" id="toc-recurrent-neural-network-rnn" class="nav-link" data-scroll-target="#recurrent-neural-network-rnn">Recurrent Neural Network (RNN)</a>
  <ul class="collapse">
  <li><a href="#rnn-cell-in-detail" id="toc-rnn-cell-in-detail" class="nav-link" data-scroll-target="#rnn-cell-in-detail">RNN cell in detail</a></li>
  </ul></li>
  <li><a href="#gated-recurrent-units-gru" id="toc-gated-recurrent-units-gru" class="nav-link" data-scroll-target="#gated-recurrent-units-gru">Gated Recurrent Units (GRU)</a>
  <ul class="collapse">
  <li><a href="#gru-cell-in-detail" id="toc-gru-cell-in-detail" class="nav-link" data-scroll-target="#gru-cell-in-detail">GRU cell in detail</a></li>
  </ul></li>
  <li><a href="#long-short-term-memory-lstm" id="toc-long-short-term-memory-lstm" class="nav-link" data-scroll-target="#long-short-term-memory-lstm">Long Short Term Memory (LSTM)</a>
  <ul class="collapse">
  <li><a href="#lstm-cell-in-detail" id="toc-lstm-cell-in-detail" class="nav-link" data-scroll-target="#lstm-cell-in-detail">LSTM cell in detail</a></li>
  </ul></li>
  <li><a href="#one-dimensional-convolutional-neural-network-1d-convnet" id="toc-one-dimensional-convolutional-neural-network-1d-convnet" class="nav-link" data-scroll-target="#one-dimensional-convolutional-neural-network-1d-convnet">One Dimensional Convolutional Neural Network (1D ConvNet)</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Build Temporal Models for Univariate Time series Data with RNN, GRU, LSTM, CNN using PyTorch</h1>
  <div class="quarto-categories">
    <div class="quarto-category">pytorch</div>
    <div class="quarto-category">dl</div>
  </div>
  </div>

<div>
  <div class="description">
    This is a practice notebook to understand and build models for time series data. We will explore some popular neural network architectures including RNN, GRU, LSTM, and 1D CNN.
  </div>
</div>


<div class="quarto-title-meta">

    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">November 7, 2022</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<p><img src="images/2022-11-07-timeseries-rnn-gru-lstm-cnn-pytorch.jpeg" class="img-fluid"></p>
<section id="credits" class="level2">
<h2 class="anchored" data-anchor-id="credits">Credits</h2>
<p>This notebook takes inspiration and ideas from the following sources.</p>
<ul>
<li>The outstanding book “Deep Learning with PyTorch Step-by-Step” by “Daniel Voigt Godoy”. You can get the book from its website: <a href="https://pytorchstepbystep.com/">pytorchstepbystep</a>. In addition, the GitHub repository for this book has valuable notebooks: <a href="https://github.com/dvgodoy/PyTorchStepByStep">github.com/dvgodoy/PyTorchStepByStep</a>. Parts of the code you see in this notebook are taken from <a href="https://colab.research.google.com/github/dvgodoy/PyTorchStepByStep/blob/master/Chapter03.ipynb">chapter 3</a> and <a href="https://github.com/dvgodoy/PyTorchStepByStep/blob/master/Chapter08.ipynb">chapter 8</a> notebooks of the same book.</li>
<li>Very helpful Kaggle notebook from ‘TARON ZAKARYAN’ to predict stock prices using LSTM. <a href="https://www.kaggle.com/code/taronzakaryan/predicting-stock-price-using-lstm-model-pytorch/notebook">Link here</a></li>
</ul>
</section>
<section id="environment" class="level2">
<h2 class="anchored" data-anchor-id="environment">Environment</h2>
<p>This notebook is prepared with Google Colab.</p>
<ul>
<li><strong>GitHub</strong>: <a href="https://github.com/hassaanbinaslam/myblog/blob/main/posts/2022-11-07-timeseries-rnn-gru-lstm-cnn-pytorch.ipynb">2022-11-07-timeseries-rnn-gru-lstm-cnn-pytorch.ipynb</a></li>
<li><strong>Open In Colab</strong>: <a href="https://colab.research.google.com/github/hassaanbinaslam/myblog/blob/main/posts/2022-11-07-timeseries-rnn-gru-lstm-cnn-pytorch.ipynb"><img src="https://colab.research.google.com/assets/colab-badge.svg" class="img-fluid" alt="Open In Colab"></a></li>
</ul>
<div id="cell-4" class="cell" data-outputid="9398dac5-fcad-4e91-a8ba-28a7d71831e2" data-execution_count="1">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1"></a><span class="im">from</span> platform <span class="im">import</span> python_version</span>
<span id="cb1-2"><a href="#cb1-2"></a><span class="im">import</span> numpy, matplotlib, pandas, torch, seaborn</span>
<span id="cb1-3"><a href="#cb1-3"></a></span>
<span id="cb1-4"><a href="#cb1-4"></a><span class="bu">print</span>(<span class="st">"python=="</span> <span class="op">+</span> python_version())</span>
<span id="cb1-5"><a href="#cb1-5"></a><span class="bu">print</span>(<span class="st">"numpy=="</span> <span class="op">+</span> numpy.__version__)</span>
<span id="cb1-6"><a href="#cb1-6"></a><span class="bu">print</span>(<span class="st">"torch=="</span> <span class="op">+</span> torch.__version__)</span>
<span id="cb1-7"><a href="#cb1-7"></a><span class="bu">print</span>(<span class="st">"matplotlib=="</span> <span class="op">+</span> matplotlib.__version__)</span>
<span id="cb1-8"><a href="#cb1-8"></a><span class="bu">print</span>(<span class="st">"seaborn=="</span> <span class="op">+</span> seaborn.__version__)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>python==3.7.15
numpy==1.21.6
torch==1.12.1+cu113
matplotlib==3.2.2
seaborn==0.11.2</code></pre>
</div>
</div>
</section>
<section id="introduction" class="level2">
<h2 class="anchored" data-anchor-id="introduction">Introduction</h2>
<p>Recurrent Neural Network (RNN) is great for exploiting data that involves one-dimensional (1D) ordered structures. We call these 1D-ordered structures <code>sequences</code>. Two main sequence problems are <code>Time series</code> and <code>Natural Language Processing (NLP)</code>. RNN and its variants are developed to work for both types of sequence problems, but in this notebook we will only deal with time series sequences.</p>
<p>I have divided this notebook into two sections. In the first section, our focus will be on understanding the structure of sequences and generating training sets and batches from them. We will develop a simple (synthetic) sequence data and then create its training set. Next, we will make batches using PyTorch DataLoaders and write a training pipeline. We will end this section by training an RNN on this data.</p>
<p>In the next section, our focus will be more on the internals of different neural architectures for sequence data problems. We will use stock price data and train multiple networks (RNN, GRU, LSTM, CNN) on it while understanding their features and behavior.</p>
</section>
<section id="section-i" class="level1">
<h1>Section I</h1>
<p>This section focuses on understanding the structure of one-dimensional ordered sequences and generating training sets from them.</p>
<section id="data-generation" class="level2">
<h2 class="anchored" data-anchor-id="data-generation">Data generation</h2>
<p>Let’s generate some one dimensional ordered sequence data.</p>
<div id="cell-8" class="cell" data-outputid="fe194f58-4b4b-429c-d6aa-c16edecfa608" data-execution_count="2">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1"></a><span class="co"># from numpy.ma.core import size</span></span>
<span id="cb3-2"><a href="#cb3-2"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb3-3"><a href="#cb3-3"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb3-4"><a href="#cb3-4"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb3-5"><a href="#cb3-5"></a></span>
<span id="cb3-6"><a href="#cb3-6"></a><span class="co"># generate 1000 data points</span></span>
<span id="cb3-7"><a href="#cb3-7"></a>n_points <span class="op">=</span> <span class="dv">1000</span></span>
<span id="cb3-8"><a href="#cb3-8"></a>noise <span class="op">=</span> <span class="fl">0.04</span></span>
<span id="cb3-9"><a href="#cb3-9"></a></span>
<span id="cb3-10"><a href="#cb3-10"></a>X_synth <span class="op">=</span> np.arange(<span class="dv">1</span>, n_points <span class="op">+</span> <span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb3-11"><a href="#cb3-11"></a>y_synth <span class="op">=</span> np.sin(X_synth <span class="op">*</span> np.pi <span class="op">/</span> <span class="dv">180</span>) <span class="op">+</span> np.random.randn(n_points) <span class="op">*</span> noise</span>
<span id="cb3-12"><a href="#cb3-12"></a></span>
<span id="cb3-13"><a href="#cb3-13"></a>df_synth <span class="op">=</span> pd.DataFrame(y_synth, index<span class="op">=</span>X_synth)</span>
<span id="cb3-14"><a href="#cb3-14"></a></span>
<span id="cb3-15"><a href="#cb3-15"></a><span class="co"># plot timeseries data</span></span>
<span id="cb3-16"><a href="#cb3-16"></a>df_synth.plot(figsize<span class="op">=</span>(<span class="dv">15</span>, <span class="dv">6</span>))</span>
<span id="cb3-17"><a href="#cb3-17"></a>plt.ylabel(<span class="st">"value"</span>)</span>
<span id="cb3-18"><a href="#cb3-18"></a>plt.xlabel(<span class="st">"step"</span>)</span>
<span id="cb3-19"><a href="#cb3-19"></a>plt.title(<span class="st">"Synthetic time series data"</span>)</span>
<span id="cb3-20"><a href="#cb3-20"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="2022-11-07-timeseries-rnn-gru-lstm-cnn-pytorch_files/figure-html/cell-3-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>In the above plot, <code>X</code> dimension represents the time or steps. And <code>y</code> dimension represents the measurements. In actual data, these measurements could represent price stocks, temperature, population, etc. If we print our DataFrame, it has only one column which shows the measurements. The DataFrame index represents the time dimension.</p>
<div id="cell-10" class="cell" data-outputid="07b0e446-934e-4d89-903c-e9707a153116" data-execution_count="3">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1"></a>df_synth.head()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="3">

  <div id="df-80d1ed8a-1b5a-4a40-9d2e-8ea4dad74c35">
    <div class="colab-df-container">
      <div>


<table class="dataframe caption-top table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">0</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">1</td>
<td>0.031713</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">2</td>
<td>-0.000675</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">3</td>
<td>0.026890</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">4</td>
<td>0.087844</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">5</td>
<td>0.057978</td>
</tr>
</tbody>
</table>

</div>
      <button class="colab-df-convert" onclick="convertToInteractive('df-80d1ed8a-1b5a-4a40-9d2e-8ea4dad74c35')" title="Convert this dataframe to an interactive table." style="display:none;">
        
  <svg xmlns="http://www.w3.org/2000/svg" height="24px" viewbox="0 0 24 24" width="24px">
    <path d="M0 0h24v24H0V0z" fill="none"></path>
    <path d="M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z"></path><path d="M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z"></path>
  </svg>
      </button>
      
  <style>
    .colab-df-container {
      display:flex;
      flex-wrap:wrap;
      gap: 12px;
    }

    .colab-df-convert {
      background-color: #E8F0FE;
      border: none;
      border-radius: 50%;
      cursor: pointer;
      display: none;
      fill: #1967D2;
      height: 32px;
      padding: 0 0 0 0;
      width: 32px;
    }

    .colab-df-convert:hover {
      background-color: #E2EBFA;
      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);
      fill: #174EA6;
    }

    [theme=dark] .colab-df-convert {
      background-color: #3B4455;
      fill: #D2E3FC;
    }

    [theme=dark] .colab-df-convert:hover {
      background-color: #434B5C;
      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);
      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));
      fill: #FFFFFF;
    }
  </style>

      <script>
        const buttonEl =
          document.querySelector('#df-80d1ed8a-1b5a-4a40-9d2e-8ea4dad74c35 button.colab-df-convert');
        buttonEl.style.display =
          google.colab.kernel.accessAllowed ? 'block' : 'none';

        async function convertToInteractive(key) {
          const element = document.querySelector('#df-80d1ed8a-1b5a-4a40-9d2e-8ea4dad74c35');
          const dataTable =
            await google.colab.kernel.invokeFunction('convertToInteractive',
                                                     [key], {});
          if (!dataTable) return;

          const docLinkHtml = 'Like what you see? Visit the ' +
            '<a target="_blank" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'
            + ' to learn more about interactive tables.';
          element.innerHTML = '';
          dataTable['output_type'] = 'display_data';
          await google.colab.output.renderOutput(dataTable, element);
          const docLink = document.createElement('div');
          docLink.innerHTML = docLinkHtml;
          element.appendChild(docLink);
        }
      </script>
    </div>
  </div>
  
</div>
</div>
</section>
<section id="data-preparation" class="level2">
<h2 class="anchored" data-anchor-id="data-preparation">Data preparation</h2>
<p>Notice that our data does not have any labels. We usually have features and labels to train our model in supervised learning problems. However, sequence data is unique as we try to predict the next value from the sequence data itself. Therefore, we don’t have to provide labels with our data separately but can generate them from the sequence itself.</p>
<p>Let’s use a simple ordered sequence of 15 integers to understand how the training set is created from it.</p>
<div id="cell-12" class="cell" data-outputid="ab1cdeb2-4240-4774-f79b-a359e33ca8a8" data-execution_count="4">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1"></a><span class="co">##</span></span>
<span id="cb5-2"><a href="#cb5-2"></a><span class="co"># generate a simple sequential data of 15 integers</span></span>
<span id="cb5-3"><a href="#cb5-3"></a>data_dummy <span class="op">=</span> np.arange(<span class="dv">15</span>)</span>
<span id="cb5-4"><a href="#cb5-4"></a><span class="bu">print</span>(data_dummy)</span>
<span id="cb5-5"><a href="#cb5-5"></a></span>
<span id="cb5-6"><a href="#cb5-6"></a><span class="co"># create a DataFrame of this sequence</span></span>
<span id="cb5-7"><a href="#cb5-7"></a>df_dummy <span class="op">=</span> pd.DataFrame(data_dummy)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14]</code></pre>
</div>
</div>
<p>We can split this long sequence into multiple smaller sequences (as training and test sets). The earlier part of the sequence will contain training features, and the last element acts as a label.</p>
<p>I have created a helper function that will take a sequence DataFrame, and split it into training features and labels.</p>
<div id="cell-14" class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1"></a><span class="co">##</span></span>
<span id="cb7-2"><a href="#cb7-2"></a><span class="co"># create a function to generate multiple sequences for training and testing</span></span>
<span id="cb7-3"><a href="#cb7-3"></a><span class="co"># look_back = size of the generated sets</span></span>
<span id="cb7-4"><a href="#cb7-4"></a><span class="kw">def</span> generate_sequences(df, test_size<span class="op">=</span><span class="fl">0.3</span>, look_back<span class="op">=</span><span class="dv">10</span>):</span>
<span id="cb7-5"><a href="#cb7-5"></a>    data <span class="op">=</span> []</span>
<span id="cb7-6"><a href="#cb7-6"></a>    df_raw <span class="op">=</span> df.values</span>
<span id="cb7-7"><a href="#cb7-7"></a></span>
<span id="cb7-8"><a href="#cb7-8"></a>    <span class="cf">for</span> index <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(df_raw) <span class="op">-</span> look_back):</span>
<span id="cb7-9"><a href="#cb7-9"></a>        data.append(df_raw[index : index <span class="op">+</span> look_back])</span>
<span id="cb7-10"><a href="#cb7-10"></a></span>
<span id="cb7-11"><a href="#cb7-11"></a>    data <span class="op">=</span> np.array(data)</span>
<span id="cb7-12"><a href="#cb7-12"></a></span>
<span id="cb7-13"><a href="#cb7-13"></a>    test_set_size <span class="op">=</span> <span class="bu">int</span>(np.<span class="bu">round</span>(test_size <span class="op">*</span> data.shape[<span class="dv">0</span>]))</span>
<span id="cb7-14"><a href="#cb7-14"></a>    train_set_size <span class="op">=</span> data.shape[<span class="dv">0</span>] <span class="op">-</span> (test_set_size)</span>
<span id="cb7-15"><a href="#cb7-15"></a></span>
<span id="cb7-16"><a href="#cb7-16"></a>    x_train <span class="op">=</span> data[:train_set_size, :<span class="op">-</span><span class="dv">1</span>, :]</span>
<span id="cb7-17"><a href="#cb7-17"></a>    y_train <span class="op">=</span> data[:train_set_size, <span class="op">-</span><span class="dv">1</span>, :]</span>
<span id="cb7-18"><a href="#cb7-18"></a>    x_test <span class="op">=</span> data[train_set_size:, :<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb7-19"><a href="#cb7-19"></a>    y_test <span class="op">=</span> data[train_set_size:, <span class="op">-</span><span class="dv">1</span>, :]</span>
<span id="cb7-20"><a href="#cb7-20"></a></span>
<span id="cb7-21"><a href="#cb7-21"></a>    <span class="cf">return</span> [x_train, y_train, x_test, y_test]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Let’s apply this function to our sequence and check the output.</p>
<div id="cell-16" class="cell" data-outputid="1692a847-9fa1-40b8-d5ee-cc6a56240efc" data-execution_count="6">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1"></a><span class="co">##</span></span>
<span id="cb8-2"><a href="#cb8-2"></a><span class="co"># generate test and train sequences</span></span>
<span id="cb8-3"><a href="#cb8-3"></a><span class="co"># x = features</span></span>
<span id="cb8-4"><a href="#cb8-4"></a><span class="co"># y = labels</span></span>
<span id="cb8-5"><a href="#cb8-5"></a>x_train_dummy, y_train_dummy, x_test_dummy, y_test_dummy <span class="op">=</span> generate_sequences(df_dummy)</span>
<span id="cb8-6"><a href="#cb8-6"></a></span>
<span id="cb8-7"><a href="#cb8-7"></a><span class="co"># view the training data. features and labels together</span></span>
<span id="cb8-8"><a href="#cb8-8"></a><span class="co"># feature 't' = labels</span></span>
<span id="cb8-9"><a href="#cb8-9"></a>df_train_dummy <span class="op">=</span> pd.DataFrame(np.squeeze(x_train_dummy))</span>
<span id="cb8-10"><a href="#cb8-10"></a>df_train_dummy[<span class="st">"t"</span>] <span class="op">=</span> np.squeeze(y_train_dummy)</span>
<span id="cb8-11"><a href="#cb8-11"></a>df_train_dummy.head(<span class="dv">10</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="6">

  <div id="df-ea40f925-c919-4111-aa00-09583144e830">
    <div class="colab-df-container">
      <div>


<table class="dataframe caption-top table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">0</th>
<th data-quarto-table-cell-role="th">1</th>
<th data-quarto-table-cell-role="th">2</th>
<th data-quarto-table-cell-role="th">3</th>
<th data-quarto-table-cell-role="th">4</th>
<th data-quarto-table-cell-role="th">5</th>
<th data-quarto-table-cell-role="th">6</th>
<th data-quarto-table-cell-role="th">7</th>
<th data-quarto-table-cell-role="th">8</th>
<th data-quarto-table-cell-role="th">t</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">0</td>
<td>0</td>
<td>1</td>
<td>2</td>
<td>3</td>
<td>4</td>
<td>5</td>
<td>6</td>
<td>7</td>
<td>8</td>
<td>9</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">1</td>
<td>1</td>
<td>2</td>
<td>3</td>
<td>4</td>
<td>5</td>
<td>6</td>
<td>7</td>
<td>8</td>
<td>9</td>
<td>10</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">2</td>
<td>2</td>
<td>3</td>
<td>4</td>
<td>5</td>
<td>6</td>
<td>7</td>
<td>8</td>
<td>9</td>
<td>10</td>
<td>11</td>
</tr>
</tbody>
</table>

</div>
      <button class="colab-df-convert" onclick="convertToInteractive('df-ea40f925-c919-4111-aa00-09583144e830')" title="Convert this dataframe to an interactive table." style="display:none;">
        
  <svg xmlns="http://www.w3.org/2000/svg" height="24px" viewbox="0 0 24 24" width="24px">
    <path d="M0 0h24v24H0V0z" fill="none"></path>
    <path d="M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z"></path><path d="M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z"></path>
  </svg>
      </button>
      
  <style>
    .colab-df-container {
      display:flex;
      flex-wrap:wrap;
      gap: 12px;
    }

    .colab-df-convert {
      background-color: #E8F0FE;
      border: none;
      border-radius: 50%;
      cursor: pointer;
      display: none;
      fill: #1967D2;
      height: 32px;
      padding: 0 0 0 0;
      width: 32px;
    }

    .colab-df-convert:hover {
      background-color: #E2EBFA;
      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);
      fill: #174EA6;
    }

    [theme=dark] .colab-df-convert {
      background-color: #3B4455;
      fill: #D2E3FC;
    }

    [theme=dark] .colab-df-convert:hover {
      background-color: #434B5C;
      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);
      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));
      fill: #FFFFFF;
    }
  </style>

      <script>
        const buttonEl =
          document.querySelector('#df-ea40f925-c919-4111-aa00-09583144e830 button.colab-df-convert');
        buttonEl.style.display =
          google.colab.kernel.accessAllowed ? 'block' : 'none';

        async function convertToInteractive(key) {
          const element = document.querySelector('#df-ea40f925-c919-4111-aa00-09583144e830');
          const dataTable =
            await google.colab.kernel.invokeFunction('convertToInteractive',
                                                     [key], {});
          if (!dataTable) return;

          const docLinkHtml = 'Like what you see? Visit the ' +
            '<a target="_blank" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'
            + ' to learn more about interactive tables.';
          element.innerHTML = '';
          dataTable['output_type'] = 'display_data';
          await google.colab.output.renderOutput(dataTable, element);
          const docLink = document.createElement('div');
          docLink.innerHTML = docLinkHtml;
          element.appendChild(docLink);
        }
      </script>
    </div>
  </div>
  
</div>
</div>
<p>Notice that our training set has smaller sequences, with the last element acting as a label denoted by column ‘t’. This is because our <code>generate_sequences</code> function acts as a <code>moving window</code> where earlier elements become features and the last element in the window acts as a <code>label</code>.</p>
<p>Let’s also check the generated testing set.</p>
<div id="cell-18" class="cell" data-outputid="f625b934-9974-45d7-93db-90b18c2d0310" data-execution_count="7">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1"></a><span class="co">##</span></span>
<span id="cb9-2"><a href="#cb9-2"></a><span class="co"># view the testing data. features and labels together</span></span>
<span id="cb9-3"><a href="#cb9-3"></a><span class="co"># feature 't' = labels</span></span>
<span id="cb9-4"><a href="#cb9-4"></a>df_test_dummy <span class="op">=</span> pd.DataFrame(np.squeeze(x_test_dummy))</span>
<span id="cb9-5"><a href="#cb9-5"></a>df_test_dummy[<span class="st">"t"</span>] <span class="op">=</span> np.squeeze(y_test_dummy)</span>
<span id="cb9-6"><a href="#cb9-6"></a>df_test_dummy.head(<span class="dv">10</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="7">

  <div id="df-9fd058e8-4ea8-47be-9e92-1707e1d62801">
    <div class="colab-df-container">
      <div>


<table class="dataframe caption-top table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">0</th>
<th data-quarto-table-cell-role="th">1</th>
<th data-quarto-table-cell-role="th">2</th>
<th data-quarto-table-cell-role="th">3</th>
<th data-quarto-table-cell-role="th">4</th>
<th data-quarto-table-cell-role="th">5</th>
<th data-quarto-table-cell-role="th">6</th>
<th data-quarto-table-cell-role="th">7</th>
<th data-quarto-table-cell-role="th">8</th>
<th data-quarto-table-cell-role="th">t</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">0</td>
<td>3</td>
<td>4</td>
<td>5</td>
<td>6</td>
<td>7</td>
<td>8</td>
<td>9</td>
<td>10</td>
<td>11</td>
<td>12</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">1</td>
<td>4</td>
<td>5</td>
<td>6</td>
<td>7</td>
<td>8</td>
<td>9</td>
<td>10</td>
<td>11</td>
<td>12</td>
<td>13</td>
</tr>
</tbody>
</table>

</div>
      <button class="colab-df-convert" onclick="convertToInteractive('df-9fd058e8-4ea8-47be-9e92-1707e1d62801')" title="Convert this dataframe to an interactive table." style="display:none;">
        
  <svg xmlns="http://www.w3.org/2000/svg" height="24px" viewbox="0 0 24 24" width="24px">
    <path d="M0 0h24v24H0V0z" fill="none"></path>
    <path d="M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z"></path><path d="M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z"></path>
  </svg>
      </button>
      
  <style>
    .colab-df-container {
      display:flex;
      flex-wrap:wrap;
      gap: 12px;
    }

    .colab-df-convert {
      background-color: #E8F0FE;
      border: none;
      border-radius: 50%;
      cursor: pointer;
      display: none;
      fill: #1967D2;
      height: 32px;
      padding: 0 0 0 0;
      width: 32px;
    }

    .colab-df-convert:hover {
      background-color: #E2EBFA;
      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);
      fill: #174EA6;
    }

    [theme=dark] .colab-df-convert {
      background-color: #3B4455;
      fill: #D2E3FC;
    }

    [theme=dark] .colab-df-convert:hover {
      background-color: #434B5C;
      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);
      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));
      fill: #FFFFFF;
    }
  </style>

      <script>
        const buttonEl =
          document.querySelector('#df-9fd058e8-4ea8-47be-9e92-1707e1d62801 button.colab-df-convert');
        buttonEl.style.display =
          google.colab.kernel.accessAllowed ? 'block' : 'none';

        async function convertToInteractive(key) {
          const element = document.querySelector('#df-9fd058e8-4ea8-47be-9e92-1707e1d62801');
          const dataTable =
            await google.colab.kernel.invokeFunction('convertToInteractive',
                                                     [key], {});
          if (!dataTable) return;

          const docLinkHtml = 'Like what you see? Visit the ' +
            '<a target="_blank" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'
            + ' to learn more about interactive tables.';
          element.innerHTML = '';
          dataTable['output_type'] = 'display_data';
          await google.colab.output.renderOutput(dataTable, element);
          const docLink = document.createElement('div');
          docLink.innerHTML = docLinkHtml;
          element.appendChild(docLink);
        }
      </script>
    </div>
  </div>
  
</div>
</div>
<p>From this sequence example, we have learned that we can generate training and test sets of different sizes using the same sequence data. The features and label aren’t provided separately but can be produced by splitting the sequence data into smaller chunks. The last element in the chunks acts as the label.</p>
<p>Let’s apply this understanding to our synthetic data and generate training and test samples.</p>
<div id="cell-20" class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1"></a><span class="co">##</span></span>
<span id="cb10-2"><a href="#cb10-2"></a><span class="co"># generate training and test data for synthetic sequence data</span></span>
<span id="cb10-3"><a href="#cb10-3"></a>x_train_synth, y_train_synth, x_test_synth, y_test_synth <span class="op">=</span> generate_sequences(df_synth)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="load-generated-data-into-pytorch-dataset-and-dataloader-class" class="level2">
<h2 class="anchored" data-anchor-id="load-generated-data-into-pytorch-dataset-and-dataloader-class">Load generated data into PyTorch Dataset and DataLoader class</h2>
<p>Now let’s load our data into <code>Dataset</code> and <code>DataLoader</code> classes. PyTorch Dataset is a helper class that converts data and labels into a list of tuples. DataLoader is another helper class to create batches from Dataset tuples. <code>batch_size</code> means the number of tuples we want in a single batch. We have used 16 here, so each fetch from DataLoader will give us a list of 16 tuples.</p>
<div id="cell-22" class="cell" data-execution_count="9">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1"></a><span class="im">import</span> torch</span>
<span id="cb11-2"><a href="#cb11-2"></a><span class="im">from</span> torch.utils.data <span class="im">import</span> DataLoader, TensorDataset</span>
<span id="cb11-3"><a href="#cb11-3"></a></span>
<span id="cb11-4"><a href="#cb11-4"></a>train_dataset_synth <span class="op">=</span> TensorDataset(</span>
<span id="cb11-5"><a href="#cb11-5"></a>    torch.as_tensor(x_train_synth).<span class="bu">float</span>(), torch.as_tensor(y_train_synth).<span class="bu">float</span>()</span>
<span id="cb11-6"><a href="#cb11-6"></a>)</span>
<span id="cb11-7"><a href="#cb11-7"></a>test_dataset_synth <span class="op">=</span> TensorDataset(</span>
<span id="cb11-8"><a href="#cb11-8"></a>    torch.as_tensor(x_test_synth).<span class="bu">float</span>(), torch.as_tensor(y_test_synth).<span class="bu">float</span>()</span>
<span id="cb11-9"><a href="#cb11-9"></a>)</span>
<span id="cb11-10"><a href="#cb11-10"></a></span>
<span id="cb11-11"><a href="#cb11-11"></a>batch_size <span class="op">=</span> <span class="dv">16</span></span>
<span id="cb11-12"><a href="#cb11-12"></a></span>
<span id="cb11-13"><a href="#cb11-13"></a>train_loader_synth <span class="op">=</span> DataLoader(</span>
<span id="cb11-14"><a href="#cb11-14"></a>    train_dataset_synth, batch_size<span class="op">=</span>batch_size, shuffle<span class="op">=</span><span class="va">True</span></span>
<span id="cb11-15"><a href="#cb11-15"></a>)</span>
<span id="cb11-16"><a href="#cb11-16"></a>test_loader_synth <span class="op">=</span> DataLoader(test_dataset_synth, batch_size<span class="op">=</span>batch_size)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="define-a-class-to-implement-training-validation-and-mini-batch-processing-pipeline" class="level2">
<h2 class="anchored" data-anchor-id="define-a-class-to-implement-training-validation-and-mini-batch-processing-pipeline">Define a class to implement training, validation, and mini-batch processing pipeline</h2>
<p>In this section we will implement a class that encapsulates all the usual steps required in training a PyTorch model. This way we can focus more on the model architecture and performance, and less concerned about the boilerplate training loop. Important parts of this class are</p>
<ul>
<li><code>__init__</code>: Class constructor to define the main actors in a training cycle including model, optimizer, loss function, training and validation DataLoaders</li>
<li><code>_make_train_step_fn</code>: Training pipeline is usually called “training step” which includes the following steps
<ol type="1">
<li>Compute our model’s predicted output - the forward pass</li>
<li>Compute the loss</li>
<li>Compute gradients i.e., find the direction and scale to update the weights to reduce the loss</li>
<li>Update weight parameters using gradients and the learning rate</li>
</ol></li>
<li><code>_make_val_step_fn</code>: Validation pipeline is usually called the “validation step” which includes the following steps
<ol type="1">
<li>Compute our model’s predicted output - the forward pass</li>
<li>Compute the loss</li>
<li>Note that during validation, we are only concerned about the loss, i.e., how well our model performs on the validation dataset. Therefore, we don’t use it to calculate the gradients.</li>
</ol></li>
<li><code>_mini_batch</code>: It defines the steps to process a single minibatch in a helper function. For a mini-batch processing, we want to
<ol type="1">
<li>Get the next batch of data and labels (x, y) from the DataLoader iterator</li>
<li>Perform a step on the batch. A step can be either training or validation</li>
<li>Compute the average batch loss</li>
</ol></li>
<li><code>train</code>: Execute training and validation steps for given number of epoch</li>
<li><code>predict</code>: Make a prediction from model on provided data</li>
</ul>
<div id="cell-24" class="cell" data-execution_count="10">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb12-2"><a href="#cb12-2"></a><span class="im">import</span> datetime</span>
<span id="cb12-3"><a href="#cb12-3"></a></span>
<span id="cb12-4"><a href="#cb12-4"></a><span class="kw">class</span> DeepLearningPipeline(<span class="bu">object</span>):</span>
<span id="cb12-5"><a href="#cb12-5"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, model, loss_fn, optimizer):</span>
<span id="cb12-6"><a href="#cb12-6"></a>        <span class="co"># Here we define the attributes of our class</span></span>
<span id="cb12-7"><a href="#cb12-7"></a></span>
<span id="cb12-8"><a href="#cb12-8"></a>        <span class="co"># We start by storing the arguments as attributes</span></span>
<span id="cb12-9"><a href="#cb12-9"></a>        <span class="co"># to use them later</span></span>
<span id="cb12-10"><a href="#cb12-10"></a>        <span class="va">self</span>.model <span class="op">=</span> model</span>
<span id="cb12-11"><a href="#cb12-11"></a>        <span class="va">self</span>.loss_fn <span class="op">=</span> loss_fn</span>
<span id="cb12-12"><a href="#cb12-12"></a>        <span class="va">self</span>.optimizer <span class="op">=</span> optimizer</span>
<span id="cb12-13"><a href="#cb12-13"></a>        <span class="va">self</span>.device <span class="op">=</span> <span class="st">"cuda"</span> <span class="cf">if</span> torch.cuda.is_available() <span class="cf">else</span> <span class="st">"cpu"</span></span>
<span id="cb12-14"><a href="#cb12-14"></a>        <span class="co"># Let's send the model to the specified device right away</span></span>
<span id="cb12-15"><a href="#cb12-15"></a>        <span class="va">self</span>.model.to(<span class="va">self</span>.device)</span>
<span id="cb12-16"><a href="#cb12-16"></a></span>
<span id="cb12-17"><a href="#cb12-17"></a>        <span class="co"># These attributes are defined here, but since they are</span></span>
<span id="cb12-18"><a href="#cb12-18"></a>        <span class="co"># not informed at the moment of creation, we keep them None</span></span>
<span id="cb12-19"><a href="#cb12-19"></a>        <span class="va">self</span>.train_loader <span class="op">=</span> <span class="va">None</span></span>
<span id="cb12-20"><a href="#cb12-20"></a>        <span class="va">self</span>.val_loader <span class="op">=</span> <span class="va">None</span></span>
<span id="cb12-21"><a href="#cb12-21"></a>        <span class="va">self</span>.writer <span class="op">=</span> <span class="va">None</span></span>
<span id="cb12-22"><a href="#cb12-22"></a></span>
<span id="cb12-23"><a href="#cb12-23"></a>        <span class="co"># These attributes are going to be computed internally</span></span>
<span id="cb12-24"><a href="#cb12-24"></a>        <span class="va">self</span>.losses <span class="op">=</span> []</span>
<span id="cb12-25"><a href="#cb12-25"></a>        <span class="va">self</span>.val_losses <span class="op">=</span> []</span>
<span id="cb12-26"><a href="#cb12-26"></a>        <span class="va">self</span>.total_epochs <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb12-27"><a href="#cb12-27"></a></span>
<span id="cb12-28"><a href="#cb12-28"></a>        <span class="co"># Creates the train_step function for our model,</span></span>
<span id="cb12-29"><a href="#cb12-29"></a>        <span class="co"># loss function and optimizer</span></span>
<span id="cb12-30"><a href="#cb12-30"></a>        <span class="co"># Note: there are NO ARGS there! It makes use of the class</span></span>
<span id="cb12-31"><a href="#cb12-31"></a>        <span class="co"># attributes directly</span></span>
<span id="cb12-32"><a href="#cb12-32"></a>        <span class="va">self</span>.train_step_fn <span class="op">=</span> <span class="va">self</span>._make_train_step_fn()</span>
<span id="cb12-33"><a href="#cb12-33"></a>        <span class="co"># Creates the val_step function for our model and loss</span></span>
<span id="cb12-34"><a href="#cb12-34"></a>        <span class="va">self</span>.val_step_fn <span class="op">=</span> <span class="va">self</span>._make_val_step_fn()</span>
<span id="cb12-35"><a href="#cb12-35"></a></span>
<span id="cb12-36"><a href="#cb12-36"></a>    <span class="kw">def</span> set_loaders(<span class="va">self</span>, train_loader, val_loader<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb12-37"><a href="#cb12-37"></a>        <span class="co"># This method allows the user to define which train_loader (and val_loader, optionally) to use</span></span>
<span id="cb12-38"><a href="#cb12-38"></a>        <span class="co"># Both loaders are then assigned to attributes of the class</span></span>
<span id="cb12-39"><a href="#cb12-39"></a>        <span class="co"># So they can be referred to later</span></span>
<span id="cb12-40"><a href="#cb12-40"></a>        <span class="va">self</span>.train_loader <span class="op">=</span> train_loader</span>
<span id="cb12-41"><a href="#cb12-41"></a>        <span class="va">self</span>.val_loader <span class="op">=</span> val_loader</span>
<span id="cb12-42"><a href="#cb12-42"></a></span>
<span id="cb12-43"><a href="#cb12-43"></a>    <span class="kw">def</span> _make_train_step_fn(<span class="va">self</span>):</span>
<span id="cb12-44"><a href="#cb12-44"></a>        <span class="co"># This method does not need ARGS... it can refer to</span></span>
<span id="cb12-45"><a href="#cb12-45"></a>        <span class="co"># the attributes: self.model, self.loss_fn and self.optimizer</span></span>
<span id="cb12-46"><a href="#cb12-46"></a></span>
<span id="cb12-47"><a href="#cb12-47"></a>        <span class="co"># Builds function that performs a step in the train loop</span></span>
<span id="cb12-48"><a href="#cb12-48"></a>        <span class="kw">def</span> perform_train_step_fn(x, y):</span>
<span id="cb12-49"><a href="#cb12-49"></a>            <span class="co"># Sets model to TRAIN mode</span></span>
<span id="cb12-50"><a href="#cb12-50"></a>            <span class="va">self</span>.model.train()</span>
<span id="cb12-51"><a href="#cb12-51"></a></span>
<span id="cb12-52"><a href="#cb12-52"></a>            <span class="co"># Step 1 - Computes our model's predicted output - forward pass</span></span>
<span id="cb12-53"><a href="#cb12-53"></a>            yhat <span class="op">=</span> <span class="va">self</span>.model(x)</span>
<span id="cb12-54"><a href="#cb12-54"></a>            <span class="co"># Step 2 - Computes the loss</span></span>
<span id="cb12-55"><a href="#cb12-55"></a>            loss <span class="op">=</span> <span class="va">self</span>.loss_fn(yhat, y)</span>
<span id="cb12-56"><a href="#cb12-56"></a>            <span class="co"># Step 3 - Computes gradients for both "a" and "b" parameters</span></span>
<span id="cb12-57"><a href="#cb12-57"></a>            loss.backward()</span>
<span id="cb12-58"><a href="#cb12-58"></a>            <span class="co"># Step 4 - Updates parameters using gradients and the learning rate</span></span>
<span id="cb12-59"><a href="#cb12-59"></a>            <span class="va">self</span>.optimizer.step()</span>
<span id="cb12-60"><a href="#cb12-60"></a>            <span class="va">self</span>.optimizer.zero_grad()</span>
<span id="cb12-61"><a href="#cb12-61"></a></span>
<span id="cb12-62"><a href="#cb12-62"></a>            <span class="co"># Returns the loss</span></span>
<span id="cb12-63"><a href="#cb12-63"></a>            <span class="cf">return</span> loss.item()</span>
<span id="cb12-64"><a href="#cb12-64"></a></span>
<span id="cb12-65"><a href="#cb12-65"></a>        <span class="co"># Returns the function that will be called inside the train loop</span></span>
<span id="cb12-66"><a href="#cb12-66"></a>        <span class="cf">return</span> perform_train_step_fn</span>
<span id="cb12-67"><a href="#cb12-67"></a></span>
<span id="cb12-68"><a href="#cb12-68"></a>    <span class="kw">def</span> _make_val_step_fn(<span class="va">self</span>):</span>
<span id="cb12-69"><a href="#cb12-69"></a>        <span class="co"># Builds function that performs a step in the validation loop</span></span>
<span id="cb12-70"><a href="#cb12-70"></a>        <span class="kw">def</span> perform_val_step_fn(x, y):</span>
<span id="cb12-71"><a href="#cb12-71"></a>            <span class="co"># Sets model to EVAL mode</span></span>
<span id="cb12-72"><a href="#cb12-72"></a>            <span class="va">self</span>.model.<span class="bu">eval</span>()</span>
<span id="cb12-73"><a href="#cb12-73"></a></span>
<span id="cb12-74"><a href="#cb12-74"></a>            <span class="co"># Step 1 - Computes our model's predicted output - forward pass</span></span>
<span id="cb12-75"><a href="#cb12-75"></a>            yhat <span class="op">=</span> <span class="va">self</span>.model(x)</span>
<span id="cb12-76"><a href="#cb12-76"></a>            <span class="co"># Step 2 - Computes the loss</span></span>
<span id="cb12-77"><a href="#cb12-77"></a>            loss <span class="op">=</span> <span class="va">self</span>.loss_fn(yhat, y)</span>
<span id="cb12-78"><a href="#cb12-78"></a>            <span class="co"># There is no need to compute Steps 3 and 4,</span></span>
<span id="cb12-79"><a href="#cb12-79"></a>            <span class="co"># since we don't update parameters during evaluation</span></span>
<span id="cb12-80"><a href="#cb12-80"></a>            <span class="cf">return</span> loss.item()</span>
<span id="cb12-81"><a href="#cb12-81"></a></span>
<span id="cb12-82"><a href="#cb12-82"></a>        <span class="cf">return</span> perform_val_step_fn</span>
<span id="cb12-83"><a href="#cb12-83"></a></span>
<span id="cb12-84"><a href="#cb12-84"></a>    <span class="kw">def</span> _mini_batch(<span class="va">self</span>, validation<span class="op">=</span><span class="va">False</span>):</span>
<span id="cb12-85"><a href="#cb12-85"></a>        <span class="co"># The mini-batch can be used with both loaders</span></span>
<span id="cb12-86"><a href="#cb12-86"></a>        <span class="co"># The argument `validation`defines which loader and</span></span>
<span id="cb12-87"><a href="#cb12-87"></a>        <span class="co"># corresponding step function is going to be used</span></span>
<span id="cb12-88"><a href="#cb12-88"></a>        <span class="cf">if</span> validation:</span>
<span id="cb12-89"><a href="#cb12-89"></a>            data_loader <span class="op">=</span> <span class="va">self</span>.val_loader</span>
<span id="cb12-90"><a href="#cb12-90"></a>            step_fn <span class="op">=</span> <span class="va">self</span>.val_step_fn</span>
<span id="cb12-91"><a href="#cb12-91"></a>        <span class="cf">else</span>:</span>
<span id="cb12-92"><a href="#cb12-92"></a>            data_loader <span class="op">=</span> <span class="va">self</span>.train_loader</span>
<span id="cb12-93"><a href="#cb12-93"></a>            step_fn <span class="op">=</span> <span class="va">self</span>.train_step_fn</span>
<span id="cb12-94"><a href="#cb12-94"></a></span>
<span id="cb12-95"><a href="#cb12-95"></a>        <span class="cf">if</span> data_loader <span class="kw">is</span> <span class="va">None</span>:</span>
<span id="cb12-96"><a href="#cb12-96"></a>            <span class="cf">return</span> <span class="va">None</span></span>
<span id="cb12-97"><a href="#cb12-97"></a></span>
<span id="cb12-98"><a href="#cb12-98"></a>        <span class="co"># Once the data loader and step function, this is the</span></span>
<span id="cb12-99"><a href="#cb12-99"></a>        <span class="co"># same mini-batch loop we had before</span></span>
<span id="cb12-100"><a href="#cb12-100"></a>        mini_batch_losses <span class="op">=</span> []</span>
<span id="cb12-101"><a href="#cb12-101"></a>        <span class="cf">for</span> x_batch, y_batch <span class="kw">in</span> data_loader:</span>
<span id="cb12-102"><a href="#cb12-102"></a>            x_batch <span class="op">=</span> x_batch.to(<span class="va">self</span>.device)</span>
<span id="cb12-103"><a href="#cb12-103"></a>            y_batch <span class="op">=</span> y_batch.to(<span class="va">self</span>.device)</span>
<span id="cb12-104"><a href="#cb12-104"></a></span>
<span id="cb12-105"><a href="#cb12-105"></a>            mini_batch_loss <span class="op">=</span> step_fn(x_batch, y_batch)</span>
<span id="cb12-106"><a href="#cb12-106"></a>            mini_batch_losses.append(mini_batch_loss)</span>
<span id="cb12-107"><a href="#cb12-107"></a></span>
<span id="cb12-108"><a href="#cb12-108"></a>        loss <span class="op">=</span> np.mean(mini_batch_losses)</span>
<span id="cb12-109"><a href="#cb12-109"></a>        <span class="cf">return</span> loss</span>
<span id="cb12-110"><a href="#cb12-110"></a></span>
<span id="cb12-111"><a href="#cb12-111"></a>    <span class="kw">def</span> set_seed(<span class="va">self</span>, seed<span class="op">=</span><span class="dv">42</span>):</span>
<span id="cb12-112"><a href="#cb12-112"></a>        torch.backends.cudnn.deterministic <span class="op">=</span> <span class="va">True</span></span>
<span id="cb12-113"><a href="#cb12-113"></a>        torch.backends.cudnn.benchmark <span class="op">=</span> <span class="va">False</span></span>
<span id="cb12-114"><a href="#cb12-114"></a>        torch.manual_seed(seed)</span>
<span id="cb12-115"><a href="#cb12-115"></a>        np.random.seed(seed)</span>
<span id="cb12-116"><a href="#cb12-116"></a></span>
<span id="cb12-117"><a href="#cb12-117"></a>    <span class="kw">def</span> train(<span class="va">self</span>, n_epochs, seed<span class="op">=</span><span class="dv">42</span>, print_loss<span class="op">=</span><span class="va">False</span>):</span>
<span id="cb12-118"><a href="#cb12-118"></a>        <span class="co"># To ensure reproducibility of the training process</span></span>
<span id="cb12-119"><a href="#cb12-119"></a>        <span class="va">self</span>.set_seed(seed)</span>
<span id="cb12-120"><a href="#cb12-120"></a></span>
<span id="cb12-121"><a href="#cb12-121"></a>        <span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(n_epochs):</span>
<span id="cb12-122"><a href="#cb12-122"></a>            <span class="co"># Keeps track of the numbers of epochs</span></span>
<span id="cb12-123"><a href="#cb12-123"></a>            <span class="co"># by updating the corresponding attribute</span></span>
<span id="cb12-124"><a href="#cb12-124"></a>            <span class="va">self</span>.total_epochs <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb12-125"><a href="#cb12-125"></a></span>
<span id="cb12-126"><a href="#cb12-126"></a>            <span class="co"># inner loop</span></span>
<span id="cb12-127"><a href="#cb12-127"></a>            <span class="co"># Performs training using mini-batches</span></span>
<span id="cb12-128"><a href="#cb12-128"></a>            loss <span class="op">=</span> <span class="va">self</span>._mini_batch(validation<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb12-129"><a href="#cb12-129"></a>            <span class="va">self</span>.losses.append(loss)</span>
<span id="cb12-130"><a href="#cb12-130"></a></span>
<span id="cb12-131"><a href="#cb12-131"></a>            <span class="cf">if</span> print_loss:</span>
<span id="cb12-132"><a href="#cb12-132"></a>                <span class="cf">if</span> epoch <span class="op">%</span> <span class="dv">10</span> <span class="op">==</span> <span class="dv">0</span> <span class="kw">and</span> epoch <span class="op">!=</span> <span class="dv">0</span>:</span>
<span id="cb12-133"><a href="#cb12-133"></a>                    <span class="bu">print</span>(<span class="st">"Epoch "</span>, epoch, <span class="st">"MSE: "</span>, loss)</span>
<span id="cb12-134"><a href="#cb12-134"></a></span>
<span id="cb12-135"><a href="#cb12-135"></a>            <span class="co"># VALIDATION</span></span>
<span id="cb12-136"><a href="#cb12-136"></a>            <span class="co"># no gradients in validation!</span></span>
<span id="cb12-137"><a href="#cb12-137"></a>            <span class="cf">with</span> torch.no_grad():</span>
<span id="cb12-138"><a href="#cb12-138"></a>                <span class="co"># Performs evaluation using mini-batches</span></span>
<span id="cb12-139"><a href="#cb12-139"></a>                val_loss <span class="op">=</span> <span class="va">self</span>._mini_batch(validation<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb12-140"><a href="#cb12-140"></a>                <span class="va">self</span>.val_losses.append(val_loss)</span>
<span id="cb12-141"><a href="#cb12-141"></a></span>
<span id="cb12-142"><a href="#cb12-142"></a>            <span class="co"># If a SummaryWriter has been set...</span></span>
<span id="cb12-143"><a href="#cb12-143"></a>            <span class="cf">if</span> <span class="va">self</span>.writer:</span>
<span id="cb12-144"><a href="#cb12-144"></a>                scalars <span class="op">=</span> {<span class="st">"training"</span>: loss}</span>
<span id="cb12-145"><a href="#cb12-145"></a>                <span class="cf">if</span> val_loss <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb12-146"><a href="#cb12-146"></a>                    scalars.update({<span class="st">"validation"</span>: val_loss})</span>
<span id="cb12-147"><a href="#cb12-147"></a>                <span class="co"># Records both losses for each epoch under the main tag "loss"</span></span>
<span id="cb12-148"><a href="#cb12-148"></a>                <span class="va">self</span>.writer.add_scalars(</span>
<span id="cb12-149"><a href="#cb12-149"></a>                    main_tag<span class="op">=</span><span class="st">"loss"</span>, tag_scalar_dict<span class="op">=</span>scalars, global_step<span class="op">=</span>epoch</span>
<span id="cb12-150"><a href="#cb12-150"></a>                )</span>
<span id="cb12-151"><a href="#cb12-151"></a></span>
<span id="cb12-152"><a href="#cb12-152"></a>        <span class="cf">if</span> <span class="va">self</span>.writer:</span>
<span id="cb12-153"><a href="#cb12-153"></a>            <span class="co"># Closes the writer</span></span>
<span id="cb12-154"><a href="#cb12-154"></a>            <span class="va">self</span>.writer.close()</span>
<span id="cb12-155"><a href="#cb12-155"></a></span>
<span id="cb12-156"><a href="#cb12-156"></a>    <span class="kw">def</span> predict(<span class="va">self</span>, x):</span>
<span id="cb12-157"><a href="#cb12-157"></a>        <span class="co"># Set is to evaluation mode for predictions</span></span>
<span id="cb12-158"><a href="#cb12-158"></a>        <span class="va">self</span>.model.<span class="bu">eval</span>()</span>
<span id="cb12-159"><a href="#cb12-159"></a>        <span class="co"># Takes aNumpy input and make it a float tensor</span></span>
<span id="cb12-160"><a href="#cb12-160"></a>        x_tensor <span class="op">=</span> torch.as_tensor(x).<span class="bu">float</span>()</span>
<span id="cb12-161"><a href="#cb12-161"></a>        <span class="co"># Send input to device and uses model for prediction</span></span>
<span id="cb12-162"><a href="#cb12-162"></a>        y_hat_tensor <span class="op">=</span> <span class="va">self</span>.model(x_tensor.to(<span class="va">self</span>.device))</span>
<span id="cb12-163"><a href="#cb12-163"></a>        <span class="co"># Set it back to train mode</span></span>
<span id="cb12-164"><a href="#cb12-164"></a>        <span class="va">self</span>.model.train()</span>
<span id="cb12-165"><a href="#cb12-165"></a>        <span class="co"># Detaches it, brings it to CPU and back to Numpy</span></span>
<span id="cb12-166"><a href="#cb12-166"></a>        <span class="cf">return</span> y_hat_tensor.detach().cpu().numpy()</span>
<span id="cb12-167"><a href="#cb12-167"></a></span>
<span id="cb12-168"><a href="#cb12-168"></a>    <span class="kw">def</span> plot_losses(<span class="va">self</span>):</span>
<span id="cb12-169"><a href="#cb12-169"></a>        fig <span class="op">=</span> plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">4</span>))</span>
<span id="cb12-170"><a href="#cb12-170"></a>        plt.plot(<span class="va">self</span>.losses, label<span class="op">=</span><span class="st">"Training Loss"</span>, c<span class="op">=</span><span class="st">"b"</span>)</span>
<span id="cb12-171"><a href="#cb12-171"></a>        plt.plot(<span class="va">self</span>.val_losses, label<span class="op">=</span><span class="st">"Validation Loss"</span>, c<span class="op">=</span><span class="st">"r"</span>)</span>
<span id="cb12-172"><a href="#cb12-172"></a>        plt.yscale(<span class="st">"log"</span>)</span>
<span id="cb12-173"><a href="#cb12-173"></a>        plt.xlabel(<span class="st">"Epochs"</span>)</span>
<span id="cb12-174"><a href="#cb12-174"></a>        plt.ylabel(<span class="st">"Loss"</span>)</span>
<span id="cb12-175"><a href="#cb12-175"></a>        plt.legend()</span>
<span id="cb12-176"><a href="#cb12-176"></a>        plt.tight_layout()</span>
<span id="cb12-177"><a href="#cb12-177"></a>        <span class="cf">return</span> fig</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
<section id="model-configuration-and-training" class="level2">
<h2 class="anchored" data-anchor-id="model-configuration-and-training">Model configuration and training</h2>
<p>We have all the pieces ready to train a neural network on the ordered sequence data. So here, I will train an RNN model on the generated data. At this point, I will not go into the details of the structure and working of RNN. But in the next section, we will discuss it in much more detail.</p>
<div id="cell-26" class="cell" data-execution_count="11">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1"></a><span class="co"># configure an RNN model</span></span>
<span id="cb13-2"><a href="#cb13-2"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb13-3"><a href="#cb13-3"></a><span class="im">import</span> torch.optim <span class="im">as</span> optim</span>
<span id="cb13-4"><a href="#cb13-4"></a></span>
<span id="cb13-5"><a href="#cb13-5"></a><span class="kw">class</span> RnnModel(nn.Module):</span>
<span id="cb13-6"><a href="#cb13-6"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, n_features, hidden_dim, n_outputs, n_layers):</span>
<span id="cb13-7"><a href="#cb13-7"></a>        <span class="bu">super</span>(RnnModel, <span class="va">self</span>).<span class="fu">__init__</span>()</span>
<span id="cb13-8"><a href="#cb13-8"></a>        <span class="va">self</span>.hidden_dim <span class="op">=</span> hidden_dim</span>
<span id="cb13-9"><a href="#cb13-9"></a>        <span class="va">self</span>.n_features <span class="op">=</span> n_features</span>
<span id="cb13-10"><a href="#cb13-10"></a>        <span class="va">self</span>.n_outputs <span class="op">=</span> n_outputs</span>
<span id="cb13-11"><a href="#cb13-11"></a>        <span class="va">self</span>.n_layers <span class="op">=</span> n_layers</span>
<span id="cb13-12"><a href="#cb13-12"></a>        <span class="va">self</span>.hidden <span class="op">=</span> <span class="va">None</span></span>
<span id="cb13-13"><a href="#cb13-13"></a></span>
<span id="cb13-14"><a href="#cb13-14"></a>        <span class="co"># Simple RNN</span></span>
<span id="cb13-15"><a href="#cb13-15"></a>        <span class="va">self</span>.basic_rnn <span class="op">=</span> nn.RNN(</span>
<span id="cb13-16"><a href="#cb13-16"></a>            <span class="va">self</span>.n_features, <span class="va">self</span>.hidden_dim, <span class="va">self</span>.n_layers, batch_first<span class="op">=</span><span class="va">True</span></span>
<span id="cb13-17"><a href="#cb13-17"></a>        )</span>
<span id="cb13-18"><a href="#cb13-18"></a>        <span class="co"># Classifier to produce as many logits as outputs</span></span>
<span id="cb13-19"><a href="#cb13-19"></a>        <span class="va">self</span>.classifier <span class="op">=</span> nn.Linear(<span class="va">self</span>.hidden_dim, <span class="va">self</span>.n_outputs)</span>
<span id="cb13-20"><a href="#cb13-20"></a></span>
<span id="cb13-21"><a href="#cb13-21"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, X):</span>
<span id="cb13-22"><a href="#cb13-22"></a>        <span class="co"># X is batch first (N, L, F)</span></span>
<span id="cb13-23"><a href="#cb13-23"></a>        <span class="co"># output is (N, L, H)</span></span>
<span id="cb13-24"><a href="#cb13-24"></a>        <span class="co"># final hidden state is (1, N, H)</span></span>
<span id="cb13-25"><a href="#cb13-25"></a>        <span class="co"># print(X.shape)</span></span>
<span id="cb13-26"><a href="#cb13-26"></a>        batch_first_output, <span class="va">self</span>.hidden <span class="op">=</span> <span class="va">self</span>.basic_rnn(X)</span>
<span id="cb13-27"><a href="#cb13-27"></a></span>
<span id="cb13-28"><a href="#cb13-28"></a>        <span class="co"># print("check1")</span></span>
<span id="cb13-29"><a href="#cb13-29"></a>        <span class="co"># only last item in sequence (N, 1, H)</span></span>
<span id="cb13-30"><a href="#cb13-30"></a>        last_output <span class="op">=</span> batch_first_output[:, <span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb13-31"><a href="#cb13-31"></a>        <span class="co"># classifier will output (N, 1, n_outputs)</span></span>
<span id="cb13-32"><a href="#cb13-32"></a>        out <span class="op">=</span> <span class="va">self</span>.classifier(last_output)</span>
<span id="cb13-33"><a href="#cb13-33"></a></span>
<span id="cb13-34"><a href="#cb13-34"></a>        <span class="co"># final output is (N, n_outputs)</span></span>
<span id="cb13-35"><a href="#cb13-35"></a>        <span class="cf">return</span> out.view(<span class="op">-</span><span class="dv">1</span>, <span class="va">self</span>.n_outputs)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>Configure model loss and optimizer.</p>
<div id="cell-28" class="cell" data-execution_count="12">
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1"></a>torch.manual_seed(<span class="dv">21</span>)</span>
<span id="cb14-2"><a href="#cb14-2"></a>rnn_model <span class="op">=</span> RnnModel(n_features<span class="op">=</span><span class="dv">1</span>, hidden_dim<span class="op">=</span><span class="dv">10</span>, n_outputs<span class="op">=</span><span class="dv">1</span>, n_layers<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb14-3"><a href="#cb14-3"></a>rnn_loss <span class="op">=</span> nn.MSELoss()</span>
<span id="cb14-4"><a href="#cb14-4"></a>rnn_optimizer <span class="op">=</span> optim.Adam(rnn_model.parameters(), lr<span class="op">=</span><span class="fl">0.01</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Run the training pipeline.</p>
<div id="cell-30" class="cell" data-outputid="9b24c6cc-0871-41b0-8c74-9ae3d17df455" data-execution_count="13">
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1"></a>dlp_rnn <span class="op">=</span> DeepLearningPipeline(rnn_model, rnn_loss, rnn_optimizer)</span>
<span id="cb15-2"><a href="#cb15-2"></a>dlp_rnn.set_loaders(train_loader_synth, test_loader_synth)</span>
<span id="cb15-3"><a href="#cb15-3"></a>dlp_rnn.train(<span class="dv">100</span>, print_loss<span class="op">=</span><span class="va">True</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch  10 MSE:  0.0043875276698434554
Epoch  20 MSE:  0.003170915104088966
Epoch  30 MSE:  0.0032213201127226716
Epoch  40 MSE:  0.003209590242477134
Epoch  50 MSE:  0.0030302550162146376
Epoch  60 MSE:  0.0031480757964097643
Epoch  70 MSE:  0.002840602589210241
Epoch  80 MSE:  0.0030571757948068394
Epoch  90 MSE:  0.0031562208208594134</code></pre>
</div>
</div>
<p>Plot the model loss.</p>
<div id="cell-32" class="cell" data-outputid="7c35213c-f58f-4668-8abf-265f4db63799" data-execution_count="14">
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1"></a>fig <span class="op">=</span> dlp_rnn.plot_losses()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="2022-11-07-timeseries-rnn-gru-lstm-cnn-pytorch_files/figure-html/cell-15-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Get predictions on the test data.</p>
<div id="cell-34" class="cell" data-execution_count="15">
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1"></a>y_test_pred_synth <span class="op">=</span> dlp_rnn.predict(x_test_synth)</span>
<span id="cb18-2"><a href="#cb18-2"></a>y_train_pred_synth <span class="op">=</span> dlp_rnn.predict(x_train_synth)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Calculate mean squared error.</p>
<div id="cell-36" class="cell" data-outputid="b551f975-9570-4149-9ade-66dd2d6c438e" data-execution_count="16">
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1"></a><span class="im">import</span> math</span>
<span id="cb19-2"><a href="#cb19-2"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> mean_squared_error</span>
<span id="cb19-3"><a href="#cb19-3"></a><span class="im">from</span> math <span class="im">import</span> sqrt</span>
<span id="cb19-4"><a href="#cb19-4"></a></span>
<span id="cb19-5"><a href="#cb19-5"></a><span class="co"># calculate root mean squared error</span></span>
<span id="cb19-6"><a href="#cb19-6"></a>trainScore <span class="op">=</span> math.sqrt(</span>
<span id="cb19-7"><a href="#cb19-7"></a>    mean_squared_error(y_train_synth[:, <span class="dv">0</span>], y_train_pred_synth[:, <span class="dv">0</span>])</span>
<span id="cb19-8"><a href="#cb19-8"></a>)</span>
<span id="cb19-9"><a href="#cb19-9"></a><span class="bu">print</span>(<span class="st">"Train Score: </span><span class="sc">%.2f</span><span class="st"> RMSE"</span> <span class="op">%</span> (trainScore))</span>
<span id="cb19-10"><a href="#cb19-10"></a>testScore <span class="op">=</span> math.sqrt(mean_squared_error(y_test_synth[:, <span class="dv">0</span>], y_test_pred_synth[:, <span class="dv">0</span>]))</span>
<span id="cb19-11"><a href="#cb19-11"></a><span class="bu">print</span>(<span class="st">"Test Score: </span><span class="sc">%.2f</span><span class="st"> RMSE"</span> <span class="op">%</span> (testScore))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Train Score: 0.05 RMSE
Test Score: 0.05 RMSE</code></pre>
</div>
</div>
<p>Plot the predicted values along with true values on the test data.</p>
<div id="cell-38" class="cell" data-outputid="6cb59e5d-be0c-4b9e-ccf2-3917b8014846" data-execution_count="17">
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1"></a><span class="kw">def</span> plot_predictions(y, y_pred, model_name<span class="op">=</span><span class="st">""</span>):</span>
<span id="cb21-2"><a href="#cb21-2"></a>    plt.figure(figsize<span class="op">=</span>(<span class="dv">15</span>, <span class="dv">7</span>))</span>
<span id="cb21-3"><a href="#cb21-3"></a></span>
<span id="cb21-4"><a href="#cb21-4"></a>    x <span class="op">=</span> np.arange(<span class="bu">len</span>(y))</span>
<span id="cb21-5"><a href="#cb21-5"></a>    plt.plot(x, y, color<span class="op">=</span><span class="st">"red"</span>, label<span class="op">=</span><span class="st">"True values"</span>)</span>
<span id="cb21-6"><a href="#cb21-6"></a>    plt.plot(x, y_pred, color<span class="op">=</span><span class="st">"blue"</span>, label<span class="op">=</span><span class="st">"Predicted values"</span>)</span>
<span id="cb21-7"><a href="#cb21-7"></a></span>
<span id="cb21-8"><a href="#cb21-8"></a>    title <span class="op">=</span> <span class="st">"Comparison of true and predicted values"</span></span>
<span id="cb21-9"><a href="#cb21-9"></a>    <span class="cf">if</span> <span class="bu">len</span>(model_name):</span>
<span id="cb21-10"><a href="#cb21-10"></a>        title <span class="op">=</span> model_name <span class="op">+</span> <span class="st">": "</span> <span class="op">+</span> title</span>
<span id="cb21-11"><a href="#cb21-11"></a></span>
<span id="cb21-12"><a href="#cb21-12"></a>    plt.title(title)</span>
<span id="cb21-13"><a href="#cb21-13"></a>    plt.xlabel(<span class="st">"Steps"</span>)</span>
<span id="cb21-14"><a href="#cb21-14"></a>    plt.ylabel(<span class="st">"Values"</span>)</span>
<span id="cb21-15"><a href="#cb21-15"></a>    plt.legend()</span>
<span id="cb21-16"><a href="#cb21-16"></a>    plt.show()</span>
<span id="cb21-17"><a href="#cb21-17"></a></span>
<span id="cb21-18"><a href="#cb21-18"></a></span>
<span id="cb21-19"><a href="#cb21-19"></a>plot_predictions(y_test_synth, y_test_pred_synth)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="2022-11-07-timeseries-rnn-gru-lstm-cnn-pytorch_files/figure-html/cell-18-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>That is the end of Section I. We have successfully trained a recurrent neural network on ordered sequence data, and our predicted values are very close to the actual values. We have also learned to use ordered sequences to generate training and test data sets with features and labels.</p>
</section>
</section>
<section id="section-ii" class="level1">
<h1>Section II</h1>
<p>In this section, we will use actual stock price data and try to predict future stock prices. I will be using Microsoft Corporation stock price data from 2006 to 2018, and it can be obtained from Kaggle using this link: <a href="https://www.kaggle.com/datasets/szrlee/stock-time-series-20050101-to-20171231?select=MSFT_2006-01-01_to_2018-01-01.csv">MSFT_2006-01-01_to_2018-01-01.csv</a>.</p>
<section id="data-preparation-1" class="level2">
<h2 class="anchored" data-anchor-id="data-preparation-1">Data preparation</h2>
<p>Let’s load this data and view the stock prices as a plot.</p>
<div id="cell-42" class="cell" data-outputid="3cc50f34-1c88-43de-8c47-032228d6a629" data-execution_count="18">
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1"></a>folder <span class="op">=</span> <span class="st">"./datasets/2022-11-07-timeseries-rnn-gru-lstm-cnn-pytorch/"</span></span>
<span id="cb22-2"><a href="#cb22-2"></a>file_name <span class="op">=</span> <span class="st">"MSFT_2006-01-01_to_2018-01-01.csv"</span></span>
<span id="cb22-3"><a href="#cb22-3"></a>df_msft <span class="op">=</span> pd.read_csv(folder <span class="op">+</span> file_name, parse_dates<span class="op">=</span><span class="va">True</span>, index_col<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb22-4"><a href="#cb22-4"></a>df_msft[[<span class="st">"Close"</span>]].plot(figsize<span class="op">=</span>(<span class="dv">15</span>, <span class="dv">6</span>))</span>
<span id="cb22-5"><a href="#cb22-5"></a>plt.ylabel(<span class="st">"stock_price"</span>)</span>
<span id="cb22-6"><a href="#cb22-6"></a>plt.title(<span class="st">"MSFT Stock"</span>)</span>
<span id="cb22-7"><a href="#cb22-7"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="2022-11-07-timeseries-rnn-gru-lstm-cnn-pytorch_files/figure-html/cell-19-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<div id="cell-43" class="cell" data-outputid="53e56fbf-eeac-46ec-f372-35e2a74df371" data-execution_count="19">
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1"></a><span class="co">##</span></span>
<span id="cb23-2"><a href="#cb23-2"></a><span class="co"># Range of the stock price</span></span>
<span id="cb23-3"><a href="#cb23-3"></a><span class="bu">print</span>(<span class="st">"Minimum stock price: "</span>, <span class="bu">min</span>(df_msft[<span class="st">'Close'</span>].values))</span>
<span id="cb23-4"><a href="#cb23-4"></a><span class="bu">print</span>(<span class="st">"Maximum stock price: "</span>, <span class="bu">max</span>(df_msft[<span class="st">'Close'</span>].values)) </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Minimum stock price:  15.15
Maximum stock price:  86.85</code></pre>
</div>
</div>
<p>From the above plot, we can see that the price value continuously increases over time, and the range of prices is roughly between 15 to 87 USD. This scale is not good news for neural networks as they work best when they get data on a scale closer to zero. Preferably -1 to 1. So in the next cell, we will convert our data to a much smaller scale.</p>
<div id="cell-45" class="cell" data-outputid="5505c4d8-e16a-445f-dc4b-b69442a5faf0" data-execution_count="20">
<div class="sourceCode cell-code" id="cb25"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1"></a><span class="im">from</span> sklearn.preprocessing <span class="im">import</span> MinMaxScaler</span>
<span id="cb25-2"><a href="#cb25-2"></a></span>
<span id="cb25-3"><a href="#cb25-3"></a>scaler <span class="op">=</span> MinMaxScaler(feature_range<span class="op">=</span>(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>))</span>
<span id="cb25-4"><a href="#cb25-4"></a></span>
<span id="cb25-5"><a href="#cb25-5"></a>df_msft <span class="op">=</span> df_msft[[<span class="st">"Close"</span>]]</span>
<span id="cb25-6"><a href="#cb25-6"></a><span class="co"># fill any missing values as a precaution</span></span>
<span id="cb25-7"><a href="#cb25-7"></a>df_msft <span class="op">=</span> df_msft.fillna(method<span class="op">=</span><span class="st">"ffill"</span>)</span>
<span id="cb25-8"><a href="#cb25-8"></a></span>
<span id="cb25-9"><a href="#cb25-9"></a><span class="co"># create a copy for scaling and keep original data</span></span>
<span id="cb25-10"><a href="#cb25-10"></a>df_msft_scaled <span class="op">=</span> df_msft.copy(deep<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb25-11"><a href="#cb25-11"></a>df_msft_scaled[<span class="st">"Close"</span>] <span class="op">=</span> scaler.fit_transform(df_msft[<span class="st">"Close"</span>].values.reshape(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>))</span>
<span id="cb25-12"><a href="#cb25-12"></a></span>
<span id="cb25-13"><a href="#cb25-13"></a><span class="bu">print</span>(<span class="st">"*** Before scaling ***</span><span class="ch">\n</span><span class="st">"</span>, df_msft.tail())</span>
<span id="cb25-14"><a href="#cb25-14"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">*** After scaling ***</span><span class="ch">\n</span><span class="st">"</span>, df_msft_scaled.tail())</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>*** Before scaling ***
             Close
Date             
2017-12-22  85.51
2017-12-26  85.40
2017-12-27  85.71
2017-12-28  85.72
2017-12-29  85.54

*** After scaling ***
                Close
Date                
2017-12-22  0.962622
2017-12-26  0.959554
2017-12-27  0.968201
2017-12-28  0.968480
2017-12-29  0.963459</code></pre>
</div>
</div>
<p>In the next step we will generate training and test sets for our data.</p>
<div id="cell-47" class="cell" data-execution_count="21">
<div class="sourceCode cell-code" id="cb27"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1"></a><span class="co"># look_back = size of a sequence in training and test set</span></span>
<span id="cb27-2"><a href="#cb27-2"></a>look_back <span class="op">=</span> <span class="dv">30</span></span>
<span id="cb27-3"><a href="#cb27-3"></a>x_train_scaled, y_train_scaled, x_test_scaled, y_test_scaled <span class="op">=</span> generate_sequences(</span>
<span id="cb27-4"><a href="#cb27-4"></a>    df_msft_scaled, look_back<span class="op">=</span>look_back</span>
<span id="cb27-5"><a href="#cb27-5"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Now let’s load this data into PyTorch Dataset and DataLoader class.</p>
<div id="cell-49" class="cell" data-execution_count="22">
<div class="sourceCode cell-code" id="cb28"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1"></a><span class="im">import</span> torch</span>
<span id="cb28-2"><a href="#cb28-2"></a><span class="im">from</span> torch.utils.data <span class="im">import</span> DataLoader, Dataset, random_split, TensorDataset</span>
<span id="cb28-3"><a href="#cb28-3"></a></span>
<span id="cb28-4"><a href="#cb28-4"></a>train_data <span class="op">=</span> TensorDataset(</span>
<span id="cb28-5"><a href="#cb28-5"></a>    torch.as_tensor(x_train_scaled).<span class="bu">float</span>(), torch.as_tensor(y_train_scaled).<span class="bu">float</span>()</span>
<span id="cb28-6"><a href="#cb28-6"></a>)</span>
<span id="cb28-7"><a href="#cb28-7"></a>test_data <span class="op">=</span> TensorDataset(</span>
<span id="cb28-8"><a href="#cb28-8"></a>    torch.as_tensor(x_test_scaled).<span class="bu">float</span>(), torch.as_tensor(y_test_scaled).<span class="bu">float</span>()</span>
<span id="cb28-9"><a href="#cb28-9"></a>)</span>
<span id="cb28-10"><a href="#cb28-10"></a></span>
<span id="cb28-11"><a href="#cb28-11"></a>batch_size <span class="op">=</span> <span class="dv">32</span></span>
<span id="cb28-12"><a href="#cb28-12"></a>train_loader <span class="op">=</span> DataLoader(train_data, batch_size<span class="op">=</span>batch_size, shuffle<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb28-13"><a href="#cb28-13"></a>test_loader <span class="op">=</span> DataLoader(test_data, batch_size<span class="op">=</span>batch_size)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="recurrent-neural-network-rnn" class="level2">
<h2 class="anchored" data-anchor-id="recurrent-neural-network-rnn">Recurrent Neural Network (RNN)</h2>
<p>Let’s train the same RNN we built in section 1 on stock prices data, and check it’s performance.</p>
<div id="cell-51" class="cell" data-execution_count="23">
<div class="sourceCode cell-code" id="cb29"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1"></a><span class="co">##</span></span>
<span id="cb29-2"><a href="#cb29-2"></a><span class="co"># configure model, its loss and optimizer</span></span>
<span id="cb29-3"><a href="#cb29-3"></a>torch.manual_seed(<span class="dv">21</span>)</span>
<span id="cb29-4"><a href="#cb29-4"></a>model <span class="op">=</span> RnnModel(n_features<span class="op">=</span><span class="dv">1</span>, hidden_dim<span class="op">=</span><span class="dv">32</span>, n_outputs<span class="op">=</span><span class="dv">1</span>, n_layers<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb29-5"><a href="#cb29-5"></a>loss <span class="op">=</span> nn.MSELoss()</span>
<span id="cb29-6"><a href="#cb29-6"></a>optimizer <span class="op">=</span> optim.Adam(model.parameters(), lr<span class="op">=</span><span class="fl">0.01</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-52" class="cell" data-outputid="8610bad6-9baf-4341-df3b-fc266453b8d8" data-execution_count="24">
<div class="sourceCode cell-code" id="cb30"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb30-1"><a href="#cb30-1"></a><span class="co">##</span></span>
<span id="cb30-2"><a href="#cb30-2"></a><span class="co"># check the dimension for one batch</span></span>
<span id="cb30-3"><a href="#cb30-3"></a>temp <span class="op">=</span> <span class="bu">next</span>(<span class="bu">iter</span>(train_loader))</span>
<span id="cb30-4"><a href="#cb30-4"></a><span class="bu">len</span>(temp[<span class="dv">0</span>]), <span class="bu">len</span>(temp[<span class="dv">0</span>][<span class="dv">0</span>]), <span class="bu">len</span>(temp[<span class="dv">0</span>][<span class="dv">0</span>][<span class="dv">0</span>]) <span class="co"># N (batch_size), L (seq len), F (n_features)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="24">
<pre><code>(32, 29, 1)</code></pre>
</div>
</div>
<div id="cell-53" class="cell" data-outputid="bd69acba-afd8-4556-d0ad-cc605e662455" data-execution_count="25">
<div class="sourceCode cell-code" id="cb32"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb32-1"><a href="#cb32-1"></a><span class="co">##</span></span>
<span id="cb32-2"><a href="#cb32-2"></a><span class="co"># start training pipeline</span></span>
<span id="cb32-3"><a href="#cb32-3"></a>dlp_rnn <span class="op">=</span> DeepLearningPipeline(model, loss, optimizer)</span>
<span id="cb32-4"><a href="#cb32-4"></a>dlp_rnn.set_loaders(train_loader, test_loader)</span>
<span id="cb32-5"><a href="#cb32-5"></a>dlp_rnn.train(<span class="dv">100</span>, print_loss<span class="op">=</span><span class="va">True</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch  10 MSE:  0.00026397304452874585
Epoch  20 MSE:  0.00022604088944993265
Epoch  30 MSE:  0.00023406338930891997
Epoch  40 MSE:  0.00029436370514855355
Epoch  50 MSE:  0.000270840552151309
Epoch  60 MSE:  0.000291384112201878
Epoch  70 MSE:  0.00026263904168665636
Epoch  80 MSE:  0.00022778069747304968
Epoch  90 MSE:  0.00023518060378123528</code></pre>
</div>
</div>
<p>Plot training and validation loss.</p>
<div id="cell-55" class="cell" data-outputid="04a8e9b6-152e-46cf-e2b1-16f1d8741e15" data-execution_count="26">
<div class="sourceCode cell-code" id="cb34"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb34-1"><a href="#cb34-1"></a>fig <span class="op">=</span> dlp_rnn.plot_losses()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="2022-11-07-timeseries-rnn-gru-lstm-cnn-pytorch_files/figure-html/cell-27-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>In the next cell, we will make predictions on the test data. After that, we will invert (or rescale) predicted and actual values to their original scale. Once that is done, we will use predicted and actual values to calculate RMSE (Root Mean Squared Error).</p>
<div id="cell-57" class="cell" data-outputid="b0a3c751-a4d1-4cec-bcae-76ec2bc2b9e8" data-execution_count="27">
<div class="sourceCode cell-code" id="cb35"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb35-1"><a href="#cb35-1"></a><span class="co">##</span></span>
<span id="cb35-2"><a href="#cb35-2"></a><span class="co"># make predictions on the test data</span></span>
<span id="cb35-3"><a href="#cb35-3"></a>y_test_pred_scaled <span class="op">=</span> dlp_rnn.predict(x_test_scaled)</span>
<span id="cb35-4"><a href="#cb35-4"></a>y_train_pred_scaled <span class="op">=</span> dlp_rnn.predict(x_train_scaled)</span>
<span id="cb35-5"><a href="#cb35-5"></a></span>
<span id="cb35-6"><a href="#cb35-6"></a><span class="co"># invert predictions and true values</span></span>
<span id="cb35-7"><a href="#cb35-7"></a>y_train_pred <span class="op">=</span> scaler.inverse_transform(y_train_pred_scaled)</span>
<span id="cb35-8"><a href="#cb35-8"></a>y_train <span class="op">=</span> scaler.inverse_transform(y_train_scaled)</span>
<span id="cb35-9"><a href="#cb35-9"></a>y_test_pred <span class="op">=</span> scaler.inverse_transform(y_test_pred_scaled)</span>
<span id="cb35-10"><a href="#cb35-10"></a>y_test <span class="op">=</span> scaler.inverse_transform(y_test_scaled)</span>
<span id="cb35-11"><a href="#cb35-11"></a></span>
<span id="cb35-12"><a href="#cb35-12"></a><span class="co"># calculate root mean squared error</span></span>
<span id="cb35-13"><a href="#cb35-13"></a>trainScore <span class="op">=</span> math.sqrt(mean_squared_error(y_train[:, <span class="dv">0</span>], y_train_pred[:, <span class="dv">0</span>]))</span>
<span id="cb35-14"><a href="#cb35-14"></a>testScore <span class="op">=</span> math.sqrt(mean_squared_error(y_test[:, <span class="dv">0</span>], y_test_pred[:, <span class="dv">0</span>]))</span>
<span id="cb35-15"><a href="#cb35-15"></a></span>
<span id="cb35-16"><a href="#cb35-16"></a><span class="bu">print</span>(<span class="st">"Train Score: </span><span class="sc">%.2f</span><span class="st"> RMSE"</span> <span class="op">%</span> (trainScore))</span>
<span id="cb35-17"><a href="#cb35-17"></a><span class="bu">print</span>(<span class="st">"Test Score: </span><span class="sc">%.2f</span><span class="st"> RMSE"</span> <span class="op">%</span> (testScore))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Train Score: 0.52 RMSE
Test Score: 2.37 RMSE</code></pre>
</div>
</div>
<p>Plot the True and Predicted values.</p>
<div id="cell-59" class="cell" data-outputid="f3d60015-733c-4e9d-934d-bd9d363b4e96" data-execution_count="28">
<div class="sourceCode cell-code" id="cb37"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb37-1"><a href="#cb37-1"></a>plot_predictions(y_test, y_test_pred)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="2022-11-07-timeseries-rnn-gru-lstm-cnn-pytorch_files/figure-html/cell-29-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>The above plot shows that the RNN model can correctly predict values till about 500 steps, but after that predictions start to diverge, and the gap keeps increasing as time passes.</p>
<section id="rnn-cell-in-detail" class="level3">
<h3 class="anchored" data-anchor-id="rnn-cell-in-detail">RNN cell in detail</h3>
<p>If you revisit section 1 topic ‘Model configuration and training’, we have built an RNN model using PyTorch <code>nn.RNN</code> class.</p>
<p><code>self.basic_rnn = nn.RNN(self.n_features, self.hidden_dim, self.n_layers, batch_first=True)</code></p>
<p>From PyTorch documentation, we don’t get much information on the internal working of this class as it only gives a short description. (<a href="https://pytorch.org/docs/stable/generated/torch.nn.RNN.html">Link here</a>)</p>
<blockquote class="blockquote">
<p>Applies a multi-layer Elman RNN with <code>tanh</code> or <code>ReLU</code> non-linearity to an input sequence.</p>
</blockquote>
<p><a href="https://github.com/dvgodoy/PyTorchStepByStep/blob/master/Chapter08.ipynb">PyTorch Step By Step Chapter 8</a> does a great job of explaining the internal working of an RNN Cell. I have taken the following image from the book’s official GitHub repo.</p>
<p><img src="images/2022-11-07-timeseries-rnn-gru-lstm-cnn-pytorch/rnn_cell_diagram.png" class="img-fluid" alt="rnn_cell_diagram.png"> <em><a href="https://github.com/dvgodoy/PyTorchStepByStep/blob/master/Chapter08.ipynb">Image Source: dvgodoy/PyTorchStepByStep/blob/master/Chapter08</a></em></p>
<p>From this image we can reason the following</p>
<ul>
<li>There are two types of weight layers
<ul>
<li>First weight layer (Wi) which processes the input. Let’s call it as ‘input linear layer’ or in short <code>linear_input</code></li>
<li>Second weight layer (Wh) which processes the hidden state. Let’s call it as hidden linear layer or in short <code>linear_hidden</code></li>
</ul></li>
<li>RNN cell processes two types of input at the same time
<ul>
<li>Sequence data input or <code>X</code></li>
<li>and the Hidden state. Hidden state is also the output of the RNN cell at each step, and it is returned as an input for the next step</li>
</ul></li>
</ul>
<p>The processing of an RNN cell can be described in the following steps</p>
<ol type="1">
<li>Pass input (<code>X=[x0, x1]</code>) to input linear layer (linear_input) and get the output (<code>tx=[t0, t1</code>])</li>
<li>Pass the last “hidden state” to the hidden linear layer (linear_hidden), and get the output (<code>th=[h0, h1</code>]). Since at the start we don’t have a hidden state from the last step, we can manually assign hidden state as zeros and pass it to hidden linear layer.</li>
<li>Add both outputs <code>tx</code> and <code>th</code>. Let’s call it <code>adding</code></li>
<li>pass the ‘adding’ to activation function <code>tanh</code>. The output is the new “hidden state” and will be used in the next step.</li>
</ol>
<p>Now that we have learned how an RNN cell works let’s build it ourselves without relying on PyTorch <code>nn.RNN</code> class. To ensure that our custom RNN cell produces the same output as nn.RNN, we will do the following test</p>
<ul>
<li>Create two linear layers that will represent input and hidden layers (<code>linear_input</code> and <code>linear_hidden</code> respectively)</li>
<li>Create an nn.RNN cell</li>
<li>Copy and assign the weights from nn.RNN cell to input and hidden linear layers</li>
<li>Train both linear layers and nn.RNN cell on an input data point</li>
<li>Compare the weight states of both. If they match, then we have successfully replicated the internal functionality of an.RNN cell.</li>
</ul>
<div id="cell-63" class="cell" data-execution_count="29">
<div class="sourceCode cell-code" id="cb38"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb38-1"><a href="#cb38-1"></a><span class="co">##</span></span>
<span id="cb38-2"><a href="#cb38-2"></a><span class="co"># create input and hidden linear layers</span></span>
<span id="cb38-3"><a href="#cb38-3"></a>torch.manual_seed(<span class="dv">19</span>)</span>
<span id="cb38-4"><a href="#cb38-4"></a>n_features <span class="op">=</span> <span class="dv">2</span></span>
<span id="cb38-5"><a href="#cb38-5"></a>hidden_dim <span class="op">=</span> <span class="dv">2</span></span>
<span id="cb38-6"><a href="#cb38-6"></a></span>
<span id="cb38-7"><a href="#cb38-7"></a>linear_input <span class="op">=</span> nn.Linear(n_features, hidden_dim)</span>
<span id="cb38-8"><a href="#cb38-8"></a>linear_hidden <span class="op">=</span> nn.Linear(hidden_dim, hidden_dim)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-64" class="cell" data-outputid="b2d244e0-7ad7-4607-9402-5d7cdb6137e5" data-execution_count="30">
<div class="sourceCode cell-code" id="cb39"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb39-1"><a href="#cb39-1"></a><span class="co">##</span></span>
<span id="cb39-2"><a href="#cb39-2"></a><span class="co"># create nn.RNN cell from PyTorch class</span></span>
<span id="cb39-3"><a href="#cb39-3"></a>rnn_cell <span class="op">=</span> nn.RNNCell(input_size<span class="op">=</span>n_features, hidden_size<span class="op">=</span>hidden_dim)</span>
<span id="cb39-4"><a href="#cb39-4"></a>rnn_state <span class="op">=</span> rnn_cell.state_dict()</span>
<span id="cb39-5"><a href="#cb39-5"></a>rnn_state</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="30">
<pre><code>OrderedDict([('weight_ih', tensor([[-0.6701, -0.5811],
                      [-0.0170, -0.5856]])),
             ('weight_hh', tensor([[ 0.1159, -0.6978],
                      [ 0.3241, -0.0983]])),
             ('bias_ih', tensor([-0.3163, -0.2153])),
             ('bias_hh', tensor([ 0.0722, -0.3242]))])</code></pre>
</div>
</div>
<p>In the last two cells, we have created two linear layers for our custom RNN cell and an instance of PyTorch nn.RNN class. In the next step, we will assign a copy of weights from nn.RNN to linear layers. This way both will have the same initial weights.</p>
<div id="cell-66" class="cell" data-execution_count="31">
<div class="sourceCode cell-code" id="cb41"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb41-1"><a href="#cb41-1"></a><span class="co">##</span></span>
<span id="cb41-2"><a href="#cb41-2"></a><span class="co"># assgin weight from nn.RNN to linear layers</span></span>
<span id="cb41-3"><a href="#cb41-3"></a><span class="cf">with</span> torch.no_grad():</span>
<span id="cb41-4"><a href="#cb41-4"></a>    linear_input.weight <span class="op">=</span> nn.Parameter(rnn_state[<span class="st">"weight_ih"</span>])</span>
<span id="cb41-5"><a href="#cb41-5"></a>    linear_input.bias <span class="op">=</span> nn.Parameter(rnn_state[<span class="st">"bias_ih"</span>])</span>
<span id="cb41-6"><a href="#cb41-6"></a>    linear_hidden.weight <span class="op">=</span> nn.Parameter(rnn_state[<span class="st">"weight_hh"</span>])</span>
<span id="cb41-7"><a href="#cb41-7"></a>    linear_hidden.bias <span class="op">=</span> nn.Parameter(rnn_state[<span class="st">"bias_hh"</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Let’s create an input data point <code>X</code> with two dimensions <code>x0</code> and <code>x1</code> * X = [x0, x1] * x0 = 1.0349 * x1 = 0.9661</p>
<div id="cell-68" class="cell" data-outputid="d789a714-1e3a-40ba-8f04-6da719bd2ce3" data-execution_count="32">
<div class="sourceCode cell-code" id="cb42"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb42-1"><a href="#cb42-1"></a>X <span class="op">=</span> torch.as_tensor(np.array([<span class="fl">1.0349</span>, <span class="fl">0.9661</span>])).<span class="bu">float</span>()</span>
<span id="cb42-2"><a href="#cb42-2"></a>X</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="32">
<pre><code>tensor([1.0349, 0.9661])</code></pre>
</div>
</div>
<p>Now let’s follow the steps we have defined for working of an RNN cell.</p>
<div id="cell-70" class="cell" data-outputid="4eda54a4-5b1c-429a-8975-a08c0c3059d6" data-execution_count="33">
<div class="sourceCode cell-code" id="cb44"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb44-1"><a href="#cb44-1"></a><span class="co">##</span></span>
<span id="cb44-2"><a href="#cb44-2"></a><span class="co"># 1. Pass input (`X=[x0, x1]`) to input linear layer (linear_input) and get the output (`tx=[t0, t1`])</span></span>
<span id="cb44-3"><a href="#cb44-3"></a>tx <span class="op">=</span> linear_input(X)</span>
<span id="cb44-4"><a href="#cb44-4"></a>tx</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="33">
<pre><code>tensor([-1.5712, -0.7985], grad_fn=&lt;AddBackward0&gt;)</code></pre>
</div>
</div>
<div id="cell-71" class="cell" data-outputid="b206fd38-012d-4815-bb15-3663307df558" data-execution_count="34">
<div class="sourceCode cell-code" id="cb46"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb46-1"><a href="#cb46-1"></a><span class="co">##</span></span>
<span id="cb46-2"><a href="#cb46-2"></a><span class="co"># 2. Pass the last "hidden state" to the hidden linear layer (linear_hidden), and get the output (`th=[h0, h1`]). </span></span>
<span id="cb46-3"><a href="#cb46-3"></a><span class="co"># Since this is the first step, and we don't have a hidden state from the last step, </span></span>
<span id="cb46-4"><a href="#cb46-4"></a><span class="co"># we can manually assign hidden state as zeros and pass it to hidden linear layer.</span></span>
<span id="cb46-5"><a href="#cb46-5"></a>initial_hidden <span class="op">=</span> torch.zeros(<span class="dv">1</span>, hidden_dim)</span>
<span id="cb46-6"><a href="#cb46-6"></a></span>
<span id="cb46-7"><a href="#cb46-7"></a>th <span class="op">=</span> linear_hidden(initial_hidden)</span>
<span id="cb46-8"><a href="#cb46-8"></a>th</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="34">
<pre><code>tensor([[ 0.0722, -0.3242]], grad_fn=&lt;AddmmBackward0&gt;)</code></pre>
</div>
</div>
<div id="cell-72" class="cell" data-outputid="b6094790-85cb-4732-d2e7-47964247f277" data-execution_count="35">
<div class="sourceCode cell-code" id="cb48"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb48-1"><a href="#cb48-1"></a><span class="co">##</span></span>
<span id="cb48-2"><a href="#cb48-2"></a><span class="co"># 3. Add both outputs `tx` and `th`. Let's call it `adding`</span></span>
<span id="cb48-3"><a href="#cb48-3"></a>t_hx <span class="op">=</span> th <span class="op">+</span> tx</span>
<span id="cb48-4"><a href="#cb48-4"></a>t_hx</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="35">
<pre><code>tensor([[-1.4991, -1.1227]], grad_fn=&lt;AddBackward0&gt;)</code></pre>
</div>
</div>
<div id="cell-73" class="cell" data-outputid="5b080184-c3ca-47a3-c31a-94771883125d" data-execution_count="36">
<div class="sourceCode cell-code" id="cb50"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb50-1"><a href="#cb50-1"></a><span class="co">##</span></span>
<span id="cb50-2"><a href="#cb50-2"></a><span class="co"># 4. pass the 'adding' to activation function `tanh`. The output is the new "hidden state" and will be used for upcoming inputs</span></span>
<span id="cb50-3"><a href="#cb50-3"></a>new_hidden_state <span class="op">=</span> torch.tanh(t_hx)</span>
<span id="cb50-4"><a href="#cb50-4"></a>new_hidden_state</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="36">
<pre><code>tensor([[-0.9050, -0.8085]], grad_fn=&lt;TanhBackward0&gt;)</code></pre>
</div>
</div>
<p>We have an output from our custom RNN cell. This is the new hidden state that will be passed to the <code>linear_hidden</code> layer in the next step.</p>
<p>Now time to compare this output with that of nn.RNN to see if they match or not.</p>
<div id="cell-75" class="cell" data-outputid="1884b68a-5c7a-43f1-de1c-638d45cd6657" data-execution_count="37">
<div class="sourceCode cell-code" id="cb52"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb52-1"><a href="#cb52-1"></a>rnn_cell(X)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="37">
<pre><code>tensor([-0.9050, -0.8085], grad_fn=&lt;SqueezeBackward1&gt;)</code></pre>
</div>
</div>
<p>Notice that the output from both the custom RNN cell and nn.RNN match. This means that we are successful in replicating the internal working of nn.RNN class.</p>
</section>
</section>
<section id="gated-recurrent-units-gru" class="level2">
<h2 class="anchored" data-anchor-id="gated-recurrent-units-gru">Gated Recurrent Units (GRU)</h2>
<p>In this section we will apply gated recurrent units on the stock price data, and compare its performance with simple RNNs.</p>
<div id="cell-78" class="cell" data-execution_count="38">
<div class="sourceCode cell-code" id="cb54"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb54-1"><a href="#cb54-1"></a><span class="co">##</span></span>
<span id="cb54-2"><a href="#cb54-2"></a><span class="co"># GRU model configuration</span></span>
<span id="cb54-3"><a href="#cb54-3"></a><span class="kw">class</span> GruModel(nn.Module):</span>
<span id="cb54-4"><a href="#cb54-4"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, n_features, hidden_dim, n_outputs, n_layers):</span>
<span id="cb54-5"><a href="#cb54-5"></a>        <span class="bu">super</span>(GruModel, <span class="va">self</span>).<span class="fu">__init__</span>()</span>
<span id="cb54-6"><a href="#cb54-6"></a>        <span class="va">self</span>.n_features <span class="op">=</span> n_features</span>
<span id="cb54-7"><a href="#cb54-7"></a>        <span class="va">self</span>.hidden_dim <span class="op">=</span> hidden_dim</span>
<span id="cb54-8"><a href="#cb54-8"></a>        <span class="va">self</span>.n_outputs <span class="op">=</span> n_outputs</span>
<span id="cb54-9"><a href="#cb54-9"></a>        <span class="va">self</span>.n_layers <span class="op">=</span> n_layers</span>
<span id="cb54-10"><a href="#cb54-10"></a>        <span class="va">self</span>.hidden <span class="op">=</span> <span class="va">None</span></span>
<span id="cb54-11"><a href="#cb54-11"></a>        <span class="co"># Simple GRU</span></span>
<span id="cb54-12"><a href="#cb54-12"></a>        <span class="va">self</span>.basic_rnn <span class="op">=</span> nn.GRU(</span>
<span id="cb54-13"><a href="#cb54-13"></a>            <span class="va">self</span>.n_features, <span class="va">self</span>.hidden_dim, <span class="va">self</span>.n_layers, batch_first<span class="op">=</span><span class="va">True</span></span>
<span id="cb54-14"><a href="#cb54-14"></a>        )</span>
<span id="cb54-15"><a href="#cb54-15"></a>        <span class="co"># Classifier to produce as many logits as outputs</span></span>
<span id="cb54-16"><a href="#cb54-16"></a>        <span class="va">self</span>.classifier <span class="op">=</span> nn.Linear(<span class="va">self</span>.hidden_dim, <span class="va">self</span>.n_outputs)</span>
<span id="cb54-17"><a href="#cb54-17"></a></span>
<span id="cb54-18"><a href="#cb54-18"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, X):</span>
<span id="cb54-19"><a href="#cb54-19"></a>        <span class="co"># X is batch first (N, L, F)</span></span>
<span id="cb54-20"><a href="#cb54-20"></a>        <span class="co"># output is (N, L, H)</span></span>
<span id="cb54-21"><a href="#cb54-21"></a>        <span class="co"># final hidden state is (1, N, H)</span></span>
<span id="cb54-22"><a href="#cb54-22"></a>        batch_first_output, <span class="va">self</span>.hidden <span class="op">=</span> <span class="va">self</span>.basic_rnn(X)</span>
<span id="cb54-23"><a href="#cb54-23"></a></span>
<span id="cb54-24"><a href="#cb54-24"></a>        <span class="co"># only last item in sequence (N, 1, H)</span></span>
<span id="cb54-25"><a href="#cb54-25"></a>        last_output <span class="op">=</span> batch_first_output[:, <span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb54-26"><a href="#cb54-26"></a>        <span class="co"># classifier will output (N, 1, n_outputs)</span></span>
<span id="cb54-27"><a href="#cb54-27"></a>        out <span class="op">=</span> <span class="va">self</span>.classifier(last_output)</span>
<span id="cb54-28"><a href="#cb54-28"></a></span>
<span id="cb54-29"><a href="#cb54-29"></a>        <span class="co"># final output is (N, n_outputs)</span></span>
<span id="cb54-30"><a href="#cb54-30"></a>        <span class="cf">return</span> out.view(<span class="op">-</span><span class="dv">1</span>, <span class="va">self</span>.n_outputs)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Configure model loss and optimizer.</p>
<div id="cell-80" class="cell" data-execution_count="39">
<div class="sourceCode cell-code" id="cb55"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb55-1"><a href="#cb55-1"></a>torch.manual_seed(<span class="dv">21</span>)</span>
<span id="cb55-2"><a href="#cb55-2"></a>gru_model <span class="op">=</span> GruModel(n_features<span class="op">=</span><span class="dv">1</span>, hidden_dim<span class="op">=</span><span class="dv">32</span>, n_outputs<span class="op">=</span><span class="dv">1</span>, n_layers<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb55-3"><a href="#cb55-3"></a>gru_loss <span class="op">=</span> nn.MSELoss()</span>
<span id="cb55-4"><a href="#cb55-4"></a>gru_optimizer <span class="op">=</span> optim.Adam(gru_model.parameters(), lr<span class="op">=</span><span class="fl">0.01</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Run the training pipeline for 100 epochs.</p>
<div id="cell-82" class="cell" data-outputid="caf8df47-ffc3-40ac-ff94-26f3edc0fc12" data-execution_count="40">
<div class="sourceCode cell-code" id="cb56"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb56-1"><a href="#cb56-1"></a>dlp_gru <span class="op">=</span> DeepLearningPipeline(gru_model, gru_loss, gru_optimizer)</span>
<span id="cb56-2"><a href="#cb56-2"></a>dlp_gru.set_loaders(train_loader, test_loader)</span>
<span id="cb56-3"><a href="#cb56-3"></a>dlp_gru.train(<span class="dv">100</span>, print_loss<span class="op">=</span><span class="va">True</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch  10 MSE:  0.00022010822761909697
Epoch  20 MSE:  0.00020518084370864514
Epoch  30 MSE:  0.00020595710090922446
Epoch  40 MSE:  0.00020482327377204925
Epoch  50 MSE:  0.00022252999384143163
Epoch  60 MSE:  0.0002140117964396874
Epoch  70 MSE:  0.00023651681564815314
Epoch  80 MSE:  0.00020522110384208094
Epoch  90 MSE:  0.0002454350946980853</code></pre>
</div>
</div>
<p>Plot the training and validation loss.</p>
<div id="cell-84" class="cell" data-outputid="0895d552-d9ee-4c73-ca37-245c8200640b" data-execution_count="41">
<div class="sourceCode cell-code" id="cb58"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb58-1"><a href="#cb58-1"></a>fig <span class="op">=</span> dlp_gru.plot_losses()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="2022-11-07-timeseries-rnn-gru-lstm-cnn-pytorch_files/figure-html/cell-42-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Make prediction on test data and calculate the loss.</p>
<div id="cell-86" class="cell" data-outputid="a6c14ceb-71d1-4071-ef08-abfa17fe47cd" data-execution_count="42">
<div class="sourceCode cell-code" id="cb59"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb59-1"><a href="#cb59-1"></a><span class="co"># make predictions</span></span>
<span id="cb59-2"><a href="#cb59-2"></a>y_test_pred_scaled <span class="op">=</span> dlp_gru.predict(x_test_scaled)</span>
<span id="cb59-3"><a href="#cb59-3"></a>y_train_pred_scaled <span class="op">=</span> dlp_gru.predict(x_train_scaled)</span>
<span id="cb59-4"><a href="#cb59-4"></a></span>
<span id="cb59-5"><a href="#cb59-5"></a><span class="co"># invert predictions</span></span>
<span id="cb59-6"><a href="#cb59-6"></a>y_train_pred <span class="op">=</span> scaler.inverse_transform(y_train_pred_scaled)</span>
<span id="cb59-7"><a href="#cb59-7"></a>y_train <span class="op">=</span> scaler.inverse_transform(y_train_scaled)</span>
<span id="cb59-8"><a href="#cb59-8"></a>y_test_pred <span class="op">=</span> scaler.inverse_transform(y_test_pred_scaled)</span>
<span id="cb59-9"><a href="#cb59-9"></a>y_test <span class="op">=</span> scaler.inverse_transform(y_test_scaled)</span>
<span id="cb59-10"><a href="#cb59-10"></a></span>
<span id="cb59-11"><a href="#cb59-11"></a><span class="co"># calculate root mean squared error</span></span>
<span id="cb59-12"><a href="#cb59-12"></a>trainScore <span class="op">=</span> math.sqrt(mean_squared_error(y_train[:, <span class="dv">0</span>], y_train_pred[:, <span class="dv">0</span>]))</span>
<span id="cb59-13"><a href="#cb59-13"></a>testScore <span class="op">=</span> math.sqrt(mean_squared_error(y_test[:, <span class="dv">0</span>], y_test_pred[:, <span class="dv">0</span>]))</span>
<span id="cb59-14"><a href="#cb59-14"></a></span>
<span id="cb59-15"><a href="#cb59-15"></a><span class="bu">print</span>(<span class="st">"Train Score: </span><span class="sc">%.2f</span><span class="st"> RMSE"</span> <span class="op">%</span> (trainScore))</span>
<span id="cb59-16"><a href="#cb59-16"></a><span class="bu">print</span>(<span class="st">"Test Score: </span><span class="sc">%.2f</span><span class="st"> RMSE"</span> <span class="op">%</span> (testScore))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Train Score: 0.48 RMSE
Test Score: 2.68 RMSE</code></pre>
</div>
</div>
<p>Plot predictions along with actual data.</p>
<div id="cell-88" class="cell" data-outputid="84fbea2e-d28d-4fed-d8cc-0008bb6a8861" data-execution_count="43">
<div class="sourceCode cell-code" id="cb61"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb61-1"><a href="#cb61-1"></a>plot_predictions(y_test, y_test_pred)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="2022-11-07-timeseries-rnn-gru-lstm-cnn-pytorch_files/figure-html/cell-44-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>GRU seems to be on par with RNN. It has a slightly better training score, but at the same time, it performed somewhat poorly on the validation data.</p>
<section id="gru-cell-in-detail" class="level3">
<h3 class="anchored" data-anchor-id="gru-cell-in-detail">GRU cell in detail</h3>
<p><img src="images/2022-11-07-timeseries-rnn-gru-lstm-cnn-pytorch/gru_cell.png" class="img-fluid" alt="gru_cell.png"> <em><a href="https://github.com/dvgodoy/PyTorchStepByStep/blob/master/Chapter08.ipynb">Image Source: dvgodoy/PyTorchStepByStep/blob/master/Chapter08</a></em></p>
<p>From the above image we can see that GRU cell is more advanced than a simple RNN cell. It has two more weight layers commonly referred as <strong>gates</strong></p>
<ul>
<li><code>Reset gate</code>: This weight layer is used to control how much of the past is needed to neglect or forget. This gate has a control <code>r</code> which is learned during training.
<ul>
<li>If we decrease r to 0, then the current state of cell <strong>n</strong> is less and less influenced by the old hidden state</li>
<li>If we increase r all the way to 1, then the current state will have maximum affect of the last hidden state.</li>
</ul></li>
<li><code>Update gate</code>: This weight layer is used to control how much of the past information is needed to be passed on to the next step. This gate has a control <code>z</code> which is also learned during training.
<ul>
<li>If we decrease z all the way to 0, then the new hidden state <strong>h`</strong> is closer and closer to current state of the cell. In the figure the current state is <strong>n</strong></li>
<li>If we increase z all the way to 1, then new hidden state <strong>h`</strong> is simply a copy of last hidden state <strong>h</strong></li>
</ul></li>
<li>If we decrease both r and z to 0, then GRU is simply a linear layer followed by an activation layer.</li>
</ul>
<p><strong>How does having two extra learnable weight layers and their controls make GRU better than RNN?</strong></p>
<ul>
<li>It is like giving more control to a neural network to decide which information it wants to retain and which to forget as time passes.</li>
<li>It may seem like both layers are trying to achieve the same thing: What information to keep or forget? But there is more to it. Suppose that we have a single weight layer as in RNN. For RNN, if the neural network has decided to forget something, then that information is gone. If it is needed in future steps, then the network will have to relearn it. In the case of GRU, the network has the luxury that the information it wants to forget can be parked in a separate layer (forget layer). If, in the future, that information is needed again, then it can simply change the gate control and make that information available.</li>
</ul>
<p><strong>Why learning and forgetting are important for recurrent neural networks?</strong></p>
<p>Patterns change over time for ordered sequence data like stock prices, and we want our networks to be sensitive to such changes. A repeating hidden state helps the network connect the dots between new information it has received and the past it has learned. If the network finds the new information it received has changed from past learning, it will try to unlearn some of the past experiences and learn the new pattern. Different variants of RNN are designed to give more and more such controls to the network and make it efficient in deciding which information to learn or forget.</p>
</section>
</section>
<section id="long-short-term-memory-lstm" class="level2">
<h2 class="anchored" data-anchor-id="long-short-term-memory-lstm">Long Short Term Memory (LSTM)</h2>
<p>In this section we will apply lstm on the stock price data, and compare its performance with RNN and GRU.</p>
<div id="cell-91" class="cell" data-execution_count="44">
<div class="sourceCode cell-code" id="cb62"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb62-1"><a href="#cb62-1"></a><span class="co">##</span></span>
<span id="cb62-2"><a href="#cb62-2"></a><span class="co"># LSTM model configuration</span></span>
<span id="cb62-3"><a href="#cb62-3"></a><span class="kw">class</span> LstmModel(nn.Module):</span>
<span id="cb62-4"><a href="#cb62-4"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, n_features, hidden_dim, n_outputs, n_layers):</span>
<span id="cb62-5"><a href="#cb62-5"></a>        <span class="bu">super</span>(LstmModel, <span class="va">self</span>).<span class="fu">__init__</span>()</span>
<span id="cb62-6"><a href="#cb62-6"></a>        <span class="va">self</span>.hidden_dim <span class="op">=</span> hidden_dim</span>
<span id="cb62-7"><a href="#cb62-7"></a>        <span class="va">self</span>.n_features <span class="op">=</span> n_features</span>
<span id="cb62-8"><a href="#cb62-8"></a>        <span class="va">self</span>.n_outputs <span class="op">=</span> n_outputs</span>
<span id="cb62-9"><a href="#cb62-9"></a>        <span class="va">self</span>.n_layers <span class="op">=</span> n_layers</span>
<span id="cb62-10"><a href="#cb62-10"></a></span>
<span id="cb62-11"><a href="#cb62-11"></a>        <span class="va">self</span>.hidden <span class="op">=</span> <span class="va">None</span></span>
<span id="cb62-12"><a href="#cb62-12"></a>        <span class="va">self</span>.cell <span class="op">=</span> <span class="va">None</span></span>
<span id="cb62-13"><a href="#cb62-13"></a>        <span class="co"># Simple LSTM</span></span>
<span id="cb62-14"><a href="#cb62-14"></a>        <span class="va">self</span>.basic_rnn <span class="op">=</span> nn.LSTM(</span>
<span id="cb62-15"><a href="#cb62-15"></a>            <span class="va">self</span>.n_features, <span class="va">self</span>.hidden_dim, <span class="va">self</span>.n_layers, batch_first<span class="op">=</span><span class="va">True</span></span>
<span id="cb62-16"><a href="#cb62-16"></a>        )</span>
<span id="cb62-17"><a href="#cb62-17"></a>        <span class="co"># Classifier to produce as many logits as outputs</span></span>
<span id="cb62-18"><a href="#cb62-18"></a>        <span class="va">self</span>.classifier <span class="op">=</span> nn.Linear(<span class="va">self</span>.hidden_dim, <span class="va">self</span>.n_outputs)</span>
<span id="cb62-19"><a href="#cb62-19"></a></span>
<span id="cb62-20"><a href="#cb62-20"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, X):</span>
<span id="cb62-21"><a href="#cb62-21"></a>        <span class="co"># X is batch first (N, L, F)</span></span>
<span id="cb62-22"><a href="#cb62-22"></a>        <span class="co"># output is (N, L, H)</span></span>
<span id="cb62-23"><a href="#cb62-23"></a>        <span class="co"># final hidden state is (1, N, H)</span></span>
<span id="cb62-24"><a href="#cb62-24"></a>        <span class="co"># final cell state is (1, N, H)</span></span>
<span id="cb62-25"><a href="#cb62-25"></a>        batch_first_output, (<span class="va">self</span>.hidden, <span class="va">self</span>.cell) <span class="op">=</span> <span class="va">self</span>.basic_rnn(X)</span>
<span id="cb62-26"><a href="#cb62-26"></a></span>
<span id="cb62-27"><a href="#cb62-27"></a>        <span class="co"># only last item in sequence (N, 1, H)</span></span>
<span id="cb62-28"><a href="#cb62-28"></a>        last_output <span class="op">=</span> batch_first_output[:, <span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb62-29"><a href="#cb62-29"></a>        <span class="co"># classifier will output (N, 1, n_outputs)</span></span>
<span id="cb62-30"><a href="#cb62-30"></a>        out <span class="op">=</span> <span class="va">self</span>.classifier(last_output)</span>
<span id="cb62-31"><a href="#cb62-31"></a></span>
<span id="cb62-32"><a href="#cb62-32"></a>        <span class="co"># final output is (N, n_outputs)</span></span>
<span id="cb62-33"><a href="#cb62-33"></a>        <span class="cf">return</span> out.view(<span class="op">-</span><span class="dv">1</span>, <span class="va">self</span>.n_outputs)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Define model loss and optimizer.</p>
<div id="cell-93" class="cell" data-execution_count="45">
<div class="sourceCode cell-code" id="cb63"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb63-1"><a href="#cb63-1"></a>torch.manual_seed(<span class="dv">21</span>)</span>
<span id="cb63-2"><a href="#cb63-2"></a>lstm_model <span class="op">=</span> LstmModel(n_features<span class="op">=</span><span class="dv">1</span>, hidden_dim<span class="op">=</span><span class="dv">32</span>, n_outputs<span class="op">=</span><span class="dv">1</span>, n_layers<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb63-3"><a href="#cb63-3"></a>lstm_loss <span class="op">=</span> nn.MSELoss()</span>
<span id="cb63-4"><a href="#cb63-4"></a>lstm_optimizer <span class="op">=</span> optim.Adam(lstm_model.parameters(), lr<span class="op">=</span><span class="fl">0.01</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Run the training pipeline.</p>
<div id="cell-95" class="cell" data-outputid="c5fce8a1-d441-44f5-ea1e-7d7a722f0e11" data-execution_count="46">
<div class="sourceCode cell-code" id="cb64"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb64-1"><a href="#cb64-1"></a>dlp_lstm <span class="op">=</span> DeepLearningPipeline(lstm_model, lstm_loss, lstm_optimizer)</span>
<span id="cb64-2"><a href="#cb64-2"></a>dlp_lstm.set_loaders(train_loader, test_loader)</span>
<span id="cb64-3"><a href="#cb64-3"></a>dlp_lstm.train(<span class="dv">100</span>, print_loss<span class="op">=</span><span class="va">True</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch  10 MSE:  0.0003263879698351957
Epoch  20 MSE:  0.000262940919569563
Epoch  30 MSE:  0.0002264043755668822
Epoch  40 MSE:  0.000254558740076997
Epoch  50 MSE:  0.0002543745165784265
Epoch  60 MSE:  0.00028126772259852396
Epoch  70 MSE:  0.00025442599127762315
Epoch  80 MSE:  0.00020528354511814982
Epoch  90 MSE:  0.00022827486629301512</code></pre>
</div>
</div>
<p>Print the training and validation loss.</p>
<div id="cell-97" class="cell" data-outputid="a5999948-5293-41cd-9b1f-5754c92ee699" data-execution_count="47">
<div class="sourceCode cell-code" id="cb66"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb66-1"><a href="#cb66-1"></a>fig <span class="op">=</span> dlp_lstm.plot_losses()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="2022-11-07-timeseries-rnn-gru-lstm-cnn-pytorch_files/figure-html/cell-48-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Make predictions on the test data and calculate the error.</p>
<div id="cell-99" class="cell" data-outputid="3bba9fb1-18a9-4914-cf70-0268cf493896" data-execution_count="48">
<div class="sourceCode cell-code" id="cb67"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb67-1"><a href="#cb67-1"></a><span class="co"># make predictions</span></span>
<span id="cb67-2"><a href="#cb67-2"></a>y_test_pred_scaled <span class="op">=</span> dlp_lstm.predict(x_test_scaled)</span>
<span id="cb67-3"><a href="#cb67-3"></a>y_train_pred_scaled <span class="op">=</span> dlp_lstm.predict(x_train_scaled)</span>
<span id="cb67-4"><a href="#cb67-4"></a></span>
<span id="cb67-5"><a href="#cb67-5"></a><span class="co"># invert predictions</span></span>
<span id="cb67-6"><a href="#cb67-6"></a>y_train_pred <span class="op">=</span> scaler.inverse_transform(y_train_pred_scaled)</span>
<span id="cb67-7"><a href="#cb67-7"></a>y_train <span class="op">=</span> scaler.inverse_transform(y_train_scaled)</span>
<span id="cb67-8"><a href="#cb67-8"></a>y_test_pred <span class="op">=</span> scaler.inverse_transform(y_test_pred_scaled)</span>
<span id="cb67-9"><a href="#cb67-9"></a>y_test <span class="op">=</span> scaler.inverse_transform(y_test_scaled)</span>
<span id="cb67-10"><a href="#cb67-10"></a></span>
<span id="cb67-11"><a href="#cb67-11"></a><span class="co"># calculate root mean squared error</span></span>
<span id="cb67-12"><a href="#cb67-12"></a>trainScore <span class="op">=</span> math.sqrt(mean_squared_error(y_train[:, <span class="dv">0</span>], y_train_pred[:, <span class="dv">0</span>]))</span>
<span id="cb67-13"><a href="#cb67-13"></a>testScore <span class="op">=</span> math.sqrt(mean_squared_error(y_test[:, <span class="dv">0</span>], y_test_pred[:, <span class="dv">0</span>]))</span>
<span id="cb67-14"><a href="#cb67-14"></a></span>
<span id="cb67-15"><a href="#cb67-15"></a><span class="bu">print</span>(<span class="st">"Train Score: </span><span class="sc">%.2f</span><span class="st"> RMSE"</span> <span class="op">%</span> (trainScore))</span>
<span id="cb67-16"><a href="#cb67-16"></a><span class="bu">print</span>(<span class="st">"Test Score: </span><span class="sc">%.2f</span><span class="st"> RMSE"</span> <span class="op">%</span> (testScore))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Train Score: 0.53 RMSE
Test Score: 0.93 RMSE</code></pre>
</div>
</div>
<p>Plot predicted values along with true values.</p>
<div id="cell-101" class="cell" data-outputid="edd7666f-d801-4c10-d1b1-ec8574a0b71e" data-execution_count="49">
<div class="sourceCode cell-code" id="cb69"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb69-1"><a href="#cb69-1"></a>plot_predictions(y_test, y_test_pred)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="2022-11-07-timeseries-rnn-gru-lstm-cnn-pytorch_files/figure-html/cell-50-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>LSTM has performed much better on the test data compared to both RNN and GRU. But it has also taken more time to get trained.</p>
<section id="lstm-cell-in-detail" class="level3">
<h3 class="anchored" data-anchor-id="lstm-cell-in-detail">LSTM cell in detail</h3>
<p><img src="images/2022-11-07-timeseries-rnn-gru-lstm-cnn-pytorch/lstm_cell.png" class="img-fluid" alt="lstm_cell.png"> <em><a href="https://github.com/dvgodoy/PyTorchStepByStep/blob/master/Chapter08.ipynb">Image Source: dvgodoy/PyTorchStepByStep/blob/master/Chapter08</a></em></p>
<p>The above image shows that LSTM network has more learnable parameters and controls compared to RNN and GRU. There is</p>
<ul>
<li><code>Forget gate</code>: Similar to the reset gate in GRU, it controls which information needs attention and which can be ignored.</li>
<li><code>Input gate and Cell state</code>: LSTM is unique in that besides the hidden state, it also maintains a separate state called <code>cell state</code>. Cell state acts as a long-term memory, while the hidden state acts like a working or short-term memory. Input gate controls how to update the cell state based on past hidden state, past cell state, and new input.</li>
<li><code>Update gate</code>: Update gate controls how to update the hidden state to generate a new hidden state value. It gets influenced by past hidden states and new input. Cell state does not affect this gate.</li>
</ul>
</section>
</section>
<section id="one-dimensional-convolutional-neural-network-1d-convnet" class="level2">
<h2 class="anchored" data-anchor-id="one-dimensional-convolutional-neural-network-1d-convnet">One Dimensional Convolutional Neural Network (1D ConvNet)</h2>
<p>In this section we will take an alternate approach and apply a type of CNN on stock price data.</p>
<div id="cell-104" class="cell" data-execution_count="50">
<div class="sourceCode cell-code" id="cb70"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb70-1"><a href="#cb70-1"></a><span class="co">##</span></span>
<span id="cb70-2"><a href="#cb70-2"></a><span class="co"># 1D CNN model configuration</span></span>
<span id="cb70-3"><a href="#cb70-3"></a><span class="kw">class</span> CNNmodel(nn.Module):</span>
<span id="cb70-4"><a href="#cb70-4"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>):</span>
<span id="cb70-5"><a href="#cb70-5"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb70-6"><a href="#cb70-6"></a>        <span class="va">self</span>.c1 <span class="op">=</span> nn.Conv1d(<span class="dv">1</span>, <span class="dv">32</span>, <span class="dv">2</span>)</span>
<span id="cb70-7"><a href="#cb70-7"></a>        <span class="va">self</span>.p1 <span class="op">=</span> nn.AvgPool1d(<span class="dv">2</span>)</span>
<span id="cb70-8"><a href="#cb70-8"></a>        <span class="va">self</span>.c2 <span class="op">=</span> nn.Conv1d(<span class="dv">32</span>, <span class="dv">64</span>, <span class="dv">1</span>)</span>
<span id="cb70-9"><a href="#cb70-9"></a>        <span class="va">self</span>.p2 <span class="op">=</span> nn.AvgPool1d(<span class="dv">2</span>)</span>
<span id="cb70-10"><a href="#cb70-10"></a>        <span class="va">self</span>.tanh <span class="op">=</span> nn.Tanh()</span>
<span id="cb70-11"><a href="#cb70-11"></a>        <span class="va">self</span>.fc1 <span class="op">=</span> nn.Linear(<span class="dv">448</span>, <span class="dv">64</span>)</span>
<span id="cb70-12"><a href="#cb70-12"></a>        <span class="va">self</span>.fc2 <span class="op">=</span> nn.Linear(<span class="dv">64</span>, <span class="dv">1</span>)</span>
<span id="cb70-13"><a href="#cb70-13"></a></span>
<span id="cb70-14"><a href="#cb70-14"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb70-15"><a href="#cb70-15"></a>        <span class="co">"""</span></span>
<span id="cb70-16"><a href="#cb70-16"></a><span class="co">        x1:  torch.Size([32, 32, 28])</span></span>
<span id="cb70-17"><a href="#cb70-17"></a><span class="co">        x2:  torch.Size([32, 32, 14])</span></span>
<span id="cb70-18"><a href="#cb70-18"></a><span class="co">        x3:  torch.Size([32, 64, 14])</span></span>
<span id="cb70-19"><a href="#cb70-19"></a><span class="co">        x4:  torch.Size([32, 64, 7])</span></span>
<span id="cb70-20"><a href="#cb70-20"></a><span class="co">        x5:  torch.Size([32, 448])</span></span>
<span id="cb70-21"><a href="#cb70-21"></a><span class="co">        x6:  torch.Size([32, 64])</span></span>
<span id="cb70-22"><a href="#cb70-22"></a><span class="co">        x7:  torch.Size([32, 1])</span></span>
<span id="cb70-23"><a href="#cb70-23"></a><span class="co">        """</span></span>
<span id="cb70-24"><a href="#cb70-24"></a></span>
<span id="cb70-25"><a href="#cb70-25"></a>        x1 <span class="op">=</span> <span class="va">self</span>.c1(x)</span>
<span id="cb70-26"><a href="#cb70-26"></a>        x2 <span class="op">=</span> <span class="va">self</span>.p1(x1)</span>
<span id="cb70-27"><a href="#cb70-27"></a>        x3 <span class="op">=</span> <span class="va">self</span>.c2(x2)</span>
<span id="cb70-28"><a href="#cb70-28"></a>        x4 <span class="op">=</span> <span class="va">self</span>.p2(x3)</span>
<span id="cb70-29"><a href="#cb70-29"></a></span>
<span id="cb70-30"><a href="#cb70-30"></a>        x4 <span class="op">=</span> <span class="va">self</span>.tanh(x4)</span>
<span id="cb70-31"><a href="#cb70-31"></a>        x5 <span class="op">=</span> x4.reshape(x4.shape[<span class="dv">0</span>], <span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb70-32"><a href="#cb70-32"></a></span>
<span id="cb70-33"><a href="#cb70-33"></a>        x6 <span class="op">=</span> <span class="va">self</span>.fc1(x5)</span>
<span id="cb70-34"><a href="#cb70-34"></a>        x7 <span class="op">=</span> <span class="va">self</span>.fc2(x6)</span>
<span id="cb70-35"><a href="#cb70-35"></a></span>
<span id="cb70-36"><a href="#cb70-36"></a>        <span class="cf">return</span> x7</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-105" class="cell" data-execution_count="51">
<div class="sourceCode cell-code" id="cb71"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb71-1"><a href="#cb71-1"></a><span class="co">##</span></span>
<span id="cb71-2"><a href="#cb71-2"></a><span class="co"># configure model loss and optimizer</span></span>
<span id="cb71-3"><a href="#cb71-3"></a>model <span class="op">=</span> CNNmodel()</span>
<span id="cb71-4"><a href="#cb71-4"></a>loss <span class="op">=</span> nn.MSELoss()</span>
<span id="cb71-5"><a href="#cb71-5"></a>optimizer <span class="op">=</span> optim.Adam(model.parameters(), lr<span class="op">=</span><span class="fl">0.01</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-106" class="cell" data-execution_count="52">
<div class="sourceCode cell-code" id="cb72"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb72-1"><a href="#cb72-1"></a><span class="co">##</span></span>
<span id="cb72-2"><a href="#cb72-2"></a><span class="co"># change the dimension of dataset</span></span>
<span id="cb72-3"><a href="#cb72-3"></a><span class="co"># for CNN use: N (batch_size), F (n_features), L (seq len)</span></span>
<span id="cb72-4"><a href="#cb72-4"></a>train_data_1d <span class="op">=</span> TensorDataset(</span>
<span id="cb72-5"><a href="#cb72-5"></a>    torch.as_tensor(x_train_scaled).<span class="bu">float</span>().permute(<span class="dv">0</span>, <span class="dv">2</span>, <span class="dv">1</span>),</span>
<span id="cb72-6"><a href="#cb72-6"></a>    torch.as_tensor(y_train_scaled).<span class="bu">float</span>(),</span>
<span id="cb72-7"><a href="#cb72-7"></a>)</span>
<span id="cb72-8"><a href="#cb72-8"></a>test_data_1d <span class="op">=</span> TensorDataset(</span>
<span id="cb72-9"><a href="#cb72-9"></a>    torch.as_tensor(x_test_scaled).<span class="bu">float</span>().permute(<span class="dv">0</span>, <span class="dv">2</span>, <span class="dv">1</span>),</span>
<span id="cb72-10"><a href="#cb72-10"></a>    torch.as_tensor(y_test_scaled).<span class="bu">float</span>(),</span>
<span id="cb72-11"><a href="#cb72-11"></a>)</span>
<span id="cb72-12"><a href="#cb72-12"></a></span>
<span id="cb72-13"><a href="#cb72-13"></a>train_loader <span class="op">=</span> DataLoader(train_data_1d, batch_size<span class="op">=</span><span class="dv">32</span>, shuffle<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb72-14"><a href="#cb72-14"></a>test_loader <span class="op">=</span> DataLoader(test_data_1d, batch_size<span class="op">=</span><span class="dv">32</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-107" class="cell" data-outputid="7d4455b4-5ba7-4fc8-c759-33cc3773b2f7" data-execution_count="53">
<div class="sourceCode cell-code" id="cb73"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb73-1"><a href="#cb73-1"></a><span class="co">##</span></span>
<span id="cb73-2"><a href="#cb73-2"></a><span class="co"># check the dimensions of one batch</span></span>
<span id="cb73-3"><a href="#cb73-3"></a>temp <span class="op">=</span> <span class="bu">next</span>(<span class="bu">iter</span>(train_loader))</span>
<span id="cb73-4"><a href="#cb73-4"></a><span class="bu">len</span>(temp[<span class="dv">0</span>]), <span class="bu">len</span>(temp[<span class="dv">0</span>][<span class="dv">0</span>]), <span class="bu">len</span>(temp[<span class="dv">0</span>][<span class="dv">0</span>][<span class="dv">0</span>]) <span class="co"># N (batch_size), F (n_features), L (seq len)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="53">
<pre><code>(32, 1, 29)</code></pre>
</div>
</div>
<div id="cell-108" class="cell" data-outputid="b2ee6cf2-f7e2-48bc-f751-70688d14b133" data-execution_count="54">
<div class="sourceCode cell-code" id="cb75"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb75-1"><a href="#cb75-1"></a><span class="co">##</span></span>
<span id="cb75-2"><a href="#cb75-2"></a><span class="co"># run the training pipeline</span></span>
<span id="cb75-3"><a href="#cb75-3"></a>dlp_conv1d <span class="op">=</span> DeepLearningPipeline(model, loss, optimizer)</span>
<span id="cb75-4"><a href="#cb75-4"></a>dlp_conv1d.set_loaders(train_loader, test_loader)</span>
<span id="cb75-5"><a href="#cb75-5"></a>dlp_conv1d.train(<span class="dv">100</span>, print_loss<span class="op">=</span><span class="va">True</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch  10 MSE:  0.000601009127973212
Epoch  20 MSE:  0.0005247063511915533
Epoch  30 MSE:  0.0004160549495171643
Epoch  40 MSE:  0.00038349507733735004
Epoch  50 MSE:  0.0005176076665520668
Epoch  60 MSE:  0.00043436023538974536
Epoch  70 MSE:  0.0004034905033415381
Epoch  80 MSE:  0.00036779195050423203
Epoch  90 MSE:  0.00027141175329840433</code></pre>
</div>
</div>
<div id="cell-109" class="cell" data-outputid="d5664b7b-af90-42d0-e573-b917beac49dc" data-execution_count="55">
<div class="sourceCode cell-code" id="cb77"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb77-1"><a href="#cb77-1"></a><span class="co">##</span></span>
<span id="cb77-2"><a href="#cb77-2"></a><span class="co"># plot the training and validation loss</span></span>
<span id="cb77-3"><a href="#cb77-3"></a>fig <span class="op">=</span> dlp_conv1d.plot_losses()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="2022-11-07-timeseries-rnn-gru-lstm-cnn-pytorch_files/figure-html/cell-56-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<div id="cell-110" class="cell" data-outputid="9cad07c0-3ba9-4ccf-bf0f-d3d53fa9aa7e" data-execution_count="56">
<div class="sourceCode cell-code" id="cb78"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb78-1"><a href="#cb78-1"></a><span class="co">##</span></span>
<span id="cb78-2"><a href="#cb78-2"></a><span class="co"># make predictions</span></span>
<span id="cb78-3"><a href="#cb78-3"></a>y_test_pred_scaled <span class="op">=</span> dlp_conv1d.predict(</span>
<span id="cb78-4"><a href="#cb78-4"></a>    torch.as_tensor(x_test_scaled).<span class="bu">float</span>().permute(<span class="dv">0</span>, <span class="dv">2</span>, <span class="dv">1</span>)</span>
<span id="cb78-5"><a href="#cb78-5"></a>)</span>
<span id="cb78-6"><a href="#cb78-6"></a>y_train_pred_scaled <span class="op">=</span> dlp_conv1d.predict(</span>
<span id="cb78-7"><a href="#cb78-7"></a>    torch.as_tensor(x_train_scaled).<span class="bu">float</span>().permute(<span class="dv">0</span>, <span class="dv">2</span>, <span class="dv">1</span>)</span>
<span id="cb78-8"><a href="#cb78-8"></a>)</span>
<span id="cb78-9"><a href="#cb78-9"></a></span>
<span id="cb78-10"><a href="#cb78-10"></a><span class="co"># invert predictions</span></span>
<span id="cb78-11"><a href="#cb78-11"></a>y_train_pred <span class="op">=</span> scaler.inverse_transform(y_train_pred_scaled)</span>
<span id="cb78-12"><a href="#cb78-12"></a>y_train <span class="op">=</span> scaler.inverse_transform(y_train_scaled)</span>
<span id="cb78-13"><a href="#cb78-13"></a>y_test_pred <span class="op">=</span> scaler.inverse_transform(y_test_pred_scaled)</span>
<span id="cb78-14"><a href="#cb78-14"></a>y_test <span class="op">=</span> scaler.inverse_transform(y_test_scaled)</span>
<span id="cb78-15"><a href="#cb78-15"></a></span>
<span id="cb78-16"><a href="#cb78-16"></a><span class="co"># calculate root mean squared error</span></span>
<span id="cb78-17"><a href="#cb78-17"></a>trainScore <span class="op">=</span> math.sqrt(mean_squared_error(y_train[:, <span class="dv">0</span>], y_train_pred[:, <span class="dv">0</span>]))</span>
<span id="cb78-18"><a href="#cb78-18"></a>testScore <span class="op">=</span> math.sqrt(mean_squared_error(y_test[:, <span class="dv">0</span>], y_test_pred[:, <span class="dv">0</span>]))</span>
<span id="cb78-19"><a href="#cb78-19"></a></span>
<span id="cb78-20"><a href="#cb78-20"></a><span class="bu">print</span>(<span class="st">"Train Score: </span><span class="sc">%.2f</span><span class="st"> RMSE"</span> <span class="op">%</span> (trainScore))</span>
<span id="cb78-21"><a href="#cb78-21"></a><span class="bu">print</span>(<span class="st">"Test Score: </span><span class="sc">%.2f</span><span class="st"> RMSE"</span> <span class="op">%</span> (testScore))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Train Score: 0.61 RMSE
Test Score: 2.07 RMSE</code></pre>
</div>
</div>
<div id="cell-111" class="cell" data-outputid="fa265bb0-92e1-414a-9d39-89e98fb4f3c9" data-execution_count="57">
<div class="sourceCode cell-code" id="cb80"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb80-1"><a href="#cb80-1"></a><span class="co">##</span></span>
<span id="cb80-2"><a href="#cb80-2"></a><span class="co"># plot true and predicted values</span></span>
<span id="cb80-3"><a href="#cb80-3"></a>plot_predictions(y_test, y_test_pred)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="2022-11-07-timeseries-rnn-gru-lstm-cnn-pytorch_files/figure-html/cell-58-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>1D ConvNet results stand between GRU and LSTM. It is performing better than RNN and GRU but less than LSTM.</p>
<p><strong>How does 1D ConvNet compare to RNN?</strong></p>
<p>1D ConvNets have 1D convolutions, i.e.&nbsp;they move the filter in one dimension from left to right like a moving window. These kernels (or filters) behave similarly to the hidden state in RNN</p>
<ul>
<li>In RNN, we process one data point at a step and move forward, incrementing the steps till we reach the end of the sequence</li>
<li>In 1D-ConvNet, we process one sequence at a time and move the filter along the entire length of the sequence</li>
</ul>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<script src="https://utteranc.es/client.js" repo="hassaanbinaslam/myblog_utterances" issue-term="pathname" theme="github-light" crossorigin="anonymous" async="">
</script>
</div> <!-- /content -->




</body></html>