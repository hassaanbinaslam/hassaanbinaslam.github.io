<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.2.269">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="dcterms.date" content="2022-10-23">
<meta name="description" content="In this notebook, we will explore how vanishing gradients can affect the training of a deep neural network. We will visualize the gradient flow from the deeper to starting layers during the backpropagation for two popular activation functions, Sigmoid and ReLU.">

<title>Random Thoughts - Detect Vanishing Gradients in Deep Neural Networks by Plotting Gradient Distributions</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../images/favicon.ico" rel="icon">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../site_libs/bootstrap/bootstrap-dark.min.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" integrity="sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>
<script type="application/javascript">define('jquery', [],function() {return window.jQuery;})</script>
<script src="https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js" crossorigin="anonymous"></script>


<link rel="stylesheet" href="../styles.css">
<meta property="og:title" content="Random Thoughts - Detect Vanishing Gradients in Deep Neural Networks by Plotting Gradient Distributions">
<meta property="og:description" content="In this notebook, we will explore how vanishing gradients can affect the training of a deep neural network.">
<meta property="og:image" content="images/2022-10-23-pytorch-vanishing-gradients-deep-neural-networks.jpeg">
<meta property="og:site-name" content="Random Thoughts">
<meta name="twitter:title" content="Random Thoughts - Detect Vanishing Gradients in Deep Neural Networks by Plotting Gradient Distributions">
<meta name="twitter:description" content="In this notebook, we will explore how vanishing gradients can affect the training of a deep neural network.">
<meta name="twitter:image" content="images/2022-10-23-pytorch-vanishing-gradients-deep-neural-networks.jpeg">
<meta name="twitter:card" content="summary_large_image">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../index.html">
    <span class="navbar-title">Random Thoughts</span>
    </a>
  </div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../about.html">
 <span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://www.linkedin.com/in/hassaanbinaslam/"><i class="bi bi-linkedin" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
</ul>
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/hassaanbinaslam/"><i class="bi bi-github" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com/hassaanbinaslam"><i class="bi bi-twitter" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
</ul>
              <div class="quarto-toggle-container">
                  <a href="" class="quarto-color-scheme-toggle nav-link" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
              </div>
              <div id="quarto-search" class="" title="Search"></div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#credits" id="toc-credits" class="nav-link active" data-scroll-target="#credits">Credits</a></li>
  <li><a href="#environment" id="toc-environment" class="nav-link" data-scroll-target="#environment">Environment</a></li>
  <li><a href="#introduction" id="toc-introduction" class="nav-link" data-scroll-target="#introduction">Introduction</a>
  <ul class="collapse">
  <li><a href="#compare-activation-functions" id="toc-compare-activation-functions" class="nav-link" data-scroll-target="#compare-activation-functions">Compare activation functions</a></li>
  <li><a href="#visualising-activation-functions-and-their-gradients" id="toc-visualising-activation-functions-and-their-gradients" class="nav-link" data-scroll-target="#visualising-activation-functions-and-their-gradients">Visualising activation functions and their gradients</a></li>
  </ul></li>
  <li><a href="#section-i" id="toc-section-i" class="nav-link" data-scroll-target="#section-i">Section I</a>
  <ul class="collapse">
  <li><a href="#download-mnist-dataset" id="toc-download-mnist-dataset" class="nav-link" data-scroll-target="#download-mnist-dataset">Download MNIST dataset</a></li>
  <li><a href="#load-generated-data-into-pytorch-dataset-and-dataloader-class" id="toc-load-generated-data-into-pytorch-dataset-and-dataloader-class" class="nav-link" data-scroll-target="#load-generated-data-into-pytorch-dataset-and-dataloader-class">Load generated data into PyTorch Dataset and DataLoader class</a></li>
  <li><a href="#define-a-class-to-implement-training-validation-and-mini-batch-processing-pipeline" id="toc-define-a-class-to-implement-training-validation-and-mini-batch-processing-pipeline" class="nav-link" data-scroll-target="#define-a-class-to-implement-training-validation-and-mini-batch-processing-pipeline">Define a class to implement training, validation, and mini-batch processing pipeline</a></li>
  <li><a href="#create-a-model-with-sigmoid-activations" id="toc-create-a-model-with-sigmoid-activations" class="nav-link" data-scroll-target="#create-a-model-with-sigmoid-activations">Create a model with sigmoid activations</a></li>
  <li><a href="#create-a-model-with-relu-activations" id="toc-create-a-model-with-relu-activations" class="nav-link" data-scroll-target="#create-a-model-with-relu-activations">Create a model with ReLU activations</a></li>
  </ul></li>
  <li><a href="#section-ii" id="toc-section-ii" class="nav-link" data-scroll-target="#section-ii">Section II</a>
  <ul class="collapse">
  <li><a href="#gradients-for-a-model-with-sigmoid-activations" id="toc-gradients-for-a-model-with-sigmoid-activations" class="nav-link" data-scroll-target="#gradients-for-a-model-with-sigmoid-activations">Gradients for a model with Sigmoid activations</a>
  <ul class="collapse">
  <li><a href="#what-do-we-get-from-these-plots" id="toc-what-do-we-get-from-these-plots" class="nav-link" data-scroll-target="#what-do-we-get-from-these-plots">What do we get from these plots?</a></li>
  <li><a href="#can-we-improve-our-sigmoidnet" id="toc-can-we-improve-our-sigmoidnet" class="nav-link" data-scroll-target="#can-we-improve-our-sigmoidnet">Can we improve our SigmoidNet?</a></li>
  <li><a href="#ridge-plot-for-gradients" id="toc-ridge-plot-for-gradients" class="nav-link" data-scroll-target="#ridge-plot-for-gradients">Ridge plot for gradients</a></li>
  </ul></li>
  <li><a href="#gradients-for-a-model-with-relu-activations" id="toc-gradients-for-a-model-with-relu-activations" class="nav-link" data-scroll-target="#gradients-for-a-model-with-relu-activations">Gradients for a model with ReLU activations</a>
  <ul class="collapse">
  <li><a href="#what-do-we-get-from-these-plots-1" id="toc-what-do-we-get-from-these-plots-1" class="nav-link" data-scroll-target="#what-do-we-get-from-these-plots-1">What do we get from these plots?</a></li>
  <li><a href="#ridge-plot-for-gradients-1" id="toc-ridge-plot-for-gradients-1" class="nav-link" data-scroll-target="#ridge-plot-for-gradients-1">Ridge plot for gradients</a></li>
  </ul></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Detect Vanishing Gradients in Deep Neural Networks by Plotting Gradient Distributions</h1>
  <div class="quarto-categories">
    <div class="quarto-category">pytorch</div>
    <div class="quarto-category">dl</div>
  </div>
  </div>

<div>
  <div class="description">
    In this notebook, we will explore how vanishing gradients can affect the training of a deep neural network. We will visualize the gradient flow from the deeper to starting layers during the backpropagation for two popular activation functions, Sigmoid and ReLU.
  </div>
</div>


<div class="quarto-title-meta">

    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">October 23, 2022</p>
    </div>
  </div>
  
    
  </div>
  

</header>

<p><img src="images/2022-10-23-pytorch-vanishing-gradients-deep-neural-networks.jpeg" class="img-fluid"></p>
<section id="credits" class="level2">
<h2 class="anchored" data-anchor-id="credits">Credits</h2>
<p>This notebook takes inspiration and ideas from the following sources. * A great post from <code>MATLAB</code> company <code>MathWorks</code> with the same title: <a href="https://www.mathworks.com/help/deeplearning/ug/detect-vanishing-gradients-in-deep-neural-networks.html">Detect Vanishing Gradients in Deep Neural Networks by Plotting Gradient Distributions</a>. That post is written for the MATLAB audience, and I have tried translating its ideas for Python and PyTorch community. * The outstanding book “Deep Learning with PyTorch Step-by-Step” by “Daniel Voigt Godoy”. You can get the book from its website: <a href="https://pytorchstepbystep.com/">pytorchstepbystep</a>. In addition, the GitHub repository for this book has valuable notebooks and can be used independently: <a href="https://github.com/dvgodoy/PyTorchStepByStep">github.com/dvgodoy/PyTorchStepByStep</a>. Parts of the code you see in this notebook are taken from <a href="https://colab.research.google.com/github/dvgodoy/PyTorchStepByStep/blob/master/Chapter03.ipynb">chapter 3 notebook</a> of the same book. * University of Amsterdam (UvA) Deep Learning Course series. <a href="https://uvadlc.github.io/">uvadlc.github.io</a>. Their lecture on “activation functions and gradients” discusses the same topic. Here is the link: <a href="https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial3/Activation_Functions.html">tutorial3/Activation_Functions.html</a>. The course is outstanding, and lectures and notebooks are also openly shared. From the course site <em>“This course is taught in the MSc program in Artificial Intelligence of the University of Amsterdam. In this course we study the theory of deep learning, namely of modern, multi-layered neural networks trained on big data.”</em></p>
</section>
<section id="environment" class="level2">
<h2 class="anchored" data-anchor-id="environment">Environment</h2>
<p>This notebook is prepared with Google Colab.</p>
<div class="cell" data-outputid="1efe3040-9935-48ea-e1d9-2a26ce2b4710" data-execution_count="1">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> platform <span class="im">import</span> python_version</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy, matplotlib, pandas, torch, seaborn</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"python=="</span> <span class="op">+</span> python_version())</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"numpy=="</span> <span class="op">+</span> numpy.__version__)</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"torch=="</span> <span class="op">+</span> torch.__version__)</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"matplotlib=="</span> <span class="op">+</span> matplotlib.__version__)</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"seaborn=="</span> <span class="op">+</span> seaborn.__version__)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>python==3.7.15
numpy==1.21.6
torch==1.12.1+cu113
matplotlib==3.2.2
seaborn==0.11.2</code></pre>
</div>
</div>
</section>
<section id="introduction" class="level2">
<h2 class="anchored" data-anchor-id="introduction">Introduction</h2>
<p><a href="https://www.mathworks.com/help/deeplearning/ug/detect-vanishing-gradients-in-deep-neural-networks.html">MathWorks post</a> explains vanishing gradients problem in deep neural networks really well, and I am sharing a passage from it.</p>
<blockquote class="blockquote">
<p>A common problem in deep network training is vanishing gradients. Deep learning training algorithms aim to minimize the loss by adjusting the network’s learnable parameters (i.e., weights) during training. Gradient-based training algorithms determine the level of adjustment using the gradients of the loss function with respect to the current learnable parameters. The gradient computation uses the propagated gradients from the previous layers for earlier layers (i.e., from the output layer to the input layer). Therefore, when a network contains activation functions that always produce gradient values less than 1 (e.g., Sigmoid), the value of the gradients can become increasingly small as the updating algorithm moves toward the initial layers. As a result, early layers in the network can receive a vanishingly small gradient, and therefore, the network is unable to learn. However, if the gradient of the activation function is always greater than or equal to 1 (e.g., ReLU), the gradients can flow through the network, reducing the chance of vanishing gradients.</p>
</blockquote>
<p>Let’s understand it better by visualizing the gradients produced by <code>Sigmoid</code> and <code>ReLU</code>.</p>
<section id="compare-activation-functions" class="level3">
<h3 class="anchored" data-anchor-id="compare-activation-functions">Compare activation functions</h3>
<p>In this section we will compare the properties of two popular activation functions: <code>Sigmoid</code> and <code>ReLU</code>.</p>
<section id="sigmoid" class="level4">
<h4 class="anchored" data-anchor-id="sigmoid">Sigmoid</h4>
<p>Sigmoid function is normally used to refer specifically to the logistic function, also called the logistic sigmoid function. It is defined as</p>
<p>$ Sigmoid(x) = $</p>
</section>
<section id="relu" class="level4">
<h4 class="anchored" data-anchor-id="relu">ReLU</h4>
<p>ReLU function is defined as</p>
<p>$ Relu(x) = max(0,x) $</p>
</section>
</section>
<section id="visualising-activation-functions-and-their-gradients" class="level3">
<h3 class="anchored" data-anchor-id="visualising-activation-functions-and-their-gradients">Visualising activation functions and their gradients</h3>
<p>Let’s plot both these functions’ outputs and visualize their gradients. In the next cell, I have created two PyTorch classes that define Sigmoid and ReLU activations.</p>
<div class="cell" data-execution_count="2">
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="co"># A class representing Sigmoid activation function</span></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> SigmoidAct(nn.Module):</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>):</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>        <span class="cf">pass</span></span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="dv">1</span> <span class="op">/</span> (<span class="dv">1</span> <span class="op">+</span> torch.exp(<span class="op">-</span>x))</span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a><span class="co"># A class representing ReLU activation function</span></span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> ReluAct(nn.Module):</span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>):</span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a>        <span class="cf">pass</span></span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb3-19"><a href="#cb3-19" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x <span class="op">*</span> (x <span class="op">&gt;</span> <span class="dv">0</span>).<span class="bu">float</span>()</span>
<span id="cb3-20"><a href="#cb3-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-21"><a href="#cb3-21" aria-hidden="true" tabindex="-1"></a><span class="co"># initialize sigmoid activation function</span></span>
<span id="cb3-22"><a href="#cb3-22" aria-hidden="true" tabindex="-1"></a>sigmoid_fn <span class="op">=</span> SigmoidAct()</span>
<span id="cb3-23"><a href="#cb3-23" aria-hidden="true" tabindex="-1"></a><span class="co"># initialize relu activation function</span></span>
<span id="cb3-24"><a href="#cb3-24" aria-hidden="true" tabindex="-1"></a>relu_fn <span class="op">=</span> ReluAct()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>I have defined a helper function to calculate the gradients for these activation functions.</p>
<div class="cell" data-execution_count="3">
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a><span class="co"># A helper function to computes the gradients of an activation function at specified positions.</span></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> get_grads(act_fn, x):</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>    x <span class="op">=</span> x.clone().requires_grad_()  <span class="co"># Mark the input as tensor for which we want to store gradients</span></span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>    out <span class="op">=</span> act_fn(x)</span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>    out.<span class="bu">sum</span>().backward()  <span class="co"># Summing results in an equal gradient flow to each element in x</span></span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> x.grad  <span class="co"># Accessing the gradients of x by "x.grad"</span></span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a><span class="co"># A helper function to plot the activation function and its gradient</span></span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> vis_act_fn(act_fn, fn_name, ax, x):</span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Run activation function</span></span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a>    y <span class="op">=</span> act_fn(x)</span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a>    y_grads <span class="op">=</span> get_grads(act_fn, x)</span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Push x, y and gradients back to cpu for plotting</span></span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a>    x, y, y_grads <span class="op">=</span> x.cpu().numpy(), y.cpu().numpy(), y_grads.cpu().numpy()</span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Plotting</span></span>
<span id="cb4-18"><a href="#cb4-18" aria-hidden="true" tabindex="-1"></a>    ax.plot(x, y, linewidth<span class="op">=</span><span class="dv">2</span>, label<span class="op">=</span><span class="st">"Activation function"</span>)</span>
<span id="cb4-19"><a href="#cb4-19" aria-hidden="true" tabindex="-1"></a>    ax.plot(x, y_grads, linewidth<span class="op">=</span><span class="dv">2</span>, label<span class="op">=</span><span class="st">"Gradient"</span>)</span>
<span id="cb4-20"><a href="#cb4-20" aria-hidden="true" tabindex="-1"></a>    ax.set_title(fn_name)</span>
<span id="cb4-21"><a href="#cb4-21" aria-hidden="true" tabindex="-1"></a>    ax.legend()</span>
<span id="cb4-22"><a href="#cb4-22" aria-hidden="true" tabindex="-1"></a>    ax.set_ylim(<span class="op">-</span><span class="fl">1.5</span>, x.<span class="bu">max</span>())</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>let’s plot the gradients.</p>
<div class="cell" data-outputid="28a9a86d-fd82-4ad9-c041-e171cc1fd4c2" data-execution_count="4">
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> torch.linspace(<span class="op">-</span><span class="dv">5</span>, <span class="dv">5</span>, <span class="dv">1000</span>)</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">2</span>, figsize<span class="op">=</span>(<span class="dv">15</span>, <span class="dv">7</span>))</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>vis_act_fn(sigmoid_fn, <span class="st">'sigmoid'</span>, ax[<span class="dv">0</span>], x)</span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>vis_act_fn(relu_fn, <span class="st">'relu'</span>, ax[<span class="dv">1</span>], x)</span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>fig.subplots_adjust(hspace<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<p><img src="2022-10-23-pytorch-vanishing-gradients-deep-neural-networks_files/figure-html/cell-5-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>We can take the following explanations from the above plots. * The sigmoid output is bounded, as it remains between 0 and 1. The gradients are the highest when the input is close to zero and diminishes as the input value moves away from it. Notice that <strong>the sigmoid gradient curve is less than 1 for the entire range; therefore, a network containing sigmoid activation functions can suffer from a vanishing gradients problem</strong>. * The output value for ReLU is not bounded. It keeps on increasing on the positive side. And for positive output values, the gradients do not diminish but remain constant at 1. As the gradient is not decreasing, it reduces the chances of the vanishing gradients problem. However, the gradient is zero for negative values, and this state is sometimes referred to as dead ReLU. It means that if ReLU ends up in this situation, it is improbable that it will recover from it. * Why will it not recover? Because the gradient from the activation function is zero for negative inputs, it will also not update the weights during the backward pass, thus leaving the weights in that perpetual state.</p>
<p>In the coming sections, we will build networks and try to visualize how gradients flow between different layers and the effect of activation functions on them.</p>
</section>
</section>
<section id="section-i" class="level1">
<h1>Section I</h1>
<p>I have split the notebook into two sections. In this first section, we will create two multi-layer models: one with sigmoids and the other with ReLU. We will train them on MNIST data and plot their losses. Then in the next section, we will plot their gradients to see how activation functions can affect the behavior of a model.</p>
<section id="download-mnist-dataset" class="level2">
<h2 class="anchored" data-anchor-id="download-mnist-dataset">Download MNIST dataset</h2>
<p>MNIST dataset can be downloaded easily from PyTorch built-in datasets provided under <code>torchvision.datasets</code>. In this section, we will download it, split it into train and test datasets, and then convert them into PyTorch tensors. * Read more about the PyTorch MNIST dataset <a href="https://pytorch.org/vision/stable/generated/torchvision.datasets.MNIST.html#torchvision.datasets.MNIST">here</a> * <code>torchvision.transforms.Compose</code> is like a container to hold a list of transformations you intend to apply. Read more about it <a href="https://pytorch.org/vision/stable/generated/torchvision.transforms.Compose.html">here</a> * <code>torchvision.transforms.ToTensor</code> converts a <code>PIL Image</code> or <code>numpy.ndarray</code> to tensor. It converts a PIL Image or numpy.ndarray <strong>(H x W x C) in the range [0, 255]</strong> to a torch.FloatTensor of shape <strong>(C x H x W) in the range [0.0, 1.0]</strong>. Here C=Channel, H=Height, W=Width. Read more about this transformation <a href="https://pytorch.org/vision/stable/generated/torchvision.transforms.ToTensor.html">here</a></p>
<div class="cell" data-outputid="1fe8ea0b-fac4-4984-e708-9f9267c80a23" data-execution_count="5">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="co">#collapse-output</span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torchvision</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>train_dataset <span class="op">=</span> torchvision.datasets.MNIST(<span class="st">'classifier_data'</span>, train<span class="op">=</span><span class="va">True</span>, download<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>test_dataset  <span class="op">=</span> torchvision.datasets.MNIST(<span class="st">'classifier_data'</span>, train<span class="op">=</span><span class="va">False</span>, download<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a>transform <span class="op">=</span> torchvision.transforms.Compose([</span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a>    torchvision.transforms.ToTensor()</span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a>])</span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a>train_dataset.transform<span class="op">=</span>transform</span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true" tabindex="-1"></a>test_dataset.transform<span class="op">=</span>transform</span>
<span id="cb6-14"><a href="#cb6-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-15"><a href="#cb6-15" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Total training images: </span><span class="sc">{</span><span class="bu">len</span>(train_dataset)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb6-16"><a href="#cb6-16" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Shape of an image: </span><span class="sc">{</span>np<span class="sc">.</span>shape(train_dataset.data[<span class="dv">7</span>])<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz
Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to classifier_data/MNIST/raw/train-images-idx3-ubyte.gz</code></pre>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"ce55a0f2e2684778812544c628163dd5","version_major":2,"version_minor":0}
</script>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Extracting classifier_data/MNIST/raw/train-images-idx3-ubyte.gz to classifier_data/MNIST/raw

Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz
Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to classifier_data/MNIST/raw/train-labels-idx1-ubyte.gz</code></pre>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"43f3bf0ad2dc45ed98ce5c9aa92935e4","version_major":2,"version_minor":0}
</script>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Extracting classifier_data/MNIST/raw/train-labels-idx1-ubyte.gz to classifier_data/MNIST/raw

Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz
Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to classifier_data/MNIST/raw/t10k-images-idx3-ubyte.gz</code></pre>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"66e1e35efd5f437289bfa38dad645dea","version_major":2,"version_minor":0}
</script>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Extracting classifier_data/MNIST/raw/t10k-images-idx3-ubyte.gz to classifier_data/MNIST/raw

Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz
Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to classifier_data/MNIST/raw/t10k-labels-idx1-ubyte.gz</code></pre>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"26d0a9e665a843ea8338906ec01708c3","version_major":2,"version_minor":0}
</script>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Extracting classifier_data/MNIST/raw/t10k-labels-idx1-ubyte.gz to classifier_data/MNIST/raw

Total training images: 60000
Shape of an image: torch.Size([28, 28])</code></pre>
</div>
</div>
<p>From the above cell output, there are <code>60,000</code> training images. The shape of each image is <code>28 x 28</code>, which means it is a 2D matrix.</p>
<div class="cell" data-outputid="dfea36b0-806e-475a-c922-9dcb823b2a65" data-execution_count="6">
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="co">##</span></span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a><span class="co"># plot a single image</span></span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a>plt.imshow(train_dataset.data[<span class="dv">7</span>], cmap<span class="op">=</span><span class="st">'gray'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="6">
<pre><code>&lt;matplotlib.image.AxesImage at 0x7fb77b73b590&gt;</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="2022-10-23-pytorch-vanishing-gradients-deep-neural-networks_files/figure-html/cell-7-output-2.png" class="img-fluid"></p>
</div>
</div>
</section>
<section id="load-generated-data-into-pytorch-dataset-and-dataloader-class" class="level2">
<h2 class="anchored" data-anchor-id="load-generated-data-into-pytorch-dataset-and-dataloader-class">Load generated data into PyTorch Dataset and DataLoader class</h2>
<p>Now let’s load our data into <code>Dataset</code> and <code>DataLoader</code> classes. PyTorch Dataset is a helper class that converts data and labels into a list of tuples. DataLoader is another helper class to create batches from Dataset tuples. <code>batch_size</code> means the number of tuples we want in a single batch. We have used 128 here, so each fetch from DataLoader will give us a list of 128 tuples.</p>
<div class="cell" data-outputid="13cedfec-2b73-41be-bf6d-e08ff8818026" data-execution_count="7">
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.utils.data <span class="im">import</span> Dataset, DataLoader, random_split</span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a>train_size<span class="op">=</span><span class="bu">len</span>(train_dataset)</span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Randomly split the data into non-overlapping train and validation set</span></span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a><span class="co"># train size = 70% and validation size = 30%</span></span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a>train_data, val_data <span class="op">=</span> random_split(train_dataset, [<span class="bu">int</span>(train_size<span class="op">*</span><span class="fl">0.7</span>), <span class="bu">int</span>(train_size <span class="op">-</span> train_size<span class="op">*</span><span class="fl">0.7</span>)])</span>
<span id="cb14-9"><a href="#cb14-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-10"><a href="#cb14-10" aria-hidden="true" tabindex="-1"></a>batch_size<span class="op">=</span><span class="dv">128</span></span>
<span id="cb14-11"><a href="#cb14-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-12"><a href="#cb14-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Load data into DataLoader class</span></span>
<span id="cb14-13"><a href="#cb14-13" aria-hidden="true" tabindex="-1"></a>train_loader <span class="op">=</span> torch.utils.data.DataLoader(train_dataset, batch_size<span class="op">=</span>batch_size)</span>
<span id="cb14-14"><a href="#cb14-14" aria-hidden="true" tabindex="-1"></a>valid_loader <span class="op">=</span> torch.utils.data.DataLoader(val_data, batch_size<span class="op">=</span>batch_size)</span>
<span id="cb14-15"><a href="#cb14-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-16"><a href="#cb14-16" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Batches in Train Loader: </span><span class="sc">{</span><span class="bu">len</span>(train_loader)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb14-17"><a href="#cb14-17" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Batches in Valid Loader: </span><span class="sc">{</span><span class="bu">len</span>(valid_loader)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb14-18"><a href="#cb14-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-19"><a href="#cb14-19" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Examples in Train Loader: </span><span class="sc">{</span><span class="bu">len</span>(train_loader.sampler)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb14-20"><a href="#cb14-20" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Examples in Valid Loader: </span><span class="sc">{</span><span class="bu">len</span>(valid_loader.sampler)<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Batches in Train Loader: 469
Batches in Valid Loader: 141
Examples in Train Loader: 60000
Examples in Valid Loader: 18000</code></pre>
</div>
</div>
</section>
<section id="define-a-class-to-implement-training-validation-and-mini-batch-processing-pipeline" class="level2">
<h2 class="anchored" data-anchor-id="define-a-class-to-implement-training-validation-and-mini-batch-processing-pipeline">Define a class to implement training, validation, and mini-batch processing pipeline</h2>
<p>In this section we will implement a class that encapsulates all the usual steps required in training a PyTorch model. This way we can focus more on the model architecture and performance, and less concerned about the boilerplate training loop. Important parts of this class are * <code>__init__</code>: Class constructor to define the main actors in a training cycle including model, optimizer, loss function, training and validation DataLoaders * <code>_make_train_step_fn</code>: Training pipeline is usually called “training step” which includes the following steps 1. Compute our model’s predicted output - the forward pass 2. Compute the loss 3. Compute gradients i.e., find the direction and scale to update the weights to reduce the loss 4. Update weight parameters using gradients and the learning rate * <code>_make_val_step_fn</code>: Validation pipeline is usually called the “validation step” which includes the following steps 1. Compute our model’s predicted output - the forward pass 2. Compute the loss 3. Note that during validation, we are only concerned about the loss, i.e., how well our model performs on the validation dataset. Therefore, we don’t use it to calculate the gradients. * <code>_mini_batch</code>: It defines the steps to process a single minibatch in a helper function. For a mini-batch processing, we want to 1. Get the next batch of data and labels (x, y) from the DataLoader iterator 2. Perform a step on the batch. A step can be either training or validation 3. Compute the average batch loss * <code>train</code>: Execute training and validation steps for given number of epoch * <code>predict</code>: Make a prediction from model on provided data</p>
<div class="cell" data-execution_count="8">
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> DeepLearningPipeline(<span class="bu">object</span>):</span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, model, loss_fn, optimizer):</span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Here we define the attributes of our class</span></span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a>        <span class="co"># We start by storing the arguments as attributes </span></span>
<span id="cb16-6"><a href="#cb16-6" aria-hidden="true" tabindex="-1"></a>        <span class="co"># to use them later</span></span>
<span id="cb16-7"><a href="#cb16-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.model <span class="op">=</span> model</span>
<span id="cb16-8"><a href="#cb16-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.loss_fn <span class="op">=</span> loss_fn</span>
<span id="cb16-9"><a href="#cb16-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.optimizer <span class="op">=</span> optimizer</span>
<span id="cb16-10"><a href="#cb16-10" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.device <span class="op">=</span> <span class="st">'cuda'</span> <span class="cf">if</span> torch.cuda.is_available() <span class="cf">else</span> <span class="st">'cpu'</span></span>
<span id="cb16-11"><a href="#cb16-11" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Let's send the model to the specified device right away</span></span>
<span id="cb16-12"><a href="#cb16-12" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.model.to(<span class="va">self</span>.device)</span>
<span id="cb16-13"><a href="#cb16-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-14"><a href="#cb16-14" aria-hidden="true" tabindex="-1"></a>        <span class="co"># These attributes are defined here, but since they are</span></span>
<span id="cb16-15"><a href="#cb16-15" aria-hidden="true" tabindex="-1"></a>        <span class="co"># not informed at the moment of creation, we keep them None</span></span>
<span id="cb16-16"><a href="#cb16-16" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.train_loader <span class="op">=</span> <span class="va">None</span></span>
<span id="cb16-17"><a href="#cb16-17" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.val_loader <span class="op">=</span> <span class="va">None</span></span>
<span id="cb16-18"><a href="#cb16-18" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.writer <span class="op">=</span> <span class="va">None</span></span>
<span id="cb16-19"><a href="#cb16-19" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb16-20"><a href="#cb16-20" aria-hidden="true" tabindex="-1"></a>        <span class="co"># These attributes are going to be computed internally</span></span>
<span id="cb16-21"><a href="#cb16-21" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.losses <span class="op">=</span> []</span>
<span id="cb16-22"><a href="#cb16-22" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.val_losses <span class="op">=</span> []</span>
<span id="cb16-23"><a href="#cb16-23" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.total_epochs <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb16-24"><a href="#cb16-24" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.grad <span class="op">=</span> []</span>
<span id="cb16-25"><a href="#cb16-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-26"><a href="#cb16-26" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Creates the train_step function for our model, </span></span>
<span id="cb16-27"><a href="#cb16-27" aria-hidden="true" tabindex="-1"></a>        <span class="co"># loss function and optimizer</span></span>
<span id="cb16-28"><a href="#cb16-28" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Note: there are NO ARGS there! It makes use of the class</span></span>
<span id="cb16-29"><a href="#cb16-29" aria-hidden="true" tabindex="-1"></a>        <span class="co"># attributes directly</span></span>
<span id="cb16-30"><a href="#cb16-30" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.train_step_fn <span class="op">=</span> <span class="va">self</span>._make_train_step_fn()</span>
<span id="cb16-31"><a href="#cb16-31" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Creates the val_step function for our model and loss</span></span>
<span id="cb16-32"><a href="#cb16-32" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.val_step_fn <span class="op">=</span> <span class="va">self</span>._make_val_step_fn()</span>
<span id="cb16-33"><a href="#cb16-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-34"><a href="#cb16-34" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> set_loaders(<span class="va">self</span>, train_loader, val_loader<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb16-35"><a href="#cb16-35" aria-hidden="true" tabindex="-1"></a>        <span class="co"># This method allows the user to define which train_loader (and val_loader, optionally) to use</span></span>
<span id="cb16-36"><a href="#cb16-36" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Both loaders are then assigned to attributes of the class</span></span>
<span id="cb16-37"><a href="#cb16-37" aria-hidden="true" tabindex="-1"></a>        <span class="co"># So they can be referred to later</span></span>
<span id="cb16-38"><a href="#cb16-38" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.train_loader <span class="op">=</span> train_loader</span>
<span id="cb16-39"><a href="#cb16-39" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.val_loader <span class="op">=</span> val_loader</span>
<span id="cb16-40"><a href="#cb16-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-41"><a href="#cb16-41" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> _make_train_step_fn(<span class="va">self</span>):</span>
<span id="cb16-42"><a href="#cb16-42" aria-hidden="true" tabindex="-1"></a>        <span class="co"># This method does not need ARGS... it can refer to</span></span>
<span id="cb16-43"><a href="#cb16-43" aria-hidden="true" tabindex="-1"></a>        <span class="co"># the attributes: self.model, self.loss_fn and self.optimizer</span></span>
<span id="cb16-44"><a href="#cb16-44" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb16-45"><a href="#cb16-45" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Builds function that performs a step in the train loop</span></span>
<span id="cb16-46"><a href="#cb16-46" aria-hidden="true" tabindex="-1"></a>        <span class="kw">def</span> perform_train_step_fn(x, y):</span>
<span id="cb16-47"><a href="#cb16-47" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Sets model to TRAIN mode</span></span>
<span id="cb16-48"><a href="#cb16-48" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.model.train()</span>
<span id="cb16-49"><a href="#cb16-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-50"><a href="#cb16-50" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Step 1 - Computes our model's predicted output - forward pass</span></span>
<span id="cb16-51"><a href="#cb16-51" aria-hidden="true" tabindex="-1"></a>            yhat <span class="op">=</span> <span class="va">self</span>.model(x)</span>
<span id="cb16-52"><a href="#cb16-52" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Step 2 - Computes the loss</span></span>
<span id="cb16-53"><a href="#cb16-53" aria-hidden="true" tabindex="-1"></a>            loss <span class="op">=</span> <span class="va">self</span>.loss_fn(yhat, y)</span>
<span id="cb16-54"><a href="#cb16-54" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Step 3 - Computes gradients for both "a" and "b" parameters</span></span>
<span id="cb16-55"><a href="#cb16-55" aria-hidden="true" tabindex="-1"></a>            loss.backward()</span>
<span id="cb16-56"><a href="#cb16-56" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Step 4 - Updates parameters using gradients and the learning rate</span></span>
<span id="cb16-57"><a href="#cb16-57" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.optimizer.step()</span>
<span id="cb16-58"><a href="#cb16-58" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.optimizer.zero_grad()</span>
<span id="cb16-59"><a href="#cb16-59" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-60"><a href="#cb16-60" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Returns the loss</span></span>
<span id="cb16-61"><a href="#cb16-61" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> loss.item()</span>
<span id="cb16-62"><a href="#cb16-62" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-63"><a href="#cb16-63" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Returns the function that will be called inside the train loop</span></span>
<span id="cb16-64"><a href="#cb16-64" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> perform_train_step_fn</span>
<span id="cb16-65"><a href="#cb16-65" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb16-66"><a href="#cb16-66" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> _make_val_step_fn(<span class="va">self</span>):</span>
<span id="cb16-67"><a href="#cb16-67" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Builds function that performs a step in the validation loop</span></span>
<span id="cb16-68"><a href="#cb16-68" aria-hidden="true" tabindex="-1"></a>        <span class="kw">def</span> perform_val_step_fn(x, y):</span>
<span id="cb16-69"><a href="#cb16-69" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Sets model to EVAL mode</span></span>
<span id="cb16-70"><a href="#cb16-70" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.model.<span class="bu">eval</span>()</span>
<span id="cb16-71"><a href="#cb16-71" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-72"><a href="#cb16-72" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Step 1 - Computes our model's predicted output - forward pass</span></span>
<span id="cb16-73"><a href="#cb16-73" aria-hidden="true" tabindex="-1"></a>            yhat <span class="op">=</span> <span class="va">self</span>.model(x)</span>
<span id="cb16-74"><a href="#cb16-74" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Step 2 - Computes the loss</span></span>
<span id="cb16-75"><a href="#cb16-75" aria-hidden="true" tabindex="-1"></a>            loss <span class="op">=</span> <span class="va">self</span>.loss_fn(yhat, y)</span>
<span id="cb16-76"><a href="#cb16-76" aria-hidden="true" tabindex="-1"></a>            <span class="co"># There is no need to compute Steps 3 and 4, </span></span>
<span id="cb16-77"><a href="#cb16-77" aria-hidden="true" tabindex="-1"></a>            <span class="co"># since we don't update parameters during evaluation</span></span>
<span id="cb16-78"><a href="#cb16-78" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> loss.item()</span>
<span id="cb16-79"><a href="#cb16-79" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-80"><a href="#cb16-80" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> perform_val_step_fn</span>
<span id="cb16-81"><a href="#cb16-81" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb16-82"><a href="#cb16-82" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> _mini_batch(<span class="va">self</span>, validation<span class="op">=</span><span class="va">False</span>):</span>
<span id="cb16-83"><a href="#cb16-83" aria-hidden="true" tabindex="-1"></a>        <span class="co"># The mini-batch can be used with both loaders</span></span>
<span id="cb16-84"><a href="#cb16-84" aria-hidden="true" tabindex="-1"></a>        <span class="co"># The argument `validation`defines which loader and </span></span>
<span id="cb16-85"><a href="#cb16-85" aria-hidden="true" tabindex="-1"></a>        <span class="co"># corresponding step function is going to be used</span></span>
<span id="cb16-86"><a href="#cb16-86" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> validation:</span>
<span id="cb16-87"><a href="#cb16-87" aria-hidden="true" tabindex="-1"></a>            data_loader <span class="op">=</span> <span class="va">self</span>.val_loader</span>
<span id="cb16-88"><a href="#cb16-88" aria-hidden="true" tabindex="-1"></a>            step_fn <span class="op">=</span> <span class="va">self</span>.val_step_fn</span>
<span id="cb16-89"><a href="#cb16-89" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb16-90"><a href="#cb16-90" aria-hidden="true" tabindex="-1"></a>            data_loader <span class="op">=</span> <span class="va">self</span>.train_loader</span>
<span id="cb16-91"><a href="#cb16-91" aria-hidden="true" tabindex="-1"></a>            step_fn <span class="op">=</span> <span class="va">self</span>.train_step_fn</span>
<span id="cb16-92"><a href="#cb16-92" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-93"><a href="#cb16-93" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> data_loader <span class="kw">is</span> <span class="va">None</span>:</span>
<span id="cb16-94"><a href="#cb16-94" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> <span class="va">None</span></span>
<span id="cb16-95"><a href="#cb16-95" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb16-96"><a href="#cb16-96" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Once the data loader and step function, this is the </span></span>
<span id="cb16-97"><a href="#cb16-97" aria-hidden="true" tabindex="-1"></a>        <span class="co"># same mini-batch loop we had before</span></span>
<span id="cb16-98"><a href="#cb16-98" aria-hidden="true" tabindex="-1"></a>        mini_batch_losses <span class="op">=</span> []</span>
<span id="cb16-99"><a href="#cb16-99" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> x_batch, y_batch <span class="kw">in</span> data_loader:</span>
<span id="cb16-100"><a href="#cb16-100" aria-hidden="true" tabindex="-1"></a>            x_batch <span class="op">=</span> x_batch.to(<span class="va">self</span>.device)</span>
<span id="cb16-101"><a href="#cb16-101" aria-hidden="true" tabindex="-1"></a>            y_batch <span class="op">=</span> y_batch.to(<span class="va">self</span>.device)</span>
<span id="cb16-102"><a href="#cb16-102" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-103"><a href="#cb16-103" aria-hidden="true" tabindex="-1"></a>            mini_batch_loss <span class="op">=</span> step_fn(x_batch, y_batch)</span>
<span id="cb16-104"><a href="#cb16-104" aria-hidden="true" tabindex="-1"></a>            mini_batch_losses.append(mini_batch_loss)</span>
<span id="cb16-105"><a href="#cb16-105" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-106"><a href="#cb16-106" aria-hidden="true" tabindex="-1"></a>        loss <span class="op">=</span> np.mean(mini_batch_losses)</span>
<span id="cb16-107"><a href="#cb16-107" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> loss</span>
<span id="cb16-108"><a href="#cb16-108" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-109"><a href="#cb16-109" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> set_seed(<span class="va">self</span>, seed<span class="op">=</span><span class="dv">42</span>):</span>
<span id="cb16-110"><a href="#cb16-110" aria-hidden="true" tabindex="-1"></a>        torch.backends.cudnn.deterministic <span class="op">=</span> <span class="va">True</span></span>
<span id="cb16-111"><a href="#cb16-111" aria-hidden="true" tabindex="-1"></a>        torch.backends.cudnn.benchmark <span class="op">=</span> <span class="va">False</span>    </span>
<span id="cb16-112"><a href="#cb16-112" aria-hidden="true" tabindex="-1"></a>        torch.manual_seed(seed)</span>
<span id="cb16-113"><a href="#cb16-113" aria-hidden="true" tabindex="-1"></a>        np.random.seed(seed)</span>
<span id="cb16-114"><a href="#cb16-114" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb16-115"><a href="#cb16-115" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> train(<span class="va">self</span>, n_epochs, seed<span class="op">=</span><span class="dv">42</span>):</span>
<span id="cb16-116"><a href="#cb16-116" aria-hidden="true" tabindex="-1"></a>        <span class="co"># To ensure reproducibility of the training process</span></span>
<span id="cb16-117"><a href="#cb16-117" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.set_seed(seed)</span>
<span id="cb16-118"><a href="#cb16-118" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-119"><a href="#cb16-119" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(n_epochs):</span>
<span id="cb16-120"><a href="#cb16-120" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Keeps track of the numbers of epochs</span></span>
<span id="cb16-121"><a href="#cb16-121" aria-hidden="true" tabindex="-1"></a>            <span class="co"># by updating the corresponding attribute</span></span>
<span id="cb16-122"><a href="#cb16-122" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.total_epochs <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb16-123"><a href="#cb16-123" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-124"><a href="#cb16-124" aria-hidden="true" tabindex="-1"></a>            <span class="co"># inner loop</span></span>
<span id="cb16-125"><a href="#cb16-125" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Performs training using mini-batches</span></span>
<span id="cb16-126"><a href="#cb16-126" aria-hidden="true" tabindex="-1"></a>            loss <span class="op">=</span> <span class="va">self</span>._mini_batch(validation<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb16-127"><a href="#cb16-127" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.losses.append(loss)</span>
<span id="cb16-128"><a href="#cb16-128" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-129"><a href="#cb16-129" aria-hidden="true" tabindex="-1"></a>            <span class="co">##########################</span></span>
<span id="cb16-130"><a href="#cb16-130" aria-hidden="true" tabindex="-1"></a>            <span class="co"># get grad at the end of each epoch</span></span>
<span id="cb16-131"><a href="#cb16-131" aria-hidden="true" tabindex="-1"></a>            imgs, labels <span class="op">=</span> <span class="bu">next</span>(<span class="bu">iter</span>(<span class="va">self</span>.train_loader))</span>
<span id="cb16-132"><a href="#cb16-132" aria-hidden="true" tabindex="-1"></a>            imgs, labels <span class="op">=</span> imgs.to(device), labels.to(device)</span>
<span id="cb16-133"><a href="#cb16-133" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-134"><a href="#cb16-134" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Pass one batch through the network, and calculate the gradients for the weights</span></span>
<span id="cb16-135"><a href="#cb16-135" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.model.zero_grad()</span>
<span id="cb16-136"><a href="#cb16-136" aria-hidden="true" tabindex="-1"></a>            preds <span class="op">=</span> <span class="va">self</span>.model(imgs)</span>
<span id="cb16-137"><a href="#cb16-137" aria-hidden="true" tabindex="-1"></a>            loss <span class="op">=</span> torch.nn.functional.cross_entropy(preds, labels)</span>
<span id="cb16-138"><a href="#cb16-138" aria-hidden="true" tabindex="-1"></a>            loss.backward()</span>
<span id="cb16-139"><a href="#cb16-139" aria-hidden="true" tabindex="-1"></a>            <span class="co"># We limit our visualization to the weight parameters and exclude the bias to reduce the number of plots</span></span>
<span id="cb16-140"><a href="#cb16-140" aria-hidden="true" tabindex="-1"></a>            grads <span class="op">=</span> {</span>
<span id="cb16-141"><a href="#cb16-141" aria-hidden="true" tabindex="-1"></a>                name: params.grad.data.view(<span class="op">-</span><span class="dv">1</span>).cpu().clone().numpy()</span>
<span id="cb16-142"><a href="#cb16-142" aria-hidden="true" tabindex="-1"></a>                <span class="cf">for</span> name, params <span class="kw">in</span> <span class="va">self</span>.model.named_parameters()</span>
<span id="cb16-143"><a href="#cb16-143" aria-hidden="true" tabindex="-1"></a>                <span class="cf">if</span> <span class="st">"weight"</span> <span class="kw">in</span> name</span>
<span id="cb16-144"><a href="#cb16-144" aria-hidden="true" tabindex="-1"></a>            }</span>
<span id="cb16-145"><a href="#cb16-145" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.model.zero_grad()</span>
<span id="cb16-146"><a href="#cb16-146" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.grad.append(grads)</span>
<span id="cb16-147"><a href="#cb16-147" aria-hidden="true" tabindex="-1"></a>            <span class="co">##########################</span></span>
<span id="cb16-148"><a href="#cb16-148" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-149"><a href="#cb16-149" aria-hidden="true" tabindex="-1"></a>            <span class="co"># VALIDATION</span></span>
<span id="cb16-150"><a href="#cb16-150" aria-hidden="true" tabindex="-1"></a>            <span class="co"># no gradients in validation!</span></span>
<span id="cb16-151"><a href="#cb16-151" aria-hidden="true" tabindex="-1"></a>            <span class="cf">with</span> torch.no_grad():</span>
<span id="cb16-152"><a href="#cb16-152" aria-hidden="true" tabindex="-1"></a>                <span class="co"># Performs evaluation using mini-batches</span></span>
<span id="cb16-153"><a href="#cb16-153" aria-hidden="true" tabindex="-1"></a>                val_loss <span class="op">=</span> <span class="va">self</span>._mini_batch(validation<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb16-154"><a href="#cb16-154" aria-hidden="true" tabindex="-1"></a>                <span class="va">self</span>.val_losses.append(val_loss)</span>
<span id="cb16-155"><a href="#cb16-155" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-156"><a href="#cb16-156" aria-hidden="true" tabindex="-1"></a>            <span class="co"># If a SummaryWriter has been set...</span></span>
<span id="cb16-157"><a href="#cb16-157" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> <span class="va">self</span>.writer:</span>
<span id="cb16-158"><a href="#cb16-158" aria-hidden="true" tabindex="-1"></a>                scalars <span class="op">=</span> {<span class="st">'training'</span>: loss}</span>
<span id="cb16-159"><a href="#cb16-159" aria-hidden="true" tabindex="-1"></a>                <span class="cf">if</span> val_loss <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb16-160"><a href="#cb16-160" aria-hidden="true" tabindex="-1"></a>                    scalars.update({<span class="st">'validation'</span>: val_loss})</span>
<span id="cb16-161"><a href="#cb16-161" aria-hidden="true" tabindex="-1"></a>                <span class="co"># Records both losses for each epoch under the main tag "loss"</span></span>
<span id="cb16-162"><a href="#cb16-162" aria-hidden="true" tabindex="-1"></a>                <span class="va">self</span>.writer.add_scalars(main_tag<span class="op">=</span><span class="st">'loss'</span>,</span>
<span id="cb16-163"><a href="#cb16-163" aria-hidden="true" tabindex="-1"></a>                                        tag_scalar_dict<span class="op">=</span>scalars,</span>
<span id="cb16-164"><a href="#cb16-164" aria-hidden="true" tabindex="-1"></a>                                        global_step<span class="op">=</span>epoch)</span>
<span id="cb16-165"><a href="#cb16-165" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb16-166"><a href="#cb16-166" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="ss">f"epoch: </span><span class="sc">{</span>epoch<span class="sc">:3}</span><span class="ss">, train loss: </span><span class="sc">{</span>loss<span class="sc">:.5f}</span><span class="ss">, valid loss: </span><span class="sc">{</span>val_loss<span class="sc">:.5f}</span><span class="ss">"</span>)</span>
<span id="cb16-167"><a href="#cb16-167" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-168"><a href="#cb16-168" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="va">self</span>.writer:</span>
<span id="cb16-169"><a href="#cb16-169" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Closes the writer</span></span>
<span id="cb16-170"><a href="#cb16-170" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.writer.close()</span>
<span id="cb16-171"><a href="#cb16-171" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-172"><a href="#cb16-172" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> predict(<span class="va">self</span>, x):</span>
<span id="cb16-173"><a href="#cb16-173" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Set is to evaluation mode for predictions</span></span>
<span id="cb16-174"><a href="#cb16-174" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.model.<span class="bu">eval</span>() </span>
<span id="cb16-175"><a href="#cb16-175" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Takes aNumpy input and make it a float tensor</span></span>
<span id="cb16-176"><a href="#cb16-176" aria-hidden="true" tabindex="-1"></a>        x_tensor <span class="op">=</span> torch.as_tensor(x).<span class="bu">float</span>()</span>
<span id="cb16-177"><a href="#cb16-177" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Send input to device and uses model for prediction</span></span>
<span id="cb16-178"><a href="#cb16-178" aria-hidden="true" tabindex="-1"></a>        y_hat_tensor <span class="op">=</span> <span class="va">self</span>.model(x_tensor.to(<span class="va">self</span>.device))</span>
<span id="cb16-179"><a href="#cb16-179" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Set it back to train mode</span></span>
<span id="cb16-180"><a href="#cb16-180" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.model.train()</span>
<span id="cb16-181"><a href="#cb16-181" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Detaches it, brings it to CPU and back to Numpy</span></span>
<span id="cb16-182"><a href="#cb16-182" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> y_hat_tensor.detach().cpu().numpy()</span>
<span id="cb16-183"><a href="#cb16-183" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-184"><a href="#cb16-184" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> plot_losses(<span class="va">self</span>):</span>
<span id="cb16-185"><a href="#cb16-185" aria-hidden="true" tabindex="-1"></a>        fig <span class="op">=</span> plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">4</span>))</span>
<span id="cb16-186"><a href="#cb16-186" aria-hidden="true" tabindex="-1"></a>        plt.plot(<span class="va">self</span>.losses, label<span class="op">=</span><span class="st">'Training Loss'</span>, c<span class="op">=</span><span class="st">'b'</span>)</span>
<span id="cb16-187"><a href="#cb16-187" aria-hidden="true" tabindex="-1"></a>        plt.plot(<span class="va">self</span>.val_losses, label<span class="op">=</span><span class="st">'Validation Loss'</span>, c<span class="op">=</span><span class="st">'r'</span>)</span>
<span id="cb16-188"><a href="#cb16-188" aria-hidden="true" tabindex="-1"></a>        plt.yscale(<span class="st">'log'</span>)</span>
<span id="cb16-189"><a href="#cb16-189" aria-hidden="true" tabindex="-1"></a>        plt.xlabel(<span class="st">'Epochs'</span>)</span>
<span id="cb16-190"><a href="#cb16-190" aria-hidden="true" tabindex="-1"></a>        plt.ylabel(<span class="st">'Loss'</span>)</span>
<span id="cb16-191"><a href="#cb16-191" aria-hidden="true" tabindex="-1"></a>        plt.legend()</span>
<span id="cb16-192"><a href="#cb16-192" aria-hidden="true" tabindex="-1"></a>        plt.tight_layout()</span>
<span id="cb16-193"><a href="#cb16-193" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> fig</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
<section id="create-a-model-with-sigmoid-activations" class="level2">
<h2 class="anchored" data-anchor-id="create-a-model-with-sigmoid-activations">Create a model with sigmoid activations</h2>
<p>Let’s define a fully connected 4 layers model with only <code>sigmoid</code> activations.</p>
<div class="cell" data-execution_count="9">
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a>SigmoidNet <span class="op">=</span> nn.Sequential()</span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a>SigmoidNet.add_module(<span class="st">"F"</span>, nn.Flatten())</span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a>SigmoidNet.add_module(<span class="st">"L1"</span>, nn.Linear(<span class="dv">28</span><span class="op">*</span><span class="dv">28</span>, <span class="dv">10</span>, bias<span class="op">=</span><span class="va">False</span>))</span>
<span id="cb17-6"><a href="#cb17-6" aria-hidden="true" tabindex="-1"></a>SigmoidNet.add_module(<span class="st">"S1"</span>, nn.Sigmoid())</span>
<span id="cb17-7"><a href="#cb17-7" aria-hidden="true" tabindex="-1"></a>SigmoidNet.add_module(<span class="st">"L2"</span>, nn.Linear(<span class="dv">10</span>, <span class="dv">10</span>, bias<span class="op">=</span><span class="va">False</span>))</span>
<span id="cb17-8"><a href="#cb17-8" aria-hidden="true" tabindex="-1"></a>SigmoidNet.add_module(<span class="st">"S2"</span>, nn.Sigmoid())</span>
<span id="cb17-9"><a href="#cb17-9" aria-hidden="true" tabindex="-1"></a>SigmoidNet.add_module(<span class="st">"L3"</span>, nn.Linear(<span class="dv">10</span>, <span class="dv">10</span>, bias<span class="op">=</span><span class="va">False</span>))</span>
<span id="cb17-10"><a href="#cb17-10" aria-hidden="true" tabindex="-1"></a>SigmoidNet.add_module(<span class="st">"S3"</span>, nn.Sigmoid())</span>
<span id="cb17-11"><a href="#cb17-11" aria-hidden="true" tabindex="-1"></a>SigmoidNet.add_module(<span class="st">"L4"</span>, nn.Linear(<span class="dv">10</span>, <span class="dv">10</span>, bias<span class="op">=</span><span class="va">False</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Print model’s summary.</p>
<div class="cell" data-execution_count="10">
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a>torch.manual_seed(<span class="dv">42</span>)</span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a>device <span class="op">=</span> torch.device(<span class="st">"cuda"</span> <span class="cf">if</span> torch.cuda.is_available() <span class="cf">else</span> <span class="st">"cpu"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-outputid="fd266f64-623e-49ef-c522-83437d833a2f" data-execution_count="11">
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="co">#collapse-output</span></span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torchsummary <span class="im">import</span> summary</span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a>model_sigmoid <span class="op">=</span> SigmoidNet</span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a>model_sigmoid <span class="op">=</span> model_sigmoid.to(device)</span>
<span id="cb19-6"><a href="#cb19-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-7"><a href="#cb19-7" aria-hidden="true" tabindex="-1"></a>summary(model_sigmoid, (<span class="dv">1</span>, <span class="dv">28</span><span class="op">*</span><span class="dv">28</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
           Flatten-1                  [-1, 784]               0
            Linear-2                   [-1, 10]           7,840
           Sigmoid-3                   [-1, 10]               0
            Linear-4                   [-1, 10]             100
           Sigmoid-5                   [-1, 10]               0
            Linear-6                   [-1, 10]             100
           Sigmoid-7                   [-1, 10]               0
            Linear-8                   [-1, 10]             100
================================================================
Total params: 8,140
Trainable params: 8,140
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 0.00
Forward/backward pass size (MB): 0.01
Params size (MB): 0.03
Estimated Total Size (MB): 0.04
----------------------------------------------------------------</code></pre>
</div>
</div>
<p>Create an optimizer and a loss function.</p>
<div class="cell" data-execution_count="12">
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.optim <span class="im">as</span> optim</span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a><span class="co"># learning rate</span></span>
<span id="cb21-4"><a href="#cb21-4" aria-hidden="true" tabindex="-1"></a>lr <span class="op">=</span> <span class="fl">0.001</span></span>
<span id="cb21-5"><a href="#cb21-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-6"><a href="#cb21-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Defines a SGD optimizer to update the parameters</span></span>
<span id="cb21-7"><a href="#cb21-7" aria-hidden="true" tabindex="-1"></a>optimizer_sigmoid <span class="op">=</span> optim.SGD(model_sigmoid.parameters(), lr<span class="op">=</span>lr)</span>
<span id="cb21-8"><a href="#cb21-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-9"><a href="#cb21-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Defines a BCE loss function</span></span>
<span id="cb21-10"><a href="#cb21-10" aria-hidden="true" tabindex="-1"></a>loss_fn <span class="op">=</span> nn.CrossEntropyLoss()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Train our model for 15 epochs.</p>
<div class="cell" data-outputid="d40bd62b-2c49-4c80-fbdd-0faaade50292" data-execution_count="13">
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="co">#collapse-output</span></span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a>n_epochs <span class="op">=</span> <span class="dv">15</span></span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-4"><a href="#cb22-4" aria-hidden="true" tabindex="-1"></a>dlp_sigmoid <span class="op">=</span> DeepLearningPipeline(model_sigmoid, loss_fn, optimizer_sigmoid)</span>
<span id="cb22-5"><a href="#cb22-5" aria-hidden="true" tabindex="-1"></a>dlp_sigmoid.set_loaders(train_loader, valid_loader)</span>
<span id="cb22-6"><a href="#cb22-6" aria-hidden="true" tabindex="-1"></a>dlp_sigmoid.train(n_epochs)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>epoch:   0, train loss: 2.38803, valid loss: 2.34602
epoch:   1, train loss: 2.37269, valid loss: 2.33644
epoch:   2, train loss: 2.36004, valid loss: 2.32891
epoch:   3, train loss: 2.34957, valid loss: 2.32299
epoch:   4, train loss: 2.34085, valid loss: 2.31831
epoch:   5, train loss: 2.33357, valid loss: 2.31463
epoch:   6, train loss: 2.32746, valid loss: 2.31172
epoch:   7, train loss: 2.32232, valid loss: 2.30943
epoch:   8, train loss: 2.31798, valid loss: 2.30762
epoch:   9, train loss: 2.31431, valid loss: 2.30620
epoch:  10, train loss: 2.31119, valid loss: 2.30508
epoch:  11, train loss: 2.30853, valid loss: 2.30420
epoch:  12, train loss: 2.30627, valid loss: 2.30351
epoch:  13, train loss: 2.30433, valid loss: 2.30297
epoch:  14, train loss: 2.30266, valid loss: 2.30254</code></pre>
</div>
</div>
<p>Let’s see how our training and validation loss looks like.</p>
<div class="cell" data-outputid="7e5ef117-77b8-4b03-d774-c7f3dc83ecff" data-execution_count="14">
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a>fig <span class="op">=</span> dlp_sigmoid.plot_losses()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="2022-10-23-pytorch-vanishing-gradients-deep-neural-networks_files/figure-html/cell-15-output-1.png" class="img-fluid"></p>
</div>
</div>
</section>
<section id="create-a-model-with-relu-activations" class="level2">
<h2 class="anchored" data-anchor-id="create-a-model-with-relu-activations">Create a model with ReLU activations</h2>
<p>This time let’s define the same model with ReLU activation functions.</p>
<div class="cell" data-execution_count="15">
<div class="sourceCode cell-code" id="cb25"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a>ReluNet <span class="op">=</span> nn.Sequential()</span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a>ReluNet.add_module(<span class="st">"F"</span>, nn.Flatten())</span>
<span id="cb25-3"><a href="#cb25-3" aria-hidden="true" tabindex="-1"></a>ReluNet.add_module(<span class="st">"L1"</span>, nn.Linear(<span class="dv">28</span><span class="op">*</span><span class="dv">28</span>, <span class="dv">10</span>, bias<span class="op">=</span><span class="va">False</span>))</span>
<span id="cb25-4"><a href="#cb25-4" aria-hidden="true" tabindex="-1"></a>ReluNet.add_module(<span class="st">"S1"</span>, nn.ReLU())</span>
<span id="cb25-5"><a href="#cb25-5" aria-hidden="true" tabindex="-1"></a>ReluNet.add_module(<span class="st">"L2"</span>, nn.Linear(<span class="dv">10</span>, <span class="dv">10</span>, bias<span class="op">=</span><span class="va">False</span>))</span>
<span id="cb25-6"><a href="#cb25-6" aria-hidden="true" tabindex="-1"></a>ReluNet.add_module(<span class="st">"S2"</span>, nn.ReLU())</span>
<span id="cb25-7"><a href="#cb25-7" aria-hidden="true" tabindex="-1"></a>ReluNet.add_module(<span class="st">"L3"</span>, nn.Linear(<span class="dv">10</span>, <span class="dv">10</span>, bias<span class="op">=</span><span class="va">False</span>))</span>
<span id="cb25-8"><a href="#cb25-8" aria-hidden="true" tabindex="-1"></a>ReluNet.add_module(<span class="st">"S3"</span>, nn.ReLU())</span>
<span id="cb25-9"><a href="#cb25-9" aria-hidden="true" tabindex="-1"></a>ReluNet.add_module(<span class="st">"L4"</span>, nn.Linear(<span class="dv">10</span>, <span class="dv">10</span>, bias<span class="op">=</span><span class="va">False</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Print the model’s summary.</p>
<div class="cell" data-outputid="255049fa-1f53-4386-ba7b-6eb7705b9758" data-execution_count="16">
<div class="sourceCode cell-code" id="cb26"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a><span class="co">#collapse-output</span></span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a>model_relu <span class="op">=</span> ReluNet</span>
<span id="cb26-3"><a href="#cb26-3" aria-hidden="true" tabindex="-1"></a>model_relu <span class="op">=</span> model_relu.to(device)</span>
<span id="cb26-4"><a href="#cb26-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-5"><a href="#cb26-5" aria-hidden="true" tabindex="-1"></a>summary(model_relu, (<span class="dv">1</span>, <span class="dv">28</span><span class="op">*</span><span class="dv">28</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
           Flatten-1                  [-1, 784]               0
            Linear-2                   [-1, 10]           7,840
              ReLU-3                   [-1, 10]               0
            Linear-4                   [-1, 10]             100
              ReLU-5                   [-1, 10]               0
            Linear-6                   [-1, 10]             100
              ReLU-7                   [-1, 10]               0
            Linear-8                   [-1, 10]             100
================================================================
Total params: 8,140
Trainable params: 8,140
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 0.00
Forward/backward pass size (MB): 0.01
Params size (MB): 0.03
Estimated Total Size (MB): 0.04
----------------------------------------------------------------</code></pre>
</div>
</div>
<p>Create an optimizer and a loss function.</p>
<div class="cell" data-execution_count="17">
<div class="sourceCode cell-code" id="cb28"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a>lr <span class="op">=</span> <span class="fl">0.001</span></span>
<span id="cb28-2"><a href="#cb28-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-3"><a href="#cb28-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Defines a SGD optimizer to update the parameters</span></span>
<span id="cb28-4"><a href="#cb28-4" aria-hidden="true" tabindex="-1"></a>optimizer_relu <span class="op">=</span> optim.SGD(model_relu.parameters(), lr<span class="op">=</span>lr)</span>
<span id="cb28-5"><a href="#cb28-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-6"><a href="#cb28-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Defines a BCE loss function</span></span>
<span id="cb28-7"><a href="#cb28-7" aria-hidden="true" tabindex="-1"></a>loss_fn <span class="op">=</span> nn.CrossEntropyLoss()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Train the model for 15 epochs.</p>
<div class="cell" data-outputid="61417d51-533b-47ea-df01-8d7cc0024a63" data-execution_count="18">
<div class="sourceCode cell-code" id="cb29"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a><span class="co">#collapse-output</span></span>
<span id="cb29-2"><a href="#cb29-2" aria-hidden="true" tabindex="-1"></a>n_epochs <span class="op">=</span> <span class="dv">15</span></span>
<span id="cb29-3"><a href="#cb29-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-4"><a href="#cb29-4" aria-hidden="true" tabindex="-1"></a>dlp_relu <span class="op">=</span> DeepLearningPipeline(model_relu, loss_fn, optimizer_relu)</span>
<span id="cb29-5"><a href="#cb29-5" aria-hidden="true" tabindex="-1"></a>dlp_relu.set_loaders(train_loader, valid_loader)</span>
<span id="cb29-6"><a href="#cb29-6" aria-hidden="true" tabindex="-1"></a>dlp_relu.train(n_epochs)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>epoch:   0, train loss: 2.30268, valid loss: 2.30202
epoch:   1, train loss: 2.30229, valid loss: 2.30165
epoch:   2, train loss: 2.30193, valid loss: 2.30122
epoch:   3, train loss: 2.30147, valid loss: 2.30069
epoch:   4, train loss: 2.30086, valid loss: 2.29998
epoch:   5, train loss: 2.30012, valid loss: 2.29905
epoch:   6, train loss: 2.29906, valid loss: 2.29793
epoch:   7, train loss: 2.29775, valid loss: 2.29667
epoch:   8, train loss: 2.29621, valid loss: 2.29525
epoch:   9, train loss: 2.29440, valid loss: 2.29363
epoch:  10, train loss: 2.29227, valid loss: 2.29176
epoch:  11, train loss: 2.28972, valid loss: 2.28957
epoch:  12, train loss: 2.28673, valid loss: 2.28703
epoch:  13, train loss: 2.28323, valid loss: 2.28408
epoch:  14, train loss: 2.27912, valid loss: 2.28062</code></pre>
</div>
</div>
<p>Now plot the model losses.</p>
<div class="cell" data-outputid="d15435ca-a8fe-4926-8c71-03fd69a8e0f2" data-execution_count="19">
<div class="sourceCode cell-code" id="cb31"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a>fig <span class="op">=</span> dlp_relu.plot_losses()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="2022-10-23-pytorch-vanishing-gradients-deep-neural-networks_files/figure-html/cell-20-output-1.png" class="img-fluid"></p>
</div>
</div>
</section>
</section>
<section id="section-ii" class="level1">
<h1>Section II</h1>
<p>This section will be more interesting than the last one. Here we will visualize and plot gradients from the trained models and understand their meanings.</p>
<section id="gradients-for-a-model-with-sigmoid-activations" class="level2">
<h2 class="anchored" data-anchor-id="gradients-for-a-model-with-sigmoid-activations">Gradients for a model with Sigmoid activations</h2>
<p>Let’s create a helper function that will plot the gradients for all the weights from each epoch. Note that our models have 4 layers, with L1 being the input layer and L4 being the output layer. Information flows from L1 to L4 during the forward pass. During the backward pass, gradients are calculated from the output layer (L4) and move toward the input layer (L1).</p>
<div class="cell" data-execution_count="20">
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb32"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb32-2"><a href="#cb32-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-3"><a href="#cb32-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> plot_gradients(grads, epoch<span class="op">=</span><span class="dv">0</span>):</span>
<span id="cb32-4"><a href="#cb32-4" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb32-5"><a href="#cb32-5" aria-hidden="true" tabindex="-1"></a><span class="co">    Args:</span></span>
<span id="cb32-6"><a href="#cb32-6" aria-hidden="true" tabindex="-1"></a><span class="co">        net: Object of class BaseNetwork</span></span>
<span id="cb32-7"><a href="#cb32-7" aria-hidden="true" tabindex="-1"></a><span class="co">        color: Color in which we want to visualize the histogram (for easier separation of activation functions)</span></span>
<span id="cb32-8"><a href="#cb32-8" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb32-9"><a href="#cb32-9" aria-hidden="true" tabindex="-1"></a>    grads <span class="op">=</span> grads</span>
<span id="cb32-10"><a href="#cb32-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-11"><a href="#cb32-11" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Plotting</span></span>
<span id="cb32-12"><a href="#cb32-12" aria-hidden="true" tabindex="-1"></a>    columns <span class="op">=</span> <span class="bu">len</span>(grads)</span>
<span id="cb32-13"><a href="#cb32-13" aria-hidden="true" tabindex="-1"></a>    fig, ax <span class="op">=</span> plt.subplots(<span class="dv">1</span>, columns, figsize<span class="op">=</span>(columns <span class="op">*</span> <span class="fl">3.5</span>, <span class="fl">2.5</span>))</span>
<span id="cb32-14"><a href="#cb32-14" aria-hidden="true" tabindex="-1"></a>    fig_index <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb32-15"><a href="#cb32-15" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> key <span class="kw">in</span> grads:</span>
<span id="cb32-16"><a href="#cb32-16" aria-hidden="true" tabindex="-1"></a>        key_ax <span class="op">=</span> ax[fig_index <span class="op">%</span> columns]</span>
<span id="cb32-17"><a href="#cb32-17" aria-hidden="true" tabindex="-1"></a>        sns.histplot(data<span class="op">=</span>grads[key], bins<span class="op">=</span><span class="dv">30</span>, ax<span class="op">=</span>key_ax, kde<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb32-18"><a href="#cb32-18" aria-hidden="true" tabindex="-1"></a>        key_ax.set_title(<span class="bu">str</span>(key))</span>
<span id="cb32-19"><a href="#cb32-19" aria-hidden="true" tabindex="-1"></a>        key_ax.set_xlabel(<span class="st">"Grad magnitude"</span>, fontsize<span class="op">=</span><span class="dv">11</span>)</span>
<span id="cb32-20"><a href="#cb32-20" aria-hidden="true" tabindex="-1"></a>        fig_index <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb32-21"><a href="#cb32-21" aria-hidden="true" tabindex="-1"></a>    fig.suptitle(</span>
<span id="cb32-22"><a href="#cb32-22" aria-hidden="true" tabindex="-1"></a>        <span class="ss">f"Epoch: </span><span class="sc">{</span>epoch<span class="sc">}</span><span class="ss">"</span>, fontsize<span class="op">=</span><span class="dv">16</span>, y<span class="op">=</span><span class="fl">1.05</span></span>
<span id="cb32-23"><a href="#cb32-23" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb32-24"><a href="#cb32-24" aria-hidden="true" tabindex="-1"></a>    fig.subplots_adjust(wspace<span class="op">=</span><span class="fl">0.45</span>)</span>
<span id="cb32-25"><a href="#cb32-25" aria-hidden="true" tabindex="-1"></a>    plt.show()</span>
<span id="cb32-26"><a href="#cb32-26" aria-hidden="true" tabindex="-1"></a>    plt.close()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>Plot the gradients for model with sigmoid activations.</p>
<div class="cell" data-outputid="3cc92299-0971-4cf6-8864-f06b6228f5ed" data-execution_count="21">
<div class="sourceCode cell-code" id="cb33"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(dlp_sigmoid.grad)):</span>
<span id="cb33-2"><a href="#cb33-2" aria-hidden="true" tabindex="-1"></a>    plot_gradients(dlp_sigmoid.grad[i], i)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="2022-10-23-pytorch-vanishing-gradients-deep-neural-networks_files/figure-html/cell-22-output-1.png" class="img-fluid"></p>
</div>
<div class="cell-output cell-output-display">
<p><img src="2022-10-23-pytorch-vanishing-gradients-deep-neural-networks_files/figure-html/cell-22-output-2.png" class="img-fluid"></p>
</div>
<div class="cell-output cell-output-display">
<p><img src="2022-10-23-pytorch-vanishing-gradients-deep-neural-networks_files/figure-html/cell-22-output-3.png" class="img-fluid"></p>
</div>
<div class="cell-output cell-output-display">
<p><img src="2022-10-23-pytorch-vanishing-gradients-deep-neural-networks_files/figure-html/cell-22-output-4.png" class="img-fluid"></p>
</div>
<div class="cell-output cell-output-display">
<p><img src="2022-10-23-pytorch-vanishing-gradients-deep-neural-networks_files/figure-html/cell-22-output-5.png" class="img-fluid"></p>
</div>
<div class="cell-output cell-output-display">
<p><img src="2022-10-23-pytorch-vanishing-gradients-deep-neural-networks_files/figure-html/cell-22-output-6.png" class="img-fluid"></p>
</div>
<div class="cell-output cell-output-display">
<p><img src="2022-10-23-pytorch-vanishing-gradients-deep-neural-networks_files/figure-html/cell-22-output-7.png" class="img-fluid"></p>
</div>
<div class="cell-output cell-output-display">
<p><img src="2022-10-23-pytorch-vanishing-gradients-deep-neural-networks_files/figure-html/cell-22-output-8.png" class="img-fluid"></p>
</div>
<div class="cell-output cell-output-display">
<p><img src="2022-10-23-pytorch-vanishing-gradients-deep-neural-networks_files/figure-html/cell-22-output-9.png" class="img-fluid"></p>
</div>
<div class="cell-output cell-output-display">
<p><img src="2022-10-23-pytorch-vanishing-gradients-deep-neural-networks_files/figure-html/cell-22-output-10.png" class="img-fluid"></p>
</div>
<div class="cell-output cell-output-display">
<p><img src="2022-10-23-pytorch-vanishing-gradients-deep-neural-networks_files/figure-html/cell-22-output-11.png" class="img-fluid"></p>
</div>
<div class="cell-output cell-output-display">
<p><img src="2022-10-23-pytorch-vanishing-gradients-deep-neural-networks_files/figure-html/cell-22-output-12.png" class="img-fluid"></p>
</div>
<div class="cell-output cell-output-display">
<p><img src="2022-10-23-pytorch-vanishing-gradients-deep-neural-networks_files/figure-html/cell-22-output-13.png" class="img-fluid"></p>
</div>
<div class="cell-output cell-output-display">
<p><img src="2022-10-23-pytorch-vanishing-gradients-deep-neural-networks_files/figure-html/cell-22-output-14.png" class="img-fluid"></p>
</div>
<div class="cell-output cell-output-display">
<p><img src="2022-10-23-pytorch-vanishing-gradients-deep-neural-networks_files/figure-html/cell-22-output-15.png" class="img-fluid"></p>
</div>
</div>
<section id="what-do-we-get-from-these-plots" class="level3">
<h3 class="anchored" data-anchor-id="what-do-we-get-from-these-plots">What do we get from these plots?</h3>
<section id="issues" class="level4">
<h4 class="anchored" data-anchor-id="issues"><u>Issues</u></h4>
<p>In this section, we will discuss the issues we can identify from these gradient plots for our <code>SigmoidNet</code></p>
<p><strong>Issue 1</strong>: Look closely at how the <code>Grad magnitude</code> scale changes from L4 to L1 in an epoch. It is diminishing at an exponential rate. This tells us that the gradient is very high at the output layer but diminishes till it reaches L1.</p>
<p><strong>Issue 2</strong>: Gradient is also spread out and not smoother. This is a bad sign because it shows that different areas of the weight layer produce gradients in different ranges. For example, from L4 plot in the last epoch, we can see that the gradients are making clusters around -0.02, 0, and 0.02. We can also find that the gradient <code>mean</code> is not centered around 0. Either it is on the left of 0, or right, or has multiple peaks.</p>
</section>
<section id="possible-explanations" class="level4">
<h4 class="anchored" data-anchor-id="possible-explanations"><u>Possible explanations</u></h4>
<p><strong>Issue 1 Reason</strong>: Our network is facing <em>Diminishing Gradient</em> problem. I have pasted below another plot for Sigmoid. <a href="https://levelup.gitconnected.com/vanishing-and-exploding-gradients-ae7fb88f3b66">Image source</a></p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/2022-10-23-pytorch-vanishing-gradients-deep-neural-networks/sigmoid.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">sigmoid.png</figcaption><p></p>
</figure>
</div>
<p>From this plot, we can see that the highest gradient produced by sigmoid is 0.25. So during backpropagation, when we calculate derivatives for deeper layers (L1, L2), there is a chain reaction where smaller and smaller numbers (less than 0.25) are multiplied to produce even smaller numbers. The result is that gradients diminish, and the weights are barely updated in deeper layers during the backward pass. We can avoid this by using a different activation function (e.g., ReLU) in hidden layers. We will do that in <code>Section II</code>.</p>
<p><strong>Issue 2 Reason</strong>: This is due to an initial weight initialization mismatch with the activation function used. By default, PyTorch uses <code>kaiming initialization</code> (<a href="https://discuss.pytorch.org/t/clarity-on-default-initialization-in-pytorch/84696">Ref here</a>) that works well for ReLU but not for sigmoid. It’s recommended to use <code>Tanh</code> or <code>Xavior</code> for sigmoid.</p>
</section>
</section>
<section id="can-we-improve-our-sigmoidnet" class="level3">
<h3 class="anchored" data-anchor-id="can-we-improve-our-sigmoidnet">Can we improve our SigmoidNet?</h3>
<p>Let’s train another SigmoidNet with the same configuration but with <code>xavior initialization</code> , and observe how it behaves now.</p>
<div class="cell" data-outputid="9ca2c3bf-dedf-4904-d3ba-ac2f8581e0de" data-execution_count="22">
<div class="sourceCode cell-code" id="cb34"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a><span class="co">#collapse-output</span></span>
<span id="cb34-2"><a href="#cb34-2" aria-hidden="true" tabindex="-1"></a>model_sigmoid_v1 <span class="op">=</span> SigmoidNet</span>
<span id="cb34-3"><a href="#cb34-3" aria-hidden="true" tabindex="-1"></a>model_sigmoid_v1 <span class="op">=</span> model_sigmoid_v1.to(device)</span>
<span id="cb34-4"><a href="#cb34-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-5"><a href="#cb34-5" aria-hidden="true" tabindex="-1"></a><span class="co"># define a function to initialize weight with xavier uniform distribution</span></span>
<span id="cb34-6"><a href="#cb34-6" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> init_weights(m):</span>
<span id="cb34-7"><a href="#cb34-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="bu">isinstance</span>(m, nn.Linear):</span>
<span id="cb34-8"><a href="#cb34-8" aria-hidden="true" tabindex="-1"></a>        torch.nn.init.xavier_uniform_(m.weight)</span>
<span id="cb34-9"><a href="#cb34-9" aria-hidden="true" tabindex="-1"></a>        <span class="co"># m.bias.data.fill_(0.01)</span></span>
<span id="cb34-10"><a href="#cb34-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-11"><a href="#cb34-11" aria-hidden="true" tabindex="-1"></a><span class="co"># reset model weights with xavier</span></span>
<span id="cb34-12"><a href="#cb34-12" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> torch.no_grad():</span>
<span id="cb34-13"><a href="#cb34-13" aria-hidden="true" tabindex="-1"></a>    model_sigmoid_v1.<span class="bu">apply</span>(init_weights)</span>
<span id="cb34-14"><a href="#cb34-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-15"><a href="#cb34-15" aria-hidden="true" tabindex="-1"></a>lr <span class="op">=</span> <span class="fl">0.001</span></span>
<span id="cb34-16"><a href="#cb34-16" aria-hidden="true" tabindex="-1"></a>optimizer_sigmoid <span class="op">=</span> optim.SGD(model_sigmoid_v1.parameters(), lr<span class="op">=</span>lr)</span>
<span id="cb34-17"><a href="#cb34-17" aria-hidden="true" tabindex="-1"></a>loss_fn <span class="op">=</span> nn.CrossEntropyLoss()</span>
<span id="cb34-18"><a href="#cb34-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-19"><a href="#cb34-19" aria-hidden="true" tabindex="-1"></a>n_epochs <span class="op">=</span> <span class="dv">15</span></span>
<span id="cb34-20"><a href="#cb34-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-21"><a href="#cb34-21" aria-hidden="true" tabindex="-1"></a>dlp_sigmoid_v1 <span class="op">=</span> DeepLearningPipeline(model_sigmoid_v1, loss_fn, optimizer_sigmoid)</span>
<span id="cb34-22"><a href="#cb34-22" aria-hidden="true" tabindex="-1"></a>dlp_sigmoid_v1.set_loaders(train_loader, valid_loader)</span>
<span id="cb34-23"><a href="#cb34-23" aria-hidden="true" tabindex="-1"></a>dlp_sigmoid_v1.train(n_epochs)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>epoch:   0, train loss: 2.49727, valid loss: 2.45576
epoch:   1, train loss: 2.46252, valid loss: 2.42681
epoch:   2, train loss: 2.43418, valid loss: 2.40356
epoch:   3, train loss: 2.41090, valid loss: 2.38474
epoch:   4, train loss: 2.39164, valid loss: 2.36943
epoch:   5, train loss: 2.37565, valid loss: 2.35693
epoch:   6, train loss: 2.36232, valid loss: 2.34670
epoch:   7, train loss: 2.35119, valid loss: 2.33831
epoch:   8, train loss: 2.34186, valid loss: 2.33143
epoch:   9, train loss: 2.33404, valid loss: 2.32578
epoch:  10, train loss: 2.32746, valid loss: 2.32113
epoch:  11, train loss: 2.32192, valid loss: 2.31731
epoch:  12, train loss: 2.31724, valid loss: 2.31417
epoch:  13, train loss: 2.31328, valid loss: 2.31159
epoch:  14, train loss: 2.30993, valid loss: 2.30946</code></pre>
</div>
</div>
<div class="cell" data-outputid="fcb7b68d-2814-455a-8f39-bc3148f15fe8" data-execution_count="23">
<div class="sourceCode cell-code" id="cb36"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb36-1"><a href="#cb36-1" aria-hidden="true" tabindex="-1"></a>fig <span class="op">=</span> dlp_sigmoid_v1.plot_losses()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="2022-10-23-pytorch-vanishing-gradients-deep-neural-networks_files/figure-html/cell-24-output-1.png" class="img-fluid"></p>
</div>
</div>
<div class="cell" data-outputid="3670ae2d-854b-434b-bb68-3afa04a3cdac" data-execution_count="24">
<div class="sourceCode cell-code" id="cb37"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb37-1"><a href="#cb37-1" aria-hidden="true" tabindex="-1"></a><span class="co">##</span></span>
<span id="cb37-2"><a href="#cb37-2" aria-hidden="true" tabindex="-1"></a><span class="co">#  SigmoidNet gradients with xavior initialization</span></span>
<span id="cb37-3"><a href="#cb37-3" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(dlp_sigmoid_v1.grad)):</span>
<span id="cb37-4"><a href="#cb37-4" aria-hidden="true" tabindex="-1"></a>    plot_gradients(dlp_sigmoid_v1.grad[i], i)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="2022-10-23-pytorch-vanishing-gradients-deep-neural-networks_files/figure-html/cell-25-output-1.png" class="img-fluid"></p>
</div>
<div class="cell-output cell-output-display">
<p><img src="2022-10-23-pytorch-vanishing-gradients-deep-neural-networks_files/figure-html/cell-25-output-2.png" class="img-fluid"></p>
</div>
<div class="cell-output cell-output-display">
<p><img src="2022-10-23-pytorch-vanishing-gradients-deep-neural-networks_files/figure-html/cell-25-output-3.png" class="img-fluid"></p>
</div>
<div class="cell-output cell-output-display">
<p><img src="2022-10-23-pytorch-vanishing-gradients-deep-neural-networks_files/figure-html/cell-25-output-4.png" class="img-fluid"></p>
</div>
<div class="cell-output cell-output-display">
<p><img src="2022-10-23-pytorch-vanishing-gradients-deep-neural-networks_files/figure-html/cell-25-output-5.png" class="img-fluid"></p>
</div>
<div class="cell-output cell-output-display">
<p><img src="2022-10-23-pytorch-vanishing-gradients-deep-neural-networks_files/figure-html/cell-25-output-6.png" class="img-fluid"></p>
</div>
<div class="cell-output cell-output-display">
<p><img src="2022-10-23-pytorch-vanishing-gradients-deep-neural-networks_files/figure-html/cell-25-output-7.png" class="img-fluid"></p>
</div>
<div class="cell-output cell-output-display">
<p><img src="2022-10-23-pytorch-vanishing-gradients-deep-neural-networks_files/figure-html/cell-25-output-8.png" class="img-fluid"></p>
</div>
<div class="cell-output cell-output-display">
<p><img src="2022-10-23-pytorch-vanishing-gradients-deep-neural-networks_files/figure-html/cell-25-output-9.png" class="img-fluid"></p>
</div>
<div class="cell-output cell-output-display">
<p><img src="2022-10-23-pytorch-vanishing-gradients-deep-neural-networks_files/figure-html/cell-25-output-10.png" class="img-fluid"></p>
</div>
<div class="cell-output cell-output-display">
<p><img src="2022-10-23-pytorch-vanishing-gradients-deep-neural-networks_files/figure-html/cell-25-output-11.png" class="img-fluid"></p>
</div>
<div class="cell-output cell-output-display">
<p><img src="2022-10-23-pytorch-vanishing-gradients-deep-neural-networks_files/figure-html/cell-25-output-12.png" class="img-fluid"></p>
</div>
<div class="cell-output cell-output-display">
<p><img src="2022-10-23-pytorch-vanishing-gradients-deep-neural-networks_files/figure-html/cell-25-output-13.png" class="img-fluid"></p>
</div>
<div class="cell-output cell-output-display">
<p><img src="2022-10-23-pytorch-vanishing-gradients-deep-neural-networks_files/figure-html/cell-25-output-14.png" class="img-fluid"></p>
</div>
<div class="cell-output cell-output-display">
<p><img src="2022-10-23-pytorch-vanishing-gradients-deep-neural-networks_files/figure-html/cell-25-output-15.png" class="img-fluid"></p>
</div>
</div>
<p>Now the plots are much smoother. Though the network is still suffering from diminishing gradients.</p>
</section>
<section id="ridge-plot-for-gradients" class="level3">
<h3 class="anchored" data-anchor-id="ridge-plot-for-gradients">Ridge plot for gradients</h3>
<p>In this section, we will use ridge plots for gradients. They provide a better perspective on how gradients evolve during epochs.</p>
<div class="cell" data-execution_count="25">
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb38"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb38-1"><a href="#cb38-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb38-2"><a href="#cb38-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-3"><a href="#cb38-3" aria-hidden="true" tabindex="-1"></a><span class="co"># A helper function to get gradients of a weight layer for all epochs.</span></span>
<span id="cb38-4"><a href="#cb38-4" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> get_layer_gradients(layer_name, layer_grads):</span>
<span id="cb38-5"><a href="#cb38-5" aria-hidden="true" tabindex="-1"></a>    df <span class="op">=</span> pd.DataFrame(columns<span class="op">=</span>[<span class="st">'x'</span>,<span class="st">'g'</span>])</span>
<span id="cb38-6"><a href="#cb38-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(layer_grads)):</span>
<span id="cb38-7"><a href="#cb38-7" aria-hidden="true" tabindex="-1"></a>        temp <span class="op">=</span> {</span>
<span id="cb38-8"><a href="#cb38-8" aria-hidden="true" tabindex="-1"></a>            <span class="st">'x'</span>: layer_grads[i][layer_name], <span class="co"># x --&gt; gradients</span></span>
<span id="cb38-9"><a href="#cb38-9" aria-hidden="true" tabindex="-1"></a>            <span class="st">'g'</span>: i <span class="co"># g --&gt; epochs</span></span>
<span id="cb38-10"><a href="#cb38-10" aria-hidden="true" tabindex="-1"></a>        }</span>
<span id="cb38-11"><a href="#cb38-11" aria-hidden="true" tabindex="-1"></a>        epoch_df <span class="op">=</span> pd.DataFrame(temp)</span>
<span id="cb38-12"><a href="#cb38-12" aria-hidden="true" tabindex="-1"></a>        df <span class="op">=</span> df.append(epoch_df, ignore_index<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb38-13"><a href="#cb38-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-14"><a href="#cb38-14" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> df</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>Print the names for the model weight layers.</p>
<div class="cell" data-outputid="86301275-9a01-448c-8df0-aefa5898cd93" data-execution_count="26">
<div class="sourceCode cell-code" id="cb39"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb39-1"><a href="#cb39-1" aria-hidden="true" tabindex="-1"></a>weight_layers_sigmoid_v1 <span class="op">=</span> <span class="bu">list</span>(dlp_sigmoid_v1.grad[<span class="dv">0</span>].keys())</span>
<span id="cb39-2"><a href="#cb39-2" aria-hidden="true" tabindex="-1"></a>weight_layers_sigmoid_v1</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="26">
<pre><code>['L1.weight', 'L2.weight', 'L3.weight', 'L4.weight']</code></pre>
</div>
</div>
<p>Store the gradients for each layer in a separate DataFrame. Each DataFrame has two columns * <code>x</code>: for the gradient value * <code>g</code>: for epoch</p>
<div class="cell" data-outputid="3810fa67-b4e1-49e0-9371-d1530aa7c90a" data-execution_count="27">
<div class="sourceCode cell-code" id="cb41"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb41-1"><a href="#cb41-1" aria-hidden="true" tabindex="-1"></a>df0_sigmoid_v1 <span class="op">=</span> get_layer_gradients(weight_layers_sigmoid_v1[<span class="dv">0</span>], dlp_sigmoid_v1.grad)</span>
<span id="cb41-2"><a href="#cb41-2" aria-hidden="true" tabindex="-1"></a>df1_sigmoid_v1 <span class="op">=</span> get_layer_gradients(weight_layers_sigmoid_v1[<span class="dv">1</span>], dlp_sigmoid_v1.grad)</span>
<span id="cb41-3"><a href="#cb41-3" aria-hidden="true" tabindex="-1"></a>df2_sigmoid_v1 <span class="op">=</span> get_layer_gradients(weight_layers_sigmoid_v1[<span class="dv">2</span>], dlp_sigmoid_v1.grad)</span>
<span id="cb41-4"><a href="#cb41-4" aria-hidden="true" tabindex="-1"></a>df3_sigmoid_v1 <span class="op">=</span> get_layer_gradients(weight_layers_sigmoid_v1[<span class="dv">3</span>], dlp_sigmoid_v1.grad)</span>
<span id="cb41-5"><a href="#cb41-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-6"><a href="#cb41-6" aria-hidden="true" tabindex="-1"></a>df3_sigmoid_v1.head()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="27">


  <div id="df-182a959d-0aab-4660-88bd-f6ed76affdea">
    <div class="colab-df-container">
      <div>

<table class="dataframe table table-sm table-striped">
  <thead>
    <tr>
      <th></th>
      <th>x</th>
      <th>g</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>-0.003590</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>-0.002747</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>-0.002291</td>
      <td>0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>-0.006168</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>-0.002711</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
</div>
      <button class="colab-df-convert" onclick="convertToInteractive('df-182a959d-0aab-4660-88bd-f6ed76affdea')" title="Convert this dataframe to an interactive table." style="display:none;">
        
  <svg xmlns="http://www.w3.org/2000/svg" height="24px" viewbox="0 0 24 24" width="24px">
    <path d="M0 0h24v24H0V0z" fill="none"></path>
    <path d="M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z"></path><path d="M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z"></path>
  </svg>
      </button>
      
  <style>
    .colab-df-container {
      display:flex;
      flex-wrap:wrap;
      gap: 12px;
    }

    .colab-df-convert {
      background-color: #E8F0FE;
      border: none;
      border-radius: 50%;
      cursor: pointer;
      display: none;
      fill: #1967D2;
      height: 32px;
      padding: 0 0 0 0;
      width: 32px;
    }

    .colab-df-convert:hover {
      background-color: #E2EBFA;
      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);
      fill: #174EA6;
    }

    [theme=dark] .colab-df-convert {
      background-color: #3B4455;
      fill: #D2E3FC;
    }

    [theme=dark] .colab-df-convert:hover {
      background-color: #434B5C;
      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);
      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));
      fill: #FFFFFF;
    }
  </style>

      <script>
        const buttonEl =
          document.querySelector('#df-182a959d-0aab-4660-88bd-f6ed76affdea button.colab-df-convert');
        buttonEl.style.display =
          google.colab.kernel.accessAllowed ? 'block' : 'none';

        async function convertToInteractive(key) {
          const element = document.querySelector('#df-182a959d-0aab-4660-88bd-f6ed76affdea');
          const dataTable =
            await google.colab.kernel.invokeFunction('convertToInteractive',
                                                     [key], {});
          if (!dataTable) return;

          const docLinkHtml = 'Like what you see? Visit the ' +
            '<a target="_blank" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'
            + ' to learn more about interactive tables.';
          element.innerHTML = '';
          dataTable['output_type'] = 'display_data';
          await google.colab.output.renderOutput(dataTable, element);
          const docLink = document.createElement('div');
          docLink.innerHTML = docLinkHtml;
          element.appendChild(docLink);
        }
      </script>
    </div>
  </div>
  
</div>
</div>
<div class="cell" data-execution_count="28">
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb42"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb42-1"><a href="#cb42-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Another helper function to create the ridge plots</span></span>
<span id="cb42-2"><a href="#cb42-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> plot_gradients_ridge_v1(df, layer_name):</span>
<span id="cb42-3"><a href="#cb42-3" aria-hidden="true" tabindex="-1"></a>    sns.set_theme(style<span class="op">=</span><span class="st">"white"</span>, rc<span class="op">=</span>{<span class="st">"axes.facecolor"</span>: (<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>)})</span>
<span id="cb42-4"><a href="#cb42-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-5"><a href="#cb42-5" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Initialize the FacetGrid object</span></span>
<span id="cb42-6"><a href="#cb42-6" aria-hidden="true" tabindex="-1"></a>    pal <span class="op">=</span> sns.cubehelix_palette(<span class="dv">10</span>, rot<span class="op">=-</span><span class="fl">.25</span>, light<span class="op">=</span><span class="fl">.7</span>)</span>
<span id="cb42-7"><a href="#cb42-7" aria-hidden="true" tabindex="-1"></a>    g <span class="op">=</span> sns.FacetGrid(df, row<span class="op">=</span><span class="st">"g"</span>, hue<span class="op">=</span><span class="st">"g"</span>, aspect<span class="op">=</span><span class="dv">15</span>, height<span class="op">=</span><span class="fl">.5</span>, palette<span class="op">=</span>pal)</span>
<span id="cb42-8"><a href="#cb42-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-9"><a href="#cb42-9" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Draw the densities in a few steps</span></span>
<span id="cb42-10"><a href="#cb42-10" aria-hidden="true" tabindex="-1"></a>    g.<span class="bu">map</span>(sns.kdeplot, <span class="st">"x"</span>,</span>
<span id="cb42-11"><a href="#cb42-11" aria-hidden="true" tabindex="-1"></a>        bw_adjust<span class="op">=</span><span class="fl">.5</span>, clip_on<span class="op">=</span><span class="va">False</span>,</span>
<span id="cb42-12"><a href="#cb42-12" aria-hidden="true" tabindex="-1"></a>        fill<span class="op">=</span><span class="va">True</span>, alpha<span class="op">=</span><span class="dv">1</span>, linewidth<span class="op">=</span><span class="fl">1.5</span>)</span>
<span id="cb42-13"><a href="#cb42-13" aria-hidden="true" tabindex="-1"></a>    g.<span class="bu">map</span>(sns.kdeplot, <span class="st">"x"</span>, clip_on<span class="op">=</span><span class="va">False</span>, color<span class="op">=</span><span class="st">"w"</span>, lw<span class="op">=</span><span class="dv">2</span>, bw_adjust<span class="op">=</span><span class="fl">.5</span>)</span>
<span id="cb42-14"><a href="#cb42-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-15"><a href="#cb42-15" aria-hidden="true" tabindex="-1"></a>    <span class="co"># passing color=None to refline() uses the hue mapping</span></span>
<span id="cb42-16"><a href="#cb42-16" aria-hidden="true" tabindex="-1"></a>    g.refline(y<span class="op">=</span><span class="dv">0</span>, linewidth<span class="op">=</span><span class="dv">2</span>, linestyle<span class="op">=</span><span class="st">"-"</span>, color<span class="op">=</span><span class="va">None</span>, clip_on<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb42-17"><a href="#cb42-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-18"><a href="#cb42-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-19"><a href="#cb42-19" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Define and use a simple function to label the plot in axes coordinates</span></span>
<span id="cb42-20"><a href="#cb42-20" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> label(x, color, label):</span>
<span id="cb42-21"><a href="#cb42-21" aria-hidden="true" tabindex="-1"></a>        ax <span class="op">=</span> plt.gca()</span>
<span id="cb42-22"><a href="#cb42-22" aria-hidden="true" tabindex="-1"></a>        ax.text(<span class="dv">0</span>, <span class="fl">.2</span>, label, fontweight<span class="op">=</span><span class="st">"bold"</span>, color<span class="op">=</span>color,</span>
<span id="cb42-23"><a href="#cb42-23" aria-hidden="true" tabindex="-1"></a>                ha<span class="op">=</span><span class="st">"left"</span>, va<span class="op">=</span><span class="st">"center"</span>, transform<span class="op">=</span>ax.transAxes)</span>
<span id="cb42-24"><a href="#cb42-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-25"><a href="#cb42-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-26"><a href="#cb42-26" aria-hidden="true" tabindex="-1"></a>    g.<span class="bu">map</span>(label, <span class="st">"x"</span>)</span>
<span id="cb42-27"><a href="#cb42-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-28"><a href="#cb42-28" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Set the subplots to overlap</span></span>
<span id="cb42-29"><a href="#cb42-29" aria-hidden="true" tabindex="-1"></a>    g.figure.subplots_adjust(hspace<span class="op">=-</span><span class="fl">.25</span>)</span>
<span id="cb42-30"><a href="#cb42-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-31"><a href="#cb42-31" aria-hidden="true" tabindex="-1"></a>    g.fig.suptitle(layer_name, ha<span class="op">=</span><span class="st">'left'</span>, fontsize<span class="op">=</span><span class="dv">16</span>, fontweight<span class="op">=</span><span class="dv">16</span>)</span>
<span id="cb42-32"><a href="#cb42-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-33"><a href="#cb42-33" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Remove axes details that don't play well with overlap</span></span>
<span id="cb42-34"><a href="#cb42-34" aria-hidden="true" tabindex="-1"></a>    g.set_titles(<span class="st">""</span>)</span>
<span id="cb42-35"><a href="#cb42-35" aria-hidden="true" tabindex="-1"></a>    g.<span class="bu">set</span>(yticks<span class="op">=</span>[], ylabel<span class="op">=</span><span class="st">""</span>)</span>
<span id="cb42-36"><a href="#cb42-36" aria-hidden="true" tabindex="-1"></a>    g.despine(bottom<span class="op">=</span><span class="va">True</span>, left<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb42-37"><a href="#cb42-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-38"><a href="#cb42-38" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> g</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>Create plots for all weight layer.</p>
<div class="cell" data-outputid="81dea6f3-3f3f-452a-872c-e00898bdbda3" data-execution_count="29">
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb43"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb43-1"><a href="#cb43-1" aria-hidden="true" tabindex="-1"></a><span class="co"># https://stackoverflow.com/questions/35042255/how-to-plot-multiple-seaborn-jointplot-in-subplot</span></span>
<span id="cb43-2"><a href="#cb43-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> warnings</span>
<span id="cb43-3"><a href="#cb43-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.image <span class="im">as</span> mpimg</span>
<span id="cb43-4"><a href="#cb43-4" aria-hidden="true" tabindex="-1"></a>warnings.filterwarnings(<span class="st">"ignore"</span>)</span>
<span id="cb43-5"><a href="#cb43-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-6"><a href="#cb43-6" aria-hidden="true" tabindex="-1"></a>g1 <span class="op">=</span> plot_gradients_ridge_v1(df0_sigmoid_v1, weight_layers_sigmoid_v1[<span class="dv">0</span>])</span>
<span id="cb43-7"><a href="#cb43-7" aria-hidden="true" tabindex="-1"></a>g2 <span class="op">=</span> plot_gradients_ridge_v1(df1_sigmoid_v1, weight_layers_sigmoid_v1[<span class="dv">1</span>])</span>
<span id="cb43-8"><a href="#cb43-8" aria-hidden="true" tabindex="-1"></a>g3 <span class="op">=</span> plot_gradients_ridge_v1(df2_sigmoid_v1, weight_layers_sigmoid_v1[<span class="dv">2</span>])</span>
<span id="cb43-9"><a href="#cb43-9" aria-hidden="true" tabindex="-1"></a>g4 <span class="op">=</span> plot_gradients_ridge_v1(df3_sigmoid_v1, weight_layers_sigmoid_v1[<span class="dv">3</span>])</span>
<span id="cb43-10"><a href="#cb43-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-11"><a href="#cb43-11" aria-hidden="true" tabindex="-1"></a>g1.savefig(<span class="st">'g1.png'</span>)</span>
<span id="cb43-12"><a href="#cb43-12" aria-hidden="true" tabindex="-1"></a>plt.close(g1.fig)</span>
<span id="cb43-13"><a href="#cb43-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-14"><a href="#cb43-14" aria-hidden="true" tabindex="-1"></a>g2.savefig(<span class="st">'g2.png'</span>)</span>
<span id="cb43-15"><a href="#cb43-15" aria-hidden="true" tabindex="-1"></a>plt.close(g2.fig)</span>
<span id="cb43-16"><a href="#cb43-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-17"><a href="#cb43-17" aria-hidden="true" tabindex="-1"></a>g3.savefig(<span class="st">'g3.png'</span>)</span>
<span id="cb43-18"><a href="#cb43-18" aria-hidden="true" tabindex="-1"></a>plt.close(g3.fig)</span>
<span id="cb43-19"><a href="#cb43-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-20"><a href="#cb43-20" aria-hidden="true" tabindex="-1"></a>g4.savefig(<span class="st">'g4.png'</span>)</span>
<span id="cb43-21"><a href="#cb43-21" aria-hidden="true" tabindex="-1"></a>plt.close(g4.fig)</span>
<span id="cb43-22"><a href="#cb43-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-23"><a href="#cb43-23" aria-hidden="true" tabindex="-1"></a><span class="co">############### 3. CREATE YOUR SUBPLOTS FROM TEMPORAL IMAGES</span></span>
<span id="cb43-24"><a href="#cb43-24" aria-hidden="true" tabindex="-1"></a>f, axarr <span class="op">=</span> plt.subplots(<span class="dv">2</span>, <span class="dv">2</span>, figsize<span class="op">=</span>(<span class="dv">25</span>, <span class="dv">16</span>))</span>
<span id="cb43-25"><a href="#cb43-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-26"><a href="#cb43-26" aria-hidden="true" tabindex="-1"></a>axarr[<span class="dv">0</span>,<span class="dv">0</span>].imshow(mpimg.imread(<span class="st">'g1.png'</span>))</span>
<span id="cb43-27"><a href="#cb43-27" aria-hidden="true" tabindex="-1"></a>axarr[<span class="dv">0</span>,<span class="dv">1</span>].imshow(mpimg.imread(<span class="st">'g2.png'</span>))</span>
<span id="cb43-28"><a href="#cb43-28" aria-hidden="true" tabindex="-1"></a>axarr[<span class="dv">1</span>,<span class="dv">0</span>].imshow(mpimg.imread(<span class="st">'g3.png'</span>))</span>
<span id="cb43-29"><a href="#cb43-29" aria-hidden="true" tabindex="-1"></a>axarr[<span class="dv">1</span>,<span class="dv">1</span>].imshow(mpimg.imread(<span class="st">'g4.png'</span>))</span>
<span id="cb43-30"><a href="#cb43-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-31"><a href="#cb43-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-32"><a href="#cb43-32" aria-hidden="true" tabindex="-1"></a><span class="co"># turn off x and y axis</span></span>
<span id="cb43-33"><a href="#cb43-33" aria-hidden="true" tabindex="-1"></a>[ax.set_axis_off() <span class="cf">for</span> ax <span class="kw">in</span> axarr.ravel()]</span>
<span id="cb43-34"><a href="#cb43-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-35"><a href="#cb43-35" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb43-36"><a href="#cb43-36" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<p><img src="2022-10-23-pytorch-vanishing-gradients-deep-neural-networks_files/figure-html/cell-30-output-1.png" class="img-fluid"></p>
</div>
</div>
<section id="what-do-we-get-from-these-ridge-plots" class="level4">
<h4 class="anchored" data-anchor-id="what-do-we-get-from-these-ridge-plots">What do we get from these ridge plots?</h4>
<ul>
<li>It also shows that the network is suffering from diminishing gradients. gradient scale ‘x’ is decreasing exponentially between weight layers</li>
<li>It shows that gradients are very spread out at the start. They keep on saturating till around epoch 10. After that, instead of getting more saturated around zero, multiple peaks start to emerge. This could be due to our learning rate. In the later epochs, our weights start to oscillate around zero. We can avoid this by using an adaptive learning rate that decreases when weights are near the global minimum.</li>
</ul>
</section>
</section>
</section>
<section id="gradients-for-a-model-with-relu-activations" class="level2">
<h2 class="anchored" data-anchor-id="gradients-for-a-model-with-relu-activations">Gradients for a model with ReLU activations</h2>
<p>In this section we will visualize gradients for our model with ReLU activations.</p>
<div class="cell" data-outputid="3fc49cf2-5b03-4485-8add-9721c946fcff" data-execution_count="30">
<div class="sourceCode cell-code" id="cb44"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb44-1"><a href="#cb44-1" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(dlp_relu.grad)):</span>
<span id="cb44-2"><a href="#cb44-2" aria-hidden="true" tabindex="-1"></a>    plot_gradients(dlp_relu.grad[i], i)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="2022-10-23-pytorch-vanishing-gradients-deep-neural-networks_files/figure-html/cell-31-output-1.png" class="img-fluid"></p>
</div>
<div class="cell-output cell-output-display">
<p><img src="2022-10-23-pytorch-vanishing-gradients-deep-neural-networks_files/figure-html/cell-31-output-2.png" class="img-fluid"></p>
</div>
<div class="cell-output cell-output-display">
<p><img src="2022-10-23-pytorch-vanishing-gradients-deep-neural-networks_files/figure-html/cell-31-output-3.png" class="img-fluid"></p>
</div>
<div class="cell-output cell-output-display">
<p><img src="2022-10-23-pytorch-vanishing-gradients-deep-neural-networks_files/figure-html/cell-31-output-4.png" class="img-fluid"></p>
</div>
<div class="cell-output cell-output-display">
<p><img src="2022-10-23-pytorch-vanishing-gradients-deep-neural-networks_files/figure-html/cell-31-output-5.png" class="img-fluid"></p>
</div>
<div class="cell-output cell-output-display">
<p><img src="2022-10-23-pytorch-vanishing-gradients-deep-neural-networks_files/figure-html/cell-31-output-6.png" class="img-fluid"></p>
</div>
<div class="cell-output cell-output-display">
<p><img src="2022-10-23-pytorch-vanishing-gradients-deep-neural-networks_files/figure-html/cell-31-output-7.png" class="img-fluid"></p>
</div>
<div class="cell-output cell-output-display">
<p><img src="2022-10-23-pytorch-vanishing-gradients-deep-neural-networks_files/figure-html/cell-31-output-8.png" class="img-fluid"></p>
</div>
<div class="cell-output cell-output-display">
<p><img src="2022-10-23-pytorch-vanishing-gradients-deep-neural-networks_files/figure-html/cell-31-output-9.png" class="img-fluid"></p>
</div>
<div class="cell-output cell-output-display">
<p><img src="2022-10-23-pytorch-vanishing-gradients-deep-neural-networks_files/figure-html/cell-31-output-10.png" class="img-fluid"></p>
</div>
<div class="cell-output cell-output-display">
<p><img src="2022-10-23-pytorch-vanishing-gradients-deep-neural-networks_files/figure-html/cell-31-output-11.png" class="img-fluid"></p>
</div>
<div class="cell-output cell-output-display">
<p><img src="2022-10-23-pytorch-vanishing-gradients-deep-neural-networks_files/figure-html/cell-31-output-12.png" class="img-fluid"></p>
</div>
<div class="cell-output cell-output-display">
<p><img src="2022-10-23-pytorch-vanishing-gradients-deep-neural-networks_files/figure-html/cell-31-output-13.png" class="img-fluid"></p>
</div>
<div class="cell-output cell-output-display">
<p><img src="2022-10-23-pytorch-vanishing-gradients-deep-neural-networks_files/figure-html/cell-31-output-14.png" class="img-fluid"></p>
</div>
<div class="cell-output cell-output-display">
<p><img src="2022-10-23-pytorch-vanishing-gradients-deep-neural-networks_files/figure-html/cell-31-output-15.png" class="img-fluid"></p>
</div>
</div>
<section id="what-do-we-get-from-these-plots-1" class="level3">
<h3 class="anchored" data-anchor-id="what-do-we-get-from-these-plots-1">What do we get from these plots?</h3>
<ul>
<li>This time, the gradient curves are nice and smooth. They are not diminishing between weight layers.</li>
<li>Gradients are shrinking between epochs. This means that learning is slowing down in later epochs, and we need to increase the learning rate.</li>
</ul>
</section>
<section id="ridge-plot-for-gradients-1" class="level3">
<h3 class="anchored" data-anchor-id="ridge-plot-for-gradients-1">Ridge plot for gradients</h3>
<p>Let’s also do ridge plots for ReluNet.</p>
<div class="cell" data-execution_count="31">
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb45"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb45-1"><a href="#cb45-1" aria-hidden="true" tabindex="-1"></a><span class="co"># a helper function relu ridge plots. </span></span>
<span id="cb45-2"><a href="#cb45-2" aria-hidden="true" tabindex="-1"></a><span class="co"># same as 'plot_gradients_ridge_v1' but with added limits</span></span>
<span id="cb45-3"><a href="#cb45-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> plot_gradients_ridge_v2(df, layer_name):</span>
<span id="cb45-4"><a href="#cb45-4" aria-hidden="true" tabindex="-1"></a>    sns.set_theme(style<span class="op">=</span><span class="st">"white"</span>, rc<span class="op">=</span>{<span class="st">"axes.facecolor"</span>: (<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>)})</span>
<span id="cb45-5"><a href="#cb45-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb45-6"><a href="#cb45-6" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Initialize the FacetGrid object</span></span>
<span id="cb45-7"><a href="#cb45-7" aria-hidden="true" tabindex="-1"></a>    pal <span class="op">=</span> sns.cubehelix_palette(<span class="dv">10</span>, rot<span class="op">=-</span><span class="fl">.25</span>, light<span class="op">=</span><span class="fl">.7</span>)</span>
<span id="cb45-8"><a href="#cb45-8" aria-hidden="true" tabindex="-1"></a>    g <span class="op">=</span> sns.FacetGrid(df, row<span class="op">=</span><span class="st">"g"</span>, hue<span class="op">=</span><span class="st">"g"</span>, aspect<span class="op">=</span><span class="dv">15</span>, height<span class="op">=</span><span class="fl">3.5</span>, palette<span class="op">=</span>pal, sharex<span class="op">=</span><span class="va">False</span>, xlim<span class="op">=</span>(<span class="op">-</span><span class="fl">0.01</span>,<span class="fl">0.01</span>)) <span class="co">## works best</span></span>
<span id="cb45-9"><a href="#cb45-9" aria-hidden="true" tabindex="-1"></a>    <span class="co"># g = sns.FacetGrid(df, row="g", hue="g", aspect=7, height=3.5, palette=pal)#, sharex=False, xlim=(-0.01,0.01)) ## works good</span></span>
<span id="cb45-10"><a href="#cb45-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb45-11"><a href="#cb45-11" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Draw the densities in a few steps</span></span>
<span id="cb45-12"><a href="#cb45-12" aria-hidden="true" tabindex="-1"></a>    g.<span class="bu">map</span>(sns.kdeplot, <span class="st">"x"</span>, bw_adjust<span class="op">=</span><span class="fl">.5</span>, clip_on<span class="op">=</span><span class="va">True</span>, fill<span class="op">=</span><span class="va">True</span>, alpha<span class="op">=</span><span class="dv">1</span>, linewidth<span class="op">=</span><span class="fl">1.5</span>)</span>
<span id="cb45-13"><a href="#cb45-13" aria-hidden="true" tabindex="-1"></a>    g.<span class="bu">map</span>(sns.kdeplot, <span class="st">"x"</span>, clip_on<span class="op">=</span><span class="va">True</span>, color<span class="op">=</span><span class="st">"w"</span>, lw<span class="op">=</span><span class="dv">2</span>, bw_adjust<span class="op">=</span><span class="fl">.5</span>)</span>
<span id="cb45-14"><a href="#cb45-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb45-15"><a href="#cb45-15" aria-hidden="true" tabindex="-1"></a>    <span class="co"># passing color=None to refline() uses the hue mapping</span></span>
<span id="cb45-16"><a href="#cb45-16" aria-hidden="true" tabindex="-1"></a>    g.refline(y<span class="op">=</span><span class="dv">0</span>, linewidth<span class="op">=</span><span class="dv">2</span>, linestyle<span class="op">=</span><span class="st">"-"</span>, color<span class="op">=</span><span class="va">None</span>, clip_on<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb45-17"><a href="#cb45-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb45-18"><a href="#cb45-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb45-19"><a href="#cb45-19" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Define and use a simple function to label the plot in axes coordinates</span></span>
<span id="cb45-20"><a href="#cb45-20" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> label(x, color, label):</span>
<span id="cb45-21"><a href="#cb45-21" aria-hidden="true" tabindex="-1"></a>        ax <span class="op">=</span> plt.gca()</span>
<span id="cb45-22"><a href="#cb45-22" aria-hidden="true" tabindex="-1"></a>        ax.text(<span class="dv">0</span>, <span class="fl">.2</span>, label, fontsize<span class="op">=</span><span class="dv">40</span>, fontweight<span class="op">=</span><span class="dv">26</span>, color<span class="op">=</span>color, ha<span class="op">=</span><span class="st">"left"</span>, va<span class="op">=</span><span class="st">"center"</span>, transform<span class="op">=</span>ax.transAxes)</span>
<span id="cb45-23"><a href="#cb45-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb45-24"><a href="#cb45-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb45-25"><a href="#cb45-25" aria-hidden="true" tabindex="-1"></a>    g.<span class="bu">map</span>(label, <span class="st">"x"</span>)</span>
<span id="cb45-26"><a href="#cb45-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb45-27"><a href="#cb45-27" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Set the subplots to overlap</span></span>
<span id="cb45-28"><a href="#cb45-28" aria-hidden="true" tabindex="-1"></a>    g.figure.subplots_adjust(hspace<span class="op">=-</span><span class="fl">.25</span>)</span>
<span id="cb45-29"><a href="#cb45-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb45-30"><a href="#cb45-30" aria-hidden="true" tabindex="-1"></a>    g.fig.suptitle(layer_name, ha<span class="op">=</span><span class="st">'left'</span>, fontsize<span class="op">=</span><span class="dv">40</span>, fontweight<span class="op">=</span><span class="dv">26</span>)</span>
<span id="cb45-31"><a href="#cb45-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb45-32"><a href="#cb45-32" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Remove axes details that don't play well with overlap</span></span>
<span id="cb45-33"><a href="#cb45-33" aria-hidden="true" tabindex="-1"></a>    g.set_titles(<span class="st">""</span>)</span>
<span id="cb45-34"><a href="#cb45-34" aria-hidden="true" tabindex="-1"></a>    g.<span class="bu">set</span>(yticks<span class="op">=</span>[], ylabel<span class="op">=</span><span class="st">""</span>)</span>
<span id="cb45-35"><a href="#cb45-35" aria-hidden="true" tabindex="-1"></a>    g.despine(bottom<span class="op">=</span><span class="va">True</span>, left<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb45-36"><a href="#cb45-36" aria-hidden="true" tabindex="-1"></a>    plt.xticks(fontsize<span class="op">=</span><span class="dv">35</span>)</span>
<span id="cb45-37"><a href="#cb45-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb45-38"><a href="#cb45-38" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> g</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div class="cell" data-execution_count="32">
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb46"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb46-1"><a href="#cb46-1" aria-hidden="true" tabindex="-1"></a>weight_layers_relu <span class="op">=</span> <span class="bu">list</span>(dlp_relu.grad[<span class="dv">0</span>].keys())</span>
<span id="cb46-2"><a href="#cb46-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-3"><a href="#cb46-3" aria-hidden="true" tabindex="-1"></a>df0_relu <span class="op">=</span> get_layer_gradients(weight_layers_relu[<span class="dv">0</span>], dlp_relu.grad)</span>
<span id="cb46-4"><a href="#cb46-4" aria-hidden="true" tabindex="-1"></a>df1_relu <span class="op">=</span> get_layer_gradients(weight_layers_relu[<span class="dv">1</span>], dlp_relu.grad)</span>
<span id="cb46-5"><a href="#cb46-5" aria-hidden="true" tabindex="-1"></a>df2_relu <span class="op">=</span> get_layer_gradients(weight_layers_relu[<span class="dv">2</span>], dlp_relu.grad)</span>
<span id="cb46-6"><a href="#cb46-6" aria-hidden="true" tabindex="-1"></a>df3_relu <span class="op">=</span> get_layer_gradients(weight_layers_relu[<span class="dv">3</span>], dlp_relu.grad)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div class="cell" data-outputid="284e39fa-5026-4b7a-a925-ff0e706e280c" data-execution_count="33">
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb47"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb47-1"><a href="#cb47-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.image <span class="im">as</span> mpimg</span>
<span id="cb47-2"><a href="#cb47-2" aria-hidden="true" tabindex="-1"></a>warnings.filterwarnings(<span class="st">"ignore"</span>)</span>
<span id="cb47-3"><a href="#cb47-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-4"><a href="#cb47-4" aria-hidden="true" tabindex="-1"></a>g1 <span class="op">=</span> plot_gradients_ridge_v2(df0_relu, weight_layers_relu[<span class="dv">0</span>])</span>
<span id="cb47-5"><a href="#cb47-5" aria-hidden="true" tabindex="-1"></a>g2 <span class="op">=</span> plot_gradients_ridge_v2(df1_relu, weight_layers_relu[<span class="dv">1</span>])</span>
<span id="cb47-6"><a href="#cb47-6" aria-hidden="true" tabindex="-1"></a>g3 <span class="op">=</span> plot_gradients_ridge_v2(df2_relu, weight_layers_relu[<span class="dv">2</span>])</span>
<span id="cb47-7"><a href="#cb47-7" aria-hidden="true" tabindex="-1"></a>g4 <span class="op">=</span> plot_gradients_ridge_v2(df3_relu, weight_layers_relu[<span class="dv">3</span>])</span>
<span id="cb47-8"><a href="#cb47-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-9"><a href="#cb47-9" aria-hidden="true" tabindex="-1"></a>g1.savefig(<span class="st">'g1.png'</span>)</span>
<span id="cb47-10"><a href="#cb47-10" aria-hidden="true" tabindex="-1"></a>plt.close(g1.fig)</span>
<span id="cb47-11"><a href="#cb47-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-12"><a href="#cb47-12" aria-hidden="true" tabindex="-1"></a>g2.savefig(<span class="st">'g2.png'</span>)</span>
<span id="cb47-13"><a href="#cb47-13" aria-hidden="true" tabindex="-1"></a>plt.close(g2.fig)</span>
<span id="cb47-14"><a href="#cb47-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-15"><a href="#cb47-15" aria-hidden="true" tabindex="-1"></a>g3.savefig(<span class="st">'g3.png'</span>)</span>
<span id="cb47-16"><a href="#cb47-16" aria-hidden="true" tabindex="-1"></a>plt.close(g3.fig)</span>
<span id="cb47-17"><a href="#cb47-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-18"><a href="#cb47-18" aria-hidden="true" tabindex="-1"></a>g4.savefig(<span class="st">'g4.png'</span>)</span>
<span id="cb47-19"><a href="#cb47-19" aria-hidden="true" tabindex="-1"></a>plt.close(g4.fig)</span>
<span id="cb47-20"><a href="#cb47-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-21"><a href="#cb47-21" aria-hidden="true" tabindex="-1"></a><span class="co">############### 3. CREATE YOUR SUBPLOTS FROM TEMPORAL IMAGES</span></span>
<span id="cb47-22"><a href="#cb47-22" aria-hidden="true" tabindex="-1"></a>f, axarr <span class="op">=</span> plt.subplots(<span class="dv">2</span>, <span class="dv">2</span>, figsize<span class="op">=</span>(<span class="dv">25</span>, <span class="dv">16</span>))</span>
<span id="cb47-23"><a href="#cb47-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-24"><a href="#cb47-24" aria-hidden="true" tabindex="-1"></a>axarr[<span class="dv">0</span>,<span class="dv">0</span>].imshow(mpimg.imread(<span class="st">'g1.png'</span>))</span>
<span id="cb47-25"><a href="#cb47-25" aria-hidden="true" tabindex="-1"></a>axarr[<span class="dv">0</span>,<span class="dv">1</span>].imshow(mpimg.imread(<span class="st">'g2.png'</span>))</span>
<span id="cb47-26"><a href="#cb47-26" aria-hidden="true" tabindex="-1"></a>axarr[<span class="dv">1</span>,<span class="dv">0</span>].imshow(mpimg.imread(<span class="st">'g3.png'</span>))</span>
<span id="cb47-27"><a href="#cb47-27" aria-hidden="true" tabindex="-1"></a>axarr[<span class="dv">1</span>,<span class="dv">1</span>].imshow(mpimg.imread(<span class="st">'g4.png'</span>))</span>
<span id="cb47-28"><a href="#cb47-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-29"><a href="#cb47-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-30"><a href="#cb47-30" aria-hidden="true" tabindex="-1"></a><span class="co"># turn off x and y axis</span></span>
<span id="cb47-31"><a href="#cb47-31" aria-hidden="true" tabindex="-1"></a>[ax.set_axis_off() <span class="cf">for</span> ax <span class="kw">in</span> axarr.ravel()]</span>
<span id="cb47-32"><a href="#cb47-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-33"><a href="#cb47-33" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb47-34"><a href="#cb47-34" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<p><img src="2022-10-23-pytorch-vanishing-gradients-deep-neural-networks_files/figure-html/cell-34-output-1.png" class="img-fluid"></p>
</div>
</div>
<section id="what-do-we-get-from-these-ridge-plots-1" class="level4">
<h4 class="anchored" data-anchor-id="what-do-we-get-from-these-ridge-plots-1">What do we get from these ridge plots?</h4>
<p>The results are consistent with earlier plots. * Gradients are not diminishing between weight layers. * Gradients are shrinking between epochs, and we need to increase the learning rate for later epochs.</p>


</section>
</section>
</section>
</section>

</main> <!-- /main -->
<script type="application/vnd.jupyter.widget-state+json">
{"038da41c108e4877971d3fe7ea495a65":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"0605da47a3b04f5eb9dba235ee9b9460":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0e93d2ce99034967834e52cef286f93b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0f8d68ef12744c72a717bc3aa2101e7b":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"14901451364d47948c60085883f148d7":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"15ef66b2e0cd405d9b814c0524929d10":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1dd05f8ed81546cc8c19ee8f74f4f560":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"210695dd9fb14af3875a3029ac909119":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_acc52db36ef84565805c7341e97126cf","max":1648877,"min":0,"orientation":"horizontal","style":"IPY_MODEL_038da41c108e4877971d3fe7ea495a65","value":1648877}},"25326f26390e4096a69b74f943a22884":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"26d0a9e665a843ea8338906ec01708c3":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_ff6633e123774feab76247186a36aa33","IPY_MODEL_8ecf7ac5007e4635a75091a554655bf4","IPY_MODEL_ac237ce3ce9045bb93ad7f3496af60fe"],"layout":"IPY_MODEL_15ef66b2e0cd405d9b814c0524929d10"}},"27fb611102fd41d7b9f3769fff06be04":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2dc9220e292c417fb9a12bfbf4bf50b0":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2e1eb1c05fa54d9b9e2811ade1e1d64a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"33d609b46d5a4177901137c6383691f4":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2dc9220e292c417fb9a12bfbf4bf50b0","placeholder":"​","style":"IPY_MODEL_2e1eb1c05fa54d9b9e2811ade1e1d64a","value":"100%"}},"36ef52f4766840049d9d3a45c352dff9":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1dd05f8ed81546cc8c19ee8f74f4f560","placeholder":"​","style":"IPY_MODEL_7b934e55e4b4473faa3641d2f1d4f091","value":" 28881/28881 [00:00&lt;00:00, 601784.92it/s]"}},"3b9deeb7becd4afa8d1d82c576a73b22":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3d20a1dcd4c042c98ad8f34266c0c7f1":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3ee4770150d94535b0511db9acc1a968":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"43f3bf0ad2dc45ed98ce5c9aa92935e4":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_33d609b46d5a4177901137c6383691f4","IPY_MODEL_aaeacc4ffcba41db8bcca0662b623184","IPY_MODEL_36ef52f4766840049d9d3a45c352dff9"],"layout":"IPY_MODEL_ac0d9aceb6a44ca1a6f73ead10dd6010"}},"5f61453474c9499f8d025d2d9df90e15":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"63d1607a80714ecda20b8d187d163a41":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"66e1e35efd5f437289bfa38dad645dea":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_f3e2a72710c34bb2b30fdb090c519002","IPY_MODEL_210695dd9fb14af3875a3029ac909119","IPY_MODEL_7800b6db2b924faf933f0aa764cf49e6"],"layout":"IPY_MODEL_740cd9fe77774fbd8e224090555db123"}},"6ef6ce9ca43e4d559cf7f89a85d686eb":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"740cd9fe77774fbd8e224090555db123":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7800b6db2b924faf933f0aa764cf49e6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_27fb611102fd41d7b9f3769fff06be04","placeholder":"​","style":"IPY_MODEL_63d1607a80714ecda20b8d187d163a41","value":" 1648877/1648877 [00:00&lt;00:00, 18331459.51it/s]"}},"7b934e55e4b4473faa3641d2f1d4f091":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"82574f6d7e634d45b795ace5424f605e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"83c4576efd254a519a3096ff424e1fa8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"83eea35b1e5648bbbc7058f2c4d70c69":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8ecf7ac5007e4635a75091a554655bf4":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_0605da47a3b04f5eb9dba235ee9b9460","max":4542,"min":0,"orientation":"horizontal","style":"IPY_MODEL_6ef6ce9ca43e4d559cf7f89a85d686eb","value":4542}},"aaeacc4ffcba41db8bcca0662b623184":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_0f8d68ef12744c72a717bc3aa2101e7b","max":28881,"min":0,"orientation":"horizontal","style":"IPY_MODEL_e0054336a32640bf8674dd9fe6580dc3","value":28881}},"ac0d9aceb6a44ca1a6f73ead10dd6010":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ac237ce3ce9045bb93ad7f3496af60fe":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_83eea35b1e5648bbbc7058f2c4d70c69","placeholder":"​","style":"IPY_MODEL_82574f6d7e634d45b795ace5424f605e","value":" 4542/4542 [00:00&lt;00:00, 138604.74it/s]"}},"acc52db36ef84565805c7341e97126cf":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b1ae274504384660a52bf0a069d4dbe5":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c3f49a9d93144df4a86b9945784d59cb":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ca1ff433bfb547719fa6456c5f4f18f2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3b9deeb7becd4afa8d1d82c576a73b22","placeholder":"​","style":"IPY_MODEL_3ee4770150d94535b0511db9acc1a968","value":"100%"}},"ce55a0f2e2684778812544c628163dd5":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_ca1ff433bfb547719fa6456c5f4f18f2","IPY_MODEL_f4a2d8966d844062912daa347f7e61ed","IPY_MODEL_dcd01c7285c44bf4b813910c74dd3679"],"layout":"IPY_MODEL_de06bdf6041243af9428e181ade1cebf"}},"dcd01c7285c44bf4b813910c74dd3679":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3d20a1dcd4c042c98ad8f34266c0c7f1","placeholder":"​","style":"IPY_MODEL_0e93d2ce99034967834e52cef286f93b","value":" 9912422/9912422 [00:00&lt;00:00, 13032361.02it/s]"}},"de06bdf6041243af9428e181ade1cebf":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e0054336a32640bf8674dd9fe6580dc3":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"f3e2a72710c34bb2b30fdb090c519002":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b1ae274504384660a52bf0a069d4dbe5","placeholder":"​","style":"IPY_MODEL_83c4576efd254a519a3096ff424e1fa8","value":"100%"}},"f4a2d8966d844062912daa347f7e61ed":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_25326f26390e4096a69b74f943a22884","max":9912422,"min":0,"orientation":"horizontal","style":"IPY_MODEL_5f61453474c9499f8d025d2d9df90e15","value":9912422}},"ff6633e123774feab76247186a36aa33":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c3f49a9d93144df4a86b9945784d59cb","placeholder":"​","style":"IPY_MODEL_14901451364d47948c60085883f148d7","value":"100%"}}}
</script>
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  let localAlternateSentinel = 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>