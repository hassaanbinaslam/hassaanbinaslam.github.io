<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.2.269">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="dcterms.date" content="2022-11-19">
<meta name="description" content="This is a practice notebook to build a character-level language model with LSTM using PyTorch. We will train a model on an input text, and our goal will be to generate some new text.">

<title>Random Thoughts - Generating Text with Recurrent Neural Networks in PyTorch</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../images/favicon.ico" rel="icon">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../site_libs/bootstrap/bootstrap-dark.min.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>

<script type="text/javascript">

(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
ga('create', 'UA-20316028', 'auto');

ga('send', {
  hitType: 'pageview',
  'anonymizeIp': true,
});
</script>


<link rel="stylesheet" href="../styles.css">
<meta property="og:title" content="Random Thoughts - Generating Text with Recurrent Neural Networks in PyTorch">
<meta property="og:description" content="This is a practice notebook to build a character-level language model with LSTM using PyTorch. We will train a model on an input text, and our goal will be to generate some new text.">
<meta property="og:image" content="images/2022-11-19-pytorch-lstm-text-generation.jpeg">
<meta property="og:site-name" content="Random Thoughts">
<meta name="twitter:title" content="Random Thoughts - Generating Text with Recurrent Neural Networks in PyTorch">
<meta name="twitter:description" content="This is a practice notebook to build a character-level language model with LSTM using PyTorch. We will train a model on an input text, and our goal will be to generate some new text.">
<meta name="twitter:image" content="images/2022-11-19-pytorch-lstm-text-generation.jpeg">
<meta name="twitter:card" content="summary_large_image">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../index.html">
    <span class="navbar-title">Random Thoughts</span>
    </a>
  </div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../about.html">
 <span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://www.linkedin.com/in/hassaanbinaslam/"><i class="bi bi-linkedin" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
</ul>
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/hassaanbinaslam/"><i class="bi bi-github" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com/hassaanbinaslam"><i class="bi bi-twitter" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
</ul>
              <div class="quarto-toggle-container">
                  <a href="" class="quarto-color-scheme-toggle nav-link" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
              </div>
              <div id="quarto-search" class="" title="Search"></div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#credits" id="toc-credits" class="nav-link active" data-scroll-target="#credits">Credits</a></li>
  <li><a href="#environment" id="toc-environment" class="nav-link" data-scroll-target="#environment">Environment</a></li>
  <li><a href="#introduction" id="toc-introduction" class="nav-link" data-scroll-target="#introduction">Introduction</a></li>
  <li><a href="#data-preparation" id="toc-data-preparation" class="nav-link" data-scroll-target="#data-preparation">Data Preparation</a>
  <ul class="collapse">
  <li><a href="#download-data" id="toc-download-data" class="nav-link" data-scroll-target="#download-data">Download data</a></li>
  <li><a href="#preprocess-data" id="toc-preprocess-data" class="nav-link" data-scroll-target="#preprocess-data">Preprocess data</a></li>
  <li><a href="#how-does-the-data-look" id="toc-how-does-the-data-look" class="nav-link" data-scroll-target="#how-does-the-data-look">How does the data look?</a></li>
  <li><a href="#preparing-data-dictionary" id="toc-preparing-data-dictionary" class="nav-link" data-scroll-target="#preparing-data-dictionary">Preparing data dictionary</a></li>
  <li><a href="#encode-input-text" id="toc-encode-input-text" class="nav-link" data-scroll-target="#encode-input-text">Encode input text</a></li>
  <li><a href="#prepare-data-sequences" id="toc-prepare-data-sequences" class="nav-link" data-scroll-target="#prepare-data-sequences">Prepare data sequences</a></li>
  </ul></li>
  <li><a href="#load-data-into-dataset-and-dataloader-class" id="toc-load-data-into-dataset-and-dataloader-class" class="nav-link" data-scroll-target="#load-data-into-dataset-and-dataloader-class">Load Data into Dataset and DataLoader class</a>
  <ul class="collapse">
  <li><a href="#load-data-into-dataset-class" id="toc-load-data-into-dataset-class" class="nav-link" data-scroll-target="#load-data-into-dataset-class">Load data into Dataset class</a></li>
  <li><a href="#load-data-into-dataloader-class-to-prepare-batches" id="toc-load-data-into-dataloader-class-to-prepare-batches" class="nav-link" data-scroll-target="#load-data-into-dataloader-class-to-prepare-batches">Load data into DataLoader class to prepare batches</a></li>
  </ul></li>
  <li><a href="#model-configuration-and-training" id="toc-model-configuration-and-training" class="nav-link" data-scroll-target="#model-configuration-and-training">Model Configuration and Training</a>
  <ul class="collapse">
  <li><a href="#configure-loss-function-and-optimizer" id="toc-configure-loss-function-and-optimizer" class="nav-link" data-scroll-target="#configure-loss-function-and-optimizer">Configure loss function and optimizer</a></li>
  <li><a href="#model-training" id="toc-model-training" class="nav-link" data-scroll-target="#model-training">Model training</a></li>
  </ul></li>
  <li><a href="#process-output-from-the-model" id="toc-process-output-from-the-model" class="nav-link" data-scroll-target="#process-output-from-the-model">Process output from the model</a></li>
  <li><a href="#generating-new-text-passages" id="toc-generating-new-text-passages" class="nav-link" data-scroll-target="#generating-new-text-passages">Generating new text passages</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Generating Text with Recurrent Neural Networks in PyTorch</h1>
  <div class="quarto-categories">
    <div class="quarto-category">pytorch</div>
    <div class="quarto-category">lstm</div>
  </div>
  </div>

<div>
  <div class="description">
    This is a practice notebook to build a character-level language model with LSTM using PyTorch. We will train a model on an input text, and our goal will be to generate some new text.
  </div>
</div>


<div class="quarto-title-meta">

    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">November 19, 2022</p>
    </div>
  </div>
  
    
  </div>
  

</header>

<p><img src="images/2022-11-19-pytorch-lstm-text-generation.jpeg" class="img-fluid"></p>
<section id="credits" class="level2">
<h2 class="anchored" data-anchor-id="credits">Credits</h2>
<p>This notebook takes inspiration and ideas from the following sources.</p>
<ul>
<li>“Machine learning with PyTorch and Scikit-Learn” by “Sebastian Raschka, Yuxi (Hayden) Liu, and Vahid Mirjalili”. You can get the book from its website: <a href="https://sebastianraschka.com/books/#machine-learning-with-pytorch-and-scikit-learn">Machine learning with PyTorch and Scikit-Learn</a>. In addition, the GitHub repository for this book has valuable notebooks: <a href="https://github.com/rasbt/machine-learning-book">github.com/rasbt/machine-learning-book</a>. Parts of the code you see in this notebook are taken from <a href="https://github.com/rasbt/machine-learning-book/blob/main/ch15/ch15_part3.ipynb">chapter 15</a> notebook of the same book.</li>
<li>“Intro to Deep Learning and Generative Models Course” lecture series from “Sebastian Raschka”. Course website: <a href="https://sebastianraschka.com/teaching/stat453-ss2021/">stat453-ss2021</a>. YouTube Link: <a href="https://www.youtube.com/playlist?list=PLTKMiZHVd_2KJtIXOW0zFhFfBaJJilH51">Intro to Deep Learning and Generative Models Course</a>. Lectures that are related to this post are <a href="https://youtu.be/k6fSgUaWUF8">L15.5 Long Short-Term Memory</a> and <a href="https://youtu.be/KgrdifrlDxg">L15.7 An RNN Sentiment Classifier in PyTorch</a></li>
</ul>
</section>
<section id="environment" class="level2">
<h2 class="anchored" data-anchor-id="environment">Environment</h2>
<p>This notebook <a href="https://github.com/hassaanbinaslam/myblog/blob/main/posts/2022-11-19-pytorch-lstm-text-generation.ipynb">GitHub link here</a> is prepared with Google Colab.</p>
<div class="cell" data-outputid="8b4cefed-e790-4b6c-9998-e49b8f2d6b47" data-execution_count="1">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1"></a><span class="im">from</span> platform <span class="im">import</span> python_version</span>
<span id="cb1-2"><a href="#cb1-2"></a><span class="im">import</span> numpy, matplotlib, pandas, torch</span>
<span id="cb1-3"><a href="#cb1-3"></a></span>
<span id="cb1-4"><a href="#cb1-4"></a><span class="bu">print</span>(<span class="st">"python=="</span> <span class="op">+</span> python_version())</span>
<span id="cb1-5"><a href="#cb1-5"></a><span class="bu">print</span>(<span class="st">"numpy=="</span> <span class="op">+</span> numpy.__version__)</span>
<span id="cb1-6"><a href="#cb1-6"></a><span class="bu">print</span>(<span class="st">"torch=="</span> <span class="op">+</span> torch.__version__)</span>
<span id="cb1-7"><a href="#cb1-7"></a><span class="bu">print</span>(<span class="st">"matplotlib=="</span> <span class="op">+</span> matplotlib.__version__)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>python==3.7.15
numpy==1.21.6
torch==1.12.1+cu113
matplotlib==3.2.2</code></pre>
</div>
</div>
</section>
<section id="introduction" class="level2">
<h2 class="anchored" data-anchor-id="introduction">Introduction</h2>
<p>Recurrent Neural Network (RNN) works well for sequence problems, i.e., predicting the next sequence item. Stock prices, for example, are a type of sequence data more commonly known as time-series data. A similar notion can be applied to the NLP domain to build a character-level language model. Here language textual data becomes the sequence data, and from our model, we try to predict the next character in the input text. For training, the input text is broken into a sequence of characters and fed to the model one character at a time. The network will process the new character in relation to previously seen characters and use this information to predict the next alphabet.</p>
</section>
<section id="data-preparation" class="level2">
<h2 class="anchored" data-anchor-id="data-preparation">Data Preparation</h2>
<section id="download-data" class="level3">
<h3 class="anchored" data-anchor-id="download-data">Download data</h3>
<p>For input text, we will use a famous English folk story (though any other text will work equally well) with the name <a href="https://en.wikipedia.org/wiki/Cinderella">Cinderella</a>. To download the story text, you may use <a href="https://www.gutenberg.org/cache/epub/10830/pg10830.txt">Project Gutenberg</a> site or <a href="https://ia600204.us.archive.org/30/items/cinderella10830gut/10830.txt">Archive.org</a>.</p>
<div class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1"></a>download_link <span class="op">=</span> <span class="st">"https://ia600204.us.archive.org/30/items/cinderella10830gut/10830.txt"</span></span>
<span id="cb3-2"><a href="#cb3-2"></a></span>
<span id="cb3-3"><a href="#cb3-3"></a><span class="co">## alternate download link</span></span>
<span id="cb3-4"><a href="#cb3-4"></a><span class="co"># download_link = "https://www.gutenberg.org/cache/epub/10830/pg10830.txt"</span></span>
<span id="cb3-5"><a href="#cb3-5"></a></span>
<span id="cb3-6"><a href="#cb3-6"></a>file_name <span class="op">=</span> <span class="st">'input.txt'</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-outputid="08ab0427-083e-441f-c596-868284544089" data-execution_count="3">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1"></a><span class="co">##</span></span>
<span id="cb4-2"><a href="#cb4-2"></a><span class="co"># download the story text and save it as {file_name}</span></span>
<span id="cb4-3"><a href="#cb4-3"></a><span class="op">!</span> curl {download_link} <span class="op">-</span>o {file_name}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
100 45278  100 45278    0     0  38865      0  0:00:01  0:00:01 --:--:-- 38831</code></pre>
</div>
</div>
<p>The download is complete. We can now open the file and read its contents.</p>
<div class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1"></a><span class="co">##</span></span>
<span id="cb6-2"><a href="#cb6-2"></a><span class="co"># Reading and processing text</span></span>
<span id="cb6-3"><a href="#cb6-3"></a><span class="cf">with</span> <span class="bu">open</span>(file_name, <span class="st">"r"</span>, encoding<span class="op">=</span><span class="st">"utf8"</span>) <span class="im">as</span> fp:</span>
<span id="cb6-4"><a href="#cb6-4"></a>    text <span class="op">=</span> fp.read()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="preprocess-data" class="level3">
<h3 class="anchored" data-anchor-id="preprocess-data">Preprocess data</h3>
<p>The downloaded text has been published as a volunteer effort under <strong>Project Gutenberg</strong>. They have added some project and license information after the original story text as part of the project requirements. We are not interested in that text (boilerplate text), so let’s omit that and limit our input text to the folk story.</p>
<div class="cell" data-outputid="b9127f6f-d88f-41bf-9b03-b29d1af1c715" data-execution_count="5">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1"></a><span class="co">##</span></span>
<span id="cb7-2"><a href="#cb7-2"></a><span class="co"># truncate text till story start and end</span></span>
<span id="cb7-3"><a href="#cb7-3"></a>start_indx <span class="op">=</span> text.find(</span>
<span id="cb7-4"><a href="#cb7-4"></a>    <span class="st">"There once lived a gentleman and his wife, who were the parents of a</span><span class="ch">\n</span><span class="st">lovely little daughter."</span></span>
<span id="cb7-5"><a href="#cb7-5"></a>)</span>
<span id="cb7-6"><a href="#cb7-6"></a>end_indx <span class="op">=</span> text.find(<span class="st">"*       *       *       *       *"</span>)</span>
<span id="cb7-7"><a href="#cb7-7"></a></span>
<span id="cb7-8"><a href="#cb7-8"></a>text <span class="op">=</span> text[start_indx:end_indx]</span>
<span id="cb7-9"><a href="#cb7-9"></a></span>
<span id="cb7-10"><a href="#cb7-10"></a><span class="co"># total length of the text</span></span>
<span id="cb7-11"><a href="#cb7-11"></a><span class="bu">print</span>(<span class="st">"Total Length (character count):"</span>, <span class="bu">len</span>(text))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Total Length (character count): 21831</code></pre>
</div>
</div>
</section>
<section id="how-does-the-data-look" class="level3">
<h3 class="anchored" data-anchor-id="how-does-the-data-look">How does the data look?</h3>
<p>Let’s view the first 500 characters from the story text.</p>
<div class="cell" data-outputid="372bb33a-850f-471e-e086-d5ffc4f86298" data-execution_count="6">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1"></a><span class="co"># view the text start</span></span>
<span id="cb9-2"><a href="#cb9-2"></a>text[:<span class="dv">500</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="6">
<pre><code>'There once lived a gentleman and his wife, who were the parents of a\nlovely little daughter.\n\nWhen this child was only nine years of age, her mother fell sick.\nFinding her death coming on, she called her child to her and said to\nher, "My child, always be good; bear every thing that happens to you\nwith patience, and whatever evil and troubles you may suffer, you will\nbe happy in the end if you are so." Then the poor lady died, and her\ndaughter was full of great grief at the loss of a mother so go'</code></pre>
</div>
</div>
<p>And the last 500 characters.</p>
<div class="cell" data-outputid="83201752-ee9e-44a6-b39e-71acbc27bec3" data-execution_count="7">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1"></a><span class="co"># view the text end</span></span>
<span id="cb11-2"><a href="#cb11-2"></a>text[<span class="op">-</span><span class="dv">500</span>:]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="7">
<pre><code>'their affection.\nShe was then taken to the palace of the young prince, in whose eyes she\nappeared yet more lovely than before, and who married her shortly after.\n\nCinderella, who was as good as she was beautiful, allowed her sisters to\nlodge in the palace, and gave them in marriage, that same day, to two\nlords belonging to the court.\n\n[Illustration: MARRIAGE OF THE PRINCE AND CINDERELLA.]\n\nThe amiable qualities of Cinderella were as conspicuous after as they\nhad been before marriage.\n\n\n\n\n       '</code></pre>
</div>
</div>
</section>
<section id="preparing-data-dictionary" class="level3">
<h3 class="anchored" data-anchor-id="preparing-data-dictionary">Preparing data dictionary</h3>
<p>Our data is a string and can’t be used to train a model. So instead, we have to convert it into integers. For this encoding, we will use a simple methodology where each unique character in the text is assigned an integer and then replaced with all occurrences of that character in the text with that integer value.</p>
<p>For this, let’s first create a set of all the unique characters in the text.</p>
<div class="cell" data-outputid="e8c52889-57ad-47cc-a8c8-b4bef4adcacb" data-execution_count="8">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb13-2"><a href="#cb13-2"></a></span>
<span id="cb13-3"><a href="#cb13-3"></a><span class="co"># find unique chars from text</span></span>
<span id="cb13-4"><a href="#cb13-4"></a>char_set <span class="op">=</span> <span class="bu">set</span>(text)</span>
<span id="cb13-5"><a href="#cb13-5"></a><span class="bu">print</span>(<span class="st">"Unique Characters:"</span>, <span class="bu">len</span>(char_set))</span>
<span id="cb13-6"><a href="#cb13-6"></a></span>
<span id="cb13-7"><a href="#cb13-7"></a><span class="co"># sort char set</span></span>
<span id="cb13-8"><a href="#cb13-8"></a>chars_sorted <span class="op">=</span> <span class="bu">sorted</span>(char_set)</span>
<span id="cb13-9"><a href="#cb13-9"></a><span class="bu">print</span>(chars_sorted)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Unique Characters: 65
['\n', ' ', '!', '"', "'", ',', '-', '.', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'Y', 'Z', '[', ']', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']</code></pre>
</div>
</div>
<p>We now know all the unique characters in our input text. Accordingly, we can create a dictionary and assign each character in <code>char_set</code> a unique integer.</p>
<div class="cell" data-outputid="0b7a8e2c-d7f7-4ae0-acc4-818af3912c53" data-execution_count="9">
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1"></a><span class="co"># encode chars</span></span>
<span id="cb15-2"><a href="#cb15-2"></a>char2int <span class="op">=</span> {ch: i <span class="cf">for</span> i, ch <span class="kw">in</span> <span class="bu">enumerate</span>(chars_sorted)}</span>
<span id="cb15-3"><a href="#cb15-3"></a></span>
<span id="cb15-4"><a href="#cb15-4"></a><span class="co"># `char2int` dictionary for char -&gt; int</span></span>
<span id="cb15-5"><a href="#cb15-5"></a><span class="bu">print</span>(char2int)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>{'\n': 0, ' ': 1, '!': 2, '"': 3, "'": 4, ',': 5, '-': 6, '.': 7, ':': 8, ';': 9, '?': 10, 'A': 11, 'B': 12, 'C': 13, 'D': 14, 'E': 15, 'F': 16, 'G': 17, 'H': 18, 'I': 19, 'J': 20, 'K': 21, 'L': 22, 'M': 23, 'N': 24, 'O': 25, 'P': 26, 'Q': 27, 'R': 28, 'S': 29, 'T': 30, 'U': 31, 'V': 32, 'W': 33, 'Y': 34, 'Z': 35, '[': 36, ']': 37, '_': 38, 'a': 39, 'b': 40, 'c': 41, 'd': 42, 'e': 43, 'f': 44, 'g': 45, 'h': 46, 'i': 47, 'j': 48, 'k': 49, 'l': 50, 'm': 51, 'n': 52, 'o': 53, 'p': 54, 'q': 55, 'r': 56, 's': 57, 't': 58, 'u': 59, 'v': 60, 'w': 61, 'x': 62, 'y': 63, 'z': 64}</code></pre>
</div>
</div>
<p>But more than just the encoding, we also need a way to convert the encoded characters back to the original form. For this, we will use a separate array that will hold the index of each <code>char</code> in the dictionary. Together with <code>char2int</code> and <code>int2char</code> we can move back and forth between encoded and decoded characters.</p>
<div class="cell" data-outputid="c3767341-eced-4fab-f94e-3e8951aa7c5d" data-execution_count="10">
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1"></a>int2char <span class="op">=</span> np.array(chars_sorted)</span>
<span id="cb17-2"><a href="#cb17-2"></a></span>
<span id="cb17-3"><a href="#cb17-3"></a><span class="co"># `int2char` for int -&gt; char</span></span>
<span id="cb17-4"><a href="#cb17-4"></a><span class="bu">print</span>(int2char)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>['\n' ' ' '!' '"' "'" ',' '-' '.' ':' ';' '?' 'A' 'B' 'C' 'D' 'E' 'F' 'G'
 'H' 'I' 'J' 'K' 'L' 'M' 'N' 'O' 'P' 'Q' 'R' 'S' 'T' 'U' 'V' 'W' 'Y' 'Z'
 '[' ']' '_' 'a' 'b' 'c' 'd' 'e' 'f' 'g' 'h' 'i' 'j' 'k' 'l' 'm' 'n' 'o'
 'p' 'q' 'r' 's' 't' 'u' 'v' 'w' 'x' 'y' 'z']</code></pre>
</div>
</div>
</section>
<section id="encode-input-text" class="level3">
<h3 class="anchored" data-anchor-id="encode-input-text">Encode input text</h3>
<p>In this step, we will use the <code>char2int</code> dictionary to encode our story text. The encoded version of <code>text</code> is called <code>text_encoded</code>.</p>
<div class="cell" data-outputid="85147933-cc61-4443-b731-926d8361ed5d" data-execution_count="11">
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1"></a><span class="co">##</span></span>
<span id="cb19-2"><a href="#cb19-2"></a><span class="co"># encode original text</span></span>
<span id="cb19-3"><a href="#cb19-3"></a>text_encoded <span class="op">=</span> np.array([char2int[ch] <span class="cf">for</span> ch <span class="kw">in</span> text], dtype<span class="op">=</span>np.int32)</span>
<span id="cb19-4"><a href="#cb19-4"></a></span>
<span id="cb19-5"><a href="#cb19-5"></a><span class="bu">print</span>(<span class="st">"Text encoded shape: "</span>, text_encoded.shape)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Text encoded shape:  (21831,)</code></pre>
</div>
</div>
<p>Let’s use <code>int2char</code> to decode and return the original text.</p>
<div class="cell" data-outputid="3dfc5e40-bfe5-4d42-c3d4-e1deb39e5bb0" data-execution_count="12">
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1"></a><span class="co">##</span></span>
<span id="cb21-2"><a href="#cb21-2"></a><span class="co"># decoding original text</span></span>
<span id="cb21-3"><a href="#cb21-3"></a><span class="cf">for</span> ex <span class="kw">in</span> text_encoded[:<span class="dv">5</span>]:</span>
<span id="cb21-4"><a href="#cb21-4"></a>    <span class="bu">print</span>(<span class="st">"</span><span class="sc">{}</span><span class="st"> -&gt; </span><span class="sc">{}</span><span class="st">"</span>.<span class="bu">format</span>(ex, int2char[ex]))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>30 -&gt; T
46 -&gt; h
43 -&gt; e
56 -&gt; r
43 -&gt; e</code></pre>
</div>
</div>
<p>Another example of encoding and decoding. This time I used multiple words together.</p>
<div class="cell" data-outputid="9e2b9f46-a56f-4901-9524-e0b10ad483c1" data-execution_count="13">
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1"></a><span class="bu">print</span>(text[:<span class="dv">18</span>], <span class="st">"     == Encoding ==&gt; "</span>, text_encoded[:<span class="dv">18</span>])</span>
<span id="cb23-2"><a href="#cb23-2"></a><span class="bu">print</span>(text_encoded[<span class="dv">19</span>:<span class="dv">41</span>], <span class="st">" == Reverse  ==&gt; "</span>, <span class="st">""</span>.join(int2char[text_encoded[<span class="dv">19</span>:<span class="dv">41</span>]]))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>There once lived a      == Encoding ==&gt;  [30 46 43 56 43  1 53 52 41 43  1 50 47 60 43 42  1 39]
[45 43 52 58 50 43 51 39 52  1 39 52 42  1 46 47 57  1 61 47 44 43]  == Reverse  ==&gt;  gentleman and his wife</code></pre>
</div>
</div>
</section>
<section id="prepare-data-sequences" class="level3">
<h3 class="anchored" data-anchor-id="prepare-data-sequences">Prepare data sequences</h3>
<p>We have our encoded data ready. Next, we will convert it into sequences of fixed length. The last sequence element will act as a target, and the remaining elements will be the input. For sequencing, we will use length 41.</p>
<ul>
<li>The first 40 characters in sequence form the input</li>
<li>The last character in sequence (41) represents the output</li>
</ul>
<div class="cell" data-execution_count="14">
<div class="sourceCode cell-code" id="cb25"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1"></a><span class="co">##</span></span>
<span id="cb25-2"><a href="#cb25-2"></a><span class="co"># make sequences of encoded text as `text_chunks`</span></span>
<span id="cb25-3"><a href="#cb25-3"></a>seq_length <span class="op">=</span> <span class="dv">40</span></span>
<span id="cb25-4"><a href="#cb25-4"></a>chunk_size <span class="op">=</span> seq_length <span class="op">+</span> <span class="dv">1</span></span>
<span id="cb25-5"><a href="#cb25-5"></a></span>
<span id="cb25-6"><a href="#cb25-6"></a>text_chunks <span class="op">=</span> [</span>
<span id="cb25-7"><a href="#cb25-7"></a>    text_encoded[i : i <span class="op">+</span> chunk_size] <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(text_encoded) <span class="op">-</span> chunk_size <span class="op">+</span> <span class="dv">1</span>)</span>
<span id="cb25-8"><a href="#cb25-8"></a>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-outputid="fe404cbb-adcc-4d22-d784-b3842a02de76" data-execution_count="15">
<div class="sourceCode cell-code" id="cb26"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1"></a><span class="co">##</span></span>
<span id="cb26-2"><a href="#cb26-2"></a><span class="co"># inspect the first chuck</span></span>
<span id="cb26-3"><a href="#cb26-3"></a><span class="cf">for</span> seq <span class="kw">in</span> text_chunks[:<span class="dv">1</span>]:</span>
<span id="cb26-4"><a href="#cb26-4"></a>    input_seq <span class="op">=</span> seq[:<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb26-5"><a href="#cb26-5"></a>    target <span class="op">=</span> seq[<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb26-6"><a href="#cb26-6"></a></span>
<span id="cb26-7"><a href="#cb26-7"></a>    <span class="bu">print</span>(input_seq, <span class="st">" -&gt; "</span>, target)</span>
<span id="cb26-8"><a href="#cb26-8"></a>    <span class="bu">print</span>(<span class="bu">repr</span>(<span class="st">""</span>.join(int2char[input_seq])), <span class="st">" -&gt; "</span>, <span class="bu">repr</span>(<span class="st">""</span>.join(int2char[target])))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[30 46 43 56 43  1 53 52 41 43  1 50 47 60 43 42  1 39  1 45 43 52 58 50
 43 51 39 52  1 39 52 42  1 46 47 57  1 61 47 44]  -&gt;  43
'There once lived a gentleman and his wif'  -&gt;  'e'</code></pre>
</div>
</div>
<div class="cell" data-outputid="a01e82c2-7a34-420d-e98f-64e573d98589" data-execution_count="16">
<div class="sourceCode cell-code" id="cb28"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1"></a><span class="co">##</span></span>
<span id="cb28-2"><a href="#cb28-2"></a><span class="co"># inspect the second chuck</span></span>
<span id="cb28-3"><a href="#cb28-3"></a><span class="cf">for</span> seq <span class="kw">in</span> text_chunks[<span class="dv">1</span>:<span class="dv">2</span>]:</span>
<span id="cb28-4"><a href="#cb28-4"></a>    input_seq <span class="op">=</span> seq[:<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb28-5"><a href="#cb28-5"></a>    target <span class="op">=</span> seq[<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb28-6"><a href="#cb28-6"></a></span>
<span id="cb28-7"><a href="#cb28-7"></a>    <span class="bu">print</span>(input_seq, <span class="st">" -&gt; "</span>, target)</span>
<span id="cb28-8"><a href="#cb28-8"></a>    <span class="bu">print</span>(<span class="bu">repr</span>(<span class="st">""</span>.join(int2char[input_seq])), <span class="st">" -&gt; "</span>, <span class="bu">repr</span>(<span class="st">""</span>.join(int2char[target])))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[46 43 56 43  1 53 52 41 43  1 50 47 60 43 42  1 39  1 45 43 52 58 50 43
 51 39 52  1 39 52 42  1 46 47 57  1 61 47 44 43]  -&gt;  5
'here once lived a gentleman and his wife'  -&gt;  ','</code></pre>
</div>
</div>
</section>
</section>
<section id="load-data-into-dataset-and-dataloader-class" class="level2">
<h2 class="anchored" data-anchor-id="load-data-into-dataset-and-dataloader-class">Load Data into Dataset and DataLoader class</h2>
<p>In this section, we will load our encoded data sequences into Dataset and DataLoader class to prepare batches for model training.</p>
<section id="load-data-into-dataset-class" class="level3">
<h3 class="anchored" data-anchor-id="load-data-into-dataset-class">Load data into Dataset class</h3>
<p>class <code>TextDataset</code> is derived from PyTorch <code>Dataset</code>. When we get a sequence using this class, it will return the sequence as a tuple of input and target.</p>
<div class="cell" data-outputid="b5befe45-930a-45fa-c85c-768c1dd6317d" data-execution_count="17">
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb30"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb30-1"><a href="#cb30-1"></a><span class="im">import</span> torch</span>
<span id="cb30-2"><a href="#cb30-2"></a><span class="im">from</span> torch.utils.data <span class="im">import</span> Dataset</span>
<span id="cb30-3"><a href="#cb30-3"></a></span>
<span id="cb30-4"><a href="#cb30-4"></a></span>
<span id="cb30-5"><a href="#cb30-5"></a><span class="kw">class</span> TextDataset(Dataset):</span>
<span id="cb30-6"><a href="#cb30-6"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, text_chunks):</span>
<span id="cb30-7"><a href="#cb30-7"></a>        <span class="va">self</span>.text_chunks <span class="op">=</span> text_chunks</span>
<span id="cb30-8"><a href="#cb30-8"></a></span>
<span id="cb30-9"><a href="#cb30-9"></a>    <span class="kw">def</span> <span class="fu">__len__</span>(<span class="va">self</span>):</span>
<span id="cb30-10"><a href="#cb30-10"></a>        <span class="cf">return</span> <span class="bu">len</span>(<span class="va">self</span>.text_chunks)</span>
<span id="cb30-11"><a href="#cb30-11"></a></span>
<span id="cb30-12"><a href="#cb30-12"></a>    <span class="kw">def</span> <span class="fu">__getitem__</span>(<span class="va">self</span>, idx):</span>
<span id="cb30-13"><a href="#cb30-13"></a>        text_chunk <span class="op">=</span> <span class="va">self</span>.text_chunks[idx]</span>
<span id="cb30-14"><a href="#cb30-14"></a>        <span class="cf">return</span> text_chunk[:<span class="op">-</span><span class="dv">1</span>].<span class="bu">long</span>(), text_chunk[<span class="dv">1</span>:].<span class="bu">long</span>()  <span class="co"># return input, target</span></span>
<span id="cb30-15"><a href="#cb30-15"></a></span>
<span id="cb30-16"><a href="#cb30-16"></a></span>
<span id="cb30-17"><a href="#cb30-17"></a>seq_dataset <span class="op">=</span> TextDataset(torch.tensor(text_chunks))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stderr">
<pre><code>/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:15: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:201.)
  from ipykernel import kernelapp as app</code></pre>
</div>
</div>
<p>Each element from the <code>seq_dataset</code> consists of</p>
<ul>
<li><code>input</code> data that we will feed to the model for training</li>
<li><code>target</code> data that we will use to compare the model output</li>
</ul>
<p>Remember that both <code>input</code> and <code>target</code> sequences are derived from the same encoded text. We train our model to predict the next character from the given input. One character is given as an input to the model, and one character output comes out of the model. In an ideal case, the model output character should represent the next character in a sequence. And our <code>target</code> sequence is just that: one next character from the input sequence.</p>
<div class="cell" data-outputid="7da935b2-0c86-4736-ca4a-26d5ed4df0f8" data-execution_count="18">
<div class="sourceCode cell-code" id="cb32"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb32-1"><a href="#cb32-1"></a><span class="cf">for</span> i, (seq, target) <span class="kw">in</span> <span class="bu">enumerate</span>(seq_dataset):</span>
<span id="cb32-2"><a href="#cb32-2"></a>    <span class="bu">print</span>(<span class="st">" Input (x):"</span>, <span class="bu">repr</span>(<span class="st">""</span>.join(int2char[seq])))</span>
<span id="cb32-3"><a href="#cb32-3"></a>    <span class="bu">print</span>(<span class="st">"Target (y):"</span>, <span class="bu">repr</span>(<span class="st">""</span>.join(int2char[target])))</span>
<span id="cb32-4"><a href="#cb32-4"></a>    <span class="bu">print</span>()</span>
<span id="cb32-5"><a href="#cb32-5"></a>    <span class="cf">if</span> i <span class="op">==</span> <span class="dv">1</span>:</span>
<span id="cb32-6"><a href="#cb32-6"></a>        <span class="cf">break</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code> Input (x): 'There once lived a gentleman and his wif'
Target (y): 'here once lived a gentleman and his wife'

 Input (x): 'here once lived a gentleman and his wife'
Target (y): 'ere once lived a gentleman and his wife,'
</code></pre>
</div>
</div>
</section>
<section id="load-data-into-dataloader-class-to-prepare-batches" class="level3">
<h3 class="anchored" data-anchor-id="load-data-into-dataloader-class-to-prepare-batches">Load data into DataLoader class to prepare batches</h3>
<p>In this step, we have prepared training batches using the PyTorch <a href="https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader">DataLoader</a> class.</p>
<div class="cell" data-execution_count="19">
<div class="sourceCode cell-code" id="cb34"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb34-1"><a href="#cb34-1"></a><span class="im">from</span> torch.utils.data <span class="im">import</span> DataLoader</span>
<span id="cb34-2"><a href="#cb34-2"></a></span>
<span id="cb34-3"><a href="#cb34-3"></a>batch_size <span class="op">=</span> <span class="dv">64</span></span>
<span id="cb34-4"><a href="#cb34-4"></a></span>
<span id="cb34-5"><a href="#cb34-5"></a>torch.manual_seed(<span class="dv">1</span>)</span>
<span id="cb34-6"><a href="#cb34-6"></a>seq_dl <span class="op">=</span> DataLoader(seq_dataset, batch_size<span class="op">=</span>batch_size, shuffle<span class="op">=</span><span class="va">True</span>, drop_last<span class="op">=</span><span class="va">True</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
</section>
<section id="model-configuration-and-training" class="level2">
<h2 class="anchored" data-anchor-id="model-configuration-and-training">Model Configuration and Training</h2>
<p>In this section, we will configure a model for character-level language modeling. This model will have an Embedding layer at the start. Next, output from the embedding layer will be passed to the LSTM layer. Finally, at the output, we have a fully connected linear layer.</p>
<p>For an in-depth analysis of the working of an Embedding layer, I recommend this article <a href="https://www.featureform.com/post/the-definitive-guide-to-embeddings">Embeddings in Machine Learning: Everything You Need to Know</a></p>
<div class="cell" data-execution_count="20">
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb35"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb35-1"><a href="#cb35-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb35-2"><a href="#cb35-2"></a></span>
<span id="cb35-3"><a href="#cb35-3"></a>device <span class="op">=</span> <span class="st">"cuda"</span> <span class="cf">if</span> torch.cuda.is_available() <span class="cf">else</span> <span class="st">"cpu"</span></span>
<span id="cb35-4"><a href="#cb35-4"></a></span>
<span id="cb35-5"><a href="#cb35-5"></a><span class="kw">class</span> RNN(nn.Module):</span>
<span id="cb35-6"><a href="#cb35-6"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, vocab_size, embed_dim, rnn_hidden_size):</span>
<span id="cb35-7"><a href="#cb35-7"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb35-8"><a href="#cb35-8"></a>        <span class="va">self</span>.embedding <span class="op">=</span> nn.Embedding(vocab_size, embed_dim)</span>
<span id="cb35-9"><a href="#cb35-9"></a>        <span class="va">self</span>.rnn_hidden_size <span class="op">=</span> rnn_hidden_size</span>
<span id="cb35-10"><a href="#cb35-10"></a>        <span class="va">self</span>.rnn <span class="op">=</span> nn.LSTM(embed_dim, rnn_hidden_size, batch_first<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb35-11"><a href="#cb35-11"></a>        <span class="va">self</span>.fc <span class="op">=</span> nn.Linear(rnn_hidden_size, vocab_size)</span>
<span id="cb35-12"><a href="#cb35-12"></a></span>
<span id="cb35-13"><a href="#cb35-13"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x, hidden, cell):</span>
<span id="cb35-14"><a href="#cb35-14"></a>        out <span class="op">=</span> <span class="va">self</span>.embedding(x).unsqueeze(<span class="dv">1</span>)</span>
<span id="cb35-15"><a href="#cb35-15"></a>        out, (hidden, cell) <span class="op">=</span> <span class="va">self</span>.rnn(out, (hidden, cell))</span>
<span id="cb35-16"><a href="#cb35-16"></a>        out <span class="op">=</span> <span class="va">self</span>.fc(out).reshape(out.size(<span class="dv">0</span>), <span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb35-17"><a href="#cb35-17"></a>        <span class="cf">return</span> out, hidden, cell</span>
<span id="cb35-18"><a href="#cb35-18"></a></span>
<span id="cb35-19"><a href="#cb35-19"></a>    <span class="kw">def</span> init_hidden(<span class="va">self</span>, batch_size):</span>
<span id="cb35-20"><a href="#cb35-20"></a>        hidden <span class="op">=</span> torch.zeros(<span class="dv">1</span>, batch_size, <span class="va">self</span>.rnn_hidden_size)</span>
<span id="cb35-21"><a href="#cb35-21"></a>        cell <span class="op">=</span> torch.zeros(<span class="dv">1</span>, batch_size, <span class="va">self</span>.rnn_hidden_size)</span>
<span id="cb35-22"><a href="#cb35-22"></a>        <span class="cf">return</span> hidden.to(device), cell.to(device)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div class="cell" data-outputid="0d50bde3-f39f-4900-900b-cc4eb488e8eb" data-execution_count="21">
<div class="sourceCode cell-code" id="cb36"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb36-1"><a href="#cb36-1"></a>torch.manual_seed(<span class="dv">1</span>)</span>
<span id="cb36-2"><a href="#cb36-2"></a></span>
<span id="cb36-3"><a href="#cb36-3"></a><span class="co"># define model dimensions</span></span>
<span id="cb36-4"><a href="#cb36-4"></a>vocab_size <span class="op">=</span> <span class="bu">len</span>(int2char)</span>
<span id="cb36-5"><a href="#cb36-5"></a>embed_dim <span class="op">=</span> <span class="dv">256</span></span>
<span id="cb36-6"><a href="#cb36-6"></a>rnn_hidden_size <span class="op">=</span> <span class="dv">512</span></span>
<span id="cb36-7"><a href="#cb36-7"></a></span>
<span id="cb36-8"><a href="#cb36-8"></a><span class="co"># initialize model</span></span>
<span id="cb36-9"><a href="#cb36-9"></a>model <span class="op">=</span> RNN(vocab_size, embed_dim, rnn_hidden_size)</span>
<span id="cb36-10"><a href="#cb36-10"></a>model <span class="op">=</span> model.to(device)</span>
<span id="cb36-11"><a href="#cb36-11"></a>model</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="21">
<pre><code>RNN(
  (embedding): Embedding(65, 256)
  (rnn): LSTM(256, 512, batch_first=True)
  (fc): Linear(in_features=512, out_features=65, bias=True)
)</code></pre>
</div>
</div>
<section id="configure-loss-function-and-optimizer" class="level3">
<h3 class="anchored" data-anchor-id="configure-loss-function-and-optimizer">Configure loss function and optimizer</h3>
<ul>
<li>For the loss function, we will use <a href="https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html">CrossEntropyLoss</a>. This is because we are dealing with a classification problem, and our model has to predict the next character from <code>vocab_size</code> of 65 classes.</li>
<li>For optimization, we will use <a href="https://pytorch.org/docs/stable/generated/torch.optim.Adam.html">torch.optim.Adam</a></li>
</ul>
<div class="cell" data-execution_count="22">
<div class="sourceCode cell-code" id="cb38"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb38-1"><a href="#cb38-1"></a>loss_fn <span class="op">=</span> nn.CrossEntropyLoss()</span>
<span id="cb38-2"><a href="#cb38-2"></a>optimizer <span class="op">=</span> torch.optim.Adam(model.parameters(), lr<span class="op">=</span><span class="fl">0.005</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="model-training" class="level3">
<h3 class="anchored" data-anchor-id="model-training">Model training</h3>
<p>All parts are ready so let’s start the training. Google Colab “CPU” runtime can take significantly longer to train. I would suggest using “GPU” runtime instead.</p>
<div class="cell" data-outputid="2f289099-3541-49b3-9e38-f124466370ca" data-execution_count="27">
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb39"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb39-1"><a href="#cb39-1"></a><span class="co"># for execution time measurement</span></span>
<span id="cb39-2"><a href="#cb39-2"></a><span class="im">from</span> timeit <span class="im">import</span> default_timer <span class="im">as</span> timer</span>
<span id="cb39-3"><a href="#cb39-3"></a></span>
<span id="cb39-4"><a href="#cb39-4"></a>num_epochs <span class="op">=</span> <span class="dv">10000</span></span>
<span id="cb39-5"><a href="#cb39-5"></a>model.train()</span>
<span id="cb39-6"><a href="#cb39-6"></a></span>
<span id="cb39-7"><a href="#cb39-7"></a>start <span class="op">=</span> timer()  <span class="co"># timer start</span></span>
<span id="cb39-8"><a href="#cb39-8"></a><span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(num_epochs):</span>
<span id="cb39-9"><a href="#cb39-9"></a>    hidden, cell <span class="op">=</span> model.init_hidden(batch_size)</span>
<span id="cb39-10"><a href="#cb39-10"></a></span>
<span id="cb39-11"><a href="#cb39-11"></a>    seq_batch, target_batch <span class="op">=</span> <span class="bu">next</span>(<span class="bu">iter</span>(seq_dl))</span>
<span id="cb39-12"><a href="#cb39-12"></a>    seq_batch <span class="op">=</span> seq_batch.to(device)</span>
<span id="cb39-13"><a href="#cb39-13"></a>    target_batch <span class="op">=</span> target_batch.to(device)</span>
<span id="cb39-14"><a href="#cb39-14"></a></span>
<span id="cb39-15"><a href="#cb39-15"></a>    optimizer.zero_grad()</span>
<span id="cb39-16"><a href="#cb39-16"></a>    loss <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb39-17"><a href="#cb39-17"></a></span>
<span id="cb39-18"><a href="#cb39-18"></a>    <span class="cf">for</span> c <span class="kw">in</span> <span class="bu">range</span>(seq_length):</span>
<span id="cb39-19"><a href="#cb39-19"></a>        pred, hidden, cell <span class="op">=</span> model(seq_batch[:, c], hidden, cell)</span>
<span id="cb39-20"><a href="#cb39-20"></a>        loss <span class="op">+=</span> loss_fn(pred, target_batch[:, c])</span>
<span id="cb39-21"><a href="#cb39-21"></a></span>
<span id="cb39-22"><a href="#cb39-22"></a>    loss.backward()</span>
<span id="cb39-23"><a href="#cb39-23"></a>    optimizer.step()</span>
<span id="cb39-24"><a href="#cb39-24"></a></span>
<span id="cb39-25"><a href="#cb39-25"></a>    loss <span class="op">=</span> loss.item() <span class="op">/</span> seq_length</span>
<span id="cb39-26"><a href="#cb39-26"></a>    <span class="cf">if</span> epoch <span class="op">%</span> <span class="dv">500</span> <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb39-27"><a href="#cb39-27"></a>        <span class="bu">print</span>(<span class="ss">f"Epoch </span><span class="sc">{</span>epoch<span class="sc">}</span><span class="ss"> loss: </span><span class="sc">{</span>loss<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb39-28"><a href="#cb39-28"></a></span>
<span id="cb39-29"><a href="#cb39-29"></a>end <span class="op">=</span> timer()  <span class="co"># timer end</span></span>
<span id="cb39-30"><a href="#cb39-30"></a><span class="bu">print</span>(<span class="st">"Total execution time in seconds: "</span>, <span class="st">"</span><span class="sc">%.2f</span><span class="st">"</span> <span class="op">%</span> (end <span class="op">-</span> start))</span>
<span id="cb39-31"><a href="#cb39-31"></a><span class="bu">print</span>(<span class="st">"Device type: "</span>, device)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 0 loss: 2.6252
Epoch 500 loss: 0.3377
Epoch 1000 loss: 0.2502
Epoch 1500 loss: 0.2403
Epoch 2000 loss: 0.2501
Epoch 2500 loss: 0.2374
Epoch 3000 loss: 0.2368
Epoch 3500 loss: 0.2499
Epoch 4000 loss: 0.2643
Epoch 4500 loss: 0.2555
Epoch 5000 loss: 0.3854
Epoch 5500 loss: 0.2326
Epoch 6000 loss: 0.2390
Epoch 6500 loss: 0.2270
Epoch 7000 loss: 0.2663
Epoch 7500 loss: 0.3403
Epoch 8000 loss: 0.2475
Epoch 8500 loss: 0.2370
Epoch 9000 loss: 0.2126
Epoch 9500 loss: 0.2308
Total execution time in seconds:  378.14
Device type:  cuda</code></pre>
</div>
</div>
</section>
</section>
<section id="process-output-from-the-model" class="level2">
<h2 class="anchored" data-anchor-id="process-output-from-the-model">Process output from the model</h2>
<p>Getting a prediction (text generation) from the model takes some extra work. Since the model is trained on encoded text, the output generated from the model is also encoded. Further, any input used for prediction itself needs to be encoded using the same encoding dictionary model it is trained with. For this, we have defined a helper function.</p>
<ul>
<li><p>This function will take the input text and encode it before passing it to the model</p></li>
<li><p>It will take the output from the model and decode it before returning</p></li>
<li><p>Note that LSTM model output has <code>logits, hidden state, and cell state</code> . Logits give us the next predicted character. Hidden state and cell state are for keeping the context (or memory) of characters processed so far and are supplied to the model for the next prediction.</p></li>
<li><p>For the output logits, we can predict the next character using the index of the highest logit value. This will make our model predict the exact text on the same input each time. To introduce some randomness, we take help from PyTorch class <a href="https://pytorch.org/docs/stable/distributions.html#categorical">torch.distributions.categorical.Categorical</a>. This is how it works</p>
<ul>
<li>We obtain output probabilities by applying softmax to logits and pass them to a Categorical object to create a distribution.</li>
<li>Generate a sample from a Categorical object. Samples generated from the same distribution may be different. This way, we get different outputs with the same input text.</li>
<li>This way, we can also control the predictability of the model output by controlling the probability distribution (calculated from logits) passed to the Categorical object. If we can make probabilities a lot more similar (through scaling), the sample generated by Categorical will also be mostly the same. On the other hand, if we can make the probabilities further apart, then we can also increase the randomness of the output from the Categorical class.</li>
</ul></li>
</ul>
<div class="cell" data-execution_count="28">
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb41"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb41-1"><a href="#cb41-1"></a><span class="im">from</span> torch.distributions.categorical <span class="im">import</span> Categorical</span>
<span id="cb41-2"><a href="#cb41-2"></a></span>
<span id="cb41-3"><a href="#cb41-3"></a><span class="kw">def</span> sample(model, starting_str, len_generated_text<span class="op">=</span><span class="dv">500</span>, scale_factor<span class="op">=</span><span class="fl">1.0</span>):</span>
<span id="cb41-4"><a href="#cb41-4"></a></span>
<span id="cb41-5"><a href="#cb41-5"></a>    encoded_input <span class="op">=</span> torch.tensor([char2int[s] <span class="cf">for</span> s <span class="kw">in</span> starting_str])</span>
<span id="cb41-6"><a href="#cb41-6"></a>    encoded_input <span class="op">=</span> torch.reshape(encoded_input, (<span class="dv">1</span>, <span class="op">-</span><span class="dv">1</span>))</span>
<span id="cb41-7"><a href="#cb41-7"></a></span>
<span id="cb41-8"><a href="#cb41-8"></a>    generated_str <span class="op">=</span> starting_str</span>
<span id="cb41-9"><a href="#cb41-9"></a></span>
<span id="cb41-10"><a href="#cb41-10"></a>    model.<span class="bu">eval</span>()</span>
<span id="cb41-11"><a href="#cb41-11"></a>    hidden, cell <span class="op">=</span> model.init_hidden(<span class="dv">1</span>)</span>
<span id="cb41-12"><a href="#cb41-12"></a>    hidden <span class="op">=</span> hidden.to(<span class="st">"cpu"</span>)</span>
<span id="cb41-13"><a href="#cb41-13"></a>    cell <span class="op">=</span> cell.to(<span class="st">"cpu"</span>)</span>
<span id="cb41-14"><a href="#cb41-14"></a>    <span class="cf">for</span> c <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(starting_str) <span class="op">-</span> <span class="dv">1</span>):</span>
<span id="cb41-15"><a href="#cb41-15"></a>        _, hidden, cell <span class="op">=</span> model(encoded_input[:, c].view(<span class="dv">1</span>), hidden, cell)</span>
<span id="cb41-16"><a href="#cb41-16"></a></span>
<span id="cb41-17"><a href="#cb41-17"></a>    last_char <span class="op">=</span> encoded_input[:, <span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb41-18"><a href="#cb41-18"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(len_generated_text):</span>
<span id="cb41-19"><a href="#cb41-19"></a>        logits, hidden, cell <span class="op">=</span> model(last_char.view(<span class="dv">1</span>), hidden, cell)</span>
<span id="cb41-20"><a href="#cb41-20"></a>        logits <span class="op">=</span> torch.squeeze(logits, <span class="dv">0</span>)</span>
<span id="cb41-21"><a href="#cb41-21"></a>        scaled_logits <span class="op">=</span> logits <span class="op">*</span> scale_factor</span>
<span id="cb41-22"><a href="#cb41-22"></a>        m <span class="op">=</span> Categorical(logits<span class="op">=</span>scaled_logits)</span>
<span id="cb41-23"><a href="#cb41-23"></a>        last_char <span class="op">=</span> m.sample()</span>
<span id="cb41-24"><a href="#cb41-24"></a>        generated_str <span class="op">+=</span> <span class="bu">str</span>(int2char[last_char])</span>
<span id="cb41-25"><a href="#cb41-25"></a></span>
<span id="cb41-26"><a href="#cb41-26"></a>    <span class="cf">return</span> generated_str</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
<section id="generating-new-text-passages" class="level2">
<h2 class="anchored" data-anchor-id="generating-new-text-passages">Generating new text passages</h2>
<p>We are processing text and model output on the ‘CPU’ device in the ‘sample’ function. So let’s also move the model to the same device.</p>
<div class="cell" data-outputid="1911eba3-6f14-466e-d427-971a2f1cc322" data-execution_count="30">
<div class="sourceCode cell-code" id="cb42"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb42-1"><a href="#cb42-1"></a><span class="co">##</span></span>
<span id="cb42-2"><a href="#cb42-2"></a><span class="co"># move model to cpu</span></span>
<span id="cb42-3"><a href="#cb42-3"></a>model.to(<span class="st">'cpu'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="30">
<pre><code>RNN(
  (embedding): Embedding(65, 256)
  (rnn): LSTM(256, 512, batch_first=True)
  (fc): Linear(in_features=512, out_features=65, bias=True)
)</code></pre>
</div>
</div>
<p>Before generating some lengthy text, let’s experiment with simple words and see if our model can complete them.</p>
<p>At first, I used the string “fat” and asked the model to generate the following three characters to complete this word. But at the same time, I have passed a tiny scaling factor meaning I have decreased the model’s predictability.</p>
<div class="cell" data-outputid="567aab7e-229d-47e7-9058-8f35b195ef39" data-execution_count="33">
<div class="sourceCode cell-code" id="cb44"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb44-1"><a href="#cb44-1"></a><span class="bu">print</span>(sample(model, starting_str<span class="op">=</span><span class="st">"fat"</span>, len_generated_text<span class="op">=</span><span class="dv">3</span>, scale_factor<span class="op">=</span><span class="fl">0.1</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>fat, i</code></pre>
</div>
</div>
<p>Next, I asked the model to use the same input and predict the following three characters, but I increased the model’s predictability ten times. So let’s see the output this time.</p>
<div class="cell" data-outputid="64b81ecd-a1f5-4133-a93b-3d7e03db9c0c" data-execution_count="32">
<div class="sourceCode cell-code" id="cb46"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb46-1"><a href="#cb46-1"></a><span class="bu">print</span>(sample(model, starting_str<span class="op">=</span><span class="st">'fat'</span>, len_generated_text<span class="op">=</span><span class="dv">3</span>, scale_factor<span class="op">=</span><span class="fl">1.0</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>father</code></pre>
</div>
</div>
<p>The second time model generated the correct word “father” it had seen before in the training text. So let’s now generate some lengthy texts.</p>
<div class="cell" data-outputid="789757fd-0635-4444-bd01-43e34ad61f48" data-execution_count="34">
<div class="sourceCode cell-code" id="cb48"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb48-1"><a href="#cb48-1"></a><span class="co">##</span></span>
<span id="cb48-2"><a href="#cb48-2"></a><span class="co"># text generation example 1</span></span>
<span id="cb48-3"><a href="#cb48-3"></a><span class="bu">print</span>(sample(model, starting_str<span class="op">=</span><span class="st">"The father"</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>The father too was she was one of those good faeries who protect children. Her
spirits revived, and she wiped away her tears.

The faery took Cinderella by the hand, and old woman, assuming her character of Queen of the
Faeries, that only jumped up behind the
carriage as nimbly as if they had been footmen and laced so tight, touched Cinderella's clothes with her wand, and said, "Now, my dear good child," said the faery, "here you have a coach and
horses, much handsomer than your sisters', to say the least</code></pre>
</div>
</div>
<div class="cell" data-outputid="9bf79006-546a-4d37-82d5-c96c81f95841" data-execution_count="35">
<div class="sourceCode cell-code" id="cb50"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb50-1"><a href="#cb50-1"></a><span class="co">##</span></span>
<span id="cb50-2"><a href="#cb50-2"></a><span class="co"># text generation example 2</span></span>
<span id="cb50-3"><a href="#cb50-3"></a><span class="bu">print</span>(sample(model, starting_str<span class="op">=</span><span class="st">"The mother"</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>The mother so good crust. But
if you like to give the household. It was she who washed the dishes, and
scrubbed down the step-sisters were very cruel to Cinderella,
that he did not eat one morsel of the supper.

Cinderella drew the fellow slipper
out of her godmother
would do with it. Her godmother took the pumpkin, and scooped out the
inside of it, leaving nothing but rind; she then struck it with her
godmother then said, "My dear Cinderella,
that he did not eat one morsel of the supper.

Cinderella drew</code></pre>
</div>
</div>
<div class="cell" data-outputid="abd71fa5-7526-41b4-dbd6-6d19032151c9" data-execution_count="37">
<div class="sourceCode cell-code" id="cb52"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb52-1"><a href="#cb52-1"></a><span class="co">##</span></span>
<span id="cb52-2"><a href="#cb52-2"></a><span class="co"># text generation example 3</span></span>
<span id="cb52-3"><a href="#cb52-3"></a><span class="bu">print</span>(sample(model, starting_str<span class="op">=</span><span class="st">"The three sisters"</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>The three sisters were very cruel to Cinderella,
that he delicacies which she had
received from the prince:  but they did not eat one morsel for a
couple of days. They spent their whole time before a looking-glass, and
they would be laced so tight, tossing her head disdainfully, "that I
should lend my clothes to a dirty Cinderella like you!"

Cinderella quite amazed; but their
astonishment at her dancing was still greater.

Gracefulness seemed to play in the attempt.

The long-wished-for evening came at last, an</code></pre>
</div>
</div>
<div class="cell" data-outputid="e1aabb58-eed7-405c-b970-fe504107e75e" data-execution_count="38">
<div class="sourceCode cell-code" id="cb54"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb54-1"><a href="#cb54-1"></a><span class="co">##</span></span>
<span id="cb54-2"><a href="#cb54-2"></a><span class="co"># text generation example 4</span></span>
<span id="cb54-3"><a href="#cb54-3"></a><span class="bu">print</span>(sample(model, starting_str<span class="op">=</span><span class="st">"The lovely prince"</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>The lovely prince
immediately jumped up behind the
carriage as nimbly as conspicuous after as they
had been before mocking me," replied the poor girl to do all the
drudgery of the household. It was she who washed the dishes, and
scrubbed down the stairs, who tried with all their might to force their unwould stration: CINDERELLA IS PRESENTED BY THE PRINCE TO THE KING AND
QUEEN, WHO WELCOME HER WITH THE HONORS DUE TO A GREAT PRINCESS, AND IS
THEN LED INTO THE ROYAL BY THE HER WITH THE HONORS DUE TO A GREAT PRINCES</code></pre>
</div>
</div>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  let localAlternateSentinel = 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<script src="https://utteranc.es/client.js" repo="hassaanbinaslam/myblog_utterances" issue-term="pathname" theme="github-light" crossorigin="anonymous" async="">
</script>
</div> <!-- /content -->



</body></html>